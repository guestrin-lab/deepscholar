
\section{Introduction}

\label{sec:introduction}


The recent developments in Large Language Models (LLMs) have introduced new paradigms for data science workflows, enabling natural language-based approaches to data interpretation, transformation, and analysis \citep{jing2025dsbench, huang-etal-2024-da, hong-etal-2025-data, wang2025largelanguagemodelbaseddata}. Existing work, however, typically assumes an idealized setting in which relevant datasets are already curated and provided to the model---an assumption that diverges substantially from the practical challenges encountered in real-world data science \citep{lai2025kramabenchbenchmarkaisystems}. In practice, a substantial fraction of effort is devoted to locating the appropriate data within large and heterogeneous data lakes, often comprising thousands of loosely organized files---a process that constitutes a major bottleneck before any downstream analysis can be performed \citep{XU202132}. We argue that this stage of data discovery is both a critical and underexplored challenge for applying LLMs effectively. 

Previous work on data science tasks that require discovery\footnote{Some example tasks are shown in Figure \ref{fig:search-example-1} and \ref{fig:search-example-2} in Appendix \ref{app:case-study}. They require computing or aggregating information from raw data within a large data lake, where the specific source files are not pre-identified.} from a data lake has primarily relied on single-agent systems in which an LLM is given access to all candidate files within its context window and is then asked to solve the problem \citep{lai2025kramabenchbenchmarkaisystems}. This method suffers from several limitations. First, it is not scalable: as the number of files grows, fitting them into the limited context window of an LLM becomes infeasible. Second, the heterogeneity of files poses a challenge, as a single agent may struggle to effectively analyze, interpret, and integrate diverse forms of information. Third, such systems lack robustness to noise, since the presence of many irrelevant files can overwhelm the model and degrade both reasoning quality and precision. One may argue that Retrieval-Augmented Generation (RAG) \citep{10.5555/3495724.3496517, kim2024retrievalenhancedmachinelearningsynthesis, 10.1145/3626772.3657957} provides a solution by choosing a subset of files in the data lake; However, current retrieval techniques are known to perform poorly on tabular and domain-specific data, which are pervasive in data science applications \citep{yu2025tableragretrievalaugmentedgeneration, ji2025targetbenchmarkingtableretrieval, huang-etal-2022-mixed, gu2025radarbenchmarkinglanguagemodels}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/figures-overview-new.pdf}
    % \vspace{-0.4cm}
    \caption{Overview of the blackboard multi-agent system for information discovery in data science. In this framework, the main agent does not assign tasks to subordinate agents. Instead, it posts requests to the blackboard, and subordinate agents autonomously decide whether to respond based on their expertise. The main agent then uses the responses to the request to solve the given task.}
    \label{fig:overview}
    % \vspace{-0.6cm}
\end{figure}

An alternative approach explores multi-agent systems, which frequently adopt a masterâ€“slave paradigm \citep{li2025autokaggle, han2025llmmultiagentsystemschallenges, xu2025comprehensivesurveydeepresearch}. In this setting, a single controller (e.g., orchestration agent) assigns subtasks to a set of subordinate agents that then execute the specified actions. While conceptually straightforward, this architecture has several drawbacks. First, this master-slave paradigm limits the agents' autonomy: subordinate agents are forced to execute instructions from the coordinator even when they lack sufficient information or hold outdated or erroneous information. Second, the central controller must maintain an accurate model of each agents capabilities to assign tasks, an assumption that is often unrealistic when agents have only partial or evolving knowledge of the problem space. Finally, when multiple agents possess overlapping expertise, the controller faces an inherent assignment ambiguity, making task routing difficult. 

Inspired by the blackboard architecture that with substantial impact on traditional AI systems since the 1980s \citep{10.1145/356810.356816, blackboard-old-3}, we adopt a new communication paradigm for LLM multi-agent systems. In this paradigm, a central agent remains responsible for solving the overall task, similar to the master-slave paradigm. However, rather than assigning subtasks to specific agents, the central agent posts a request on a shared \textit{blackboard} that describes the task or information needed, as shown in Figure~\ref{fig:overview}. Subordinate agents monitoring the blackboard can independently decide whether they possess the capability, knowledge, or interest to contribute to solving the task. This design shifts decision-making from a single coordinator to a distributed model whose agents autonomously determine their participation, enabling more flexible collaborations. This differs from the conventional shared-memory paradigm in multi-agent systems. In shared-memory \citep{sagirova2025shared}, agents perform assigned tasks based on information in the shared memory, effectively being asked to execute tasks determined by a central coordinator. Conversely, \textit{in the blackboard architecture, there is no task assignment; instead, requests are broadcast on the blackboard, and each agent retains full autonomy to decide whether to participate in solving the task or not.}



While the blackboard architecture can be applied broadly within multi-agent frameworks, its application to data science with data discovery is particularly compelling and underexplored. As shown in Figure~\ref{fig:overview}, the data lake can be partitioned into smaller clusters, e.g., based on similarity, homogeneity, or any criteria that facilitate efficient handling, each assigned to a subordinate agent responsible for understanding and processing that subset. The main agent, which is tasked with solving the given problem, posts requests on the blackboard specifying the data or general information required. Subordinate agents with the relevant knowledge or capability then autonomously volunteer to respond. This design ensures that each sub-agent manages only a subset of files or web-based information, enhancing scalability compared to approaches that require all data to be loaded into the main agent's prompt. Importantly, the main agent does not need prior knowledge of sub agents knowledge or capability to solve the task, simplifying coordination and improving flexibility in large-scale data lake environments. Here, the main agent's role is primarily to describe the information it requires and define tasks for the sub agents, without directly managing or assigning tasks to them.



We conduct experiments on three datasets for data science tasks that require an explicit information discovery phase. KramaBench \citep{lai2025kramabenchbenchmarkaisystems} is a recently released benchmark designed for this purpose and, to the best of our knowledge, the only publicly available dataset that directly evaluates data discovery in data science. In addition, we repurpose DS-Bench \citep{jing2025dsbench} and DA-Code \citep{huang-etal-2024-da} by introducing a data discovery component, thereby making them more challenging than their original formulations. Experimental results across these datasets demonstrate that the proposed blackboard architecture consistently outperforms strong baselines, including RAG and the master-slave multi-agent framework, achieving 13\% to 57\% relative improvement over the best performing baseline in end-to-end problem solving depending on the backbone LLM. Notably, this improvement is observed across both proprietary and open-source LLMs, highlighting the generalizability of the approach. Furthermore, our method also surpasses baselines in data discovery performance, yielding up to a 9\% relative gain in F1 score for correctly identifying relevant files from the data lake. These results underscore the effectiveness of the blackboard architecture as a communication paradigm for multi-agent systems in data science.