@inproceedings{yao2023react,
  title = {{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle = {International Conference on Learning Representations (ICLR) },
  year = {2023},
  html = {https://arxiv.org/abs/2210.03629},
}

@misc{lai2025kramabenchbenchmarkaisystems,
      title={KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes}, 
      author={Eugenie Lai and Gerardo Vitagliano and Ziyu Zhang and Sivaprasad Sudhir and Om Chabra and Anna Zeng and Anton A. Zabreyko and Chenning Li and Ferdi Kossmann and Jialin Ding and Jun Chen and Markos Markakis and Matthew Russo and Weiyang Wang and Ziniu Wu and Michael J. Cafarella and Lei Cao and Samuel Madden and Tim Kraska},
      year={2025},
      eprint={2506.06541},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.06541}, 
}

@article{wang2022text,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2212.03533},
  year={2022}
}

@inproceedings{
Holtzman2020The,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@misc{gu2025radarbenchmarkinglanguagemodels,
      title={RADAR: Benchmarking Language Models on Imperfect Tabular Data}, 
      author={Ken Gu and Zhihan Zhang and Kate Lin and Yuwei Zhang and Akshay Paruchuri and Hong Yu and Mehran Kazemi and Kumar Ayush and A. Ali Heydari and Maxwell A. Xu and Girish Narayanswamy and Yun Liu and Ming-Zher Poh and Yuzhe Yang and Mark Malhotra and Shwetak Patel and Hamid Palangi and Xuhai Xu and Daniel McDuff and Tim Althoff and Xin Liu},
      year={2025},
      eprint={2506.08249},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2506.08249}, 
}

@misc{comanici2025gemini25pushingfrontier,
      title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities}, 
      author={Gemini-Team},
      year={2025},
      eprint={2507.06261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.06261}, 
}

@misc{anthropic2025claude4,
  title = {System Card: Claude Opus 4 \& Claude Sonnet 4},
  author = {Anthropic},
  howpublished = {\url{https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf}},
  year = {2025},
  month = {May},
  note = {Also referenced in the official release blog post: \url{https://www.anthropic.com/news/claude-4}}
}

@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={Qwen-Team},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}


@inproceedings{
jing2025dsbench,
title={{DSB}ench: How Far Are Data Science Agents from Becoming Data Science Experts?},
author={Liqiang Jing and Zhehui Huang and Xiaoyang Wang and Wenlin Yao and Wenhao Yu and Kaixin Ma and Hongming Zhang and Xinya Du and Dong Yu},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=DSsSPr0RZJ}
}


@inproceedings{huang-etal-2024-da,
    title = "{DA}-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
    author = "Huang, Yiming  and
      Luo, Jianwen  and
      Yu, Yan  and
      Zhang, Yitong  and
      Lei, Fangyu  and
      Wei, Yifan  and
      He, Shizhu  and
      Huang, Lifu  and
      Liu, Xiao  and
      Zhao, Jun  and
      Liu, Kang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.748/",
    doi = "10.18653/v1/2024.emnlp-main.748",
    pages = "13487--13521",
    abstract = "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, including Python and SQL, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously designed the evaluation suite to ensure the accuracy and robustness of the evaluation. We developed the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5{\%} accuracy, leaving ample room for improvement. We release our benchmark at [link](https://github.com/yiyihum/dabench)"
}

@article{XU202132,
title = {Data science: connotation, methods, technologies, and development},
journal = {Data Science and Management},
volume = {1},
number = {1},
pages = {32-37},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000035},
author = {Zongben Xu and Niansheng Tang and Chen Xu and Xueqi Cheng},
keywords = {data science, data science methodology, big data, technical directions},
abstract = {The rapid development of big data breeds data science. Understanding and mastering the internal pattern of the value generation of big data is important for improving digitization and the covergence of data science with management science, computer science, and other disciplines. In this study, we discuss the significance of data science for the development of science and technology and social progress. Based on the interpretation of the connotation of data science, we introduce the definition of data science and review its formation by summarizing the major progress of related disciplines. We also discuss the research methodologies, development patterns and trends, and technical directions of data science. Finally, we make suggestions for data science to promote the development of data-based science and technology.}
}

@inproceedings{hong-etal-2025-data,
    title = "Data Interpreter: An {LLM} Agent for Data Science",
    author = "Hong, Sirui  and
      Lin, Yizhang  and
      Liu, Bang  and
      Liu, Bangbang  and
      Wu, Binhao  and
      Zhang, Ceyao  and
      Li, Danyang  and
      Chen, Jiaqi  and
      Zhang, Jiayi  and
      Wang, Jinlin  and
      Zhang, Li  and
      Zhang, Lingyao  and
      Yang, Min  and
      Zhuge, Mingchen  and
      Guo, Taicheng  and
      Zhou, Tuo  and
      Tao, Wei  and
      Tang, Robert  and
      Lu, Xiangtao  and
      Zheng, Xiawu  and
      Liang, Xinbing  and
      Fei, Yaying  and
      Cheng, Yuheng  and
      Ni, Yongxin  and
      Gou, Zhibin  and
      Xu, Zongze  and
      Luo, Yuyu  and
      Wu, Chenglin",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1016/",
    doi = "10.18653/v1/2025.findings-acl.1016",
    pages = "19796--19821",
    ISBN = "979-8-89176-256-5",
    abstract = "Large Language Model (LLM)-based agents have excelled in various domains but face significant challenges when applied to data science workflows due to their complex, multi-stage nature. Current LLM-based agents struggle with non-linear relationships, recursive dependencies, implicit data- and logic-dependent reasoning, and managing extensive context. In this paper, we introduce Data Interpreter, an LLM-based agent that addresses these challenges through hierarchical graph-based modeling to represent the complexity and a progressive strategy for step-by-step verification, refinement, and consistent context management. Extensive experiments confirm the effectiveness of Data Interpreter. On InfiAgent-DABench, it boosts performance by 25{\%} (from 75.9{\%} to 94.9{\%}), and on machine learning and open-ended tasks, it lifts accuracy from 88{\%} to 95{\%} and from 60{\%} to 97{\%}, respectively. Moreover, our method surpasses state-of-the-art baselines by 26{\%} on the MATH dataset. We will release the code upon publication."
}

@misc{wang2025largelanguagemodelbaseddata,
      title={Large Language Model-based Data Science Agent: A Survey}, 
      author={Peiran Wang and Yaoning Yu and Ke Chen and Xianyang Zhan and Haohan Wang},
      year={2025},
      eprint={2508.02744},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.02744}, 
}

@inproceedings{10.5555/3495724.3496517,
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {793},
numpages = {16},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@misc{kim2024retrievalenhancedmachinelearningsynthesis,
      title={Retrieval-Enhanced Machine Learning: Synthesis and Opportunities}, 
      author={To Eun Kim and Alireza Salemi and Andrew Drozdov and Fernando Diaz and Hamed Zamani},
      year={2024},
      eprint={2407.12982},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.12982}, 
}

@inproceedings{10.1145/3626772.3657957,
author = {Salemi, Alireza and Zamani, Hamed},
title = {Evaluating Retrieval Quality in Retrieval-Augmented Generation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657957},
doi = {10.1145/3626772.3657957},
abstract = {Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's tau correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2395–2400},
numpages = {6},
keywords = {evaluation, retrieval quality, retrieval-augmented generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@misc{yu2025tableragretrievalaugmentedgeneration,
      title={TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning}, 
      author={Xiaohan Yu and Pu Jian and Chong Chen},
      year={2025},
      eprint={2506.10380},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.10380}, 
}

@misc{ji2025targetbenchmarkingtableretrieval,
      title={TARGET: Benchmarking Table Retrieval for Generative Tasks}, 
      author={Xingyu Ji and Parker Glenn and Aditya G. Parameswaran and Madelon Hulsebos},
      year={2025},
      eprint={2505.11545},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2505.11545}, 
}

@inproceedings{huang-etal-2022-mixed,
    title = "Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in {O}pen{QA}",
    author = "Huang, Junjie  and
      Zhong, Wanjun  and
      Liu, Qian  and
      Gong, Ming  and
      Jiang, Daxin  and
      Duan, Nan",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.303/",
    doi = "10.18653/v1/2022.findings-emnlp.303",
    pages = "4117--4129",
    abstract = "Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity problem. To address the above challenges, we introduce an optimized OpenQA Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences. Firstly, we propose to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed-modality negative sampling strategy. Secondly, to alleviate data sparsity problem and enhance the general retrieval ability, we conduct retrieval-centric mixed-modality synthetic pre-training. Experimental results demonstrate that OTTeR substantially improves the performance of table-and-text retrieval on the OTT-QA dataset. Comprehensive analyses examine the effectiveness of all the proposed mechanisms. Besides, equipped with OTTeR, our OpenQA system achieves the state-of-the-art result on the downstream QA task, with 10.1{\%} absolute improvement in terms of the exact match over the previous best system."
}

@misc{li2025autokaggle,
      title={AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions}, 
      author={Ziming Li and Qianbo Zang and David Ma and Jiawei Guo and Tuney Zheng and Minghao Liu and Xinyao Niu and Yue Wang and Jian Yang and Jiaheng Liu and Wanjun Zhong and Wangchunshu Zhou and Wenhao Huang and Ge Zhang},
      year={2024},
      eprint={2410.20424},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.20424}, 
}

@misc{han2025llmmultiagentsystemschallenges,
      title={LLM Multi-Agent Systems: Challenges and Open Problems}, 
      author={Shanshan Han and Qifan Zhang and Yuhang Yao and Weizhao Jin and Zhaozhuo Xu},
      year={2025},
      eprint={2402.03578},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2402.03578}, 
}

@misc{xu2025comprehensivesurveydeepresearch,
      title={A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications}, 
      author={Renjun Xu and Jingwen Peng},
      year={2025},
      eprint={2506.12594},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.12594}, 
}

@INPROCEEDINGS{blackboard-old-1,
  author={Dong, J. and Chen, S. and Jeng, J.-J.},
  booktitle={International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II}, 
  title={Event-based blackboard architecture for multi-agent systems}, 
  year={2005},
  volume={2},
  number={},
  pages={379-384 Vol. 2},
  keywords={Multiagent systems;Computer architecture;Centralized control;Application software;Open systems;Broadcasting;Computer science;Software architecture;Software systems;Intelligent agent;Multi-agent systems;architectural pattern;implicit invocation;blackboard;software architecture},
  doi={10.1109/ITCC.2005.149}}
  
 @inproceedings{blackboard-old-2,
  author    = "Daniel D Corkill",
  title     = "{Collaborating Software: Blackboard and Multi-Agent
               Systems \& the Future}",
  booktitle = "Proceedings of the International Lisp Conference",
  month     = "October",
  year      = "2003",
  address   = "New York, New York",
  url       = "http://mas.cs.umass.edu/paper/265",
}

@article{blackboard-old-3,
title = {A temporal blackboard for a multi-agent environment},
journal = {Data \& Knowledge Engineering},
volume = {15},
number = {3},
pages = {189-211},
year = {1995},
issn = {0169-023X},
doi = {https://doi.org/10.1016/0169-023X(95)00007-F},
url = {https://www.sciencedirect.com/science/article/pii/0169023X9500007F},
author = {V. Botti and F. Barber and A. Crespo and E. Onaindia and A. Garcia-Fornes and I. Ripoll and D. Gallardo and L. Hernández},
keywords = {Blackboard, Knowledge-Based Systems, Real-time, Multi-agent, Temporal reasoning, Temporal Representation},
abstract = {The multi-agent system paradigm emerges as an interesting approach in the Knowledge Based System (KBS) field, when distributed problem-solving techniques are required for solving problems that can be represented as a collection of groups of cooperating intelligent individuals. A key concept in the multi-agent systems is the interaction between agents. On the other hand time plays a crucial role in a wide range of KBS applications. Temporal reasoning and representations consists of formalizing the notion of time and providing means to represent and reason about the temporal aspects of knowledge. This paper presents a framework for agent communication based on the blackboard paradigm which is able to manage temporal information, and it provides its multiple access and coherence management protocols.}
}

@misc{
sagirova2025shared,
title={Shared Memory for Multi-agent Lifelong Pathfinding},
author={Alsu Sagirova and Yuri Kuratov and Mikhail Burtsev},
year={2025},
url={https://openreview.net/forum?id=9DrPvYCETp}
}

@inproceedings{10.5555/3618408.3619164,
author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
title = {DS-1000: a natural and reliable benchmark for data science code generation},
year = {2023},
publisher = {JMLR.org},
abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS- 1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from Stack-Overflow. Second, our automatic evaluation is highly specific (reliable) - across all Codex-002- predicted solutions that our evaluation accepts, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {756},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{yin-etal-2023-natural,
    title = "Natural Language to Code Generation in Interactive Data Science Notebooks",
    author = "Yin, Pengcheng  and
      Li, Wen-Ding  and
      Xiao, Kefan  and
      Rao, Abhishek  and
      Wen, Yeming  and
      Shi, Kensen  and
      Howland, Joshua  and
      Bailey, Paige  and
      Catasta, Michele  and
      Michalewski, Henryk  and
      Polozov, Oleksandr  and
      Sutton, Charles",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.9/",
    doi = "10.18653/v1/2023.acl-long.9",
    pages = "126--173",
    abstract = "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at \url{https://github.com/google-research/arcade-nl2code/}."
}

@misc{zhang2025datascibenchllmagentbenchmark,
      title={DataSciBench: An LLM Agent Benchmark for Data Science}, 
      author={Dan Zhang and Sining Zhoubian and Min Cai and Fengzu Li and Lekang Yang and Wei Wang and Tianjiao Dong and Ziniu Hu and Jie Tang and Yisong Yue},
      year={2025},
      eprint={2502.13897},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.13897}, 
}

@inproceedings{zhang-etal-2024-benchmarking-data,
    title = "Benchmarking Data Science Agents",
    author = "Zhang, Yuge  and
      Jiang, Qiyang  and
      XingyuHan, XingyuHan  and
      Chen, Nan  and
      Yang, Yuqing  and
      Ren, Kan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.308/",
    doi = "10.18653/v1/2024.acl-long.308",
    pages = "5677--5700",
    abstract = "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval {--} a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field."
}

@inproceedings{
jimenez2024swebench,
title={{SWE}-bench: Can Language Models Resolve Real-world Github Issues?},
author={Carlos E Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R Narasimhan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=VTF8yNQM66}
}

@misc{
tang2025mlbench,
title={{ML}-Bench: Evaluating Large Language Models for Code Generation in Repository-Level Machine Learning Tasks},
author={Xiangru Tang and Yuliang Liu and Zefan Cai and Yanjun Shao and Junjie Lu and Yichi Zhang and Zexuan Deng and Helan Hu and Kaikai An and Ruijun Huang and Shuzheng Si and Chen Sheng and Haozhe Zhao and Liang Chen and Tianyu Liu and Yin Fang and Yujia Qin and Wangchunshu Zhou and Yilun Zhao and Zhiwei Jiang and Baobao Chang and Arman Cohan and Mark Gerstein},
year={2025},
url={https://openreview.net/forum?id=sf1u3vTRjm}
}

@inproceedings{
zhuo2025bigcodebench,
title={BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions},
author={Terry Yue Zhuo and Vu Minh Chien and Jenny Chim and Han Hu and Wenhao Yu and Ratnadira Widyasari and Imam Nur Bani Yusuf and Haolan Zhan and Junda He and Indraneil Paul and Simon Brunner and Chen GONG and James Hoang and Armel Randy Zebaze and Xiaoheng Hong and Wen-Ding Li and Jean Kaddour and Ming Xu and Zhihan Zhang and Prateek Yadav and Naman Jain and Alex Gu and Zhoujun Cheng and Jiawei Liu and Qian Liu and Zijian Wang and David Lo and Binyuan Hui and Niklas Muennighoff and Daniel Fried and Xiaoning Du and Harm de Vries and Leandro Von Werra},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=YrycTjllL0}
}

@inproceedings{gu-etal-2024-blade,
    title = "{BLADE}: Benchmarking Language Model Agents for Data-Driven Science",
    author = "Gu, Ken  and
      Shang, Ruoxi  and
      Jiang, Ruien  and
      Kuang, Keying  and
      Lin, Richard-John  and
      Lyu, Donghe  and
      Mao, Yue  and
      Pan, Youran  and
      Wu, Teng  and
      Yu, Jiaqian  and
      Zhang, Yikun  and
      Zhang, Tianmai M.  and
      Zhu, Lanyi  and
      Merrill, Mike A  and
      Heer, Jeffrey  and
      Althoff, Tim",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.815/",
    doi = "10.18653/v1/2024.findings-emnlp.815",
    pages = "13936--13971",
    abstract = "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents' multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents' analysis approaches."
}

@inproceedings{
chen2025scienceagentbench,
title={ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery},
author={Ziru Chen and Shijie Chen and Yuting Ning and Qianheng Zhang and Boshi Wang and Botao Yu and Yifei Li and Zeyi Liao and Chen Wei and Zitong Lu and Vishal Dey and Mingyi Xue and Frazier N. Baker and Benjamin Burns and Daniel Adu-Ampratwum and Xuhui Huang and Xia Ning and Song Gao and Yu Su and Huan Sun},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=6z4YKr0GK6}
}

@misc{mitchener2025bixbenchcomprehensivebenchmarkllmbased,
      title={BixBench: a Comprehensive Benchmark for LLM-based Agents in Computational Biology}, 
      author={Ludovico Mitchener and Jon M Laurent and Benjamin Tenmann and Siddharth Narayanan and Geemi P Wellawatte and Andrew White and Lorenzo Sani and Samuel G Rodriques},
      year={2025},
      eprint={2503.00096},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2503.00096}, 
}

@inproceedings{10.5555/3495724.3495883,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@misc{rubavicius2025conversationalcodegenerationcase,
      title={Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles}, 
      author={Rimvydas Rubavicius and Antonio Valerio Miceli-Barone and Alex Lascarides and Subramanian Ramamoorthy},
      year={2025},
      eprint={2410.09829},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.09829}, 
}

@article{
li2023starcoder,
title={StarCoder: may the source be with you!},
author={Raymond Li and Loubna Ben allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia LI and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Joel Lamy-Poirier and Joao Monteiro and Nicolas Gontier and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Ben Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason T Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Urvashi Bhattacharyya and Wenhao Yu and Sasha Luccioni and Paulo Villegas and Fedor Zhdanov and Tony Lee and Nadav Timor and Jennifer Ding and Claire S Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro Von Werra and Harm de Vries},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=KoFOg41haE},
note={Reproducibility Certification}
}

@misc{codellama,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.12950}, 
}

@inproceedings{
chen2018executionguided,
title={Execution-Guided Neural Program Synthesis},
author={Xinyun Chen and Chang Liu and Dawn Song},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gfOiAqYm},
}

@inproceedings{
schick2023toolformer,
title={Toolformer: Language Models Can Teach Themselves to Use Tools},
author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Eric Hambro and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Yacmpz84TH}
}

@inproceedings{
patil2024gorilla,
title={Gorilla: Large Language Model Connected with Massive {API}s},
author={Shishir G Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=tBRNC6YemY}
}


@inproceedings{
chen2024teaching,
title={Teaching Large Language Models to Self-Debug},
author={Xinyun Chen and Maxwell Lin and Nathanael Sch{\"a}rli and Denny Zhou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=KuPixIqPiq}
}

@inproceedings{
shinn2023reflexion,
title={Reflexion: language agents with verbal reinforcement learning},
author={Noah Shinn and Federico Cassano and Ashwin Gopinath and Karthik R Narasimhan and Shunyu Yao},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vAElhFcKW6}
}

@misc{salemi2025planandrefinediversecomprehensiveretrievalaugmented,
      title={Plan-and-Refine: Diverse and Comprehensive Retrieval-Augmented Generation}, 
      author={Alireza Salemi and Chris Samarinas and Hamed Zamani},
      year={2025},
      eprint={2504.07794},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.07794}, 
}

@inproceedings{10.1145/3731120.3744584,
author = {Salemi, Alireza and Zamani, Hamed},
title = {Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization},
year = {2025},
isbn = {9798400718618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731120.3744584},
doi = {10.1145/3731120.3744584},
abstract = {This paper investigates the design of a unified search engine to serve multiple retrieval-augmented generation (RAG) agents, each with a distinct task, backbone large language model (LLM), and RAG strategy. We introduce an iterative approach where the search engine generates retrieval results for the RAG agents and gathers feedback on the quality of the retrieved documents during an offline phase. This feedback is then used to iteratively optimize the search engine using an expectation-maximization algorithm, with the goal of maximizing each agent's utility function. Additionally, we adapt this to an online setting, allowing the search engine to refine its behavior based on real-time individual agents feedback to better serve the results for each of them. Experiments on datasets from the Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our approach significantly on average outperforms baselines across 18 RAG models. We demonstrate that our method effectively ''personalizes'' the retrieval for each RAG agent based on the collected feedback. Finally, we provide a comprehensive ablation study to explore various aspects of our method.},
booktitle = {Proceedings of the 2025 International ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR)},
pages = {183–193},
numpages = {11},
keywords = {ranking optimization, retrieval-augmented generation, retrieval-enhanced machine learning, search engine for agents},
location = {Padua, Italy},
series = {ICTIR '25}
}

@inproceedings{10.1145/3626772.3657733,
author = {Salemi, Alireza and Zamani, Hamed},
title = {Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657733},
doi = {10.1145/3626772.3657733},
abstract = {This paper introduces uRAG-a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {741–751},
numpages = {11},
keywords = {large language model, neural ranking model, retrieval augmentation, retrieval-enhanced machine learning, text generation},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@article{10.1145/356810.356816,
author = {Erman, Lee D. and Hayes-Roth, Frederick and Lesser, Victor R. and Reddy, D. Raj},
title = {The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty},
year = {1980},
issue_date = {June 1980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/356810.356816},
doi = {10.1145/356810.356816},
journal = {ACM Comput. Surv.},
month = jun,
pages = {213–253},
numpages = {41}
}

@article{Nii_1986, title={The Blackboard Model of Problem Solving and the Evolution of Blackboard Architectures}, volume={7}, url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/537}, DOI={10.1609/aimag.v7i2.537}, abstractNote={The first blackboard system was the HEARSAY-II speech understanding system (Erman et al.,1980) that evolved between 1971 and 1976. Subsequently, many systems have been built that have similar system organization and run-time behavior. The objectives of this article are (1) to define what is meant by &quot;blackboard systems&quot; and (2) to show the richness and diversity of blackboard system designs. The article begins with a discussion of the underlying concept behind all blackboard systems, the blackboard model of problem solving. In order to bridge the gap between a model and working systems, the blackboard framework, an extension of the basic blackboard model is introduced, including a detailed description of the model’s components and their behavior. A model does not come into existence on its own, and is usually an abstraction of many examples. In Section 2 the history of ideas is traced, and the designs of some application systems that helped shape the blackboard model are detailed. Part 2 of this article which will appear in the next issue of AI Magazine, describes and contrasts some blackboard systems and discusses the characteristics of application problems suitable for the blackboard method of problem solving.}, number={2}, journal={AI Magazine}, author={Nii, H. Penny}, year={1986}, month={Jun.}, pages={38} }

@article{Nii_Feigenbaum_Anton_1982, title={Signal-to-Symbol Transformation: HASP/SIAP Case Study}, volume={3}, url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/368}, DOI={10.1609/aimag.v3i2.368}, abstractNote={Artificial intelligence is that part of computer science that concerns itself with the concepts and methods of symbolic inference and symbolic representation of knowledge. Its point of departure -- it’s most fundamental concept -- is what Newell and Simon called (in their Turing Award Lecture) &quot;the physical symbol system.&quot; But within the last fifteen years, it has concerned itself also with signals -- with the interpretation or understanding of signal data. AI researchers have discussed &quot;signal-to symbol transformations,&quot; and their programs have shown how appropriate use of symbolic manipulations can be of great use in making signal processing more effective and efficient. Indeed, the programs for signal understanding have been fruitful, powerful, and among the most widely recognized of AI’s achievements.}, number={2}, journal={AI Magazine}, author={Nii, H. Penny and Feigenbaum, Edward A. and Anton, John J.}, year={1982}, month={Jun.}, pages={23} }

@article{HAYESROTH1985251,
title = {A blackboard architecture for control},
journal = {Artificial Intelligence},
volume = {26},
number = {3},
pages = {251-321},
year = {1985},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(85)90063-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370285900633},
author = {Barbara Hayes-Roth},
abstract = {The control problem—which of its potential actions should an AI system perform at each point in the problem-solving process?—is fundamental to all cognitive processes. This paper proposes eight behavioral goals for intelligent control and a ‘blackboard control architecture’ to achieve them. The architecture distinguishes domain and control problems, knowledge, and solutions. It enables AI systems to operate upon their own knowledge and behavior and to adapt to unanticipated problem-solving situations. The paper shows how opm, a blackboard control system for multiple-task planning, exploits these capabilities. It also shows how the architecture would replicate the control behavior of hearsay-ii and hasp. The paper contrasts the blackboard control architecture with three alternatives and shows how it continues an evolutionary progression of control architectures. The paper concludes with a summary of the blackboard control architecture's strengths and weaknesses.}
}