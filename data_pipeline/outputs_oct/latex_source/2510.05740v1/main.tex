
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{array}
\usepackage{adjustbox}
\usepackage{rotating}
\usepackage{amsmath} % For \text
\usepackage{soul} % For underlining
\usepackage[table]{xcolor} % Required for row color
\usepackage{pifont} % For checkmark and x symbols
\usepackage{subcaption}
\usepackage{arydshln} % Required for dashed lines in tables
\usepackage{wrapfig}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy \& Hamid R. Rabiee \\
Department of Computer Engineering\\
Sharif University of Technology\\
% Tehran, Iran \\
\texttt{\{amir.aman,zahra.dehghanian97,beigy,rabiee\}@sharif.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% Custom command with color for rAcc (blue) and fAcc (red)
\newcommand{\result}[2]{%
  \begin{tabular}{@{}c@{}}%
    #1 / #2 %
  \end{tabular}%
}

% Custom command with color for rAcc (blue) and fAcc (red)
\newcommand{\resultscell}[2]{%
  \begin{tabular}{@{}c@{}}%
    \textcolor{blue!50!black}{#1} / \textcolor{red!50!black}{#2}%
  \end{tabular}%
}


% Custom command with color and vertical stacking
% #1: rAcc (blue), #2: fAcc (red)
\newcommand{\resultscellcolor}[2]{%
  \begin{tabular}{@{}c@{}}%
    \textcolor{blue!50!black}{#1} \\ % rAcc on the first line
    \textcolor{red!50!black}{#2}  % fAcc on the second line
  \end{tabular}%
}

% Command to rotate column headers
\newcommand{\rotateheader}[1]{\rotatebox{60}{#1}}
% Define custom symbols for checkmark and x
\newcommand{\cmark}{\textcolor{black}{\ding{51}}}%
\newcommand{\xmark}{\textcolor{black}{\ding{55}}}%


\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.


\begin{document}


\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
% While the prodigious emergence of generative modeling approaches has made synthetic-image detectors increasingly important, we believe that the conventional approach in this sphere- an emphasis on cross-generator generalization- is inadequate. Most synthetic-image detection models are bound to fail under an equally demanding second orthogonal axis of problematic generalizations, cross-semantic generalization between different visual domains. To close this vital gap, we propose \textbf{OmniGen} Benchmark, a new and much harder testbed with 12 state-of-the-art generators, to offer a more realistic perspective on determining how well a detector might perform in the wild. We further propose \texttt{FusionDetect}, which solves the two-axis generalization problem by fusing complementary features from two powerful frozen foundational models. By integrating the rich semantic understanding of a language-aligned vision encoder (CLIP) with highly detailed structural and textural information from a self-supervised model (DINOv2), \texttt{FusionDetect} forms a holistic feature space resilient to transformations in both content and generator architecture. Extensive experiments show that \texttt{FusionDetect} establishes a new state-of-the-art, outperforming its closest competitor by a significant margin of $\textbf{+3.87\%}$ in accuracy and $\textbf{+6.13\%}$ in average precision on a diverse collection of established benchmarks, while also demonstrating unparalleled robustness to common image perturbations. Our work not only provides a top-performing detector but also establishes a new benchmark and conceptual framework to guide the future of universal AI image detection.

% The rapid development of generative models has made it increasingly crucial to create detectors capable of reliably detecting synthetic images. Although most of the work has now focused on cross-generator generalization, this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap, we present the \textbf{OmniGen Benchmark}, a comprehensive and challenging platform that incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, \texttt{FusionDetect}, aimed at addressing both vectors of generalization. \texttt{FusionDetect} draws on the benefits of two frozen foundation models: CLIP \& Dinov2. By deriving features from both complementary models, we develop a cohesive feature space that naturally adapts to changes in both the content and design of the generator. Our extensive experiments show that \texttt{FusionDetect} delivers not only a new state of the art, which is $\textbf{+3.87\%}$ more accurate than its closest competitor and $\textbf{+6.13\%}$ average precision better on established benchmarks, but also $\textbf{+4.48\%}$ increase in accuracy on OmniGen, along with exceptional robustness to common image perturbations. Our work supplies not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection.

The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap, we present the \textbf{OmniGen Benchmark}. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, \texttt{FusionDetect}, aimed at addressing both vectors of generalization. \texttt{FusionDetect} draws on the benefits of two frozen foundation models: CLIP \& Dinov2. By deriving features from both complementary models, we develop a cohesive feature space that naturally adapts to changes in both the content and design of the generator. Our extensive experiments demonstrate that \texttt{FusionDetect} delivers not only a new state-of-the-art, which is $\textbf{3.87\%}$ more accurate than its closest competitor and $\textbf{6.13\%}$ more precise on average on established benchmarks, but also achieves a $\textbf{4.48\%}$ increase in accuracy on OmniGen, along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available \href{https://github.com/amir-aman/FusionDetect}{here}.
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Introduction}
\label{intro}

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{images/teaser.png}
    \caption{\texttt{FusionDetect} performance on OmniGen and established benchmarks from previous works \cite{genimage, imaginet, aide} compared to other detectors. The size of the bubble indicates the standard deviation of accuracy between all generators in the dataset (smaller is better).}
    \label{fig:teaser}
\end{wrapfigure}


The field of artificial intelligence has entered an era of unprecedented creative capacity, primarily driven by the rapid maturation of text-to-image generative models \cite{genaisurvey}. Recently, diffusion-based architectures such as Stable Diffusion \cite{ldm}, Midjourney \cite{midjourney}, and Imagen \cite{imagen} have achieved a level of photorealism and artistic flexibility that was once the domain of science fiction. These models have democratized content creation, empowering users to generate complex, high-fidelity images from simple textual descriptions. This technological leap has unlocked vast potential in domains ranging from digital art \cite{imagen, glide} and entertainment \cite{animation} to product design \cite{designdiffusion} and scientific visualization \cite{mindfularch}. However, this accessibility is a double-edged sword. The same tools that foster creativity can be wielded for malicious purposes, including the generation of convincing disinformation, the creation of synthetic media to erode public trust, and the violation of copyright and personal identity \cite{xu2023combating, ren2024copyright, faceawsp}. Consequently, the development of robust, reliable, and universal methods for detecting AI-generated images has become a critical imperative for ensuring the integrity of our digital ecosystem \cite{mahara2025methods}.


The academic pursuit of AI-generated image detection has evolved significantly, yet it faces persistent challenges that limit its real-world applicability. Early research \cite{cnndet, spec, f3net} focused heavily on identifying artifacts from Generative Adversarial Networks (GANs) \cite{gan}. Although fundamental, this focus is increasingly obsolete due to the overwhelming dominance of diffusion models \cite{ddpm,ddim}. These models \cite{ddpm, adm, ddim, ldm} are the backbone of nearly all state-of-the-art (SOTA) commercial, open-source, and community-driven projects. A modern, practical detector must therefore be also engineered for the unique and subtle characteristics of this new paradigm.
%\done{i change a bit this paragraph}


% More critically, we argue that the community's understanding of "generalization" has been incomplete. The prevailing research paradigm has been to test for what we term the first axis of generalization: \textit{cross-generator} performance. This typically involves training a detector on images from a single generator, such as ProGAN \cite{progan} or an early version of Stable Diffusion \cite{ldm}, and evaluating its ability to identify images from a variety of other known generators (e.g., ADM \cite{adm}, DDPM \cite{ddpm}, StyleGAN \cite{stylegan}). While important, this approach overlooks a second, equally crucial axis: \textit{cross-semantic} generalization. A detector may perform flawlessly when trained and tested on images from different generators within the same dataset, but fail when faced with images that have a different semantic context.

More critically, we argue that the community's understanding of generalization is dangerously incomplete as they focus on only one aspect of it. This typically involves training a detector on images from a single generator and evaluating its ability to identify images from a variety of other generators \cite{unifd, dire, aide}. To rectify this, we formalize the problem as a two-axis generalization challenge: a truly universal detector must demonstrate robustness not only on the well-studied \textit{cross-generator axis} (handling unseen generators) but also on the often-neglected \textit{cross-semantic axis} (handling unseen visual domains). As we will show, prior works often fails on the second axis, rendering it unreliable for real-world  \cite{aide}. This semantic gap is not merely theoretical. As our t-SNE \cite{tsne} projection in Figure \ref{fig:tsne_datasets} visualizes, popular datasets like GenImage \cite{genimage}, ImagiNet \cite{imaginet}, and the challenging Chameleon \cite{aide} form distinct, non-overlapping clusters in our proposed \texttt{FusionDetect} embedding space. As shown, there is no to little overlap between each dataset cluster. This demonstrates that a model trained exclusively on the feature distribution of one dataset will fail to recognize the patterns of another, regardless of the generator used.

% For example, a detector trained on a dataset of AI-generated landscapes might successfully identify landscapes created by other, unseen diffusion models. However, its performance often collapses when applied to a different domain, like paintings, medical scans or digital artwork, even if the images were generated by the same models. This proves that current detectors overfit to content-specific features, rather than learning the universal, intrinsic artifacts of the diffusion process itself. This fundamental lack of semantic-agnosticism is a critical failure, rendering many existing detection methods unreliable for real-world use.
% %\done{i add this paragraph}%


\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{images/dataset-emb.png}
    \caption{T-SNE \cite{tsne} projection of GenImage \cite{genimage}, ImagiNet \cite{imaginet}, and Chameleon \cite{aide} dataset.}
    \label{fig:tsne_datasets}
\end{wrapfigure}



To solve this two-axis challenge, we propose \texttt{FusionDetect}, a powerful fusion model engineered for universal AI image detection. We hypothesize that a truly robust and generalizable representation can only be created by combining the complementary strengths of large-scale, foundational models with orthogonal training objectives. Instead of hunting for a single, elusive universal artifact, \texttt{FusionDetect} fuses deep features from two distinct and powerful vision encoders: CLIP \cite{clip} for its unparalleled semantic breadth and DINOv2 \cite{dinov2} for its profound understanding of fine-grained structure and texture.

To facilitate a more rigorous and realistic evaluation of detector performance, we introduce the \textbf{OmniGen} Benchmark, a new, open-source test set designed to capture the modern generative landscape. The OmniGen benchmark directly addresses the weaknesses of prior benchmarks by including images from 12 SOTA generators, such as closed-source models, the latest open-source architectures, and popular community fine-tunes. By curating this benchmark with high semantic variety, we provide a robust framework to validate a model's performance along both axes of generalization, ensuring that our evaluations reflect a detector's true capabilities in real-world scenarios. \ref{fig:teaser}

% \begin{figure*}[t]
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=\textwidth]{images/teaser2.png}

%    \caption{\textbf{FusionDetect pipeline:} caption}
%    \label{fig:teaser}
% \end{figure*}


In summary, the primary contributions of this paper are fourfold:

\begin{enumerate}
    \item We formalize the "two-axis generalization" problem in AI image detection, highlighting the critical need for models to generalize across both unseen generators and semantic domains.
    \item We introduce \texttt{FusionDetect} as a strong proof-of-concept for this framework. It demonstrates that fusion of complementary foundational features can decisively outperform more complex architectures when evaluated under the two-axis setting.
    \item We release the \textbf{OmniGen Benchmark}, the first test set explicitly designed to test two-axis generalization, featuring 12 diverse SOTA generators and high semantic variance.
    \item We demonstrate through extensive experiments that \texttt{FusionDetect} establishes a new SOTA, achieving superior generalization and robustness to common image perturbations compared to existing methods.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Related Work}
\label{sec:related_works}

% The task of distinguishing AI-generated images from real ones is a dynamic co-evolution between generative capabilities and forensic detection. To contextualize our work, we survey the key developments in this field by first outlining the evolution of generative, then categorizing the major paradigms in detection methodology.

The field of AI-generated image detection is in a constant race against generative technology. To provide context for our work, we'll first review the evolution of generative models, from older GANs \cite{gan, progan, biggan} to modern diffusion models \cite{adm, ldm, glide}. We'll then look at the detection methods, highlighting how each has responded to the shifting capabilities of generative architectures. Our review shows that existing detection methods have consistently lagged behind generative advancements, a critical gap that our work aims to close by addressing the "two-axis generalization" problem. %\done{i rewrite it}%

\subsection{Image Generation}
% \todod{in this section you should name all works you want to bring in your comparison and 5 more work}


The field of synthetic image generation has been reshaped over the last decade. It has transitioned from early breakthroughs with GANs \cite{progan, stylegan, biggan} to the current dominance of Diffusion Models (DMs). The advent of Denoising Diffusion Probabilistic Models (DDPMs) \cite{ddpm} marked a significant paradigm shift. Diffusion Models (DMs) and their subsequent variants have now surpassed GANs in terms of image quality, diversity, and text-to-image coherence \cite{adm}. The initial wave of practical diffusion models was led by the Latent Diffusion Model (LDM) architecture \cite{ldm}, which underpins the widely popular Stable Diffusion series. These models made high-fidelity generation accessible to the public and became a foundational tool for both research and creative applications.

The pace of innovation has since accelerated, leading to a new generation of even more sophisticated architectures. Architectural upgrades in models like Stable Diffusion XL (SDXL) \cite{sdxl}, such as a larger UNet \cite{unet} and dual text-encoders, have led to significant improvements in image quality and prompt fidelity. The field continues to evolve rapidly with new open-source models like FLUX \cite{flux}, SD3.5 \cite{sd3.5}, HiDream \cite{hidream}, CogView4 \cite{cogview}, Kandinsky3 \cite{kandinsky}, PixArt-$\delta$, alongside closed-source counterparts like Google's Imagen \cite{imagen} and Midjourney \cite{midjourney} and also community finetuned models such as Juggernaut \cite{juggernaut} and Dreamshaper \cite{dreamshaper}. This model shift from GANs to diffusions generates a new class of synthetic images with distinct statistical fingerprints that challenge existing detection methodologies, a primary focus of this work.
%\done{i rewrite the enitr subsection}


\subsection{Image Detection}
Detection methodologies can be broadly categorized into two main paradigms: those that seek to identify specific, inherent artifacts of the generation process, and those that leverage the general-purpose feature representations of large pretrained foundational models.


\subsubsection*{Artifact-Based Detection}

This paradigm is founded on the principle that the synthetic generation process, regardless of its sophistication, leaves behind subtle, machine-detectable traces or "fingerprints" \cite{dif}. Researchers have pursued these artifacts across various domains. A significant body of work targets universal image properties, analyzing inconsistencies in the frequency domain (\cite{fredect, npr, dif, f3net}), exploring local texture and patch-level correlations (\cite{patchcraft, lgrad, ssp}), or extracting unique residual noise patterns left by the generation process (\cite{dnf, lnp}). More recently, a modern class of artifact-based detectors leverages the internal mechanics of the diffusion process itself as a forensic tool. This approach is broadly divided into error-based and non-error-based methods. Error-based detectors operate on the principle that diffusion models reconstruct their own outputs with lower error than real images, using this discrepancy in pixel space (\cite{dire, sedid}), in latent space (\cite{aeroblade}), or as a guiding feature (\cite{lare}). In contrast, non-error-based methods use the diffusion pipeline in other ways, such as to generate hard negative training samples (\cite{drct}), to extract internal representations like noise maps as features (\cite{fakeinversion}), or to distill a slow, error-based model into a faster one (\cite{distildire}). A detailed overview of these detection paradigms is provided in Appendix \ref{previous_detectors}.


Despite their successes, our experiments indicate that artifact-based detection methods face significant limitations. First, their performance is often brittle, demonstrating poor cross-generator generalization. As generative models evolve, the specific artifacts these methods rely on change, making the detectors quickly outdated. Second, they are highly sensitive to common image perturbations, like compression, which can easily destroy the subtle fingerprints they detect.


\subsubsection*{Pretrained Feature-Based Detection}

A more recent and increasingly dominant paradigm moves away from specialized artifact detection and instead leverages the rich feature spaces of large-scale, pretrained foundational models \cite{aide, unifd, bilora}. The core idea is that these models, having been trained on web-scale data, have learned robust and generalizable representations of the visual world. A pioneering work in this area is \texttt{UniFD} \cite{unifd}, which demonstrated that a simple linear classifier trained on CLIP \cite{clip} features can achieve impressive generalization across unseen generators. This highlighted the power of semantic features for the detection task. Other works have explored this vision-language connection further; for example, \texttt{Bi-LoRa} \cite{bilora} reframes the detection problem as a visual question-answering or captioning task. Methods like \texttt{LASTED} \cite{lasted} also leverage language-guided contrastive learning. \texttt{AIDE} \cite{aide}, proposed a hybrid model that combined semantic features from a pretrained CLIP model with specialized, hand-crafted modules (DCT \cite{dct} and SRM \cite{srm} filters) to capture low-level texture statistics. 

% Despite their impressive generalization capabilities, these pretrained feature-based methods have notable limitations. Their effectiveness is highly dependent on the quality and robustness of the underlying foundation model; any biases or weaknesses in the base model's feature space can be inherited by the detector. A further concern is that such complex, dense architectures can introduce computational overhead, potentially leading to longer training and inference times. 

The success of sophisticated hybrid approaches like AIDE \cite{aide}, raises a critical question: is it necessary to design hand-crafted modules for low-level features, or can a more effective and less complex solution be found by fusing the features of two distinct, general-purpose foundational models? To answer this question we proposed \texttt{FusionDetect} that utilized feature fusion of foundation models and experiment on the impact of such approach.

% \subsection{Research Gap and Our Contribution}
% The literature reveals a clear trajectory towards leveraging pretrained models, yet an open question remains: what is the most effective and efficient way to harness their power? Previous works have either relied on a single model's feature space \cite{unifd} or combined it with complex, specialized modules \cite{aide}. In this paper, we explore a different path. We hypothesize that a more powerful and robust representation can be achieved by directly fusing the features of two distinct, general-purpose foundational models with complementary strengths. Our proposed model, \textbf{FusionDetect}, combines the high-level semantic features of CLIP with the rich, low-level structural features from the self-supervised model DINOv2. This simple fusion strategy is designed to create a holistic feature space that is inherently robust along both the cross-generator and cross-semantic axes of generalization, without the need for specialized modules or complex architectures.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}
\label{method}

% The core idea of our proposed method, \texttt{FusionDetect}, is to create a holistic and generalizable image representation for detecting AI-generated content, instead of designing a complex, end-to-end architecture from scratch. Our approach is founded on the principle of leveraging and fusing the rich, pre-existing knowledge encoded within large-scale foundational models. We hypothesize that by combining features from models trained with fundamentally different objectives—one with language-vision supervision and the other with self-supervision—we can create a hybrid feature space that is robust along both the semantic and generator axes of generalization.

This section details \texttt{FusionDetect}, a model explicitly designed to solve the two-axis generalization problem. We formally define this as training a detector $D$ on a distribution of generators $G_{train}$ and semantic domains $S_{train}$ that must generalize to a test set drawn from $G_{test}$ and $S_{test}$, where $G_{train} \cap G_{test}=\emptyset$ and $S_{train} \cap S_{test} = \emptyset$. \texttt{FusionDetect} addresses this by creating a hybrid feature space engineered to be a strong baseline detector invariant to shifts in both $G$ and $S$.

% \begin{figure*}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    % \includegraphics[width=\textwidth]{images/pipeline.png}

%    \caption{\textbf{FusionDetect pipeline:} An input image feeds into two parallel, frozen encoders (CLIP and DINOv2). Their output vectors are concatenated and then passed to a small, trainable MLP classifier head which outputs a final real/fake prediction.}
%    \label{fig:pipeline}
% \end{figure*}


\subsubsection*{FusionDetect}

The feature extraction backbone of \texttt{FusionDetect} consists of two distinct, powerful vision encoders. A key design choice is that both of these pretrained backbones remain frozen during training. This helps with computation efficiency and faster training time as well as preventing the model from overfitting to the training data and preserving the highly generalizable, world-knowledge features learned by these models during their original large-scale pretraining.

The two branches of our feature extractor are as follows:
\begin{enumerate}
    \item \textbf{Semantic Feature Encoder (CLIP):} We employ a CLIP vision encoder to capture high-level semantic, contextual, and object-level information. Its rich understanding, derived from large-scale image-text pretraining, is crucial for achieving cross-semantic generalization. Given an input image $I$, the CLIP encoder $E_{CLIP}$ produces a semantic feature vector.

    \item \textbf{Structural Feature Encoder (DINOv2)}: We use a DINOv2 vision transformer to capture fine-grained structural and textural details. As a self-supervised model, it is highly sensitive to the low-level patterns and artifacts that betray a synthetic origin, which is vital for achieving cross-generator generalization. The DINOv2 encoder $E_{DINO}$ processes the same input image $I$ to produce a structural feature vector.
\end{enumerate}

These two feature vectors are then fused via concatenation to form a comprehensive hybrid feature vector, $z_{f}$, assuming $d_{clip}$ and $d_{dino}$ are the image encoders output dimensions:

\begin{equation}
    z_{f} = [E_{CLIP}(I) \in \mathbb{R}^{d_{clip}} \mathbin\Vert E_{DINO}(I) \in \mathbb{R}^{d_{dino}}] \in \mathbb{R}^{d_{clip} + d_{dino}}
\end{equation}

$z_{f}$, is then processed by the only trainable component of our model: a lightweight Multi-Layer Perceptron (MLP) classifier head, denoted by the function $f_{\theta}$ with parameters $\theta$. The model is trained end-to-end to minimize the binary cross-entropy (BCE) loss $L(\theta)$ over a batch of $N$ images, defined as:
\begin{equation}
    L(\theta) = - \frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
\end{equation}
where $y_i \in \{0,1\}$ is the ground-truth label and $p_i = \sigma(f_{\theta}(z_{f,i}))$ is the predicted probability from the sigmoid function $\sigma$.



% \subsection{Feature Extraction}

% The feature extraction backbone of \texttt{FusionDetect} consists of two distinct, powerful vision encoders. A key design choice is that both of these pretrained backbones remain frozen during training. This strategy provides two significant advantages: first, it is computationally efficient, dramatically reducing the number of trainable parameters and accelerating the training process; second, and more importantly, it prevents the model from overfitting to the training data and preserves the highly generalizable, world-knowledge features learned by these models during their original large-scale pretraining.

% The two branches of our feature extractor are as follows:
% \begin{enumerate}
%     \item \textbf{Semantic Feature Encoder (CLIP):} To capture the high-level semantic content of an image, we employ the vision encoder from the CLIP. Having been trained on hundreds of millions of image-text pairs from the web, the CLIP encoder excels at producing representations that are rich in contextual and object-level information. This semantic understanding is crucial for tackling the semantic generalization challenge, where a detector must perform reliably across different content domains (e.g., faces, animals, landscapes). Given an input image $I$, the CLIP encoder $E_{CLIP}$ produces a semantic feature vector.


%     \item \textbf{Structural Feature Encoder (DINOv2)}: To capture the fine-grained structural and textural details of an image, we use the DINOv2 vision transformer. DINOv2 is trained via self-supervision, learning features directly from pixel data without human labels. This allows it to develop a powerful understanding of low-level patterns, object parts, and textures, making it highly sensitive to the subtle artifacts and statistical inconsistencies that are characteristic of synthetic images. This capability is vital for addressing generator generalization, as these low-level patterns often betray the generative process. The DINOv2 encoder $E_{DINO}$ processes the same input image $I$ to produce a structural feature vector.
    
% \end{enumerate}

% These two feature vectors, capturing complementary aspects of the input image, are then fused via simple concatenation to form a single, comprehensive hybrid feature vector, $z_{f}$ assuming $d_{clip}$ and $d_{dino}$ are the image encoders output dimensions:


% \begin{equation}
%     z_{f} = [E_{CLIP}(I) \in \mathbb{R}^{d_{clip}} \mathbin\Vert E_{DINO}(I) \in \mathbb{R}^{d_{dino}}] \in \mathbb{R}^{d_{clip} + d_{dino}}
% \end{equation}



% \subsection{Classification Head}
% While the feature extractors remain frozen, the classification of the hybrid feature vector is handled by a small, lightweight classifier head, which is the only trainable component of our model. This head is a Multi-Layer Perceptron (MLP), denoted by the function $f_\theta$ with trainable parameters $\theta$. The role of the MLP is to learn a decision boundary within the high-dimensional hybrid feature space to effectively separate real images from their AI-generated counterparts.

% The MLP takes the concatenated feature vector $z_{f}$ as input and outputs a single logit. This logit is passed through a sigmoid function $\sigma$ to produce a final probability score $p$, which indicates the likelihood of the image being AI-generated:

% \begin{equation}
%     p = \sigma(f_{\theta}(z_{f}))
% \end{equation}

% The model is trained to minimize the binary cross-entropy (BCE) loss between the predicted probabilities and the ground-truth labels. For a batch of $N$ images, the loss function $L$ is defined as:

% \begin{equation}
%     L(\theta) = - \frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1 - y_i) \log(1 - p_i)]
% \end{equation}

% where $y_i\in \{0,1\}$ is the ground truth label for the $i$-th image (0 for real, 1 for fake), and $p_i$ is the corresponding predicted probability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% The development of a truly universal detector is critically dependent on the data used for both training and evaluation. To address the limitations of prior work, we introduce a comprehensive data strategy that involves curating a new, semantically diverse training set and establishing a novel, challenging benchmark for evaluation. This section details the composition and motivation behind these datasets.

% \todod{only talk about omnnigen. for train and testsets only briefly talk about them in results tables}



\section{Experiments}
\label{experiments}

To empirically validate the effectiveness of our proposed \texttt{FusionDetect} model, we conduct a series of comprehensive experiments. Our evaluation is designed to rigorously test performance along the two-axis generalization problem, assess robustness to real-world image perturbations, and dissect the model's architecture to understand the contribution of its core components.

\subsection{Experimental Setup}

\textbf{Implementation Details:} The \texttt{FusionDetect} model was trained for an efficient 10 epochs using an AdamW optimizer on a single NVIDIA RTX 3090 GPU. The final architecture consists of frozen \textit{CLIP-ViT-L14} and \textit{DINOv2-L14} backbones and a 4-layer MLP classifier head. To enhance robustness, we applied random JPEG compression and Gaussian blur to 10\% of the images during training.

\textbf{Baselines for Comparison:} We compare \texttt{FusionDetect} against a comprehensive suite of recent detectors which their code and pretrained weights were publicly accessible: \texttt{DIF} \cite{dif}, \texttt{UNIFD} \cite{unifd}, \texttt{DNF} \cite{dnf}, \texttt{LASTED} \cite{lasted}, \texttt{BiLORA} \cite{bilora}, \texttt{AIDE} \cite{aide}, \texttt{SSP}, and \texttt{NPR}. To ensure a thorough and fair evaluation, these models are tested in two settings where applicable: using their original, off-the-shelf pretrained weights, and after being retrained from scratch on our custom dataset.

\textbf{Training Dataset:} To directly address the semantic generalization gap, we curated a custom, balanced training set of 60,000 images (30k real, 30k fake). This dataset was constructed to maximize categorical and stylistic diversity by combining three distinct sources: samples derived from the large-scale ImagiNet \cite{imaginet} and GenImage \cite{genimage} benchmarks, and a challenging set of images generated using prompts derived from the hyper-realistic Chameleon dataset \cite{aide}. To follow the training scheme of previous work for cross-generator generalization, only images generated by SD1.4 and SD2.1 \cite{ldm} were used in the train dataset and tested on others generators and datasets. 

\textbf{Evaluation Metrics:} Similar to previous work, we report performance using Accuracy (Acc) and Average Precision (AP) \cite{aide, lasted, npr, unifd}. To specifically measure generalization, we compute the both the Mean and Standard Deviation (STD) of these metrics across diverse benchmarks. A lower STD is a critical indicator of a robust detector, as it signifies consistent performance across different semantic domains. Accuracy of each class (real/fake) has also been reported in Appendix \ref{app_results}.


\subsection{Comparative Analysis on Established Benchmarks}

To validate the generalization capabilities of \texttt{FusionDetect} and provide a comparison point to prior work, we evaluated it on a collection of diverse and established test sets. The test set contains 8000, 10000, and 2595 images from GenImage \cite{genimage}, ImagiNet \cite{imaginet} and Chameleon \cite{aide} datasets respectively, each containing equal number of real and synthetic images. To ensure a fair and comprehensive evaluation, the test set is composed of an equal number of images sampled from every available generator within each source dataset. A detailed overview can be found in Appendix \ref{app_established}.

As shown in Table \ref{tab:established_results_main}, \texttt{FusionDetect} achieves the best overall performance, attaining the highest mean accuracy and average precision, and crucially, the lowest standard deviation. It surpasses the closest competitor by $3.87\%$ in accuracy, $6.13\%$ in average precision. An important note is that although other detectors perform better on GenImage \cite{genimage}, but on ImagiNet \cite{imaginet} and the difficult chameleon \cite{aide} dataset our detector outperforms others by almost $10\%$ on average which indicates that previous models were specifically designed to perform well on the GenImage benchmark since it was seen as the standard benchmark for synthetic image detection;  and not as a universal detector. Acheiving low STD and high mean accuracy on three different datasets indicates the domain generalization capabilities of \texttt{FusionDetect}.
% Moreover, Its strong performance on the OmniGen benchmark—which is out-of-distribution compared to our training set—combined with these results, provides strong evidence of its robustness on both the semantic and generator axes of generalization.

\input{tables/gen-img-cham_results}



\subsection{The OmniGen Benchmark}
A core contribution of our work is the creation of a new, open-source benchmark designed to reflect the practical challenges of AI image detection. Our motivation was to address the shortcomings of existing benchmarks, which often lack semantic diversity and lag behind the rapid pace of generator development. The OmniGen benchmark was designed to be more practical and challenging by focusing on the latest SOTA generators, including both closed-source APIs and popular fine-tuned community models.


\textbf{Generator Selection:} The benchmark contains 11,550 fake images from a curated list of 12 relevant and powerful generative models, categorized as follows:
\begin{itemize}
    \item \textit{Closed-Source}: GPT-4o \cite{gpt4o}, Imagen 4 \cite{imagen}, Imagen 4 ultra \cite{imagen}, MidJourney v7 \cite{midjourney}.
    \item \textit{Open-Source}: FLUX 1 \cite{flux}, Kandinsky 3 \cite{kandinsky}, PixArt-$\delta$ \cite{pixart}, SD3.5-medium \cite{sd3.5}, HiDream-I1 \cite{hidream}, CogView4-6B \cite{cogview}.
    \item \textit{Community Fine-tuned:} Juggernaut \cite{juggernaut}, Dreamshaper \cite{dreamshaper}.
\end{itemize}


\textbf{Benchmark Curation:} To ensure high semantic diversity and prevent model overfitting to specific concepts, the synthetic images were generated using a structured, randomized prompt template:

\textit{"A richly detailed, high-resolution and photorealistic image depicting: \{subject\} during the \{time\}. The scene includes \{setting\}, \{visual\}, and lifelike rendering. The image style resembles \{style\}. Use \{light\}."}

Each bracketed variable was populated from a large pool of options (e.g., over 400 different subjects), resulting in highly unique prompts for each image. For each of the 12 generators, we generated 1000 images with $1024\times1024$ resolution which were evaluated against a set of 1000 real images from Unsplash \cite{unsplash}. A detailed overview and examples generated can be found in Appendix \ref{app_omnigen}.
% The generation process involved a mix of using official APIs, sourcing from existing datasets, and running models locally, mirroring the diverse ways synthetic content is produced and encountered in the real world. 


Our secondary evaluation is conducted on the new OmniGen benchmark, which is designed to test detectors against the modern, real-world generative landscape. The results shown in Table \ref{tab:omnigen_results_acc} demonstrate the superior performance of \texttt{FusionDetect}, specifically in accuracy where we see $+4.48\%$ increase in performance. Among the detectors, \texttt{FusionDetect} is the top performer, achieving the highest mean accuracy of $97.38\%$ and a remarkably low standard deviation of $3.26$. This indicates its consistent ability to generalize across a wide variety of SOTA generators, from closed-source APIs to open-source and community. AP and class accuracies are reported in \ref{app_results}.

% \texttt{FusionDetect}'s strong performance on the OmniGen benchmark—which is out-of-distribution compared to our training set—combined with results from previous section on established evaluation sets, provides strong evidence of its robustness on both the semantic and generator axes of generalization.

\input{tables/omnigen_results_acc}
% \input{tables/omnigen_results_ap}



\subsection{Robustness to Common Perturbations}

A critical attribute of a practical detector is its resilience to image degradations commonly encountered online. We subjected the detectors to two stress tests: JPEG compression and Gaussian blur. The results, shown in Table \ref{tab:robustness_results}, highlight a key weakness in many artifact-based detectors. Models that rely on high-frequency spatial artifacts, such as \texttt{SSP} \cite{ssp}, \texttt{NPR} \cite{npr} and \texttt{DNF} \cite{dnf}, fail drastically under both compression and blur, despite their high scores on clean images. In contrast, \texttt{FusionDetect}'s performance barely sees any degradation. This remarkable stability confirms that our model's decisions are based on more fundamental, robust features rather than fragile, easily-disrupted artifacts, making it far more suitable for real-world deployment.

\input{tables/robustness}


\subsection{Ablation and Sensitivity Analysis}

We evaluated the performance of the individual components of our model. As shown in Table \ref{tab:ablation_study}, while only using the CLIP \cite{clip} model as feature extractor performs well, consistent with findings from prior work, and only using the DINOv2 \cite{dinov2} model is also effective, their fusion in \texttt{FusionDetect} yields the best results. This confirms our core hypothesis that the two models provide complementary features. The t-SNE visualization of these three embeddings is shown in Figure \ref{fig:tsne} which indicates that that CLIP+DINO embedding can easily separate not only real and fake images but also their underlying dataset. We also explored incorporating feature up-scaling via FeatUp \cite{featup}, but this did not improve performance, suggesting that the raw, powerful features from the foundational backbones are more discriminative for this task.

Moreover, we analyzed the impact of different backbones and classifier depths on performance and the results are shown in Table \ref{tab:sensitivity_analysis}. The choice of \textit{ViT-L/14} for both backbones was made for consistency with prior work and to leverage the power of large-scale architectures. The results show that a relatively simple 4-layer MLP classifier is sufficient to achieve SOTA performance. This indicates that the true power of \texttt{FusionDetect} lies in its robust feature extractor, which is so effective that it does not require a complex classifier to learn a decision boundary.

\input{tables/ablation}
% \input{tables/component_ablation}
% \input{tables/sensitivity}



\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\textwidth]{images/all-emb-vert.png}

   \caption{T-SNE \cite{tsne} projection of GenImage \cite{genimage}, ImagiNet \cite{imaginet}, and Chameleon \cite{aide} dataset. The CLIP+DINO (bottom) encoder successfully separates real and fake classes for each dataset unlike the other two options.  (Top left: CLIP, Top right: DINOv2)}
   \label{fig:tsne}
\end{figure*}




\section{Discussion}
The empirical findings presented here strongly substantiate our central thesis regarding the "two-axis generalization" issue. \texttt{FusionDetect} exhibits remarkable stability across the cross-semantic axis, as evidenced by its minimal standard deviation on benchmarks such as GenImage \cite{genimage}, ImagiNet \cite{imaginet}, and Chameleon \cite{patchcraft}. This consistency indicates that its performance is not restricted by the underlying visual domain. Additionally, \texttt{FusionDetect} maintains impressive accuracy on the OmniGen benchmark which can be considered out-of-distribution both semantically and also unseen generators used, affirming its robustness across the two axes. Our proposed benchmark itself plays a pivotal role in this analysis. By incorporating both cutting-edge closed-source models and a variety of community fine-tuned models, it reveals the limitations of detectors that only excel on outdated test sets. These results underscore the necessity of benchmarks that capture both semantic and generator diversity to meaningfully assess a detector’s real-world effectiveness.



\section{Conclusion}

In this paper, we redefined the task of AI-generated image detection by formalizing the “two-axis generalization” task that warrants robustness to both previously unseen generators and different semantic domains. To tackle the two-axis generalization task, we presented the \textbf{OmniGen Benchmark}, a new challenging test set consisting of 12 SOTA generators, and the \texttt{FusionDetect}, a robust detector that solves the two-axis problem by learning representations in a fusion model composed of complementary features extracted from foundational models. We show, empirically, that \texttt{FusionDetect} sets a state-of-the-art within generalization and robustness stages that indicate intelligently fusing complementing features extracted from foundational models is a better paradigm than building from scratch with special architectures. More generally, the two-axis framework offers a valuable method for evaluating model robustness that is broadly applicable, and we hope our contributions lay the groundwork for the next generation of universal fake media detectors.


% This research strongly re-evaluated AI-generated image detection by casting the problem under what we formalize as the "Two-axis generalization" challenge, requiring the created detector to be robust to unseen generators and different semantic domains. In an attempt to satisfy the above requirements, we have proposed the \textbf{OmniGen} benchmark, a new and challenging test set with 12 new SOTA generators that better represent the modern generative landscape of today. Further, we introduced \texttt{FusionDetect}, a suprisingly strong and reliable detection tool that solves our two-axis problem by fusion of complementary features extracted from foundational models. Based on the comprehensive experiments conducted, \texttt{FusionDetect} is proved to be at the forefront, outperforming existing sofisticated methods with a great margin in the generalization aspect and robustness to common types of perturbations.

% Our findings suggest a new paradigm: intelligently combining the knowledge in diverse foundational models is a more efficient and powerful strategy than designing specialized architectures, establishing \texttt{FusionDetect} as a strong baseline for future research. The two-axis generalization framework we propose extends beyond fake image detection, offering a valuable paradigm for evaluating model robustness in any multi-distribution setting, making it relevant to the broader research community. We hope our contributions—the framework, the detector, and our modern benchmark—will lay the ground for the next generation of truly universal fake media detectors.

% Our findings point towards a promising line of work in media forensics: combining the rich knowledge which is already embedded in diverse large-scale foundation models seems a more efficient paradigm that is more powerful than designing specialized architectures from scratch. While \texttt{FusionDetect} sets a new bar for performance, it remains a cat-and-mouse game, and future researchers might want to explore more complex fusion mechanisms based on cross-attention, or generalize this hybrid feature approach to the more challenging domain of deepfake video detection. We hope that our contributions—the conceptual framework, the robust detector, and the modern benchmark—would lay the ground for the next generation of universal fake media detectors.

\newpage
\bibliography{references}
\bibliographystyle{iclr2026_conference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage
\section{Appendix}

\subsection{Detailed overview of OmniGen Benchmark}
\label{app_omnigen}
This appendix provides supplementary details for the OmniGen Benchmark introduced in Section 3. To offer a comprehensive overview of its composition, Table \ref{tab:appendix_omnigen} lists all 12 state-of-the-art generators used in its creation, along with their respective image counts, resolutions, and sourcing methods. Furthermore, Figure \ref{fig:omnigen_examples} presents a selection of example images generated by these models, visually demonstrating the high degree of realism and semantic diversity that makes the OmniGen benchmark a challenging and realistic testbed for modern AI image detectors.



\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\linewidth]{images/all.jpg}

   \caption{OmniGen Benchmark Images. \textbf{Top row:} Midjourney v7 \cite{midjourney}, HiDream \cite{hidream}, Imagine 4 \cite{imagen}, Kandinsky 3 \cite{kandinsky}; \textbf{Middle row:} Flux 1 \cite{flux}, Dreamshaper \cite{dreamshaper}, Pixart-$\delta$ \cite{pixart}, Cogview 4 \cite{cogview}; \textbf{Bottom row:} Juggernaut \cite{juggernaut}, SD3.5 \cite{sd3.5}, Imagen 4 ultra \cite{imagen}, GPT4o \cite{gpt4o}.}
   \label{fig:omnigen_examples}
\end{figure*}

\input{tables/appendix_omnigen}


\subsection{Detailed overview of Established datasets}
\label{app_established}
We provide additional information regarding the test sets used for GenImage \cite{genimage}, ImagiNet \cite{imaginet}, and Chameleon \cite{aide} in Section \ref{experiments}. It includes generators used, number of images, image resolution, semantic categories, and the source for real images utilized in these datasets which is shown in Tables \ref{tab:appendix_genimage}, \ref{tab:appendix_imaginet}, and \ref{tab:appendix_chameleon}. Note that the numbers reported in these tables indicate the count of fake images only.


\input{tables/appendix_genimage}
\input{tables/appendix_imaginet}
\input{tables/appendix_chameleon}


\subsection{Detailed overview of previous detectors}
\label{previous_detectors}

Detectors for synthetically generated images leverage a variety of signals, from low-level artifacts to high-level semantic features. Below is a detailed overview of prominent methods and the core ideas behind their approaches.

\begin{itemize}
\item \textbf{NPR} \cite{npr}: This method focuses on \textit{Frequency and Spectral Analysis}. It operates on the principle that up-sampling operations in generative models introduce predictable artifacts in the frequency domain. NPR analyzes an image's frequency spectrum to identify these high-frequency inconsistencies.

\item \textbf{DIF} \cite{dif}: Also a method based on \textit{Frequency and Spectral Analysis}, DIF aims to extract a "Deep Image Fingerprint." It leverages frequency-aware clues to find unique, model-specific signatures for detection and lineage analysis.

\item \textbf{PatchCraft} \cite{patchcraft}: This detector is based on \textit{Texture and Patch Analysis}. It posits that artifacts are more pronounced at a local level and works by analyzing the inter-pixel correlation contrast between rich and poor texture regions within an image.

\item \textbf{SSP} \cite{ssp}: Following the \textit{Texture and Patch Analysis} approach, SSP operates on local patches and gradients to identify discriminative features that separate real images from generated ones.

\item \textbf{DNF} \cite{dnf}: This method uses \textit{Noise Pattern Analysis}. It is designed to extract the unique residual noise patterns present in synthetic images by estimating and analyzing the noise added during the diffusion process.

\item \textbf{DIRE} \cite{dire}: A \textit{Diffusion Process-Based Method} that relies on reconstruction error. It is founded on the principle that diffusion models can reconstruct their own generated images with significantly lower error than they can reconstruct real-world images. This pixel-space error is used as the primary feature for detection.

\item \textbf{AEROBLADE} \cite{aeroblade}: This method also uses reconstruction error but measures it in the latent space of the model's autoencoder, providing a different perspective on the reconstruction fidelity.

\item \textbf{LaRE$^2$} \cite{lare}: This approach uses the latent reconstruction error not as a direct feature, but as a guiding signal for a larger, more complex classification network.

\item \textbf{DRCT} \cite{drct}: A non-error-based diffusion method that employs contrastive training. It cleverly uses the diffusion model's reconstruction ability to generate hard negative training samples (reconstructed real images labeled as fake) to improve detector robustness.

\item \textbf{DistilDIRE} \cite{distildire}: This method addresses the slow speed of error-based detectors by distilling the knowledge from a large, slow DIRE-based detector into a much smaller and faster one, which operates without a full reconstruction cycle.

\item \textbf{UniFD} \cite{unifd}: A pioneering \textit{Pretrained Feature-Based} detector. It leverages the rich, semantic feature space of large vision-language models like CLIP, demonstrating that a simple linear classifier trained on these general-purpose features can achieve impressive generalization across unseen generators.

\item \textbf{LASTED} \cite{lasted}: This detector also leverages vision-language models but through the specific mechanism of language-guided contrastive learning to better align features for the detection task.

\item \textbf{Bi-LoRa} \cite{bilora}: This approach creatively reframes the detection problem as an image captioning task. It fine-tunes a VLM to output a simple caption of "real" or "fake," leveraging the model's generative language capabilities for classification.

\item \textbf{AIDE} \cite{aide}: This detector proposes a \textit{Hybrid Model} that combines the best of both worlds. It fuses high-level semantic features from a pretrained CLIP model with specialized, hand-crafted modules (like DCT and SRM filters) designed to capture low-level, artifact-based texture statistics.

\end{itemize}


\subsection{Additional experiment results}
\label{app_results}
Here we included the supplementary experiment results on our evaluation sets and report AP and accuracy of each class: Real \& Fake.

\input{tables/appendix_gen-img-cham_results_fAcc-rAcc}
\input{tables/appendix_omnigen_results}
\input{tables/omnigen_results_ap}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
