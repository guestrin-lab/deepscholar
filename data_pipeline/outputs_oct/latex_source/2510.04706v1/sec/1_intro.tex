\section{Introduction}
\label{sec:intro}

The rapid advancement of generative models, beginning with GANs \cite{goodfellow2020generative} and now dominated by diffusion models \cite{ddpm_Ho, song2021Score-SDE, dhariwal2021diffusion}, has significantly elevated the quality and versatility of synthetic visual content. Powered by high-performance computing and trained on vast amounts of data, foundation diffusion models can generate high-quality and diverse images of human characters, achieving remarkable realism and adaptability across domains. The impact of such technology spans multiple industries, from advertising to education and virtual environments. Yet, one of the most transformative applications lies in film and entertainment, where human performance is central to storytelling. Despite progress in human-centric generation, the existing paradigm in diffusion models focuses on identity preservation, often overlooking controllability.

Facial expressions, however, play a vital role in human communication, conveying emotions, intentions, and personality traits. As such, they have become a central focus in face modeling research. A long line of work has explored facial expression editing with GAN-based methods \cite{StarGAN, sdapolito2021GANmut, ding2017exprgan, Tripathy_ICface, paraperas2022ned}, though often with limited realism. More recent approaches leverage powerful pre-trained text-to-image diffusion models like Stable Diffusion \cite{rombach2022high}, extending them with additional mechanisms for identity and expression control to generate faces with desired emotional states. These methods typically rely on emotion representations such as categorical labels (\eg, happy, sad), the continuous Valence-Arousal 2D space \cite{russell1977evidence}, or Action Units (AUs) \cite{ekman1978facial} that describe localized facial muscle movements. However, while these descriptors offer semantic editing, they often fall short in capturing the full precision and nuance of facial expressions, limiting their usefulness for applications requiring fine-grained control.

In this work, we target precise and disentangled facial expression control. To achieve this, we adopt a parametric representation based on expression blendshapes from the FLAME 3D face model \cite{FLAME:SiggraphAsia2017}, which offers continuous, high-dimensional control over expression. We build upon Arc2Face \cite{paraperas2024arc2face}, a foundation diffusion model which can synthesize diverse, realistic faces of a single identity using identity embeddings as conditioning input. To incorporate detailed control, we inject the expression parameters into the model's cross-attention layers, while the pretrained backbone remains frozen, preserving its strong identity prior. To support a wide range of expressive capabilities, including rare, asymmetric, subtle, and extreme expressions, we carefully curate training datasets with rich expression diversity. Additionally, we introduce a Reference Adapter trained on top of the base model that conditions generation on an input image, allowing precise expression editing while maintaining the subjectâ€™s original appearance and background.

