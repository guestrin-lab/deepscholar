\section{Related Work}
\label{sec:related_work}


\subsection{Facial Expression Generation and Editing}

Early efforts to edit facial expressions in ``in-the-wild'' images primarily employed GAN-based architectures. These approaches typically used conditional GANs for image-to-image translation. For instance, StarGAN \cite{StarGAN} enabled multi-domain facial attribute transfer using categorical emotion labels, while ExprGAN \cite{ding2017exprgan} and Lindt \etal \cite{Lindt2019FacialEE} introduced expression editing along emotion intensity levels. Deviating from hand-crafted emotion representations, GANmut \cite{sdapolito2021GANmut} proposed a framework that implicitly learns a continuous and interpretable emotion space from categorical labels, by associating classification confidence with expression intensity. Despite promising results, these methods are limited by the quality and diversity of their training data, as well as the inherent instability of GAN training. 

At the same time, various works focus on geometry-driven expression editing, addressing the task of face reenactment. Models like ICface \cite{Tripathy_ICface} and GANimation \cite{GANimation_ijcv2019} leverage Action Units (AUs) as an interpretable representation for expression control, whereas Neural Emotion Director \cite{paraperas2022ned} introduces a 3D-based Emotion Manipulator that translates expressions into basic emotions or reference-driven styles using a 3D Morphable Model (3DMM)-conditioned GAN. Another direction explores the latent space of the pre-trained StyleGAN \cite{stylegan} network, for facial editing. InterFaceGAN \cite{shen2020interpreting, shen2020interfacegan} disentangles facial attributes using pre-trained classifiers, enabling semantic control over identity, age, or expression. StyleCLIP \cite{patashnik2021styleclip} builds on this by incorporating CLIP \cite{radford2021learning}, allowing text-driven facial edits within StyleGANâ€™s latent space, while EmoStyle \cite{EmoStyle_2024_WACV} takes this further by explicitly separating expression from other facial attributes, using Valence-Arousal values to modify expressions while preserving identity and appearance.

\subsection{ID and Attribute Control in Diffusion Models}

Following the advent of diffusion models, research has shifted towards adding controllability to frontier text-to-image models like Stable Diffusion \cite{rombach2022high}. One major area of focus has been identity preservation. A popular approach involves using the CLIP image encoder \cite{radford2021learning} to extract features from reference subjects, which are then injected into pre-trained models via cross-attention layers. Notable examples include FastComposer \cite{xiao2023fastcomposer}, PhotoVerse \cite{chen2023photoverse}, MoA \cite{wang2024moa}, and PhotoMaker \cite{li2023photomaker}. A more robust alternative to CLIP embeddings is the use of face recognition networks, which extract stronger ID-specific features. This approach has been leveraged in works such as Face0 \cite{valevski2023face0}, DreamIdentity \cite{chen2023dreamidentity}, IP-Adapter-FaceID \cite{ye2023ip}, PortraitBooth \cite{peng2023portraitbooth}, and InstantID \cite{wang2024instantid} for both 2D and 3D face generation \cite{gerogiannis2025arc2avatar}. Among these, the pioneering Arc2Face work~\cite{paraperas2024arc2face}, showed how the powerful Stable Diffusion model can be transformed into an ID-consistent face foundation model capable of generating highly realistic and diverse images with compelling identity similarity, by leveraging WebFace \cite{zhu2021webface260m} - the largest public dataset for face recognition. 

In parallel, numerous efforts explore the integration of expression features. These methods vary in the type of representation used, including 2D landmarks \cite{han2024faceadapter, mishima2025facecrafter}, 3DMMs \cite{liang2024caphuman}, Action Units \cite{wei2025magicface, varanka2024fineface}, continuous emotion spaces \cite{emotiondiffusion}, or natural language instructions \cite{wang2024instructavatar, liu2024towards}. Yet, they often fall short in expression fidelity - particularly when handling asymmetric or extreme expressions and typically introduce identity distortion.