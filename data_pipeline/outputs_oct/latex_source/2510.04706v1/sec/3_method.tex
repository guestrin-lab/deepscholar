\section{Method}
\label{sec:method}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{figures/pipeline_compressed.pdf}
\caption{\textbf{Overview of the proposed expression-control framework.} Our approach builds on the Arc2Face diffusion model~\cite{paraperas2024arc2face}, which conditions the denoising UNet on ID embeddings. \textbf{(a)} We introduce an \textbf{Expression Adapter} that guides the generation using explicit FLAME~\cite{FLAME:SiggraphAsia2017} blendshape parameters extracted from reference images via an off-the-shelf 3D reconstruction method~\cite{smirk_2024_CVPR}. The adapter consists of two components: (1) an MLP that maps 3DMM parameters into the CLIP latent space used by Stable Diffusion, and (2) a dual attention mechanism that integrates expression information alongside identity using separate key and value matrices in the cross-attention layers. \textbf{(b)} We further incorporate a \textbf{Reference Adapter} that conditions the model on the input image itself. A dedicated Reference UNet extracts multi-scale features, which are injected into the denoising UNet via self-attention layers modulated by added LoRA weights. By enabling this adapter at inference, we support image-based expression editing. The proposed modules are trained on expression-rich image and video data, achieving strong generalization across a wide range of facial expressions.}
\label{fig:method}
\end{figure*}

Our method generates facial images with disentangled control over identity and expression. Given an input facial identity, represented by ArcFace \cite{deng2019arcface} embeddings, and a target expression, encoded as FLAME blendshape parameters extracted from an expressive target image, our diffusion model synthesizes diverse and photorealistic images that faithfully reflect the source subject with the target expression, as shown in \cref{fig:teaser}. To further enhance applicability, we introduce an optional inference-time mechanism that conditions the generation directly on the source image pixels, enabling expression editing on real images while preserving background and appearance. Specific details of our approach are provided in the subsequent sections. For a high-level overview, see \cref{fig:method}.

\subsection{Preliminary: Arc2Face}
Arc2Face \cite{paraperas2024arc2face} is a recent diffusion foundation model for human faces, built on top of the Stable Diffusion framework. It achieves identity-consistent synthesis by integrating ArcFace \cite{deng2019arcface} embeddings into the generation pipeline, enabling the creation of diverse, high-quality $512 \times 512$ facial images with a significantly higher degree of identity similarity compared to prior methods.  Starting from a face image $\mathbf{x}\in \mathbb{R}^{H\times W\times C}$ as input, the ArcFace network \cite{deng2019arcface} $\phi$ is used, following cropping and alignment, to extract the identity features, $\mathbf{v} = \phi(\mathbf{x}) \in \mathbb{R}^{512}$. In order to project these features into the cross-attention layers of the UNet for conditioning, Arc2Face repurposes the original CLIP text encoder $\tau$ to act as a facial identity encoder tailored to ArcFace embeddings. The identity vector $\mathbf{v}$ replaces a placeholder token within a fixed text prompt and is mapped by the fine-tuned encoder to a sequence in the CLIP latent space: $\mathbf{v}_c = \tau(\mathbf{v}) \in \mathbb{R}^{77 \times 768}$. The entire model - comprising the adapted CLIP encoder and the UNet - is extensively fine-tuned on WebFace260M \cite{zhu2021webface260m}, the largest publicly available face recognition dataset, enabling Arc2Face to function as a powerful prior model. Its strong decoupling between identity and other visual attributes makes it well-suited for our task, which aims to introduce fine-grained, expression-level control while maintaining subject fidelity.

\subsection{Expression Adapter}
To achieve fine-grained and disentangled control over facial expressions, we leverage a parametric 3DMM, specifically FLAME \cite{FLAME:SiggraphAsia2017}, which decomposes facial geometry into distinct components for identity and expression. This formulation maps the emotion control problem from image space to the subject-agnostic parameter space of a 3D model. We construct an expression representation vector by concatenating FLAME’s expression parameters $\mathbf{e} \in \mathbb{R}^{50}$, eyelid pose parameters $\mathbf{a} \in \mathbb{R}^{2}$, and jaw articulation parameters $\mathbf{p} \in \mathbb{R}^{3}$. The resulting vector $\mathbf{s} = \text{concat}(\mathbf{e}, \mathbf{a}, \mathbf{p}) \in \mathbb{R}^{55}$ serves as the input to our \textbf{Expression Adapter}.

Our approach follows the paradigm of IP-Adapter \cite{ye2023ip}, and integrates expression information into the cross-attention layers of the UNet in a disentangled manner. First, we use a lightweight MLP $\epsilon$ to project the expression vector into the CLIP latent space $\mathbf{s}_c = \epsilon(\mathbf{s}) \in \mathbb{R}^{4 \times 768}$. This projected embedding is then used to produce additional key and value matrices, $\mathbf{K}_{exp}, \mathbf{V}_{exp}$, which are incorporated into each cross-attention layer of the UNet. Specifically, at each layer, we extend the original cross-attention mechanism $\text{Attention}_{\text{id}}(\mathbf{Q}, \mathbf{K}, \mathbf{V})$ - conditioned on the identity embedding - with an additional branch conditioned on expression, where the two attention outputs are computed in parallel using the same query matrix $\mathbf{Q}$. The final attention output is the sum of both contributions:
\begin{equation}
\begin{split}
    \mathbf{S} = \text{Attention}_{\text{id}}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \text{Attention}_{\text{exp}}(\mathbf{Q}, \mathbf{K}_{exp}, \mathbf{V}_{exp}) \\
    = \text{Softmax}(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d}})\mathbf{V}+\text{Softmax}(\frac{\mathbf{Q}(\mathbf{K}_{exp})^{\top}}{\sqrt{d}})\mathbf{V}_{exp}
\end{split}
\end{equation}
To maintain the strong prior of the pre-trained Arc2Face backbone, we freeze all original model weights and train only the additional parameters: the expression projection MLP $\epsilon$, and the linear projection layers $\mathbf{W}_{k_{exp}}$, $\mathbf{W}_{v_{exp}}$ used to compute the expression key and value matrices $\mathbf{K}_{exp}=\mathbf{s}_{c}\mathbf{W}_{k_{exp}}, \mathbf{V}_{exp}=\mathbf{s}_{c}\mathbf{W}_{v_{exp}}$. 

\subsection{Reference Adapter}
While our base model enables stochastic portrait generation conditioned on identity and expression features, certain use cases, such as real image editing, require preserving the subject’s appearance and background. To support this, we introduce a \textbf{Reference Adapter}, which can be seamlessly integrated into the base model to condition the generation on the source image itself. This is implemented via a Reference UNet, a frozen copy of the original denoising UNet that mirrors its architecture. The Reference UNet serves as a feature encoder for the input reference image, extracting spatially-aligned representations that capture the subject’s appearance and background at each layer. Since these features are dimensionally aligned with those produced by the main (expression-conditioned) UNet during the denoising process, we concatenate the two feature maps within the self-attention layers. This allows the model to blend appearance cues from the reference image with the evolving features of the generated image. While this modification achieves reference conditioning at inference time, in practice, it may introduce conflicts with expression control - especially when the reference expression differs from the target. To resolve this, we incorporate lightweight LoRA layers into the self-attention modules of the denoising UNet. These are fine-tuned with both the \textbf{Expression Adapter} and the \textbf{Reference Adapter} active, enabling the model to harmonize identity, expression, and appearance without retraining the full network. During inference, our approach supports flexible generation: the model can switch between reference-driven and stochastic expression synthesis by adding or removing the Reference UNet and the optimized LoRA weights.

\subsection{Training Details}
Training is performed in two stages. In the first phase, we train the \textbf{Expression Adapter} using expression-rich datasets. We use AffectNet \cite{mollahosseini2017affectnet} with 450K images which we upsample with a face restoration network \cite{wang2021gfpgan}, and augment it with 250K frames from the FEED video dataset \cite{Drobyshev_2024_CVPR} containing extreme expressions. We also include 70K high-resolution images from FFHQ \cite{stylegan} for enhanced quality. For each image, we extract identity embeddings using ArcFace \cite{deng2019arcface} and expression parameters using the state-of-the-art SMIRK method \cite{smirk_2024_CVPR}, for conditioning, while the image itself is used as the supervision target for the denoising loss.

The second phase includes adding the LoRA layers of the \textbf{Reference Adapter}, and training them while keeping the trained \textbf{Expression Adapter} frozen. This requires cross-paired training samples where the same individual appears with different expressions in the source and target images, as using the same image would lead the model to simply ``copy-paste'' the reference image, bypassing the target expression. Therefore, we utilize the FEED video dataset, as well as the large-scale HDTF video dataset \cite{zhang2021hdtf}, containing approximately 1.5 million frames of talking faces. We sample reference–target pairs by selecting two random frames within short (2-second) clips, ensuring expression variation while maintaining identity consistency. Again, we annotate the reference images with identity embeddings and the target images with expression parameters.