\section{Experiments}
\label{sec:experiments}

\begin{figure*}[h]
\centering
\includegraphics[width=1.0\textwidth]{figures/comp_compressed.pdf}
\caption{Visual comparison of our method with competing models \cite{varanka2024fineface, liang2024caphuman} for expression-controlled generation conditioned on identity features.}
\label{fig:comp}
\end{figure*}

We conduct comprehensive comparisons with state-of-the-art open-source approaches for face generation with emotion control. Below, we outline the experimental setup, evaluation metrics, and baseline methods used in each setting. Additional qualitative results are provided in the Supplementary Material.

\subsection{Identity-Driven Expression Generation}
We first focus on the unconstrained generation setting, where the goal is to synthesize novel expressive facial images, conditioned on a given identity and target expression. We compare against recent similar methods that achieve expression control on top of Stable Diffusion: FineFace~\cite{varanka2024fineface}, which uses Action Units (AUs) as expression guidance, and CapHuman~\cite{liang2024caphuman}, which leverages rendered normals and albedo maps from FLAME with a ControlNet-like \cite{controlnet} structure. Both incorporate identity embeddings to preserve subject identity. For benchmarking, we collect 1K ``in-the-wild'' face images of different individuals. For each subject, we randomly sample 5 target expression images from the AffectNet validation set~\cite{mollahosseini2017affectnet}, resulting in 5K expression-conditioned generations per method. We evaluate performance across three main axes: image quality, expression fidelity, and identity preservation, using the following metrics: 1) \textbf{FID}~\cite{heusel2017gans, Seitzer2020FID}: Fréchet Inception Distance to assess realism by comparing distributions of generated and input identity images; 2) \textbf{VA-MSE}: MSE of Valence-Arousal values between generated and target expression images; 3) \textbf{AU-MSE}: MSE between target and generated Action Unit intensities; 4) \textbf{Emotion Accuracy}: Classification accuracy for 8 emotion categories (anger, contempt, disgust, fear, happiness, neutral, sadness, surprise) between intended and generated emotions; 5) \textbf{3D Exp-MSE}: MSE of reconstructed FLAME expression parameters using \cite{smirk_2024_CVPR} between target and synthesized images; 6) \textbf{ID Similarity}: Cosine similarity of identity embeddings between source and generated faces. To extract Valence-Arousal values, AU intensities, and emotion labels for all images, we employ off-the-shelf models from the Facetorch toolkit~\cite{facetorch, kim2022optimal, luo2022learning, Savchenko_2021}.

As shown in \cref{tab:quant}, our method significantly outperforms the baselines in terms of expression consistency across all metrics, while also achieving a lower FID score, indicating higher visual quality. This highlights the effectiveness of our precise, parametric expression representation in accurately transferring expressions against AU-based and render-based alternatives. Moreover, as illustrated in \cref{fig:id_sim}, our model consistently achieves higher identity similarity to the input subjects, owing to the strong Arc2Face prior and our careful integration of expression conditioning avoiding interference with identity. To further validate the superiority of our approach, we conducted a user study with 37 participants who were asked to choose the method whose result best resembles the desired expression across a randomly chosen set of 25 ID/expression pairs from our dataset. As reported in \cref{fig:user_study}, our method received 72\% of the votes, indicating its strong expression fidelity. For a visual comparison, we provide examples in \cref{fig:comp}. It is evident that other methods struggle to transfer varying target expressions to the input subjects, whereas our method performs reliably, even for subtle or uncommon facial expressions.

\begin{table}
    \begin{center}
    \setlength{\tabcolsep}{2.3pt}
    \scriptsize
    \begin{tabular}{lccccc} 
        \toprule
         & FID$\downarrow$ & Em.~Acc.~(\%)$\uparrow$ & AU-MSE$\downarrow$ & VA-MSE$\downarrow$ & 3D Exp-MSE$\downarrow$\\
         \midrule
         FineFace \cite{varanka2024fineface} & 0.367 & 35.85 & 0.038 & 0.164 & 1.250\\
         CapHuman \cite{liang2024caphuman} & 0.701 & 23.50 & 0.045 & 0.208 & 1.076\\
         \textbf{Ours} & \textbf{0.300} & \textbf{46.59} & \textbf{0.032} & \textbf{0.086} & \textbf{0.696}\\
        \bottomrule
    \end{tabular}
    \end{center}   
    \caption{\textbf{ID-conditioned generation:} Comparison of our method with ID-conditioned diffusion models for expression control \cite{varanka2024fineface, liang2024caphuman} on a dataset of 1K IDs. For each ID, we generate 5 expressive samples using target expressions shared across all methods. We report expression accuracy using metrics based on 3D reconstruction, emotion classification, Action Unit (AU), and Valence-Arousal (VA) predictions. Image quality is assessed using FID. Bold values indicate the best performance for each metric.}
    \label{tab:quant}
\end{table}

\begin{figure}[t]
  \begin{minipage}[t]{0.55\linewidth}
    \centering
    \includegraphics[width=\linewidth, trim={2.5cm 0.8cm 1.8cm 0.5cm}, clip]{figures/id_sim.pdf}
    \caption{Cosine similarity distribution between identity features of input and generated faces.}
    \label{fig:id_sim}
  \end{minipage}%
  \hfill
  \begin{minipage}[t]{0.42\linewidth}
    \centering
    \includegraphics[width=0.88\linewidth, trim={2.5cm 1.0cm 1.0cm 0.5cm}, clip]{figures/user_study.pdf}
      \caption{Users' preference on accuracy between generated/intended expressions.}
      \label{fig:user_study}
  \end{minipage}
  %\vspace{-0.3cm}
\end{figure}

\subsection{Reference-Driven Expression Generation}
We further evaluate our method in a reference-driven setting, where the objective is to modify the facial expression of a given image while preserving the subject’s appearance and background. The evaluation setup mirrors that of the stochastic generation experiment, using the same metrics and test data. However, in this case, we activate the \textbf{Reference Adapter} and its associated LoRA layers for our method, allowing our model to condition not only on the ArcFace embedding but also directly on the source image itself. We compare against recent methods designed for this task: MagicFace \cite{wei2025magicface} which incorporates Action Units along with reference image features into the Stable Diffusion pipeline and Face-Adapter \cite{han2024faceadapter} which uses 3D landmarks instead, along with the source image. Finally, we include EmoStyle \cite{EmoStyle_2024_WACV}, a GAN-based approach which manipulates StyleGAN2 \cite{karras2020analyzing} latent codes using Valence-Arousal controls via a learned expression-guidance network.

We present the results in \cref{tab:quant_ref} and \cref{fig:id_sim_ref} and provide visual comparisons in \cref{fig:comp_ref}. Our method again achieves superior results than the baselines in all aspects. While our primary design targets stochastic generation with expression and identity guidance, we show how our architecture can be seamlessly extended to reference-based editing via the proposed optional mechanism, outperforming models explicitly designed for this task.

\begin{table}[h]
    \begin{center}
    \setlength{\tabcolsep}{1.8pt}
    \scriptsize
    \begin{tabular}{lccccc} 
        \toprule
         & FID$\downarrow$ & Em.~Acc.~(\%)$\uparrow$ & AU-MSE$\downarrow$ & VA-MSE$\downarrow$ & 3D Exp-MSE$\downarrow$\\
         \midrule
         MagicFace \cite{wei2025magicface} & 0.138 & 34.08 & 0.044 & 0.166 & 1.595\\  
         EmoStyle \cite{azari2024emostyle} & 0.159 & 42.42 & 0.043 & 0.091 & 1.223\\ 
         Face-Adapter \cite{han2024faceadapter} & 0.166 & 43.85 & 0.040 & 0.093 & 0.986\\
         \textbf{Ours (w/ Ref.)} & \textbf{0.128} & \textbf{44.19} & \textbf{0.038} & \textbf{0.090} & \textbf{0.849}\\
        \bottomrule
    \end{tabular}
    \end{center}   
    \caption{\textbf{Reference-driven generation:} Comparison of our method - extended with the Reference Adapter - against approaches for expression editing in reference images \cite{wei2025magicface, EmoStyle_2024_WACV, han2024faceadapter}. The evaluation setup is identical to that in \cref{tab:quant}.  Bold values indicate the best performance for each metric.}
    \label{tab:quant_ref}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth, trim={5.0cm 0.8cm 4.0cm 1.0cm}, clip]{figures/id_sim_ref.pdf}
\caption{Comparison of cosine similarity distributions between identity features of input and generated faces for the reference-driven setting.}
\label{fig:id_sim_ref}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/comp_ref_compressed.pdf}
\caption{Visual comparison with recent methods for reference-driven expression control~\cite{wei2025magicface, han2024faceadapter, azari2024emostyle}. Our method achieves more faithful expression transfer while better preserving both identity and visual consistency with the reference image.}
\label{fig:comp_ref}
\end{figure}

\subsection{Ablation Studies}
To further validate the effectiveness of our expression representation, we conduct ablation studies comparing our method against two alternative variants. First, we replace the FLAME blendshape parameters $\mathbf{s} \in \mathbb{R}^{55}$ with deep perceptual expression features. Specifically, we use EmoNet~\cite{toisoul2021estimation}, a widely used expression recognition model, to extract perceptual features $c \in \mathbb{R}^{256}$ from its final convolutional layers, just before classification. These features are then used as input to the \textbf{Expression Adapter}, instead of the original 3DMM-based parameters. Second, we evaluate a straightforward baseline that integrates ControlNet \cite{controlnet} with the Arc2Face model \cite{paraperas2024arc2face}. For this, we use the official pre-trained ControlNet variant provided by Arc2Face, which is conditioned on normal maps derived from the FLAME 3D model. We evaluate both baselines using the same expression consistency protocol as in the main experiments. \cref{tab:ablation} shows that both alternatives fall short in accurately encoding the intended expressions, leading to a noticeable performance drop. We attribute this to the highly discriminative nature of emotion recognition networks in the first case and ControlNet’s reliance on low-dimensional geometric cues, such as normal maps, in the second case, which provide only coarse surface information about facial structure. In contrast, the use of explicit and compact blendshape parameters, while simple in design, proves to be the most efficient approach for capturing and re-generating detailed expressions.

Furthermore, we examine the importance of expressive variability in the training data. As discussed earlier, in addition to FFHQ, we incorporate AffectNet~\cite{mollahosseini2017affectnet}, which is tailored for facial expression recognition (FER), and FEED~\cite{Drobyshev_2024_CVPR}, a dataset featuring exaggerated expressions performed by actors. These datasets are crucial for capturing a broad range of facial expressions and enabling the model to generalize beyond subtle or common emotions. To demonstrate this, we retrain our model using only the high-quality but expression-limited FFHQ dataset. \cref{fig:ablation} provides examples where this model fails to accurately reproduce more nuanced reference expressions that go beyond prototypical emotions such as happiness or anger, and instead reflect micro-expressions or facial morphs. The integration of AffectNet and FEED proves essential for robust expression synthesis across the full spectrum of human affect.

\begin{table}[h]
    \begin{center}
    \setlength{\tabcolsep}{2.5pt}
    \scriptsize
    \begin{tabular}{lccccc} 
        \toprule
         & Em.~Acc.~(\%)$\uparrow$ & AU-MSE$\downarrow$ & VA-MSE$\downarrow$ & 3D Exp-MSE$\downarrow$\\
         \midrule
         Arc2Face + ControlNet & 31.92 & 0.038 & 0.154 & 1.167\\
         Ours w/ EmoNet & 44.28 & 0.037 & 0.105 & 1.192\\
         \textbf{Ours} & \textbf{46.59} & \textbf{0.032} & \textbf{0.086} & \textbf{0.696}\\
        \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-0.3cm}
    \caption{Ablation study comparing the proposed approach with two alternative variants: (1) integrating a ControlNet into Arc2Face, using normal maps for control instead of expression blendshapes; and (2) using the proposed Expression Adapter, but conditioned on deep features from a Facial Expression Recognition (FER) network rather than explicit 3DMM parameters.}
    \label{tab:ablation}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=1.0\linewidth]{figures/ablation_compressed.pdf}
\caption{Comparison of our Expression Adapter trained on the full dataset (AffectNet + FEED + FFHQ) vs.~trained only on FFHQ.}
\vspace{-0.3cm}
\label{fig:ablation}
\end{figure}