·@misc{Grinstein:2002:IVV,
  author = {Georges Grinstein and Daniel Keim and Matthew Ward},
  title = {Information Visualization, Visual Data Mining, and Its Application to Drug Design},
  howpublished = {IEEE Visualization Course \#1 Notes},
  month = {October},
  year = {2002},
	url = {http://vis.computer.org/vis2002/program/tutorials/tutorial_01_abstract.html}
}

@Article{Isenberg:2017:VMC,
  author = {Petra Isenberg and Florian Heimerl and Steffen Koch and Tobias Isenberg and Panpan Xu and Chad Stolper and Michael Sedlmair and Jian Chen and Torsten M{\"o}ller and John Stasko},
	title = {{vispubdata.org: A Metadata Collection about {IEEE} Visualization ({VIS}) Publications}},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	year = {2017},
	volume = {23},
	OPTnumber = {},
	OPTmonth = {},
	OPTpages = {},
	doi = {https://doi.org/10.1109/TVCG.2016.2615308},
	note = {To appear},
}

@mastersthesis{Kindlmann:1999:SAG,
  author = {Gordon Kindlmann},
  title = {Semi-Automatic Generation of Transfer Functions for Direct Volume Rendering},
  school = {Cornell University},
	address = {USA},
	url = {http://www.graphics.cornell.edu/pubs/1999/Kin99.html},
  year = 1999,
}

@manual{Kitware:2003,
  title = {The Visualization Toolkit User's Guide},
  organization = {Kitware, Inc.},
  year = {2003},
  month = {January},
	url = {http://www.kitware.com/publications/item/view/1269}
}

@phdthesis{Levoy:1989:DSV,
  author = {Marc Levoy},
  title = {Display of Surfaces from Volume Data},
  school = {University of North Carolina at Chapel Hill},
	address = {USA},
	url = {http://www.cs.unc.edu/techreports/89-022.pdf},
  year = {1989},
}

@article{Lorensen:1987:MCA,
 author = {Lorensen, William E. and Cline, Harvey E.},
 title = {Marching Cubes: A High Resolution {3D} Surface Construction Algorithm},
 journal = {SIGGRAPH Computer Graphics},
 volume = {21},
 number = {4},
 month = aug,
 year = {1987},
 pages = {163--169},
 doi = {10.1145/37402.37422},
}

@article{Max:1995:OMF,
 author = {Max, Nelson},
 title = {Optical Models for Direct Volume Rendering},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 volume = {1},
 number = {2},
 month = jun,
 year = {1995},
 pages = {99--108},
 doi = {10.1109/2945.468400},
} 

@inproceedings{Nielson:1991:TAD,
  author = {Gregory M. Nielson and Bernd Hamann},
  title = {The Asymptotic Decider: Removing the Ambiguity in Marching Cubes},
  year = {1991},
  booktitle = {Proc.\ Visualization},
  pages = {83--91},
  doi={10.1109/VISUAL.1991.175782},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos},
}

@book{Ware:2004:IVP,
  author = {Colin Ware},
  title = {Information Visualization: Perception for Design},
  edition = {2\textsuperscript{nd}},
  year = 2004,
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco},
	doi = {https://doi.org/10.1016/B978-155860819-1/50001-7}
}

@ARTICLE{Wyvill:1986:DSS,
  author = {Wyvill, Geoff and McPheeters, Craig and Wyvill, Brian},
  title = {Data Structure for \emph{soft} Objects},
  journal = {The Visual Computer},
  year = {1986},
  volume = {2},
  pages = {227--234},
  number = {4},
  month = aug,
  doi = {10.1007/BF01900346}
}

@inproceedings{Lui2021iText,
author = {Lu, Xueshi and Yu, Difeng and Liang, Hai-Ning and Goncalves, Jorge},
title = {iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474788},
doi = {10.1145/3472749.3474788},
abstract = {Text entry is an important and frequent task in interactive devices including augmented reality head-mounted displays (AR HMDs). In current AR HMDs, there are still two main open challenges to overcome for efficient and usable text entry: arm fatigue due to mid-air input and visual occlusion because of their small see-through displays. To address these challenges, we present iText, a technique for AR HMDs that is hands-free and is based on an imaginary (invisible) keyboard. We first show that it is feasible and practical to use an imaginary keyboard on AR HMDs. Then, we evaluated its performance and usability with three hands-free selection mechanisms: eye blinks (E-Type), dwell (D-Type), and swipe gestures (G-Type). Our results show that users could achieve an average text entry speed of 11.95, 9.03 and 9.84 words per minutes (WPM) with E-Type, D-Type, and G-Type, respectively. Given that iText with E-Type outperformed the other two selection mechanisms in text entry rate and subjective feedback, we ran a third, 5-day study. Our results show that iText with E-Type can achieve an average text entry rate of 13.76 WPM with a mean word error rate of 1.5\%. In short, iText can enable efficient eyes-free text entry and can be useful for various application scenarios in AR HMDs.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {815–825},
numpages = {11},
keywords = {typing, text entry, head-mounted displays, hands-free, eye blink, dwell, augmented reality},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{pakov2019collaborative,
author = {\v{S}pakov, Oleg and Istance, Howell and R\"{a}ih\"{a}, Kari-Jouko and Viitanen, Tiia and Siirtola, Harri},
title = {Eye gaze and head gaze in collaborative games},
year = {2019},
isbn = {9781450367097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317959.3321489},
doi = {10.1145/3317959.3321489},
abstract = {We present an investigation of sharing the focus of visual attention between two players in a collaborative game, so that where one player was looking was visible to the other. The difference between using head-gaze and eye-gaze to estimate the point of regard was studied, the motive being that recording head-gaze is easier and cheaper than eye-gaze. Two experiments are reported, the first investigates the effect of a high immersion presentation of the game in VR Head Mounted Display compared with a lower immersion desktop presentation. The second examines the high immersion condition in more detail. The studies show that in spite of there being many factors that could affect the outcome of a relatively short period of game play, sharing eye-gaze in the high immersion condition produces shorter overall durations and better subjective ratings of team work than does sharing head-gaze. This difference is not apparent in the low immersion condition. The findings are a good argument for exploiting the opportunities for including and using eye tracking within head mounted displays in the context of collaborative games.},
booktitle = {Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
articleno = {85},
numpages = {9},
keywords = {gaze sharing, gaze in teamwork, collaborative games, VR},
location = {Denver, Colorado},
series = {ETRA '19}
}

@INPROCEEDINGS{meng2022textselection,
  author={Meng, Xuanru and Xu, Wenge and Liang, Hai-Ning},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={An Exploration of Hands-free Text Selection for Virtual Reality Head-Mounted Displays}, 
  year={2022},
  volume={},
  number={},
  pages={74-81},
  doi={10.1109/ISMAR55827.2022.00021}
}

@inproceedings{Stellmach2012gazeui,
author = {Stellmach, Sophie and Dachselt, Raimund},
title = {Designing gaze-based user interfaces for steering in virtual environments},
year = {2012},
isbn = {9781450312219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2168556.2168577},
doi = {10.1145/2168556.2168577},
abstract = {Since eye gaze may serve as an efficient and natural input for steering in virtual 3D scenes, we investigate the design of eye gaze steering user interfaces (UIs) in this paper. We discuss design considerations and propose design alternatives based on two selected steering approaches differing in input condition (discrete vs. continuous) and velocity selection (constant vs. gradient-based). The proposed UIs have been iteratively advanced based on two user studies with twelve participants each. In particular, the combination of continuous and gradient-based input shows a high potential, because it allows for gradually changing the moving speed and direction depending on a user's point-of-regard. This has the advantage of reducing overshooting problems and dwell-time activations. We also investigate discrete constant input for which virtual buttons are toggled using gaze dwelling. As an alternative, we propose the Sticky Gaze Pointer as a more flexible way of discrete input.},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
pages = {131–138},
numpages = {8},
keywords = {3D interaction, gaze input, steering, traveling, virtual environments},
location = {Santa Barbara, California},
series = {ETRA '12}
}

@inproceedings{piotrowski2019gaze,
  title={Gaze-based interaction for VR environments},
  author={Piotrowski, Patryk and Nowosielski, Adam},
  booktitle={International Conference on Image Processing and Communications},
  pages={41--48},
  year={2019},
  organization={Springer}
}

@inproceedings{kang2024rayhand,
  title={The rayhand navigation: A virtual navigation method with relative position between hand and gaze-ray},
  author={Kang, Sei and Jeong, Jaejoon and Lee, Gun A and Kim, Soo-Hyung and Yang, Hyung-Jeong and Kim, Seungwon},
  booktitle={Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2024}
}

@ARTICLE{Sidenmark2023Comparing,
  author={Sidenmark, Ludwig and Prummer, Franziska and Newn, Joshua and Gellersen, Hans},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Comparing Gaze, Head and Controller Selection of Dynamically Revealed Targets in Head-Mounted Displays}, 
  year={2023},
  volume={29},
  number={11},
  pages={4740-4750},
  keywords={Mathematical models;Three-dimensional displays;Task analysis;Resists;Lenses;Performance evaluation;Visualization;Pointing;Selection Performance;Virtual Reality;3D Interaction},
  doi={10.1109/TVCG.2023.3320235}}

@article{Scott2004motor,
  author = {Scott, Stephen H.},
  title = {Optimal feedback control and the neural basis of volitional motor control},
  journal = {Nature Reviews Neuroscience},
  year = {2004},
  volume = {5},
  number = {7},
  pages = {532--545},
  doi = {10.1038/nrn1427},
  url = {https://doi.org/10.1038/nrn1427},
  issn = {1471-0048},
  abstract = {The motor system can be considered at three levels: motor behaviour, limb mechanics and neural control. Although our understanding at each level continues to grow, linking these levels into a cohesive framework is an important challenge for neuroscientists. The peripheral motor system is a complex filter that converts patterns of muscle activity into movements. This conversion depends on the properties of muscle, multi-joint mechanics with uncoupled joint motion and muscular torque, as well as environmental forces acting on the limbs. Despite these complexities, body movements are smooth and graceful. For example, hand trajectories during reaching movements are fairly straight, and hand velocity follows a smooth, bell-shaped profile. The motor system compensates for the complexities of limb mechanics, and also can adapt to perceptual or mechanical perturbations. There is considerable noise in the motor system, which is reflected in trial-to-trial variability, but task-relevant features are less variable. Motor control involves the spinal cord, brainstem and cerebral cortex. Neurophysiological studies of motor control often involve recording the activity of neurons in different brain regions and relating such activity to aspects of sensorimotor function. Such recordings indicate that neural activity in primary motor cortex (M1) correlates with hand direction, speed and distance of movement. However, theoretical studies have shown that such correlations can arise even if neurons code other details such as muscle activity or joint motion. There is also evidence that M1 behaves like or forms part of an inverse internal model that converts spatial goals or trajectories into detailed motor patterns. Optimal feedback control might provide a link between the different levels of motor control. An optimal feedback controller uses an optimal estimate of the state of the system, generated through sensory feedback and efferent copy, and uses this feedback to adjust its output towards a specific goal. 'Errors' are corrected only if they adversely affect motor performance. This control theory is consistent with a number of features of motor behaviour and of neural processing in M1. For example, the activity of M1 neurons varies depending on the behavioural context, and M1 receives a rich mix of sensory inputs. Although the mathematics to compute an optimal feedback controller are quite complex, such controllers provide an important bridge to link limb mechanics, motor behaviour and the neural basis of motor control. As a theory, optimal feedback control generates a number of neurophysiological questions on how such a controller is created by the highly distributed circuitry involved in sensorimotor function.}
}

@article{Prablanc1978saccades,
title = {Error-correcting mechanisms in large saccades},
journal = {Vision Research},
volume = {18},
number = {5},
pages = {557-560},
year = {1978},
issn = {0042-6989},
doi = {https://doi.org/10.1016/0042-6989(78)90202-X},
url = {https://www.sciencedirect.com/science/article/pii/004269897890202X},
author = {C. Prablanc and D. Massé and J.F. Echallier},
abstract = {The initial saccade toward a very peripheral stimulus is generally hypometric and necessitates a corrective saccade. In a first experiment the stimulus was cut off at the onset of the initial saccade. If the hypometry was important a secondary saccade occurred reducing the error, but not truly corrective. If the hypometry was normal (10%), almost no secondary saccade occurred. In a second experiment the stimulus was cut of during the deceleration phase of the initial saccade. Secondary saccades (which were truly corrective) occurred very often, provided the velocity of the eye at the time of cut off was inferior to about 100°/s. It is suggested that extraretinal signals for error detection are not very sensitive, and that in normal conditions reafferent visual feedback can be taken into account before the end of the saccade, to generate a secondary saccade encoded in absolute position, though the information is still available in retinal position.
Résumé
]L'hypométrie générale d'une saccade en direction d'un stimulus très périphérique rend nécessaire l'exécution d'une saccade de correction. Dans une première expérience onéteint le stimulus au départ de la saccade. Dans le cas d'une hypométrie importante on voit apparaître une saccade secondaire, qui bien que réduisant l'erreur, ne la corrige pas totalement. Lorsque l'hypométrie est normale (10%) on n'observe pratiquement pas de saccade secondaire. Dans la deuxième expérience, onéteint le stimulus durant la phase de décélération de la saccade initiale. De véritables saccades de correction apparaissent alors très fréquemment,àcondition que la vitesse de l'oeilàl'instant de l'extinction n'excède pas environ 100°/s. Les signaux extra-rétiniens responsables de la détection de l'erreur saccadique semblent relativement peu précis. Concernant les saccades effectuées dans des conditions normales, la reprise d'information visuelle se fait avant la fin de la saccade et génère une saccade secondaire codée en position absolue, bien que l'information soit disponible en position rétinienne.}
}

@book{Kenneth2011eyemovement,
title = "Eye Tracking : A Comprehensive Guide to Methods and Measures",
abstract = "This book is written by and for researchers who are still in that part of their careers where they are actively using the eye-tracker as a tool; those who have to deal with the technology, the signals, the filters, the algorithms, the experimental design, the programming of stimulus presentation, instructions to participants, working the varying tools for data analysis, and of course, worrying about all the different things that must not go wrong! A central theme of the book concerns the wide range of fields eye tracking covers. Suppose an educational psychologist wishes to use eye tracking to evaluate a new software pack- age designed to support learning to read. She may have an excellent idea as a starting point, and some understanding of the kind of results eye tracking could provide to tackle her re- search question, but unless she and the group around her are also adept in computer science, it is unlikely she will know how the eye movement data she collects is generated: How raw data samples are converted into fixations and saccades using event detection algorithms, how the different representations of eye movement data are calculated, and how all the measures of eye movements relate to these processes. All this is important because subtleties involved in working with eye-tracking data can have large consequences for the final results, and thus whether our educational psychologist can confidently conclude that her software package is effective or not in supporting the development of reading skills. This is not to say that hard-core computer science skills are the crux of good eye-tracking research, for this is certainly not the case. One can equally envisage a situation where an expert in programming and the manipulation of data plans and executes an eye-tracking study poorly, simply because she is not trained in the principles of experimental design, and the associated literature on the visual system and oculomotor control. There are many contrasts between the diverging schools of thought which use eye track- ing; practices and preferences vary, but certainly experts in different fields do not draw on each other{\textquoteright}s strengths enough. We felt there was a need to pinpoint the relative merits of adopting methods based in one field alone, whilst highlighting that the lack of synergy be- tween different disciplines can lead to sub-optimal research practices, and new advancements being overlooked. Besides technical details and theory, however, the heart of this book revolves around practicality. At the Humanities Laboratory at Lund University we have been teaching eye- tracking methodology regularly since 2000. We commonly see newcomers to the technique run aground when encountering just the sort of issues raised above, but beginners struggle with problems which are even more practical in nature. Hands-on advice for how to actually use eye-trackers is very limited. Setting up the eye camera and performing a good calibration routine is just as important as the design of the study and how data is handled, for if the recording is poor your options are limited from the outset. There are fundamental methodological skills which underpin using eye-trackers, but at the other end of the spectrum there is also the vast choice of measures available to the eye-tracking researcher. For the present text to be complete, therefore, we felt a require- ment should also be to draw together eye-tracking measures, as well as methods, into an understandable structure. So, starting around 2005, we began producing a taxonomy of all eye-movement methods and measures used by researchers, examining how the measures are related to each other, what type of data quality they rely on, and previous data processing they require. Our classification work thus consisted of searching the method sections from thousands of journal papers, book chapters, PhD theses and conference proceedings. Every measure and method we found was catalogued and put into a growing system. Some of the measures were extremely elusive, as they are known by different names, not only between research fields, but even within, and often the precise implementations are missing in the WHY WE WROTE THIS BOOK | v vi | WHY WE WROTE THIS BOOK published texts. At first, we were very unclear how to classify measures. Some varieties of taxonomic structures that we rejected can be found on p. 463. We ended up with a classifica- tion structure where the operational definitions are at the centre. Users of eye-trackers often lack proficient training because there is little or no teaching community to rely on. As a result people are often self taught, or depend on second-hand knowledge which may be out of date or even incorrect. When they participate in our eye- tracking methodology courses, we find that many new users are very focused on their re- search questions, but are surprised how much time they need to invest in order to master eye tracking properly. Often people attending have just purchased an eye-tracker to compliment their research, or for use in their company to tackle ergonomic and marketing-related ques- tions. Our aim for this book is to make learning to use eye-trackers a much easier process for these readers. If you have a solid background in experimental psychology, computer sci- ence, or mathematics you will often find it straightforward to embrace the technologies and workflows surrounding eye tracking. But whatever your background, you should be able to achieve the same level of knowledge and understanding from this book as you would from training on eye tracking in-house in a fully competent laboratory. More specifically, this book has been written to be a support when: 1. Evaluating or acquiring a commercial eye-tracker, 2. Planning an experiment where eye tracking is used as a tool, 3. About to record eye-movement data, 4. Planning how to process and interpret the recorded data, before carrying out statistical tests on it, 5. Reading or reviewing eye-movement research. In our efforts to classify eye-tracking methods and measures, combined with useful prac- tical hints and tips, we hope to provide the reader with the first comprehensive textbook on methodology for new users of eye tracking, but which also caters for the advanced researcher. Previous versions of this book have been used in eye-tracking education in Lund. Also, col- leagues of ours in Potsdam, T{\"u}bingen, and Helsinki have used earlier manuscripts of the book when teaching and training masters and PhD level students in eye tracking. Lastly, although not the target audience, manufacturers have already shown a great interest in the book at the manuscript stage, which we hope may lead to even better eye-trackers in the future.",
author = "Kenneth Holmqvist and Marcus Nystr{\"o}m and Richard Andersson and Richard Dewhurst and Jarodzka Halszka and {van de Weijer}, Joost",
year = "2011",
language = "English",
isbn = "9780199697083",
publisher = "Oxford University Press",
address = "United Kingdom",
}


@inproceedings{Zhang2010dwell-based,
author = {Zhang, Xinyong and Ren, Xiangshi and Zha, Hongbin},
title = {Modeling dwell-based eye pointing target acquisition},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753645},
doi = {10.1145/1753326.1753645},
abstract = {We propose a quantitative model for dwell-based eye pointing tasks. Using the concepts of information theory to analogize eye pointing, we define an index of difficulty (IDeye) for the corresponding tasks in a similar manner to the definition that Fitts made for hand pointing. According to our validations in different situations, IDeye, which takes account of the distinct characteristics of rapid saccades and involuntary eye jitters, can accurately and meaningfully describe eye pointing tasks. To the best of our knowledge, this work is the first successful attempt to model eye gaze interactions.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2083–2092},
numpages = {10},
keywords = {modeling, information theory, fitts' law, eye pointing},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}


@inproceedings{xuning2025CHILBW,
author = {Hu, Xuning and Zhang, Yichuan and Wei, Yushi and Li, Yue and Stuerzlinger, Wolfgang and Liang, Hai-Ning},
title = {Exploring and Modeling Gaze-Based Steering Behavior in Virtual Reality},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720273},
doi = {10.1145/3706599.3720273},
abstract = {Gaze-based interaction is a common input method in virtual reality (VR). Eye movements, such as fixations and saccades, result in different behaviors compared to other input methods. Previous studies on selection tasks showed that, unlike the mouse, the human gaze is insensitive to target distance and does not fully utilize target width due to the characteristics of saccades and micro-saccades of the eyes. However, its application in steering tasks remains unexplored. Since steering tasks are widely used in VR for menu adjustments and object manipulation, this study examines whether the findings from selection tasks apply to steering tasks. We also model and compare the Steering Law based on eye movement characteristics. To do this, we use data on movement time, average speed, and re-entry count. Our analysis investigates the impact of path width and length on performance. This work proposes three candidate models that incorporate gaze characteristics, which achieve a superior fit (R² > 0.964) compared to the original Steering Law, improving the accuracy of time prediction, AIC, and BIC by 7\%, 26\%, and 10\%, respectively. These models offer valuable insights for game and interface designers who implement gaze-based controls in VR environments.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {260},
numpages = {8},
keywords = {Gaze Input, Steering Law, Virtual Reality, Modeling},
location = {
},
series = {CHI EA '25}
}

@inproceedings{accot1997beyond,
  title={Beyond Fitts' law: models for trajectory-based HCI tasks},
  author={Accot, Johnny and Zhai, Shumin},
  booktitle={Proceedings of the ACM SIGCHI Conference on Human factors in computing systems},
  pages={295--302},
  year={1997}
}


@INPROCEEDINGS{Xuning2024Spatial,
  author={Hu, Xuning and Yan, Xinan and Wei, Yushi and Xu, Wenxuan and Li, Yue and Liu, Yue and Liang, Hai-Ning},
  booktitle={2024 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploring the Effects of Spatial Constraints and Curvature for 3D Piloting in Virtual Environments}, 
  year={2024},
  volume={},
  number={},
  pages={505-514},
  keywords={Solid modeling;Three-dimensional displays;Virtual environments;Games;Predictive models;Aircraft navigation;Trajectory;Complexity theory;Aircraft;Aerospace control;Virtual Reality;Game Controller;Steering Law;Graphical User Interfaces;Modeling;Navigation},
  doi={10.1109/ISMAR62088.2024.00065}}

@INPROCEEDINGS{Xuning2025Selection,
  author={Hu, Xuning and Xu, Wenxuan and Wei, Yushi and Zhang, Hao and Huang, Jin and Liang, Hai-Ning},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
  title={Optimizing Moving Target Selection in VR by Integrating Proximity-Based Feedback Types and Modalities}, 
  year={2025},
  volume={},
  number={},
  pages={52-62},
  keywords={Visualization;Three-dimensional displays;Error analysis;Entertainment industry;Virtual reality;User interfaces;User experience;Real-time systems;Haptic interfaces;Virtual Reality;Moving target selection;Multi-modal interaction and perception;Feedback Mechanism},
  doi={10.1109/VR59515.2025.00030}}

@article{Rongkai2024Selection,
author = {Shi, Rongkai and Wei, Yushi and Hu, Xuning and Liu, Yu and Yue, Yong and Yu, Lingyun and Liang, Hai-Ning},
title = {Experimental Analysis of Freehand Multi-object Selection Techniques in Virtual Reality Head-Mounted Displays},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {ISS},
url = {https://doi.org/10.1145/3698129},
doi = {10.1145/3698129},
abstract = {Object selection is essential in virtual reality (VR) head-mounted displays (HMDs). Prior work mainly focuses on enhancing and evaluating techniques for selecting a single object in VR, leaving a gap in the techniques for multi-object selection, a more complex but common selection scenario. To enable multi-object selection, the interaction technique should support group selection in addition to the default pointing selection mode for acquiring a single target. This composite interaction could be particularly challenging when using freehand gestural input. In this work, we present an empirical comparison of six freehand techniques, which are comprised of three mode-switching gestures (Finger Segment, Multi-Finger, and Wrist Orientation) and two group selection techniques (Cone-casting Selection and Crossing Selection) derived from prior work. Our results demonstrate the performance, user experience, and preference of each technique. The findings derive three design implications that can guide the design of freehand techniques for multi-object selection in VR HMDs.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {529},
numpages = {19},
keywords = {Freehand Interaction, Gestural Input, Head-Mounted Display, Mid-Air Interaction, Multi-object Selection, Object Selection, Target Acquisition, Virtual Reality}
}
@article{zavichi2025gaze,
  title={Gaze--Hand Steering for Travel and Multitasking in Virtual Environments},
  author={Zavichi, Mona and Santos, Andr{\'e} and Moreira, Catarina and Maciel, Anderson and Jorge, Joaquim},
  journal={Multimodal Technologies and Interaction},
  volume={9},
  number={6},
  pages={61},
  year={2025},
  publisher={MDPI}
}

@inproceedings{huang2020modeling,
  title={Modeling the endpoint uncertainty in crossing-based moving target selection},
  author={Huang, Jin and Tian, Feng and Fan, Xiangmin and Tu, Huawei and Zhang, Hao and Peng, Xiaolan and Wang, Hongan},
  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2020}
}

@inproceedings{pfeuffer2017gaze+,
  title={Gaze+ pinch interaction in virtual reality},
  author={Pfeuffer, Ken and Mayer, Benedikt and Mardanbegi, Diako and Gellersen, Hans},
  booktitle={Proceedings of the 5th symposium on spatial user interaction},
  pages={99--108},
  year={2017}
}

@inproceedings{wei2023predicting,
  title={Predicting gaze-based target selection in augmented reality headsets based on eye and head endpoint distributions},
  author={Wei, Yushi and Shi, Rongkai and Yu, Difeng and Wang, Yihong and Li, Yue and Yu, Lingyun and Liang, Hai-Ning},
  booktitle={Proceedings of the 2023 CHI conference on human factors in computing systems},
  pages={1--14},
  year={2023}
}


@inproceedings{Yamanaka2016Narrowing,
author = {Yamanaka, Shota and Miyashita, Homei},
title = {Modeling the Steering Time Difference between Narrowing and Widening Tunnels},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858037},
doi = {10.1145/2858036.2858037},
abstract = {The performance of trajectory-based tasks is modeled by the steering law, which predicts the required time from the index of difficulty (ID). This paper focuses on the fact that the time required to pass through a straight path with linearly-varying width alters depending on the direction of the movement. In this study, an expression for the relationship between the ID of narrowing and widening paths has been developed. This expression can be used to predict the movement time needed to pass through in the opposite direction from only a few data points, after measuring the time needed in the other direction. In the experiment, the times for five IDs were predicted with high precision from the measured time for one ID, thereby illustrating the effectiveness of the proposed method.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1846–1856},
numpages = {11},
keywords = {steering law, pointing, motor control, modeling, human performance, graphical user interfaces},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{Abeysinghe2025QuestProRefreshRate,
author = {Abeysinghe, Yasasi and Cauchi, Kevin and Ashok, Vikas and Jayarathna, Sampath},
title = {Framework for Measuring Visual Attention in Gaze-Driven VR Learning Environments Using Meta Quest Pro},
year = {2025},
isbn = {9798400714870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715669.3726795},
doi = {10.1145/3715669.3726795},
abstract = {The growing availability of consumer-grade devices equipped with eye-tracking optics, including Augmented/virtual reality (AR/VR) headsets, has brought eye-tracking technology into wider use than ever before. Understanding the focus and visual scanning behavior of users can help optimize users’ engagement in immersive environments. In this study, we present a framework for measuring visual attention in a gaze-driven VR learning environment using a consumer-grade Meta Quest Pro VR headset. This system generates and presents basic and advanced eye-tracking measures such as fixation duration, saccade amplitude, and ambient/focal attention coefficient ( mathcal {K} ) as indicators of visual attention in the VR environment.},
booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications},
articleno = {54},
numpages = {3},
keywords = {Eye Tracking, Virtual Reality, Visual Attention, Advanced Gaze Measures, Meta Quest Pro},
location = {
},
series = {ETRA '25}
}

@inproceedings{Kim2025CHIGazeSemiPinch,
author = {Kim, Jinwook and Park, Sangmin and Zhou, Qiushi and Gonzalez-Franco, Mar and Lee, Jeongmi and Pfeuffer, Ken},
title = {PinchCatcher: Enabling Multi-selection for Gaze+Pinch},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713530},
doi = {10.1145/3706598.3713530},
abstract = {This paper investigates multi-selection in XR interfaces based on eye and hand interaction. We propose enabling multi-selection using different variations of techniques that combine gaze with a semi-pinch gesture, allowing users to select multiple objects, while on the way to a full-pinch. While our exploration is based on the semi-pinch mode for activating a quasi-mode, we explore four methods for confirming subselections in multi-selection mode, varying in effort and complexity: dwell-time (SemiDwell), swipe (SemiSwipe), tilt (SemiTilt), and non-dominant hand input (SemiNDH), and compare them to a baseline technique. In the user study, we evaluate their effectiveness in reducing task completion time, errors, and effort. The results indicate the strengths and weaknesses of each technique, with SemiSwipe and SemiDwell as the most preferred methods by participants. We also demonstrate their utility in file managing and RTS gaming application scenarios. This study provides valuable insights to advance 3D input systems in XR.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {853},
numpages = {16},
keywords = {Extended Reality, Selection, Grouping, Gaze, Gestures, Eye-Hand interaction},
location = {
},
series = {CHI '25}
}

@inproceedings{Xueshi2021UISTHandsfree,
author = {Lu, Xueshi and Yu, Difeng and Liang, Hai-Ning and Goncalves, Jorge},
title = {iText: Hands-free Text Entry on an Imaginary Keyboard for Augmented Reality Systems},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474788},
doi = {10.1145/3472749.3474788},
abstract = {Text entry is an important and frequent task in interactive devices including augmented reality head-mounted displays (AR HMDs). In current AR HMDs, there are still two main open challenges to overcome for efficient and usable text entry: arm fatigue due to mid-air input and visual occlusion because of their small see-through displays. To address these challenges, we present iText, a technique for AR HMDs that is hands-free and is based on an imaginary (invisible) keyboard. We first show that it is feasible and practical to use an imaginary keyboard on AR HMDs. Then, we evaluated its performance and usability with three hands-free selection mechanisms: eye blinks (E-Type), dwell (D-Type), and swipe gestures (G-Type). Our results show that users could achieve an average text entry speed of 11.95, 9.03 and 9.84 words per minutes (WPM) with E-Type, D-Type, and G-Type, respectively. Given that iText with E-Type outperformed the other two selection mechanisms in text entry rate and subjective feedback, we ran a third, 5-day study. Our results show that iText with E-Type can achieve an average text entry rate of 13.76 WPM with a mean word error rate of 1.5\%. In short, iText can enable efficient eyes-free text entry and can be useful for various application scenarios in AR HMDs.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {815–825},
numpages = {11},
keywords = {augmented reality, dwell, eye blink, hands-free, head-mounted displays, text entry, typing},
location = {Virtual Event, USA},
series = {UIST '21}
}

@INPROCEEDINGS{Xueshi2020IsmarHandsfree,
  author={Lu, Xueshi and Yu, Difeng and Liang, Hai-Ning and Xu, Wenge and Chen, Yuzheng and Li, Xiang and Hasan, Khalad},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Exploration of Hands-free Text Entry Techniques For Virtual Reality}, 
  year={2020},
  volume={},
  number={},
  pages={344-349},
  keywords={Performance evaluation;Head-mounted displays;Resists;Gaze tracking;Indexes;Augmented reality;Virtual reality;Text Entry;Dwelling;Eye Blinking;NeckType;Head-Mounted Display;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Text input},
  doi={10.1109/ISMAR50242.2020.00061}}

@Inbook{Majaranta2014Tracking,
author="Majaranta, P{\"a}ivi
and Bulling, Andreas",
title="Eye Tracking and Eye-Based Human--Computer Interaction",
bookTitle="Advances in Physiological Computing",
year="2014",
publisher="Springer London",
address="London",
pages="39--65",
abstract="Eye tracking has a long history in medical and psychological research as a tool for recording and studying human visual behavior. Real-time gaze-based text entry can also be a powerful means of communication and control for people with physical disabilities. Following recent technological advances and the advent of affordable eye trackers, there is a growing interest in pervasive attention-aware systems and interfaces that have the potential to revolutionize mainstream human-technology interaction. In this chapter, we provide an introduction to the state-of-the art in eye tracking technology and gaze estimation. We discuss challenges involved in using a perceptual organ, the eye, as an input modality. Examples of real life applications are reviewed, together with design solutions derived from research results. We also discuss how to match the user requirements and key features of different eye tracking systems to find the best system for each task and application.",
isbn="978-1-4471-6392-3",
doi="10.1007/978-1-4471-6392-3_3",
url="https://doi.org/10.1007/978-1-4471-6392-3_3"
}

@inproceedings{SibertCHI00GazeSelection,
author = {Sibert, Linda E. and Jacob, Robert J. K.},
title = {Evaluation of eye gaze interaction},
year = {2000},
isbn = {1581132166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/332040.332445},
doi = {10.1145/332040.332445},
abstract = {Eye gaze interaction can provide a convenient and natural addition to user-computer dialogues. We have previously reported on our interaction techniques using eye gaze [10]. While our techniques seemed useful in demonstration, we now investigate their strengths and weaknesses in a controlled setting. In this paper, we present two experiments that compare an interaction technique we developed for object selection based on a where a person is looking with the most commonly used selection method using a mouse. We find that our eye gaze interaction technique is faster than selection with a mouse. The results show that our algorithm, which makes use of knowledge about how the eyes behave, preserves the natural quickness of the eye. Eye gaze interaction is a reasonable addition to computer interaction and is convenient in situations where it is important to use the hands for other tasks. It is particularly beneficial for the larger screen workspaces and virtual environments of the future, and it will become increasingly practical as eye tracker technology matures.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {281–288},
numpages = {8},
keywords = {eye movements, eye tracking, interaction techniques, user interfaces},
location = {The Hague, The Netherlands},
series = {CHI '00}
}

@article{isokoski2009gazeGame,
  title={Gaze controlled games},
  author={Isokoski, Poika and Joos, Markus and Spakov, Oleg and Martin, Beno{\^\i}t},
  journal={Universal Access in the Information Society},
  volume={8},
  number={4},
  pages={323--337},
  year={2009},
  publisher={Springer}
}

@inproceedings{Kurauchi2016GazeTextEntry,
author = {Kurauchi, Andrew and Feng, Wenxin and Joshi, Ajjen and Morimoto, Carlos and Betke, Margrit},
title = {EyeSwipe: Dwell-free Text Entry Using Gaze Paths},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858335},
doi = {10.1145/2858036.2858335},
abstract = {Text entry using gaze-based interaction is a vital communication tool for people with motor impairments. Most solutions require the user to fixate on a key for a given dwell time to select it, thus limiting the typing speed. In this paper we introduce EyeSwipe, a dwell-time-free gaze-typing method. With EyeSwipe, the user gaze-types the first and last characters of a word using the novel selection mechanism "reverse crossing." To gaze-type the characters in the middle of the word, the user only needs to glance at the vicinity of the respective keys. We compared the performance of EyeSwipe with that of a dwell-time-based virtual keyboard. EyeSwipe afforded statistically significantly higher typing rates and more comfortable interaction in experiments with ten participants who reached 11.7 words per minute (wpm) after 30 min typing with EyeSwipe.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1952–1956},
numpages = {5},
keywords = {text entry, target selection, eye typing, eye tracking, dwell-free typing},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{HedeshyCHI21GazeHumTextEntry,
author = {Hedeshy, Ramin and Kumar, Chandan and Menges, Raphael and Staab, Steffen},
title = {Hummer: Text Entry by Gaze and Hum},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445501},
doi = {10.1145/3411764.3445501},
abstract = {Text entry by gaze is a useful means of hands-free interaction that is applicable in settings where dictation suffers from poor voice recognition or where spoken words and sentences jeopardize privacy or confidentiality. However, text entry by gaze still shows inferior performance and it quickly exhausts its users. We introduce text entry by gaze and hum as a novel hands-free text entry. We review related literature to converge to word-level text entry by analysis of gaze paths that are temporally constrained by humming. We develop and evaluate two design choices: “HumHum” and “Hummer.” The first method requires short hums to indicate the start and end of a word. The second method interprets one continuous humming as an indication of the start and end of a word. In an experiment with 12 participants, Hummer achieved a commendable text entry rate of 20.45 words per minute, and outperformed HumHum and the gaze-only method EyeSwipe in both quantitative and qualitative measures.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {741},
numpages = {11},
keywords = {swipe, humming, hands-free interaction, eye typing, eye tracking},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{Difeng2019TOGSelection,
author = {Yu, Difeng and Liang, Hai-Ning and Lu, Xueshi and Fan, Kaixuan and Ens, Barrett},
title = {Modeling endpoint distribution of pointing selection tasks in virtual reality environments},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356544},
doi = {10.1145/3355089.3356544},
abstract = {Understanding the endpoint distribution of pointing selection tasks can reveal the underlying patterns on how users tend to acquire a target, which is one of the most essential and pervasive tasks in interactive systems. It could further aid designers to create new graphical user interfaces and interaction techniques that are optimized for accuracy, efficiency, and ease of use. Previous research has explored the modeling of endpoint distribution outside of virtual reality (VR) systems that have shown to be useful in predicting selection accuracy and guide the design of new interactive techniques. This work aims at developing an endpoint distribution of selection tasks for VR systems which has resulted in EDModel, a novel model that can be used to predict endpoint distribution of pointing selection tasks in VR environments. The development of EDModel is based on two users studies that have explored how factors such as target size, movement amplitude, and target depth affect the endpoint distribution. The model is built from the collected data and its generalizability is subsequently tested in complex scenarios with more relaxed conditions. Three applications of EDModel inspired by previous research are evaluated to show the broad applicability and usefulness of the model: correcting the bias in Fitts's law, predicting selection accuracy, and enhancing pointing selection techniques. Overall, EDModel can achieve high prediction accuracy and can be adapted to different types of applications in VR.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {218},
numpages = {13},
keywords = {Fitts's law, endpoint distribution, error prediction, selection modeling, target selection}
}

@article{HuTVCG2025GazeSteering,
  author    = {Xuning Hu and Yichuan Zhang and Yushi Wei and Liangyuting Zhang and Yue Li and Wolfgang Stuerzlinger and Hai{-}Ning Liang},
  title     = {Exploring and Modeling the Effects of Eye-Tracking Accuracy and Precision on Gaze-Based Steering in Virtual Environments},
  journal   = {IEEE Transactions on Visualization and Computer Graphics},
  note      = {Proceedings of IEEE International Symposium on Mixed and Augmented Reality (ISMAR 2025)}, 
  year      = {2025}
}

@inproceedings{HuaweiCHI2021Crossing,
author = {Tu, Huawei and Huang, Jin and Liang, Hai-Ning and Skarbez, Richard and Tian, Feng and Duh, Henry Been-Lirn},
title = {Distractor Effects on Crossing-Based Interaction},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445340},
doi = {10.1145/3411764.3445340},
abstract = {Task-irrelevant distractors affect visuo-motor control for target acquisition and studying such effects has already received much attention in human-computer interaction. However, there has been little research into distractor effects on crossing-based interaction. We thus conducted an empirical study on pen-based interfaces to investigate six crossing tasks with distractor interference in comparison to two tasks without it. The six distractor-related tasks differed in movement precision constraint (directional/amplitude), target size, target distance, distractor location and target-distractor spacing. We also developed and experimentally validated six quantitative models for the six tasks. Our results show that crossing targets with distractors had longer average times and similar accuracy than that without distractors. The effects of distractors varied depending on distractor location, target-distractor spacing and movement precision constraint. When spacing is smaller than 11.27 mm, crossing tasks with distractor interference can be regarded as pointing tasks or a combination of pointing and crossing tasks, which could be better fitted with our proposed models than Fitts’ law. According to these results, we provide practical implications to crossing-based user interface design.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {192},
numpages = {13},
keywords = {pointing, models., distractor effects, Fitts’ law, Crossing},
location = {Yokohama, Japan},
series = {CHI '21}
}

@ARTICLE{Yushi25TVCGSteeringLatency,
  author={Wei, Yushi and Shi, Rongkai and Batmaz, Anil Ufuk and Li, Yue and Huang, Mengjie and Yang, Rui and Liang, Hai-Ning},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Evaluating and Modeling the Effect of Frame Rate on Steering Performance in Virtual Reality}, 
  year={2025},
  volume={31},
  number={9},
  pages={5447-5461},
  keywords={Task analysis;Predictive models;Solid modeling;Three-dimensional displays;Computational modeling;Behavioral sciences;Adaptation models;Virtual reality;human performance modeling;steering law;frame rate;head-mounted display},
  doi={10.1109/TVCG.2024.3451491}}

@ARTICLE{Yushi24TVCGSteeringDirection,
  author={Wei, Yushi and Xu, Kemu and Li, Yue and Yu, Lingyun and Liang, Hai-Ning},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Exploring and Modeling Directional Effects on Steering Behavior in Virtual Reality}, 
  year={2024},
  volume={30},
  number={11},
  pages={7107-7117},
  keywords={Three-dimensional displays;Solid modeling;Predictive models;Computational modeling;Mathematical models;Performance evaluation;Muscles;Virtual reality;human performance modeling;steering law;barehand interaction;head-mounted display},
  doi={10.1109/TVCG.2024.3456166}}
