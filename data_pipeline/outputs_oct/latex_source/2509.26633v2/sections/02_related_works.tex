\section{Related Works}
\subsection{Motion Retargeting}\label{sec:motion_retargeting}
In computer graphics, transferring motions across characters has been extensively explored. Researchers have employed optimization-based methods to retarget human motions to avatars by preserving distances and orientations between keypoints \cite{cheynel2023sparse}, minimizing deformation energy \cite{Ho2010Spatial, kim2016retargeting}, or scaling the motions to satisfy hard constraints \cite{gleicher1998retargetting}. Others leverage data-driven methods that map diverse skeletons to a canonical representation \cite{aberman2020skeleton}, solve inverse kinematics with neural networks \cite{villegas2018neural}, or use reinforcement learning to preserve an interaction graph \cite{zhang2023simulation}.

% Retargeting motions for physical humanoid robots introduces challenges beyond character animation, most notably the need to enforce physical constraints. A direct adaptation of a graphics method PHC \cite{Luo2023PerpetualHC}, widely used in RL training \cite{he2025asap, chen2025gmt, he2024omnih2o}, which uses a common approach of, keypoint matching, minimizes key body distances to human demonstrators but could violate physical constraints. solves an unconstrained optimization, yielding motions with interpenetration, foot skating, and no awareness of objects or environments. 
Retargeting motions to humanoid robots introduces challenges beyond character animation, particularly the need to enforce physical constraints. For example, PHC \cite{Luo2023PerpetualHC}, a graphics method adopted in robotics~\cite{he2025asap, he2024omnih2o}, uses keypoint matching with unconstrained optimization, often leading to penetration, foot skating, and lack of object or scene awareness.
Similarly, GMR~\cite{ze2025twist} extends keypoint matching to orientations but suffer the same issues. 
VideoMimic~\cite{videomimic} improves realism with soft contact and collision penalties but offers no guarantees and requires careful tuning.

The closest method to ours is Interaction Mesh based Motion Adaptation (IMMA) \cite{Nakaoka2012Interaction}, which also leverages an interaction mesh \cite{Ho2010Spatial} to preserve the spatial relationship between body parts. However, it is not open-sourced and ignores kinematic limits and interactions with the environment or manipulated objects. 
In contrast, \OmniRetarget unifies all hard constraints, including foot sticking, non-penetration, and joint and velocity limits, while explicitly preserving environment and object interactions.

\subsection{Learning-Based Humanoid Whole-Body Control}
% margolis2023walk xie2018feedback
% \guanya{\begin{itemize}
%     \item Humanoid WBC learning is promising
%     \item Motion tracking methods have advanced, levearing data... (Fousing on flat terrian)
%     \item Few work extent do interaction setting
%     \item In summary, all these methods lack XXX
% \end{itemize}}

Recent learning-based whole-body control has enabled humanoids to traverse dynamic scenes and manipulate objects~\cite{dao2024sim, long2024learning, he2025attention, he2025learning, kuang2025skillblender, zhang2025unleashing, xue2025unified, zhang2406wococo, zhang2025falcon}. These methods typically train with hand-crafted rewards or task interfaces (e.g., velocity tracking, contact schedules, end-effector targets) but depend on extensive reward engineering and mostly fail to yield natural, human-level motions.
% These interfaces enable a wide range of behaviors, including getting up from the ground, stepping stones, predefined locomotion gaits with upper body movements, scene traversal, and object manipulation. Yet, these methods typically require extensive reward engineering and still struggle to produce natural, expressive motions that achieve human-level agility and behavior. 

Motion imitation offers a promising alternative. In graphics, DeepMimic~\cite{peng2018deepmimic} shows that using human references yields natural, human-like behaviors with agile, dynamic motions. 
However, applying this approach to humanoid robots remains difficult due to the lack of reliable open-source kinematic retargeting pipelines.
With suboptimal reference motions, practitioners are forced to either manually clean the data~\cite{zhang2025hub} or re-introduce extensive reward engineering, such as ad-hoc regularizers for contact, slipping, and air time, to compensate for artifacts~\cite{ze2025twist, he2025asap, li2025reinforcement}. In contrast, trackers with minimal reward formulation like BeyondMimic~\cite{liao2025beyondmimic} achieve state-of-the-art results on hardware with high-fidelity references~\cite{unitree_lafan1_retargeting_dataset}, but those are scarce and robot-only, without interactions.

Beyond single-character motion, human–scene interaction data has proven effective for terrain traversal and loco-manipulation in character animation~\cite{xu2025parc, wu2024human}, but translating this to robotics remains challenging. 
VideoMimic~\cite{videomimic} applies this idea to human–terrain traversal by reconstructing motions and terrains from video, but suffers from artifacts and is limited to static–scene interactions. To bridge this gap, \OmniRetarget enables natural, agile robot-object-scene interactions with high-quality reference from retargeting without manual post-processing or reward engineering.
% Beyond single-character motion, human–scene interaction data has also proven effective for terrain traversal and loco-manipulation in character animation~\cite{xu2025parc, wu2024human}.  
% However, translating this success to robotics has proven challenging.
% One of the prior works, VideoMimic~\cite{videomimic}, applies this approach to human-terrain traversal by reconstructing motions and terrains from video, but suffers from limited quality and diversity due to reconstruction and retargeting artifacts. Furthermore, their approach only deal with static environment and does not address human-object interactions. 
% Bridging this gap, \OmniRetarget enables natural and agile robot-scene interactions by tracking high-quality reference motions without manual post-processing or reward engineering.

\subsection{Data Generation for Humanoid Loco-Manipulation}
% The demand for large-scale data to train versatile whole-body loco-manipulation policies has motivated a variety of data generation strategies. 
The demand for whole-body interaction data has motivated many prior works on data generation.
One approach is direct human teleoperation \cite{seo2023deep, fu2024humanplus, he2024omnih2o, ze2025twist, ben2025homie}. While it provides online feedback, teleoperation is difficult to scale: it's labor-intensive, prone to operator fatigue, and limited by the embodiment gap between human and robot kinematics. The lack of rich haptic feedback and difficulty stabilizing extreme motions (e.g., deep squats) further constrain its applicability. To address these scaling challenges, automated data augmentation has been explored, particularly for robotic manipulation. Many works leverage state-of-the-art generative models for visual \cite{zhang2024diffusion, tian2024view, chen2024rovi} and semantic \cite{mandi2022cacti, chen2023genaug, yu2023scaling} augmentations, while others rely on simple open-loop kinematic replay of base trajectories  \cite{mandlekar2023mimicgen, jiang2024dexmimicgen, garrett2024skillmimicgen} or trajectory optimization \cite{yang2025physics} in simulation. 
Despite the advances in manipulation, data augmentation for whole-body loco-manipulation remains largely unexplored. 
The closest prior work~\cite{starke2019neural} interpolates keypoints to augment objects of different shape, but it cannot deal with varied object poses either. 
\OmniRetarget directly addresses this gap.


% \vspace{-0.05cm}