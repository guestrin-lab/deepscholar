\section{Introduction}
The quest to enable humanoid robots to perform complex whole-body scene- and object-interaction tasks has long been constrained by a fundamental data bottleneck. While deep reinforcement learning (RL) has shown remarkable success in robot control, efficient exploration is highly sensitive to reward engineering~\cite{lee2020learning}.
This challenge is further amplified on humanoids, whose high-dimensional action spaces and complex dynamics make learning natural, expressive behaviors from scratch both difficult and inefficient.
% \karen{It is true that RL is sample inefficient (btw I prefer "sample inefficient" over "data-hungry" becuase RL technically does not require any label data), but our solution, better retargetting and data augmentation, is not directly aiming for solving sample inenfficiency issue. Instead, we are solving the challenging of reward tuning by giving a good reference motion to track.} \lujie{I think better reference motion also helps with the sampling problem since for every reset, the algorithm now can sample a better state that more likely leads to task success.} \ak{Lujie that's great, in that case do you have a curve that demonstrates this? I do see Karen's point too, bc the first paragraph isnt hinting the issue of data quality. You could hint this smthlike "fundamental data bottleneck, which comes both in quantity as well as quality"}
% \guanya{I will suggest that let's focus on the notorious reward/DR/curriculum tuning issue and don't discuss sample efficiency here. Sampling efficiency is not that meaningful for sim2real massively parallel PG anyways.}

% \carlo{Some teleoperation methods are also based on retargeting, so these are not two entirely different strategies. I think the main difference lies in leveraging scale -- 1) "online retargeting + teleoperation" allows you to gather a few trajectories directly on the robot, but it doesn't scale well because of what you mentioned, 2) "offline retargeting + motion tracking" allows you to scale on large human motion datasets while keeping RL simple due to accurate retargeting.}

To address these challenges, imitating human motions offers a powerful alternative for learning whole-body control, especially for complex scene interactions. Human demonstrations capture dynamic coordination, such as lifting objects while walking on uneven terrain, and have been used effectively in animation~\cite{peng2018deepmimic, xu2025parc, wu2024human}. A critical challenge arises in robotics: unlike virtual characters, physical humanoids only approximate human morphology, with significant differences in shape, proportion and degrees of freedom. This embodiment gap means that simply adapting human motions is insufficient; it is essential to also adapt their scene interactions to the robot's specific form to generate usable references.
% To address these challenges, imitating human motions offers a powerful alternative. Human demonstrations provide a rich source of data for complex, whole-body coordination, such as lifting objects while walking or navigating uneven terrain. While highly successful in character animation~\cite{peng2018deepmimic, xu2025parc, wu2024human}, direct use of such data in robotics is hindered by the embodiment gap between humans and robots. To bridge this gap, researchers have pursued two primary strategies. One approach is teleoperation \cite{fu2024humanplus, he2024omnih2o, ze2025twist}, where a human operator directly controls the robot to generate data. While this provides robot-specific trajectories, the process is labor-intensive, difficult to scale, and often still requires a tracking policy to handle the kinematic differences between the operator and the robot. A more scalable path toward automated data generation is kinematic retargeting, which adapts existing human motions to the robot's morphology.

To this end, researchers have pursued two main strategies. The first one is teleoperation~\cite{fu2024humanplus, he2024omnih2o, ze2025twist}, where only a human operator's motions are retargeted to control the robot online. This approach leverages the human operator for real-time adaptation, which sidesteps the need for automatic interaction retargeting. However, despite the advantage of online feedback, the method remains labor-intensive and does not scale well for large-scale data generation. The second and more scalable strategy is offline interaction retargeting, which holistically adapts both the human's motion and their scene interactions to the robot's specific embodiment.
% sidestepping interaction retargeting by relying on the operator to adapt to the scene, which is effective but also labor-intensive and hard to scale.
% The second, more scalable approach is interaction retargeting offline, which adapts both human motions and their scene interactions to the robot’s embodiment.
% However, existing motion retargeting methods \cite{Luo2023PerpetualHC, ze2025twist, videomimic} suffer from fundamental limitations that severely constrain their utility for large-scale training data generation. These approaches predominantly rely on unconstrained optimization frameworks, lacking principled mechanisms to enforce hard physical constraints, resulting in physically implausible motions plagued by artifacts such as foot skating, ground penetration, and joint limit violations. More critically, they typically focus solely on simple locomotion while neglecting the rich interactions between the robot, manipulated objects, and the environment--precisely the interactions that are essential for practical loco-manipulation tasks. 
% This limitation forces practitioners to either manually clean retargeted motions or extensively engineer rewards during RL policy training, both of which are labor-intensive processes that limit the diversity and scale of usable training data.

However, most existing retargeting methods~\cite{Luo2023PerpetualHC, ze2025twist, videomimic} fall short in this regard. They predominantly rely on unconstrained or softly-penalized optimization, resulting in implausible motions with artifacts such as foot skating and penetration. More importantly, they do not explicitly consider interaction preservation---i.e., maintaining spatial and contact relationship---in the retargeting formulation, relying instead on simple keypoint matching. Consequently, the resulting references are of lower quality, which in turn complicates the downstream RL policy training~\cite{zhang2025hub, he2024omnih2o, he2025asap}.

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figures/pipeline_v2.pdf}
	\caption{\textbf{\OmniRetarget overview.} Human demonstrations are retargeted to the robot via interaction-mesh–based constrained optimization. Each spatial and shape augmentation is solved as a new optimization, producing diverse trajectories that serve as references for RL training with minimal reward design and domain randomization, enabling zero-shot transfer to real-world humanoids.} 
    % \guanya{I will update a new version on Monday}}
    % \karen{The text above the arrows is supposed to describe a process and the text above images is to describe data? If that's the format we are using, "interaction Mesh" does not sound like a process. Also, it is not clear from the figure where retargeting happens. One way to make it more clear is to put interactive mesh (both text and figure) below the Human Mocap Demo, and their outgoing arrows merge into one with text "constrained optimization" above it. This arrow points to the g1 robot figure with text "Retargeted Motion" above it.} \ak{Agree! Also I would like to suggest improving the real estate for the the super cool pictures  so they can be bigger by reducing the arrow width. You can wrap things in a box}
	\label{fig:banner}
    % \vspace*{-0.5cm}
\end{figure*}

% \ak{Current format of the intro is one where you list all the way other people do it and how they are bad, and then you say what you do. Instead, paper intros reads a lot more clearly if you directly state what your method can do as soon as possible, like do this "In this work.." in the second paragraph and then describe how its better than prior work in the other paragraphs. It's a much more direct way of writing and helps make things clear. It also leads to a first paragraph that's more directly relevant to the contribution. Here's a suggestion.
% P1: How can we teach humanoids how to move about in the world and interact with objects? Arguably the simplest method is to simply have the humans demonstrate, where the motion can be captured with motion capture or from video. This approach of taking human motion as a motion tracking target for training a policy in RL has shown promising results~\cite{..}. However, the morphological gap between humans and robots require retargeting, whose process may introduce physically implausible artifacts such as xyz particularly when humans interact with the object or the scenes. ---
% P2 (What we can do): In this work, we present OmniRetarget, it can do ABC and awesome results---
% P3 (How we do it): We achieve this by interaction mesh that preserves human object relationship and proper optimization technique. + augmentation. --
% P4 Our approach is much more scalable than teleop, and do not exhibit artifacts introduced by existing retargeting methods (thanks to constrained optimization), which as we show hinder policy training. In fact, w e show that our retargetting is so good that we can train a very good policy with very simple RL rewards. (discuss more )> }

In this work, we introduce \OmniRetarget, an open-source data generation engine that transforms human demonstrations into diverse, high-quality kinematic references for humanoid whole-body control. By modeling spatial and contact relationships between robots, objects, and terrains via an interaction mesh \cite{Ho2010Spatial}, \OmniRetarget preserves essential interactions and generates kinematically feasible variations. While existing methods require separate demonstrations for each variation---making data collection costly and limiting coverage---\OmniRetarget addresses this bottleneck directly. Inspired by data augmentation frameworks for contact-rich manipulation~\cite{yang2025physics}, our framework automatically augments a single demonstration into a large number of training examples across object configurations, shapes, robot embodiments, and environments.

Our pipeline employs constrained optimization to enforce physical feasibility, including collision avoidance, joint limits, and foot contact stability, while minimizing interaction mesh deformation. The resulting motions are interaction-preserving and exhibit only minimal kinematic artifacts, providing dense learning signals that accelerate RL with minimal reward engineering. On a diverse suite of whole-body interaction tasks such as box lifting, platform climbing, and slope crawling, policies trained on \OmniRetarget datasets outperform those from prior retargeting methods in both motion quality and robustness, with successful zero-shot sim-to-real transfer onto a physical humanoid robot.

Our contributions are fourfold:
\begin{enumerate}
    \item The first interaction-preserving humanoid retargeting framework that handles rich robot-object-terrain interactions while enforcing hard physical constraints.
    \item A systematic data augmentation pipeline that transforms a single human demonstration into a diverse, large-scale set of high-quality kinematic trajectories on various robot embodiments.
    \item A large-scale, open-source dataset of retargeted, kinematically-feasible loco-manipulation trajectories.
    \item Successful zero-shot sim-to-real transfer of proprioceptive RL policies on a physical humanoid, demonstrating a diverse set of scene-interaction tasks, including a long, agile sequence of object carrying, platform climbing, jumping, rolling and wall-flipping.
\end{enumerate}

