\section*{APPENDIX}
\subsection{Different Sources of Human Motion Data}
% Human motion datasets contain rich pose and shape information but come in various formats that require standardization before use in a retargeting pipeline. 
% The primary goal of preprocessing is to convert these diverse formats into a consistent representation—typically a time series of global 3D keypoint positions $\{p^\text{source}_{0:T, i}\}$. This process must also account for morphological differences between the human demonstrator and the target robot. 
Human motion datasets contain rich pose and shape information, but they differ both in data format and in the physical attributes (e.g., height, body proportions) of the demonstrators. To make them compatible across different sources and suitable for retargeting, we need to convert these inputs into a consistent representation, typically a time series of global 3D keypoint positions $\{p^\text{source}_{0:T, i}\}$. This process must account for differences between human demonstrators and the target robot.

The datasets used in this work represent two common formats:
\begin{itemize}
    \item \textbf{Parametric Human Models}: The OMOMO dataset uses the SMPL format \cite{SMPL:2015}, a parametric model representing human body shape and pose-dependent variations using shape ($\beta$) and pose ($q$) parameters.
    \item \textbf{Skeleton Hierarchy}: Both our in-house MoCap data and the LAFAN1 dataset utilize the skeleton hierarchy defined in the BVH format.
\end{itemize}
Different retargeting pipelines use different strategies to handle these formats. We detail these preprocessing steps below, denoting the human demonstrator's pose as $q^\text{demo}_{t}$, the SMPL forward model for the $i$-th keypoint as $M_i$, the original demonstrator shape as $\beta^{\text{demo}}$, and the demonstrator's $i$-th keypoint position as $p^\text{demo}_{t,i}$.


\subsubsection{SMPL Data}
To handle data from parametric models like SMPL, methods typically follow one of two strategies: fitting the model to the robot's morphology or directly scaling the human's keypoints.
\begin{algorithm}
\caption{Fit SMPL Shape (PHC)}
\begin{algorithmic}[1]\label{alg:phc_fit_smpl_shape}
\REQUIRE SMPL model $M$, robot urdf with forward kinematics $f$, $\bar q^\text{smpl} = 0_{n_s}, \bar q^\text{robot} = 0_{n_x}$
\ENSURE scaling factors $\alpha, \beta$
\STATE $\alpha, \beta \leftarrow 1, 0_{10}$
\FOR{$\text{iter} = 1, \ldots, \text{max\_iter}$}
    \STATE $L(\alpha, \beta) = \sum_i \left\|(f_i(\bar q^\text{robot}) - \alpha \cdot M_i(\bar q^\text{smpl}; \beta)\right\|^2$
    \STATE $\alpha \leftarrow \alpha - \eta_{\alpha} \cdot \nabla_{\alpha} L$
    \STATE $\beta \leftarrow \beta - \eta_{\beta} \cdot \nabla_{\beta} L$
\ENDFOR
\end{algorithmic}
\end{algorithm}
\paragraph{Model Fitting (PHC, VideoMimic)} This strategy fits a scaled SMPL model to the robot's morphology. PHC first optimizes for an overall scaling factor $\alpha$, and a set of SMPL shape parameters $\beta$ that best match the robot's link length in a canonical T-pose, as detailed in Alg. \ref{alg:phc_fit_smpl_shape}. The final source keypoint positions are then generated from this fitted model:\looseness=-1
\begin{equation}
p^\text{source}_{t,i} = \alpha \cdot M_i(q^\text{demo}_t; \beta).
\end{equation}
VideoMimic adopts a similar philosophy but integrates the scaling directly into its main retargeting optimization, solving for per-link scale factors jointly with the robot's motion.
% \begin{itemize}
%     \item \textbf{Model Fitting:} PHC and VideoMimic fit a scaled SMPL model to the robot's morphology by optimizing shape parameters $\beta$ and a scaling factor $\alpha$.
%     \item \textbf{Direct Scaling:} GMR and OmniRetarget directly extract keypoint positions from the human's SMPL parameters and scale them to the robot's size, often using a height ratio.
% \end{itemize}


\paragraph{Direct Scaling (GMR \& \OmniRetarget)}
In contrast, GMR and \OmniRetarget use a more direct approach. They generate keypoints from the human's \emph{original} SMPL parameters $\beta^{\text{demo}}$ and then scale them to the robot's proportions. 
Both methods support detailed morphological adaptation via per-bone scaling factors based on corresponding human-robot link lengths. For simplicity in this work, however, we adopt a single global scaling factor $\alpha$, set to the robot-to-human height ratio:
% To scale each link correctly for inverse kinematics, GMR requires manual tuning of per-bone scaling factors based on the corresponding human–robot link length ratios. In essence, GMR fits a human model onto the robot's shape manually. 

% In contrast, OmniRetarget does not require using an exact human model to begin with, thus avoiding this tedious process and only needs a single global scaling factor, $\alpha$, defined as the robot-to-human height ratio:
\begin{equation}\label{eq:gmr_scaling}
    p^\text{source}_{t,i} = \alpha \cdot M_i(q^\text{demo}_t; \beta^{\text{demo}}), \alpha=\frac{h_{\text{robot}}}{h_{\text{demo}}}.
\end{equation}

\begin{table*}[tb]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Method} & \textbf{Optimization Type} & \textbf{Primary Objective} & \textbf{Preprocessing} & \textbf{Data Formats} \\ \midrule
PHC & Trajectory-wise Optimization & Keypoint Position Matching & Model Fitting & SMPL \\
GMR & Per-Frame Optimization & Keypoint Position \& Orientation Matching  & Direct Scaling & SMPL, BVH  \\
VideoMimic & Trajectory-wise Optimization & Pairwise Distance \& Orientation Preservation & Model Fitting & SMPL \\
IMMA & Multi-Stage Trajectory-wise Optimization & Interaction Mesh Deformation + IK & Unknown & Unknown \\ \midrule
\textbf{\OmniRetarget} & Per-Frame Optimization & Interaction Mesh Deformation & Direct Scaling & SMPL, BVH   \\ \bottomrule
\end{tabular} 
\caption{\label{tab:method_comparison}Comparison of different retargeting methodologies}
\end{table*}

\subsubsection{Skeleton Hierarchy Data}
For formats like BVH, keypoint positions are derived from the skeleton's forward kinematics $f^\text{skeleton}$. This data is then typically scaled to the robot's size using the height ratio:
\begin{equation}
    p^\text{source}_{t, i} = \frac{h_{\text{robot}}}{h_{\text{demo}}} \cdot f_i^\text{skeleton}(q^\text{demo}_t).
\end{equation}

A key distinction among methods is their data compatibility. While GMR and \OmniRetarget are designed to process both parametric model data and raw skeleton hierarchies, frameworks like PHC and VideoMimic are primarily designed for SMPL data. Fitting other data formats to the SMPL format is yet another tedious process. 

\subsection{Different Kinematic Retargeting Formulations}
Once human motion is preprocessed into a series of source keypoint positions $\{p^\text{source}_{0:T, i}\}$, different methods formulate the retargeting problem in distinct ways. As summarized in Tab.~\ref{tab:method_comparison}, these approaches vary in their optimization strategy and objectives. The following sections detail the mathematical formulation of each baseline method and our proposed approach, \OmniRetarget.
% As mentioned in Sec. \ref{sec:motion_retargeting}, researchers have adopted various formulations to retarget human motions to robot trajectories. PHC uses keypoint matching, while GMR extends this approach to match both keypoint positions and orientations. VideoMimic preserves the pariwise distance and orientation between keypoint pairs.
\begin{algorithm}
\caption{Retarget Robot Motion (PHC)}
\begin{algorithmic}[1]\label{alg:phc_fit_robot_motion}
\REQUIRE Robot urdf, source keypoint positions $\{p^\text{source}_{0:T, i}\}$
\ENSURE $q_{0:T}$
\STATE $q_{0:T} \leftarrow [0_{n_x}]_T$
\FOR{$\text{iter} = 1, \ldots, \text{max\_iter}$}
    \STATE $\mathcal{L}(q_{0:T}) = \sum_{t=0}^T  \sum_i \left\|f_i(q_t) - p^\text{source}_{t, i}\right\|^2$
    \STATE $q_{0:T} \leftarrow \text{clamp}(q_{0:T} - \nabla_{q_{0:T}} \mathcal{L}(q_{0:T}), q_{\min}, q_{\max})$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{PHC}
PHC formulates retargeting as a large-scale trajectory-wise optimization problem. It applies gradient descent to minimize the error between the source keypoint positions and the robot's keypoint positions over the entire trajectory, as shown in Alg.~\ref{alg:phc_fit_robot_motion}. \looseness=-1



\subsubsection{GMR}
GMR performs retargeting by solving an inverse kinematics (IK) problem at each frame (\ref{alg:gmr_retarget_robot_motion}).
At each timestep, GMR finds the robot configuration $q_t$ that matches the source keypoint positions and orientations via the following optimization program:
\begin{equation} \label{eq:gmr_qp}
    \begin{aligned}
        q_t^\star = \argmin_{q_t} & \sum_i \left\|f_i^p(q_t) - p^\text{source}_{t, i}\right\|^2 + \left\|f_i^\theta(q_t) - \theta^\text{source}_{t, i}\right\|^2 \\
      \text{s.t. } & q_{\min} \leq q_t \leq q_{\max},
    \end{aligned}
\end{equation}
where $f_i^p$ and $f_i^\theta$ are the robot forward kinematics for the $i$-th keypoint's position and orientation, respectively. Leveraging the mink \cite{Zakka_Mink_Python_inverse_2025} library, GMR solves this program in a Sequential Quadratic Programming fashion.
\begin{algorithm}
\caption{Retarget Robot Motion (GMR)}
\begin{algorithmic}[1]\label{alg:gmr_retarget_robot_motion}
\REQUIRE Robot urdf, source keypoint positions $\{p^\text{source}_{0:T, i}\}$ and orientations $\{\theta^\text{source}_{0:T, i}\}$
\ENSURE $q_{0:T}$
\FOR{$t = 0, \ldots, T$}
    \STATE $q_{t} \leftarrow$ Solve IK \eqref{eq:gmr_qp}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{VideoMimic}
% VideoMimic performs a joint optimization over the robot's trajectory $q_{0:T}$ and per-link scaling factors $\beta_{ij}$. Its primary objective is to preserve the scaled pairwise distances and orientations between keypoints. The full objective includes multiple penalty terms for physical constraints:

VideoMimic jointly optimizes for the robot motion $q_{0:T}$ and SMPL per-link scaling factor $\beta$ over the entire trajectory. The primary objective is to preserve the scaled pairwise distance and orientation between each keypoint pair $(i, j)$:
\begin{equation}
    \mathcal{L}_{\text{pairwise}} = \sum_{t, i \in \mathcal{N}(j)} \|\beta_{ij} \cdot (p^\text{demo}_{t,i} -  p^\text{demo}_{t,j}) - (f_i(q_t) - f_j(q_t))\|_2^2,
\end{equation}
with soft penalties on foot contact matching $\mathcal{L}_{\text{contact}}$, foot skating $\mathcal{L}_{\text{skating}}$, collision $\mathcal{L}_{\text{collision}}$, joint limits $\mathcal{L}_{\text{joint}}$ and temporal smoothness $\mathcal{L}_{\text{smooth}}$:
\begin{equation} \label{eq:videomimic_opt}
    \begin{aligned}
        q_{0:T}^\star, \beta^\star = & \argmin_{q_{0:T}, \beta} \quad  \mathcal{L}_{\text{pairwise}} + \lambda_c \cdot \mathcal{L}_{\text{contact}} + \lambda_s \cdot \mathcal{L}_{\text{skate}} + \\
        & \lambda_{cl} \cdot \mathcal{L}_{\text{collision}} + \lambda_j \cdot \mathcal{L}_{\text{joint}} + \lambda_{sm} \cdot \mathcal{L}_{\text{smooth}} + \ldots
    \end{aligned}
\end{equation}
\begin{algorithm}
\caption{Retarget Robot Motion (VideoMimic)}\label{alg:videomimic_retarget}
\begin{algorithmic}[1]
\REQUIRE Robot urdf, demonstrator's original keypoint positions $\{p^\text{demo}_{0:T, i}\}$
\ENSURE $q_{0:T}, \beta$ $\leftarrow$ Solve \eqref{eq:videomimic_opt} for the entire trajectory
\end{algorithmic}
\end{algorithm}

\subsubsection{IMMA Multi-stage Optimization}
IMMA relies on a complex, multi-stage pipeline: first, it optimizes the intermediate robot keypoint positions to warp the interaction mesh from the human to the robot with minimal deformation by solving the following program
\begin{equation}
    \begin{aligned}
    p_{t,i}^\star = \argmin_{p_{t_i}} \quad & \sum_{i} \|L(p_{t,i} ^{\text{source}})-L(p_{t,i})\|^2 \\
    \text{s.t.} \quad & \phi_j(q_t) \geq 0, \forall j \\
    & \|p_{t,i} - p_{t,j}\|_2 = l_{ij}, \forall \text{bone} \\
    & p_t^{F} = p_{t-1}^{F}, \forall \text{stance foot}, 
\end{aligned}
\end{equation}
where $l_{ij}$ is the bone length between the $i$-th and $j$-th joints. Then, it solves a separate IK problem to recover joint angles that best match the intermediate keypoints: 
\begin{equation}
    \begin{aligned}
        q_t^\star = \argmin_{q_t} \sum_i \|f_i(q_t) - p_{t,i}^\star \|_2^2.
    \end{aligned}
\end{equation}
In later stages, additional hard constraints on the feet and waist are imposed to prevent foot slipping and ensure dynamic balancing. This sequential and fragmented approach produces dynamically consistent motions but fails to consider crucial kinematic constraints like joint and velocity limits.

\begin{algorithm}
\caption{Retarget Robot Motion (\OmniRetarget)}
\begin{algorithmic}[1]\label{alg:omniretarget_retarget_robot_motion}
\REQUIRE Robot urdf, source keypoint positions $\{p^\text{source}_{0:T, i}\}$
\ENSURE $q_{0:T}$
\FOR{$t = 0, \ldots, T$}
    \STATE $q_{t} \leftarrow$ Solve interaction mesh optimization \eqref{eq:interaction_mesh_opt}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{\OmniRetarget}
\OmniRetarget, as outlined in Alg. \ref{alg:omniretarget_retarget_robot_motion}, operates frame-by-frame by minimizing the Laplacian deformation of the interaction meshes. The core objective \eqref{eq:interaction_mesh_cost} is flexible and can be augmented with task-specific costs, such as the orientation matching term from GMR, providing a unified and extensible framework for motion retargeting.


\subsection{Data Augmentation Details}
% We denote the time step where the box starts moving as $t_m$, the time constants for translational and rotational exponential decay as $\tau_p$ and $\tau_\theta$ respectively. The exponential interpolation scheme for the augmented object trajectory is
% \begin{subequations}\label{eq:aug_obj_traj}
% \begin{align}
% \tilde{p}_{obj}[t] &=
% \left\{
% \begin{aligned}
% & \Delta p_{obj} + p_{obj}[0] \\
% & \hfill \text{(before object starts moving)} \\[2pt]
% & \Delta p_{obj}\, e^{-(t-t_m)/\tau_p} + p_{obj}[t] \\
% & \hfill \text{(after object starts moving)}
% \end{aligned}
% \right. \\[0.75em]
% \tilde{\theta}_{obj}[t] &=
% \left\{
% \begin{aligned}
% & \Delta \theta_{obj} + \theta_{obj}[0] \\
% & \hfill \text{(before object starts moving)} \\[2pt]
% & \Delta \theta_{obj}\, e^{-(t-t_m)/\tau_{\theta}} + \theta_{obj}[t] \\
% & \hfill \text{(after object starts moving)}
% \end{aligned}
% \right.
% \end{align}
% \end{subequations}
\subsubsection{Augmented Object Trajectory}\label{sec:aug_obj_traj}
To generate a perturbed object trajectory, we introduce a transient offset that decays exponentially over time. Let the original trajectory be denoted by $(p_{obj}(t), \theta_{obj}(t))$. We define an initial positional offset $\Delta p_{obj}$ and rotational offset $\Delta \theta_{obj}$ that are applied at the onset of object motion, $t_m$. The augmented trajectory, $(\tilde{p}_{obj}(t), \tilde{\theta}_{obj}(t))$, is then formulated as:

\begin{subequations}\label{eq:aug_obj_traj}
\begin{align}
\tilde{p}_{obj}(t) &=
\begin{cases}
\Delta p_{obj} + p_{obj}(0) & \text{if } t < t_m \\
\Delta p_{obj}\, e^{-(t-t_m)/\tau_p} + p_{obj}(t)& \text{if } t \ge t_m
\end{cases}
\\[0.75em]
\tilde{\theta}_{obj}(t) &=
\begin{cases}
\Delta \theta_{obj} \oplus \theta_{obj}(0) & \text{if } t < t_m \\
\Delta \theta_{obj}\, e^{-(t-t_m)/\tau_{\theta}} \oplus \theta_{obj}(t) & \text{if } t \ge t_m
\end{cases}
\end{align}
\end{subequations}
where $\tau_p$ and $\tau_\theta$ are time constants governing the rate of decay for the translational and rotational perturbations, respectively. The $\oplus$ operator denotes composition for orientations (e.g., quaternion multiplication).


\subsubsection{Interaction Mesh Construction in Object Frame} \label{sec:im_in_obj_frame}
For robot-object interactions, it is crucial to construct the interaction mesh in the object's local coordinate frame. This ensures that the Laplacian coordinates, which encode relative spatial relationships, are invariant to the object's global rotation and translation. As illustrated in Fig. \ref{fig:obj_frame_im}, when the object rotates by \ang{180} (indicated by the black arrow), the Laplacian coordinate of the object in the world frame $L_W$ changes from $(0, 1)$ to $(0, -1)$, while the Laplacian coordinate calculated in the object frame $L_O$ remains constant. Using object-frame coordinates is therefore essential for preserving the intended interaction geometry during object spatial transformation.
\begin{figure}
\centering
\includegraphics[width=0.20\textwidth]{figures/obj_frame_im.png}
	\caption{The Laplacian coorinate should stay the same when the object rotates \ang{180}. }
	\label{fig:obj_frame_im}
    \vspace*{-0.4cm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.23\textwidth]{figures/interaction_mesh.png}
	\caption{The actual target (left) and source (right) interaction meshes used for optimization.}
    % \ak{btw, this figure is very similar to the original paper in that it's about a link but since the paper is about human object interaction I think it would be better with two long arms holding a box that has to be regarted to two shorter arms + box }
	\label{fig:interaction_mesh}
    \vspace*{-0.4cm}
\end{figure}
\subsection{Sequential SOCP Details}
At each time step $t$, we iteratively solve a Second-Order Cone Program (SOCP) for the optimal change in the robot's configuration $dq$ util convergence for up to 10 iterations. For conciseness, we omit the time index $t$ for variables within the Sequential SOCP loop. 

The optimization finds the increment $dq_n$ at the $n$-th iteration. The configuration is updated using this increment, starting from the previous time step's solution ($\bar q_0 = q_{t-1}^\star$):
$$\bar q_{n+1} = \bar q_n + dq_n^\star.$$
The optimal increment $dq_n^\star$ is the solution to the following SOCP, which is linearized around the current iterate  
$\bar q_n$:
\begin{subequations}
\begin{align}
    dq_n^\star = \argmin_{dq_n} \quad & \|L^{\text{source}} - (J_L^n\cdot dq_n +\bar L_n^{\text{target}})\|^2 \\
    & + \|\bar{q}_n + dq_n - q_{t-1}\|_{Q}^2 \\
    \text{s.t.} \quad & J_j^n \cdot dq_n + \phi_j(\bar{q}_n) \geq 0, \forall j \\
    & q_{\min} \leq \bar{q}_n + dq_n \leq q_{\max} \\
    & v_{\min} \cdot dt \leq \bar{q}_n + dq_n - q_{t-1} \leq v_{\max} \cdot dt \\
    & p_t^{F}(\bar{q}_n) + J_{F}^n \cdot dq_n = p_{t-1}^{F}, \forall \text{stance foot} \\
    & \|dq_n\|_2 \leq \epsilon, \label{eq:trust_region_cstr}
\end{align}
\end{subequations}
where 
\begin{itemize}
    \item $L^{\text{source}} = vec(\{L(p_{t,i}^{\text{source}})\})$
    \item $L^{\text{target}}(q) = vec(\{L(p_{t,i}^{\text{target}}(q))\})$
    \item $\bar L_n^{\text{target}} = vec(\{L( p_{t,i}^{\text{target}}(\bar q_n))\})$
    \item $J_L^n = \partial L^\text{target}/\partial q|_{q=\bar q_n}$
    \item $J_j^n = \partial \phi_j/\partial q|_{q=\bar q_n}$
    \item $J_F^n = \partial p_t^F /\partial q|_{q=\bar q_n}$.
\end{itemize}
The second-order cone constraint \eqref{eq:trust_region_cstr} is a trust region constraint with radius $\epsilon$ (we use $\epsilon=0.2$) that keeps the step size small, ensuring the linear approximations remain valid.



\subsection{Downstream RL Evaluation Breakdown}
Shown in Fig.~\ref{fig:histogram}, we present histograms from the downstream RL evaluation (Sec.~\ref{sec:downstream-rl}) to illustrate failure patterns and variance across OmniRetarget and baselines. These histograms break down failure rates by each motion for two tasks: robot–object interaction and robot–terrain interaction, highlighting not only overall averages but also how failures distribute across different motions. We do not include augmented motions as baselines do not support augmentation. 

\begin{figure}
\centering
\includegraphics[width=0.49\textwidth]{figures/histo-crop.pdf}
	\caption{Histograms from the downstream RL evaluation showing the failure patterns for the baselines in different tasks. }
    % \ak{btw, this figure is very similar to the original paper in that it's about a link but since the paper is about human object interaction I think it would be better with two long arms holding a box that has to be regarted to two shorter arms + box }
	\label{fig:histogram}
    \vspace*{-0.4cm}
\end{figure}

In robot–object interaction, the motions are modest while object properties are heavily randomized. Since the motions are not aggressive, most policies can adapt even to low-quality references and achieve at least one success, except VideoMimic, which fails systematically due to poor interaction preservation. This task therefore measures robustness rather than accuracy. We see that GMR shows broader failure spread with lower success rates, likely due to penetration issues that reduce robustness under placement changes. PHC shows improved robustness, while OmniRetarget achieves the most robust performance, with results concentrated in the high-success region.

In contrast, climbing terrains requires much more agile and challenging motions and thus, demands precise reference motions: if the quality is low, the agent fails outright with no successes. Here, PHC and VideoMimic perform the worst, with nearly half the motions failing entirely. GMR delivers somewhat better references but still fails on four motions, while OmniRetarget fails on only one. These results show that OmniRetarget not only provides superior robustness under variation but also higher reference accuracy.

For the one remaining failure, we believe that it is limited by the simple RL formulation we use. For future work, an interesting direction could be to extend the current RL formulation with curriculum learning to support these extremely difficult motions. 