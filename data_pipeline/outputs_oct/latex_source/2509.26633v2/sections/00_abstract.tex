\begin{abstract}
% Background and problems of existing methods
A dominant paradigm for teaching humanoid robots complex skills is to retarget human motions as kinematic references to train reinforcement learning (RL) policies. However, existing retargeting pipelines often struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating and penetration. More importantly, common retargeting methods neglect the rich human-object and human-environment interactions essential for expressive locomotion and loco-manipulation.
% Our algorithm and key insights
To address this, we introduce \OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, the terrain, and manipulated objects. By minimizing the Laplacian deformation between the human and robot meshes while enforcing kinematic constraints, \OmniRetarget generates kinematically feasible trajectories. Moreover, preserving task-relevant interactions enables efficient data augmentation, from a single demonstration to different robot embodiments, terrains, and object configurations. 
% Retargeting performance
We comprehensively evaluate \OmniRetarget by retargeting motions from OMOMO \cite{li2023object}, LAFAN1 \cite{harvey2020robust}, and our in-house MoCap datasets, generating over 8-hour trajectories that achieve better kinematic constraint satisfaction and contact preservation than widely used baselines.
% So what? The implication for downstream tasks.
% \guanya{(I moved the RL part here) More detailed: RL only 4-5 reward terms, minimum DR, no curriculum, minumum sim2real gap.} These high-quality references provide dense and informative learning signals that dramatically accelerate downstream RL policy learning with minimal reward engineering.
Such high-quality data enables proprioceptive RL policies to successfully execute long-horizon (up to 30 seconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained with only 5 reward terms and simple domain randomization shared by all tasks, without any learning curriculum. 
% Contribution to the community
All code, retargeted datasets, and trained policies will be publicly released. Result videos can be found at \href{https://omniretarget.github.io}{https://omniretarget.github.io}
\end{abstract}

% \guanya{Major high-level comments:
% \begin{itemize}
%     \item It is a retargeting paper. Strange that we don't have many ``human motion $\rightarrow$ robot kinematic motion $\rightarrow$ RL in sim $\rightarrow$ RL in real'' figures! Emphasize from 2 to 3 to 4 there is minimum gap. Check out ASAP Fig 3. I would suggest the current ``OmniRetarget overview'' figure should focus on algorithm. We need a new figure 1 for teaser. We also need a new figure showing the previously mentioned retargeting process for different motions.
%     \item Figure plan: \begin{itemize}
%         \item Figure 1: Pure real-world teaser, $\geq$3 sequences @zhen
%         \item Figure 2: Algorithm pipeline, a diagram emphasize what we are optimizing and how simple RL is. @lujie
%         \item Figure 3: Illustrate the whole retargeting and RL process: human motion $\rightarrow$ robot kinematic motion $\rightarrow$ RL in sim $\rightarrow$ RL in real  @lujie
%         \item Figure 4: A small figuring showing H1/T1. @xiaoyu
%         \item Figure 5: One figure for many augmentation. original + 2 should be enough. @zhen 
%         \item Figure 6: Showing side-by-side comparison with baselines emphasize artifact. 2x4 @xiaoyu, @zhen
%         \item Figure 7: Other real results not included in teaser. Different style. @zhen
%     \end{itemize}
%     \item Need to make a macro for our algorithm, sth like $\textbf{\texttt{OmniRetarget}}$
%     \item All figures need to be PDFs.
%     \item Need to prepare an anonymous website for ICRA.
% \end{itemize}}



% \begin{abstract}
% A dominant paradigm for teaching robots complex skills is to retarget human motion as a kinematic reference for reinforcement learning (RL) policies. However, existing retargeting pipelines struggle with the significant embodiment gap between humans and robots, producing physically implausible artifacts like foot-skating, and critically, most neglect the rich human-object-environment interactions essential for loco-manipulation. We argue that a principled, interaction-preserving retargeting framework with rigorous kinematic constraints is the key to unlocking robust, whole-body control and scalable data generation. To this end, we introduce OmniRetarget, an interaction-preserving data generation engine based on an interaction mesh that explicitly models and preserves the crucial spatial and contact relationships between an agent, its environment, and manipulated objects. By optimizing to maintain these interaction geometries across augmented object configurations and shapes, our method transforms a single human demonstration into diverse sets of kinematically feasible and task-relevant trajectories. These high-quality references provide dense, informative learning signals that dramatically accelerate downstream policy learning with minimal reward engineering. We comprehensively evaluate our approach by retargeting motions from LAFAN, OMOMO, and our in-house motion capture datasets, generating over 10,000 augmented trajectory variations across different tasks and environmental conditions. We demonstrate the effectiveness of our data generation pipeline with a Unitree G1 humanoid robot that learns robust, dynamically-consistent policies for challenging whole-body tasks, including box lifting and transport, stair climbing, and crawling on uneven terrain. To facilitate future research, we open-source our complete framework, including code, retargeted datasets, and trained policies.
% \end{abstract}