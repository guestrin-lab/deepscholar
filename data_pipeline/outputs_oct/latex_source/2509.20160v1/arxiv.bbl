% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{kuutti2020survey}
S.~Kuutti, R.~Bowden, Y.~Jin, P.~Barber, and S.~Fallah, ``A survey of deep learning applications to autonomous vehicle control,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, 2020.

\bibitem{chen2019survey}
Q.~Chen, W.~Wang, F.~Wu, S.~De, R.~Wang, B.~Zhang, and X.~Huang, ``A survey on an emerging area: Deep learning for smart city data,'' \emph{IEEE Transactions on Emerging Topics in Computational Intelligence}, 2019.

\bibitem{kollias2018deep}
D.~Kollias \emph{et~al.}, ``Dimitrios kollias and athanasios tagaris and andreas stafylopatis and stefanos kollias and georgios tagaris,'' \emph{Complex \& Intelligent Systems}, 2018.

\bibitem{trainedge}
N.~Kukreja, A.~Shilova, O.~Beaumont, J.~Huckelheim, N.~Ferrier, P.~Hovland, and G.~Gorman, ``Training on the edge: The why and the how,'' in \emph{2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 2019.

\bibitem{edge_review}
J.~Chen and X.~Ran, ``Deep learning with edge computing: A review,'' \emph{Proceedings of the IEEE}, vol. 107, no.~8, 2019.

\bibitem{paise22}
{Prashanthi S.K}, A.~Khochare, S.~A. Kesanapalli, R.~Bhope, and Y.~Simmhan, ``Don't miss the train: A case for systems research into training on the edge,'' in \emph{2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, 2022.

\bibitem{zhou2019edge}
Z.~Zhou \emph{et~al.}, ``Edge intelligence: Paving the last mile of artificial intelligence with edge computing,'' \emph{Proceedings of the IEEE}, vol. 107, no.~8, 2019.

\bibitem{mohan2021analyzing}
J.~Mohan, A.~Phanishayee, A.~Raniwala, and V.~Chidambaram, ``Analyzing and mitigating data stalls in dnn training,'' \emph{Proc. VLDB Endow.}, vol.~14, no.~5, 2021.

\bibitem{wang2019benchmarking}
Y.~E. Wang, G.-Y. Wei, and D.~Brooks, ``Benchmarking tpu, gpu, and cpu platforms for deep learning,'' \emph{arXiv preprint arXiv:1907.10701}, 2019.

\bibitem{baller2021deepedgebench}
S.~Baller, A.~Jindal, M.~Chadha, and M.~Gerndt, ``Deepedgebench: Benchmarking deep neural networks on edge devices,'' in \emph{2021 IEEE International Conference on Cloud Engineering (IC2E)}, 2021.

\bibitem{edge_config}
S.~Holly, A.~Wendt, and M.~Lechner, ``Profiling energy consumption of deep neural networks on nvidia jetson nano,'' in \emph{2020 11th International Green and Sustainable Computing Workshops (IGSC)}, 2020.

\bibitem{coral}
Google, ``Google coral products,'' \url{https://coral.ai/products/}, 2022.

\bibitem{movidius}
Intel, ``Intel movidius vpus,'' \url{https://www.intel.com/content/www/us/en/products/details/processors/movidius-vpu.html}, 2022.

\bibitem{dev_datasheet}
Google, ``Dev board datasheet,'' \url{https://coral.ai/docs/dev-board/datasheet/}, 2022.

\bibitem{TFvsPy}
Assemblyai, ``Tf v/s pytorch,'' \url{https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/}, 2022.

\bibitem{tx2_training}
J.~Liu, J.~Liu, W.~Du, and D.~Li, ``Performance analysis and characterization of training deep learning models on mobile device,'' in \emph{2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS)}, 2019, pp. 506--515.

\bibitem{beutel2020flower}
D.~J. Beutel, T.~Topal, A.~Mathur, X.~Qiu, J.~Fernandez-Marques, Y.~Gao, L.~Sani, K.~H. Li, T.~Parcollet, P.~P.~B. de~Gusmão, and N.~D. Lane, ``Flower: A friendly federated learning research framework,'' \emph{arXiv preprint arXiv:2007.14390}, 2020.

\bibitem{tk1_europar}
H.~Halawa, H.~A. Abdelhafez, A.~Boktor, and M.~Ripeanu, ``Nvidia jetson platform characterization,'' in \emph{Euro-Par 2017: Parallel Processing}.\hskip 1em plus 0.5em minus 0.4em\relax Cham: Springer International Publishing, 2017, pp. 92--105.

\bibitem{MLSYS2020_02522a2b}
\BIBentryALTinterwordspacing
P.~Mattson, C.~Cheng, G.~Diamos, C.~Coleman, P.~Micikevicius, D.~Patterson, H.~Tang, G.-Y. Wei, P.~Bailis, V.~Bittorf, D.~Brooks, D.~Chen, D.~Dutta, U.~Gupta, K.~Hazelwood, A.~Hock, X.~Huang, D.~Kang, D.~Kanter, N.~Kumar, J.~Liao, D.~Narayanan, T.~Oguntebi, G.~Pekhimenko, L.~Pentecost, V.~Janapa~Reddi, T.~Robie, T.~St~John, C.-J. Wu, L.~Xu, C.~Young, and M.~Zaharia, ``Mlperf training benchmark,'' vol.~2, 2020, pp. 336--349. [Online]. Available: \url{https://proceedings.mlsys.org/paper/2020/file/02522a2b2726fb0a03bb19f2d8d9524d-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{frqswitching}
H.~A. Abdelhafez and M.~Ripeanu, ``Studying the impact of {CPU} and memory controller frequencies on power consumption of the {Jetson} {TX1},'' in \emph{IEEE Intl. Conf. on Fog and Mobile Edge Comp. (FMEC)}, 2019.

\bibitem{snowflakes}
H.~A. Abdelhafez, H.~Halawa, K.~Pattabiraman, and M.~Ripeanu, ``Snowflakes at the edge: A study of variability among {NVIDIA} {Jetson} {AGX} {Xavier} boards,'' in \emph{ACM EdgeSys Workshop}, 2021.

\bibitem{kumar2020quiver}
A.~V. Kumar and M.~Sivathanu, ``Quiver: An informed storage cache for deep learning,'' in \emph{18th $\{$USENIX$\}$ Conference on File and Storage Technologies ($\{$FAST$\}$ 20)}, 2020.

\bibitem{google-sysml}
\BIBentryALTinterwordspacing
K.~Bonawitz, H.~Eichner, W.~Grieskamp, D.~Huba, A.~Ingerman, V.~Ivanov, C.~Kiddon, J.~Kone\v{c}n\'{y}, S.~Mazzocchi, B.~McMahan, T.~Van~Overveldt, D.~Petrou, D.~Ramage, and J.~Roselander, ``Towards federated learning at scale: System design,'' in \emph{Proceedings of Machine Learning and Systems}, A.~Talwalkar, V.~Smith, and M.~Zaharia, Eds., vol.~1, 2019, pp. 374--388. [Online]. Available: \url{https://proceedings.mlsys.org/paper/2019/file/bd686fd640be98efaae0091fa301e613-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{powermodes_nano}
Nvidia, ``Power modes for nano,'' \url{https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3261/index.html\#page/Tegra\%20Linux\%20Driver\%20Package\%20Development\%20Guide/power_management_nano.html\#}, 2021.

\bibitem{powermodes_nxagx}
------, ``Power modes for nx and agx,'' \url{https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3261/index.html\#page/Tegra\%20Linux\%20Driver\%20Package\%20Development\%20Guide/power_management_jetson_xavier.html\#}, 2021.

\bibitem{powermodes_orin}
------, ``Technical brief: Nvidia jetson agx orin,'' \url{https://www.nvidia.com/content/dam/en-zz/Solutions/gtcf21/jetson-orin/nvidia-jetson-agx-orin-technical-brief.pdf}, 2021.

\bibitem{AGX}
Nvidia., ``Jetson agx xavier developer kit,'' \url{https://developer.nvidia.com/embedded/jetson-agx-xavier-developer-kit}, 2021.

\bibitem{NX}
Nvidia, ``Jetson nx xavier developer kit,'' \url{https://developer.nvidia.com/embedded/jetson-xavier-nx}, 2021.

\bibitem{Nano}
------, ``Jetson nano developer kit,'' \url{https://developer.nvidia.com/embedded/jetson-nano-developer-kit}, 2021.

\bibitem{Orin}
Nvidia., ``Jetson agx orin developer kit,'' \url{https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/}, 2022.

\bibitem{pytorch}
\BIBentryALTinterwordspacing
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~K\"{o}pf, E.~Yang, Z.~DeVito, M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning library,'' in \emph{Advances in Neural Information Processing Systems}, vol.~32, 2019. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf}
\BIBentrySTDinterwordspacing

\bibitem{dataloader}
pytorch, ``Torch.utils.data,'' \url{https://pytorch.org/docs/stable/data.html}, 2021.

\bibitem{summary}
S.~Chandel, ``Pytorch model summary,'' \url{https://github.com/sksq96/pytorch-summary}, 2022.

\bibitem{LeNet}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning applied to document recognition,'' \emph{Proceedings of the IEEE}, vol.~86, no.~11, pp. 2278--2324, 1998.

\bibitem{ding2018auto}
X.~Ding, G.~Ding, J.~Han, and S.~Tang, ``Auto-balanced filter pruning for efficient convolutional neural networks,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~32, no.~1, 2018.

\bibitem{howard2019searching}
A.~Howard, M.~Sandler, G.~Chu, L.-C. Chen, B.~Chen, M.~Tan, W.~Wang, Y.~Zhu, R.~Pang, V.~Vasudevan, Q.~V. Le, and H.~Adam, ``Searching for mobilenetv3,'' in \emph{IEEE/CVF International Conference on Computer Vision (ICCV)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019.

\bibitem{mobnet_flops}
papers~with code, ``Mobilenet v3,'' \url{https://paperswithcode.com/lib/torchvision/mobilenet-v3}, 2021.

\bibitem{tensorflow_gld23k}
TensorFlow, ``Tff gldv2,'' \url{https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/gldv2/load_data}, 2022.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image recognition,'' in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2016, pp. 770--778.

\bibitem{flops}
V.~Sovrasov, ``Flops counter,'' \url{https://pypi.org/project/ptflops/}, 2021.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton, ``Learning multiple layers of features from tiny images,'' 2009.

\bibitem{vgg}
\BIBentryALTinterwordspacing
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for large-scale image recognition,'' 2014. [Online]. Available: \url{https://arxiv.org/abs/1409.1556}
\BIBentrySTDinterwordspacing

\bibitem{weyand2020google}
T.~Weyand, A.~Araujo, B.~Cao, and J.~Sim, ``Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval,'' in \emph{IEEE/CVF conference on computer vision and pattern recognition}, 2020.

\bibitem{golmant2018computational}
N.~Golmant, N.~Vemuri, Z.~Yao, V.~Feinberg, A.~Gholami, K.~Rothauge, M.~W. Mahoney, and J.~Gonzalez, ``On the computational inefficiency of large batch sizes for stochastic gradient descent,'' \emph{arXiv preprint arXiv:1811.12941}, 2018.

\bibitem{charles2021large}
Z.~Charles, Z.~Garrett, Z.~Huo, S.~Shmulyian, and V.~Smith, ``On large-cohort training for federated learning,'' \emph{Advances in Neural Information Processing Systems}, vol.~34, 2021.

\bibitem{shallue2018measuring}
C.~J. Shallue, J.~Lee, J.~Antognini, J.~Sohl-Dickstein, R.~Frostig, and G.~E. Dahl, ``Measuring the effects of data parallelism on neural network training,'' \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem{chen2022demon}
J.~Chen, C.~Wolfe, Z.~Li, and A.~Kyrillidis, ``Demon: Improved neural network training with momentum decay,'' in \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 3958--3962.

\bibitem{tegrastats}
Nvidia, ``tegrastats,'' \url{https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3231/index.html\#page/Tegra\%20Linux\%20Driver\%20Package\%20Development\%20Guide/AppendixTegraStats.html}, 2021.

\bibitem{iostat}
man page, ``iostat,'' \url{https://man7.org/linux/man-pages/man1/iostat.1.html}, 2021.

\bibitem{vmtouch}
man pages, ``vmtouch,'' \url{https://linux.die.net/man/8/vmtouch}, 2021.

\bibitem{Cuda_event}
PyTorch, ``Cuda event,'' \url{https://pytorch.org/docs/stable/generated/torch.cuda.Event.html}, 2022.

\bibitem{linux-page-lru}
\BIBentryALTinterwordspacing
R.~van Riel, ``Page replacement in linux 2.4 memory management,'' in \emph{2001 USENIX Annual Technical Conference (USENIX ATC 01)}.\hskip 1em plus 0.5em minus 0.4em\relax Boston, MA: USENIX Association, Jun. 2001. [Online]. Available: \url{https://www.usenix.org/conference/2001-usenix-annual-technical-conference/page-replacement-linux-24-memory-management}
\BIBentrySTDinterwordspacing

\bibitem{bengio_bs}
Y.~Bengio, ``Practical recommendations for gradient-based training of deep architectures,'' in \emph{Neural networks: Tricks of the trade}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2012, pp. 437--478.

\bibitem{small_bs}
D.~Masters and C.~Luschi, ``Revisiting small batch training for deep neural networks,'' \emph{arXiv preprint arXiv:1804.07612}, 2018.

\bibitem{JJ}
T.~Prabhakar, N.~Bhaskar, T.~Pande, and C.~Kulkarni, ``Joule jotter: An interactive energy meter for metering, monitoring and control,'' in \emph{International Workshop on Demand Response, co-located with the ACM e-Energy}, 2014.

\bibitem{KL_mobile}
\BIBentryALTinterwordspacing
S.~Kim, S.~Oh, and Y.~Yi, ``Minimizing gpu kernel launch overhead in deep learning inference on mobile gpus,'' in \emph{Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications}, ser. HotMobile '21.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2021, p. 57–63. [Online]. Available: \url{https://doi.org/10.1145/3446382.3448606}
\BIBentrySTDinterwordspacing

\end{thebibliography}
