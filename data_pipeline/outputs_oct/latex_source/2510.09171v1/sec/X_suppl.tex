\clearpage
\renewcommand{\thetable}{S\arabic{table}}  
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{page}{1}
\appendix

$\Large{\textbf{Supplementary material}}$
\bigskip

This supplementary material reports further experiment details discussed in the main paper. The document is organized into the following sections:
\begin{itemize}[noitemsep,topsep=0pt]
    \item Section \ref{sec:sup:data_generation}: Data generation.
    \item Section \ref{sec:sup:train_details}: Training details.
    \item Section \ref{sec:sup:additional_study}: Additional study.
    \item Section \ref{sec:sup:dataset_samples}: Evaluation dataset examples.
\end{itemize}


\section{Data generation}
\label{sec:sup:data_generation}

\paragraph{Prompts on object categories generation}

\cref{tab:sup:prompts} provides the designed and template prompts used to generate object categories for each domain.
Designed prompts are used in all cases except for ID-3 in \cref{tab:ablation}, where we use template prompts for ablation study. 
The template is ``\textit{Provide a raw list of} $2000$ \textit{object names from the domain of} \texttt{<domain>} \textit{objects. Here are some examples, which you can include in your list too but also get inspired by} \texttt{<examples>}.'' 
For each domain, we substitute the \texttt{<domain>} and \texttt{<examples>} accordingly. We randomly sample images to match the number of instances per domain in \cref{tab:gene_details}.

\begin{table*}[h]
    \centering
    \caption{Designed and template prompts for object categories generation.}
    \input{tables/suppl_prompts}
    \label{tab:sup:prompts}
\end{table*}

\paragraph{Examples with difference inference steps}
Examples generated by Stable Diffusion Turbo with different inference steps are shown in \cref{fig:sup:hq}. Increasing the number of steps enhances the quality of the objects, but also results in more detailed backgrounds. To balance the higher quality of the object and a less detailed background, we choose images with $5$ inference steps for the ablation study.

\begin{figure*}[h]
\centering
\hspace{-2cm}
\input{fig/suppl_hq}
\caption{Images generated by GDM with inference steps ranging from $1$ to $9$. To minimize artifacts in the object while preventing a detailed background, we select $5$ steps for the ablation study.
 \label{fig:sup:hq}}
\end{figure*}

\paragraph{Background generation details}
The background removal tool used in background generation is RMBG v1.4\footnote{\url{https://huggingface.co/briaai/RMBG-1.4}}. 
We use ICLight with its default parameters and condition the generation on the object category name.


\section{Training Details}
\label{sec:sup:train_details} 

\paragraph{Backbone details}
\cref{tab:sup:backbones} includes further details of the three backbones.

\begin{table}[t]
    \centering
    \caption{Details of backbones.}
    \input{tables/suppl_backbones}
    \label{tab:sup:backbones}
\end{table}

\paragraph{Recall@k loss}
We use $k=\{1,2,4,8\}$ for the recall@k loss, with the two temperatures set to $0.01$ and $1.0$ as in the original work.

\paragraph{InfoNCE loss}
For infoNCE loss, we follow the same strategy for batch construction and training parameters as for the recall@k loss. Each image acts as one query, and its positive and negative pairs depend on the instance-level class label. We use a 0.05 temperature during training.

\paragraph{Contrastive loss}
Regarding the contrastive loss, we treat each image as a query and randomly sample one positive among images with the same instance-level class label. For the negatives, we mine the hardest one in the dataset based on the cosine similarity of the image descriptors. We use a margin of $1$. The learning rate is $10^{-7}$. The batch size is $8$. Other settings are following recall@k loss.

\paragraph{Softmax margin loss}
Softmax margin loss is a classification loss. Following the training process proposed in UnED \citep{ypsilantis2021met}, the backbone remains frozen for the first two epochs, and only the classifier is trained with a learning rate of $10^{-3}$. In the following epochs, the network is trained end-to-end with a $10^{-6}$ learning rate. Since this is a classification loss, no specific curation of the batches is necessary. We use a batch size of $16$. The scale and margin parameters are set at $16$ and $0$, respectively, as in UnED. %Other settings are the same as recall loss.

\paragraph{Training time}
The training process of \oursplus{} with SigLIP takes approximately $2.5$ hours on a single A100 GPU.


\section{Additional study}
\label{sec:sup:additional_study}


\paragraph{Additional study on Objaverse}

When training on Objavese, we match the same dataset sizes ($20$K) as \oursplus{} to ensure a fair comparison (ID2). 
As shown in \cref{tab:sup:objaverse}, increasing the number of Objaverse images to $40$K performs a bit worse (ID-S10). Therefore, we did not increase further.
We additionally tested the impact of real viewpoint variations from Objaverse by switching from 4 views of our main experiment to one view, where only the background differs (ID2 vs ID-S11). There is a small impact. Richer viewpoint variations are for sure worth further investigation.

\input{tables/suppl_additional}

\paragraph{Training real-labeled images with softmax margin loss}
\cref{tab:sup:real_smm} presents the results of training real-labeled images with softmax margin loss on four domains using SigLIP. We observe a consistent improvement over the pre-trained model on average. Real-ALL achieves the best overall performance, particularly on ROP and SOP. Similar to the trend observed with recall@k loss, training on the artwork domain yields the most improvement on MET and mini-ILIAS.

\begin{table*}[h]
    \centering
    \caption{Training on real-labeled images using SigLIP with softmax margin loss.}
% \vspace{-5pt}
\input{tables/suppl_real_smm}
\label{tab:sup:real_smm}
\end{table*}



\section{Evaluation dataset examples}
\label{sec:sup:dataset_samples}

\cref{fig:sup:eval_examples} shows some examples of the queries and positives from the seven test sets.
The examples illustrate the diversity within the image domains and highlight the challenges posed by variations in viewpoint and background between queries and positive matches.
In detail, in the MET dataset, queries consist of photos taken by visitors inside the museum, often have complex backgrounds, whereas the database images, collecting from the museum's website, typically have clean backgrounds. 
R-Oxford, R-Paris, and GLDv2 are landmark datasets where both the query and database images have complex backgrounds.
In the SOP dataset, retailed product images are collected from e-commerce websites, with both query and database having clean backgrounds.
Lastly, INSTRE and mini-ILIAS include multi-domain objects. In INSTRE, both query and database have diverse backgrounds, while in mini-ILIAS, queries have clean backgrounds.

\begin{figure*}[t]
\input{fig/suppl_examples}
\caption{Examples of queries (column 1) and positives (columns 2 $\sim$ 4) from all the test sets.
}
\label{fig:sup:eval_examples}
\end{figure*}

