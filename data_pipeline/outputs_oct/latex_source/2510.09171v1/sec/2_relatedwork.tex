\section{Related work}
\label{sec:relatedwork}

\paragraph{Instance-level representations}
%
Instance-level recognition requires image representations that capture fine-grained object details while distinguishing them from numerous semantically similar classes. Generic models like ResNet~\citep{hzr+16} and CLIP~\citep{rkh+21} struggle in this setting, as they prioritize high-level semantics over instance-specific features. A common solution is fine-tuning pre-trained backbones on domain-specific datasets—such as artwork~\citep{ypsilantis2021met}, landmarks~\citep{lsl+22,sck+23,cas20,ski+24}, or products~\citep{ptm22,ramzi2022hierarchical}—to enhance their ability to differentiate individual instances. Recent efforts focus on universal embeddings~\citep{ypsilantis2023towards} that cover jointly a whole range of domains and tasks. However, models still require fine-tuning with class-supervised learning to acquire the necessary discriminative properties, making the scarcity of high-quality labeled datasets a major challenge. Data augmentation techniques~\citep{ypsilantis2021met} help mitigate this issue by generating diverse variations of an instance from limited samples.
%
The only prior work that also leverages generative models for instance-level tasks~\citep{Sundaram25iclr} fine-tunes a separate model for each instance, requiring a few real images as input. In contrast, our approach trains a single model that generalizes well across objects and domains without relying on any real images.\looseness=-1

\paragraph{Training with synthetic images}
Synthetic data has been used in a variety of computer vision problems, such as object detection~\citep{peng2015learning,rozantsev2015rendering,georgakis2017synthesizing}, segmentation~\citep{chen2019learning,ros2016synthia}, autonomous driving~\citep{abu2018augmented}, object pose estimation~\citep{cai2022ove6d, labbe2020cosypose}, 3D-tasks~\citep{chang2015shapenet}, and recently for representation learning~\citep{tian2024stablerep, wu2023not}. An early practice is to cut the real objects and paste them onto backgrounds to generate synthetic images for instance or object detection~\citep{dwibedi2017cut, georgakis2017synthesizing}. However, challenges remain in reducing the boundary artifacts and achieving consistent lighting conditions between the object and background, as these problems often result in unrealistic composite images.
More recently, the main sources of synthetic images are computer graphics pipeline or rendering engines~\citep{mahmood2019amass}, generative adversarial networks (GAN)~\citep{besnier2020dataset,brock2018large}, and text-to-image GDM~\citep{fan2024scaling,sariyildiz2023fake}. Images generated through rendering engines often suffer from domain gap when compared to real-world test images, requiring domain adaptation techniques to mitigate the gap during training. In contrast, GAN and GDM produce more realistic images that do not typically require post-generation domain adaptation~\citep{wang2020self6d}. Text-to-image GDM, in particular, offers a higher degree of control in the image generation process, for example, changing the background of the target object using text prompts~\citep{mokady2023null,raj2023dreambooth3d,geng2024instructdiffusion,zhang2023adding}. This ability to control image features through text makes GDM particularly valuable for generating diverse images, which is crucial for representation learning~\citep{tian2024stablerep, wu2023not}. However, synthesizing images for instance-level task is not trivial, as it requires generating a synthetic object under various conditions while preserving its structure and texture. 

\paragraph{Metric learning for image retrieval}
%
Given a training dataset, the most common approach for training deep representation networks for image retrieval is supervised learning using categorical labels.
As a result, a large number of methods have proposed classification-based losses~\citep{zw18,dgx+19,tdt20,qss+19,kim2020proxy}.
Despite not directly optimizing the pairwise distance metric that is used at test time, such approaches achieve very good performance, especially when combined with propagating the representation across examples~\citep{evt+20,sel21,kpm+23}. 
Other methods directly optimize the distance metric with pairwise losses. 
These most often rely on hand-crafted loss functions, such as the most popular contrastive~\citep{hcl06}, and triplet loss~\citep{skp+15}, by postulating a correlation between such a training objective and the test time objective which is typically an information retrieval metric. 
Finding informative pairs and triplets~\citep{mbl20,rms+20,sxj+15,sohn16} appears to be very important.
As a natural follow-up, a few recent methods directly optimized differentiable approximations of retrieval metrics, such as average precision~\citep{rmp+20,hls18,rar+19,ramzi2021robust,ramzi2022hierarchical} and recall~\citep{ptm22}. 
In this work, we rely on recall@k~\citep{ptm22} as a loss function which is demonstrating top results on a variety of benchmarks in the literature and does not require hard negative mining.
%
Self-supervised~\citep{kkc+} methods exist as well and are shown effective, but are tested only on training data from the target distributions, which is not a realistic setup. 
%
A recent alternative to CLIP~\citep{rkh+21}, called Unicom~\citep{anunicom}, trains on LAION 400M \citep{schuhmann2021laion}, treats captions as weak annotations to perform text-based clustering, and reformulates the learning as a classification task. Their results show improvements in a set of different retrieval datasets, including instance-level ones.
Alternatively, we propose leveraging synthetic data to introduce an extensive collection of objects with diverse variations into the training dataset.


