\section{Discussion}
\label{sec:discuss}


\begin{comment}
    

In this section, we discuss relevant aspects of the our approach.

\textbf{Can LLMs Interpret Ethical Information in Textual Prompts? \todo{OR} Ethical reasoning capabilities of LLMs (since this is similar with RQ1.)}

\todo{The answer will be a summary of the results, not sure. Patrizio, please check this.}


\textbf{Toward Model-Based Ethical Evaluation}

This is also 


\textbf{Constructing Ethical Profiles through Passive Interaction}

The end goal of our approach is to automatically generate user ethical profiles, leveraging the ethical reasoning abilities of LLMs to analyze user behavior in real time. If LLMs can consistently interpret ethically relevant scenarios, then they can serve as inference engines for future systems that monitor user interactions (e.g., via AR glasses or smart agents) and extract preference signals from context.

These profiles need not be static or deterministic. Rather, they can evolve as a form of passive ethical memory that captures the userâ€™s stance across dimensions such as fairness, harm, or loyalty. This enables the design of agents that reason within user-defined moral constraints, without requiring explicit rule engineering or ongoing manual feedback.


\textbf{Implications for Software Engineering Practice}

From a software engineering perspective, our approach serves as a practical and scalable method to incorporate an ethical reasoning mechanism into the development of software systems. By using LLMs as modular evaluators, ethical reasoning mechanisms can be embedded as an operational layer in software systems, enabling them to make decisions that align with user preferences.


*******************
This framework aligns with software engineering research in two directions. First, it provides a method for rapidly probing the ethical capabilities of generative systems that may later be integrated into user-facing software agents. Second, it sets the foundation for ethically-informed profiling methods that can be automated, auditable, and adaptable to different application domains, including healthcare assistants, AR interfaces, and autonomous agents. By decoupling the evaluation from manual supervision, we advance toward scalable, trustworthy AI modules that operate within well-defined moral boundaries.

\end{comment}


%\subsection{RQ1: Do LLMs demonstrate the capacity for ethical reasoning when presented with ethically charged scenarios?}

\textbf{RQ1.} Our findings provide evidence that state-of-the-art LLMs can engage in ethical reasoning when presented with complex, real-world made \major{explanations of acceptability}. Without fine-tuning or examples, models consistently identified the most applicable ethical theory and made acceptability explanations with substantial inter-model agreement. This capability suggests that LLMs possess an implicit grasp of moral reasoning principles, grounded in their pretraining on large-scale textual corpora. From an SE perspective, this opens the door to using LLMs as ethical reasoning modules in decision-making pipelines, such as requirement negotiation, user modeling, or system auditing.

%\subsection{RQ2: Do LLMs Reason Consistently Across Models and Scenarios?}

\smallskip

\textbf{RQ2.} Quantitative results showed that models converge more strongly on binary moral acceptability (86.7\% BAR) than on ethical theory classification (73.3\% TCR). While this difference reflects the higher abstraction level of theoretical judgments, the level of agreement observed is non-trivial. The scenario-dependent variability in TCR reveals an important feature: model disagreement tends to reflect ethical ambiguity inherent in the scenario rather than arbitrary noise. This suggests that ensemble disagreement can be used as a proxy for moral uncertainty, enabling software systems to trigger escalation or human intervention when LLMs disagree sharply.

%\subsection{RQ3: How Do LLMs Compare to Human Experts?}

\smallskip

\textbf{RQ3.} %Our comparison revealed substantial alignment between LLMs and human experts. 
\major{Overall, LLMs exhibit non-trivial agreement with experts that is more pronounced in prevalent classes and weaker on rare or edge cases.} Scenarios that elicited strong agreement among experts tended to also show high inter-model LLM agreement, and vice versa. This convergence reinforces the reliability of LLMs in interpreting familiar or structurally simple moral scenarios. Divergences, especially in edge cases, underscore the importance of hybrid systems that combine automated reasoning with human oversight. For SE applications involving legal, regulatory, or safety-critical implications, LLM-based profiling should not be deployed as an isolated decision-maker but as a complementary module.

%\subsection{RQ4: What Characterizes the Structure of LLM Moral Expanations?}

\smallskip

\textbf{RQ4.} Qualitative analyses demonstrated that LLMs generate explanations that are lexically diverse but conceptually coherent. Despite low textual similarity across models, explanations consistently aligned with the chosen moral theory in over 90\% of cases. Models blended terminology from multiple ethical traditions in natural, context-sensitive ways, %mirroring 
\major{tending to reflect} how human reasoners combine principles, consequences, and character-based considerations. This expressive flexibility is critical for ethical profiling, as it enables the detection of user-aligned reasoning patterns across different moral framings. Moreover, the compactness of most explanations (single sentences) and their theoretical consistency suggest that LLMs are capable of producing tractable, auditable moral outputs suitable for runtime interpretation and logging.
%required by metareview
\begin{tcolorbox}[colback=gray!10,
                  colframe=black,
                  arc=4mm,
                  boxrule=0.8pt,
                  left=2mm, right=2mm, top=1mm, bottom=1mm]
                  \major{\textbf{Agreement $\ne$ correctness.} Our design surfaces stability and ambiguity signals, it does not certify normative accuracy. In SE practice, high agreement supports automation with audit, while low agreement recommends human-in-the-loop escalation.}
\end{tcolorbox}                  

\smallskip


\textbf{Limitations and Scope of Validity.} While promising, our findings are bounded by some limitations:

\smallskip

\noindent\textit{Theoretical coverage.} We focus on three major ethical theories utilitarianism, deontology, and virtue ethics due to their widespread adoption in software engineering practice and education~\cite{vaniea2018securitytrolley}. These ethical theories provide well-established foundations for analyzing ethical dilemmas in technology contexts. While alternative theories such as care ethics or contractualism are less commonly applied, they offer valuable perspectives that could enrich ethical analyses. Future work may explore the integration of these additional frameworks to capture a broader spectrum of moral reasoning in software engineering.

\smallskip

\noindent\textit{Scenario framing.} Our prompts use concise, decontextualized scenarios. Richer formats (e.g., dialogues, system logs) may affect model interpretation. Our current prompts are decontextualized statements. An important next step is to apply the same ethical reasoning pipeline (Figure~\ref{fig:approach2}) to richer input modalities, including:
(i) chat transcripts from developer-agent interactions;
(ii) logs of user decisions in ethically sensitive configurations;
(iii) behavioral signals from simulation environments or system telemetry.
This would move the profiling process closer to real-time, context-aware ethical inference.

\smallskip

\noindent\textit{Zero-shot constraints.} All reasoning is performed without memory or clarification. Interactive or multi-turn reasoning may yield different profiles. An ethical profile need not be static. As users interact with a system, their decisions may reveal shifts in priorities, trade-offs, or ethical boundaries. Future work should implement an ethical memory module that incrementally updates a user's profile over time, capturing both stable dispositions and contextual shifts. This requires designing a temporal profiling architecture that tracks ethical indicators across scenarios and resolutions.

\smallskip
   
\noindent\textit{Agreement $\ne$ correctness.} Convergence does not imply normative accuracy. Human biases and model alignment may coincide but remain ethically questionable. In real deployments, users may reject or revise the moral judgments made by the system. Building on our current architecture, we envision an interactive loop in which: (i) the system proposes an ethical explanation; (ii) the user confirms, modifies, or rejects the reasoning; (iii) the profile is updated accordingly. This would enable both user agency and model refinement over time, reducing the risk of misaligned ethical personalization. 

\smallskip

\major{\noindent\textit{Prompt sensitivity.} Our zero-shot, single-turn protocol deliberately controls for instruction complexity; however, model behavior can still be sensitive to seemingly innocuous variations in prompt phrasing, formatting, or input length. We therefore treat prompt sensitivity as a threat to validity and an explicit boundary of our claims. A systematic sensitivity analysis is left as future work. In practice, we recommend freezing prompt templates in repositories and reporting all formatting details that might affect reproducibility.}

\smallskip

Even if the findings support the potential viability, these limitations suggest caution in direct deployment and highlight the need for further validation before integrating LLM-based profiling into high-stakes SE systems. Our results position LLMs as viable components for modular ethical reasoning in SE. Possible use cases include: \textit{decision auditing} for moral rationales generation for SE tool outputs (e.g., in requirements prioritization or resource allocation); \textit{autonomy triage} to route decisions to humans when LLMs disagree, reducing risk in ethically charged contexts; \textit{agent personalization} to tailor behavior of autonomous SE agents based on learned ethical user profiles. More broadly, the ability to extract consistent moral structure from language enables a shift from static ethics-as-checklists to adaptive, traceable, and user-aligned ethical cognition in engineered systems.