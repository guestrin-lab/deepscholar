\section{Introduction}
\label{sec:intro}

%\noindent\textbf{\major{Autonomous systems and the ethical design challenge.}}
Autonomous systems are increasingly becoming an integral part of our daily lives across diverse domains~\cite{suri2023software,jedlickova2024ensuring}. These systems can operate independently without any human intervention and make decisions acting on behalf of their users~\cite{InsightsISSRE,waldman2019power,sharma2022recent,anderson2018artificial}. Their rapid growth brings both opportunities and challenges. From a software engineering perspective, as these systems become pervasive, a key challenge is designing systems that, beyond meeting technical requirements, also account for ethical considerations~\cite{autili2025engineering,de2024engineering,hendrycks2020aligning,alidoosti2022incorporating,svegliato2021ethically}.

\smallskip
\noindent\textbf{\major{SE ethics.}}
%: principles and alignment mechanisms.}}
Recently, various studies have focused on the ethical implications of these software-intensive systems on individuals and society~\cite{alidoosti2022incorporating,tolmeijer2020implementations,inverardi2019ethics,inverardi2022ethical,cervantes2020toward}. Software engineering ethics encompasses principles and rules that guide engineers' decisions throughout the design and development process~\cite{alidoosti2025exploring}. Various approaches have also been introduced that ensure that systems align with broad ethical values like fairness, transparency, and safety~\cite{jedlivckova2024ethical,bremner2019proactive,winfield2014towards,Winfield2019,alidoosti2021ethics,townsend2022pluralistic}.

\smallskip
%\noindent\textbf{\major{From abstract norms to user-aligned ethical profiles.}}
\noindent\textbf{\major{Ethics operationalization.}}
Moreover, beyond abstract ethical norms, recent studies focus on operationalizing end users' ethical preferences directly into the software engineering process~\cite{winter2019advancing,memon2023automated,MemonAFSI24,donati2024representing,de2024engineering,barn2016you}. Through profiling techniques \major{(}including questionnaires, surveys, product reviews\major{,} etc.\major{)}, these studies propose approaches to capture user ethical preferences, generate their ethical profiles, and integrate \major{them} into these systems~\cite{alfieri2022exosoul,autili2025engineering,cacmInverardi2019,InverardiMP23,DiRuscioIMN24}. The ethical profile is a structured representation of the ethical preferences of the user that autonomous systems can leverage to adjust their behavior and make decisions aligned with the user\major{'s} ethical values~\cite{alfieri2022exosoul}. Embedding user ethical profiles into system design would not only enhance trust and accountability, as highlighted by regulatory bodies including GDPR~\cite{voigt2017eu}, the AI Act~\cite{EU_AI_Act_Proposal}, and the Ethics Guidelines for Trustworthy AI~\cite{ai2019high}, but also ensure that systems reflect and respect the ethical preferences of their users. However, relying on manual input from users to generate their profiles limits their scope and adaptability, as users' ethical preferences vary with the change in context. Hence, it becomes impractical to expect users to provide input for every possible situation, introducing the challenge of automating the generation of ethical profiles.

\smallskip
%\noindent\textbf{\major{LLMs as candidates for ethical reasoning.}}
\noindent\textbf{\major{LLM-based ethical reasoning.}}
Recent advances in generative AI, especially large language models (LLMs), have positioned these models as powerful tools capable of engaging in ethical reasoning~\cite{han2022aligning}. Various studies have explored the use of LLMs to assess their moral reasoning abilities in specific applications~\cite{alshami2023harnessing,DBLP:conf/emnlp/RaoKTAC23,DBLP:conf/iclr/TennantHM25}. Building on this, we take a step toward evaluating whether large language models (LLMs) can effectively reason about ethically significant content in real-life scenarios. To this end, we propose a lightweight, fully automated framework that examines the potential of LLMs to identify ethically relevant information, to support the automated generation of user-aligned ethical profiles, and \major{to integrate them} into software engineering (SE) pipelines.

\smallskip
\noindent\textbf{\major{Setup and RQs.}}
We present 16 LLMs (as shown in Table~\ref{tab:llms_summary}) with 30 ethically charged statements. For each statement, the models are prompted to identify the most applicable ethical theory according to the action detailed in the statement, assess whether the action is morally acceptable according to the selected theory, and explain the reasoning behind their choice. Importantly, as discussed in Section \ref{sec:scenario}, selecting an ethical theory does not imply that the action described in the statement is justified by that theory; rather, it serves as a normative lens through which the moral acceptability of the action is evaluated. This distinction enables meaningful binary judgments (acceptable/unacceptable) based on whether the action complies with the principles of the selected theory. To establish a \major{comparative baseline}, we replicate the same process with three professors, experts with extensive knowledge in applied ethics and philosophy. We then evaluated the responses of LLMs and experts, exploring both alignment and divergence in their judgments and their explanations. To assess the potential of LLMs as ethical reasoning modules in software engineering, we pose the following research questions:

\smallskip
\noindent\textbf{RQ1:} Do LLMs demonstrate the capacity for ethical reasoning when presented with ethically charged scenarios?\\
\textbf{RQ2:} To what extent do different LLMs agree on ethical theories and moral acceptability of the scenarios?\\
\textbf{RQ3:} How do the agreements among LLMs compare to those among the human experts?\\
\textbf{RQ4:} What qualitative characteristics emerge in the explanations produced by LLMs?

\smallskip
\noindent\textbf{\major{Methodology.}}
These questions are designed to evaluate whether current generative models can function not only as isolated agents but also as components in robust, transparent, and auditable decision pipelines for software engineering. To address RQ1, we prompted 16 LLMs with 30 ethical scenarios covering a range of common, real-life contexts that involve ethically charged situations. We then analyzed LLMs' responses to determine their ability to recognize the action described in the scenario, its correspondence to one of the ethical theories (utilitarianism, deontology, and virtue ethics), and determine whether the action described is morally acceptable according to the selected ethical theory. To address RQ2, we computed inter-model agreement using Theory Consistency Rate (TCR) to identify the percentage of prompts for which the models selected the same ethical theory and Binary Agreement Rate \major{metrics} (BAR) to identify the percentage of prompts for which different models agreed on whether the action is morally acceptable in accordance with the selected ethical theory. These metrics assess whether different LLM models apply comparable ethical reasoning structures under identical conditions. To address RQ3, we collected questionnaires from the three professors, expert ethicists, and we presented them with the same scenarios previously shown to the LLMs. We surveyed them by replicating the same process we followed with LLMs. We then compared the experts’ judgments with the responses generated by the LLMs using z-score\major{s}. To address RQ4, we conducted a multi-layered qualitative analysis of the free-text explanations provided by the LLMs. We applied lexical similarity metrics (TF-IDF and cosine similarity), dimensionality reduction (PCA, t-SNE), and topic modeling (LDA) to examine variation in linguistic form and underlying conceptual structure. A manual alignment study further assessed whether the explanations were consistent with the ethical theories selected by the models. This combined approach enabled the evaluation of both the coherence and the diversity of the explanations.

\smallskip
\noindent\textbf{\major{LLM-aided ethical reasoning.}}
From a software engineering perspective, the results indicate that LLMs are capable of serving as modular evaluators of ethical context. They show capabilities that can be utilized as a possible way to automate the generation of user ethical profiles. \major{This paper does not present a full automated ethical profile generator. Instead, it evaluates whether state-of-the-art LLMs can serve as ethical reasoning modules, that is, whether they can consistently select an interpretive moral lens and produce theory-aligned acceptability explanations under zero-shot conditions. Within SE pipelines, this component enables: (i) decision auditing with concise theory-grounded rationales; (ii) triage and escalation when model disagreement signals moral ambiguity; (iii) seed signals for user-aligned profiling, where interpretive patterns accumulate into dynamic profiles.}

\smallskip
\noindent\textbf{\major{Contributions.}}
The main contributions of the paper are:
\begin{itemize}[left=0pt]
\item An automated framework for quantifying agreement and divergence among LLMs on non-trivial reasoning tasks, using ethical scenarios as a representative benchmark.
\item An empirical analysis of the consistency and diversity of 16 LLMs, measuring both classification agreement and the qualitative variety in explanations.
\item An evidence-based discussion on the trade-offs of single-LLM versus multi-LLM approaches for judgment and reasoning in SE support systems.
\item A comparison of the LLM outcomes and those of expert human judgments, identifying areas of alignment and persistent divergence.
\item A fully reproducible experimental pipeline and dataset, with all artifacts released for independent verification.
\end{itemize}

\smallskip
\noindent\textbf{\major{Paper roadmap.}}
The rest of the paper is organized as follows.
Section~\ref{sec:settingcontext} introduces the theoretical foundations of ethical reasoning and motivates the need for automated ethical profiling in SE.
Section~\ref{sec:approach} provides an overview of our evaluation framework and its main components.
Section~\ref{sec:quantitative} reports inter-model and human–LLM agreement metrics to assess consistency in ethical reasoning.
Section~\ref{sec:qualitative} analyzes the LLM-generated explanations through lexical, conceptual, and theory-alignment perspectives.
Section~\ref{sec:discuss} discusses the implications of our findings, identifies limitations, and outlines practical use cases.
Section~\ref{sec:related} reviews related work on ethical AI, moral reasoning, and LLM evaluation.
Finally, Section~\ref{sec:conclude} concludes the paper and highlights directions for future research.


\begin{comment}


\section{Introduction}
\label{sec:intro}


Autonomous systems are increasingly becoming an integral part of our daily lives across diverse domains~\cite{suri2023software,jedlickova2024ensuring}. These systems can operate independently without any human intervention and make decisions acting on behalf of their users~\cite{InsightsISSRE,waldman2019power,sharma2022recent,anderson2018artificial}. Their rapid growth brings both opportunities and challenges. From a software engineering perspective, as these systems become pervasive, a key challenge is designing systems that, beyond meeting technical requirements, also account for ethical considerations~\cite{autili2025engineering,de2024engineering,hendrycks2020aligning,alidoosti2022incorporating,svegliato2021ethically}. Recently, various studies have focused on the ethical implications of these software-intensive systems on individuals and society~\cite{alidoosti2022incorporating,tolmeijer2020implementations,inverardi2019ethics,inverardi2022ethical,cervantes2020toward}. Software engineering ethics encompasses principles and rules that guide engineers' decisions throughout the design and development process~\cite{alidoosti2025exploring}. Various approaches have also been introduced that ensure that systems align with broad ethical values like fairness, transparency, and safety~\cite{jedlivckova2024ethical,bremner2019proactive,winfield2014towards,Winfield2019,alidoosti2021ethics,townsend2022pluralistic}. Moreover, beyond abstract ethical norms, recent studies focus on operationalizing end users' ethical preferences directly into the software engineering process~\cite{winter2019advancing,memon2023automated,MemonAFSI24,donati2024representing,de2024engineering,barn2016you}. Through profiling techniques \major{(}including questionnaires, surveys, product reviews\major{,} etc.\major{)}, these studies propose approaches to capture user ethical preferences, generate their ethical profiles, and integrate \major{them} into these systems~\cite{alfieri2022exosoul,autili2025engineering,cacmInverardi2019,InverardiMP23,DiRuscioIMN24}. The ethical profile is a structured representation of the ethical preferences of the user that autonomous systems can leverage to adjust their behavior and make decisions aligned with the user\major{'s} ethical values~\cite{alfieri2022exosoul}. Embedding user ethical profiles into system design would not only enhance trust and accountability, as highlighted by regulatory bodies including GDPR~\cite{voigt2017eu}, the AI Act~\cite{EU_AI_Act_Proposal}, and the Ethics Guidelines for Trustworthy AI~\cite{ai2019high}, but also ensure that systems reflect and respect the ethical preferences of their users. However, relying on manual input from users to generate their profiles limits their scope and adaptability, as users' ethical preferences vary with the change in context. Hence, it becomes impractical to expect users to provide input for every possible situation, introducing the challenge of automating the generation of ethical profiles.


Recent advances in generative AI, especially large language models (LLMs), have positioned these models as powerful tools capable of engaging in ethical reasoning~\cite{han2022aligning}. Various studies have explored the use of LLMs to assess their moral reasoning abilities in specific applications~\cite{alshami2023harnessing,DBLP:conf/emnlp/RaoKTAC23,DBLP:conf/iclr/TennantHM25}.
%
%
%Building on this, in this work, we take a step toward our long-term goal of automated ethical profile generation by proposing a lightweight, fully automated framework designed to explore whether LLMs are capable of reasoning about ethically significant content and if they can be leveraged to automate the generation of ethical profiles.
Building on this, we take a step toward evaluating whether large language models (LLMs) can effectively reason about ethically significant content in real-life scenarios. To this end, we propose a lightweight, fully automated framework that examines the potential of LLMs to identify ethically relevant information, to support the automated generation of user-aligned ethical profiles, and %their integration
\major{to integrate them} into software engineering (SE) pipelines. 
%
%
We present 16 LLMs (as shown in Table~\ref{tab:llms_summary}) with 30 ethically charged statements. For each statement, the models are prompted to identify the most applicable ethical theory according to the action detailed in the statement, assess whether the action is morally acceptable according to the selected theory, and explain the reasoning behind their choice.
Importantly, as discussed in Section \ref{sec:scenario}, selecting an ethical theory does not imply that the action described in the statement is justified by that theory; rather, it serves as a normative lens through which the moral acceptability of the action is evaluated. This distinction enables meaningful binary judgments (acceptable/unacceptable) based on whether the action complies with the principles of the selected theory. To establish a %ground truth
\major{comparative baseline}, we replicate the same process with three professors, experts with extensive knowledge in applied ethics and philosophy. We then evaluated the responses of LLMs and experts, exploring both alignment and divergence in their judgments and their explanations. To assess the potential of LLMs as ethical reasoning modules in software engineering, we pose the following research questions:

\smallskip
\noindent\textbf{RQ1:} Do LLMs demonstrate the capacity for ethical reasoning when presented with ethically charged scenarios?\\
\textbf{RQ2:} To what extent do different LLMs agree on ethical theories and moral acceptability of the scenarios?\\
\textbf{RQ3:} How do the agreements among LLMs compare to those among the human experts?\\
\textbf{RQ4:} What qualitative characteristics emerge in the explanations produced by LLMs?


\smallskip
These questions are designed to evaluate whether current generative models can function not only as isolated agents but also as components in robust, transparent, and auditable decision pipelines for software engineering. To address RQ1, we prompted 16 LLMs with 30 ethical scenarios %that capture everyday ethical decisions across a spectrum of normative frameworks,
covering a range of common, real-life contexts that involve ethically charged situations. 
%and analyze their responses to determine whether they can recognize the applicability of any of the three ethical theories including utilitarianism, deontology, and virtue ethics to the scenario
We then analyzed LLMs' responses to determine their ability to recognize the action described in the scenario, its correspondence to one of the ethical theories (utilitarianism, deontology, and virtue ethics), and determine whether the action described is morally acceptable according to the selected ethical theory. To address RQ2, we computed inter-model agreement using Theory Consistency Rate (TCR) to identify the percentage of prompts for which %both
the models selected the same ethical theory and Binary Agreement Rate %metrics
(BAR) \major{metrics} to identify the percentage of prompts for which different models agreed on whether the action is morally acceptable in accordance with the selected ethical theory. These metrics assess whether different LLM models apply comparable ethical reasoning structures under identical conditions. To address RQ3, we collected questionnaires from the three professors, expert ethicists, and we presented them with the same scenarios previously shown to the LLMs. We surveyed them by replicating the same process we followed with LLMs. We then compared the experts’ judgments with the responses generated by the LLMs using z-score\major{s}. To address RQ4, we conducted a multi-layered qualitative analysis of the free-text explanations provided by the LLMs. We applied lexical similarity metrics (TF-IDF and cosine similarity), dimensionality reduction (PCA, t-SNE), and topic modeling (LDA) to examine variation in linguistic form and underlying conceptual structure. A manual alignment study further assessed whether the explanations were consistent with the ethical theories selected by the models. This combined approach enabled the evaluation of both the coherence and the diversity of the explanations.

From a software engineering perspective, the results indicate that LLMs are capable of serving as modular evaluators of ethical context. They show capabilities that can be utilized as a possible way to automate the generation of user ethical profiles. % the following incorporates Rev A on significance and Rev B on focus of contribution; consistent with the current description of the paper's sections
\major{This paper does not present a full automated ethical profile generator. Instead, it evaluates whether state-of-the-art LLMs can serve as ethical reasoning modules, that is, whether they can consistently select an interpretive moral lens and produce theory-aligned acceptability judgments and explanations under zero-shot conditions. Within SE pipelines, this component enables: (i) decision auditing with concise theory-grounded rationales; (ii) triage and escalation when model disagreement signals moral ambiguity; (iii) seed signals for user-aligned profiling, where interpretive patterns accumulate into dynamic profiles.}


The main contributions of the paper are:
\begin{itemize}[left=0pt]
    \item An automated framework for quantifying agreement and divergence among LLMs on non-trivial reasoning tasks, using ethical scenarios as a representative benchmark.
    \item An empirical analysis of the consistency and diversity of 16 LLMs, measuring both classification agreement and the qualitative variety in explanations.
    \item An evidence-based discussion on the trade-offs of single-LLM versus multi-LLM approaches for judgment and reasoning in SE support systems.
    \item A comparison of the LLM outcomes and those of expert human judgments, identifying areas of alignment and persistent divergence.
    \item A fully reproducible experimental pipeline and dataset, with all artifacts released for independent verification.
    %\item We release the full dataset, code, and analysis pipeline for transparent benchmarking and future extension.
\end{itemize}


The rest of the paper is organized as follows. 
Section~\ref{sec:settingcontext} introduces the theoretical foundations of ethical reasoning and motivates the need for automated ethical profiling in SE. 
Section~\ref{sec:approach} provides an overview of our evaluation framework and its main components. 
Section~\ref{sec:quantitative} reports inter-model and human–LLM agreement metrics to assess consistency in ethical reasoning. 
Section~\ref{sec:qualitative} analyzes the LLM-generated explanations through lexical, conceptual, and theory-alignment perspectives. 
Section~\ref{sec:discuss} discusses the implications of our findings, identifies limitations, and outlines practical use cases. 
Section~\ref{sec:related} reviews related work on ethical AI, moral reasoning, and LLM evaluation. 
Finally, Section~\ref{sec:conclude} concludes the paper and highlights directions for future research.


\end{comment}
