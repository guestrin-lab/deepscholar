\section{Qualitative Analysis of LLM Explanations}
\label{sec:qualitative}

%\subsection{Motivation and Analytical Strategy}

The ability of an LLM to select an appropriate ethical theory or to judge an action's acceptability (as quantitatively analyzed in Section~\ref{sec:quantitative}) is only part of what constitutes ethical reasoning. In software engineering contexts where systems must explain decisions to users, auditors, or regulators, the \textit{explanatory layer} becomes essential. That is, this section addresses \textbf{RQ4} by complementing our quantitative agreement analysis with a qualitative investigation of the linguistic and conceptual properties of the explanations generated by LLMs. The structure, content, and coherence of such explanations directly affect the trustworthiness, transparency, and usefulness of the AI system involved. We aim to determine whether models produce morally meaningful, internally coherent, and theory-consistent explanations. We also explore how such explanations vary across models, and whether surface-level linguistic diversity masks deeper conceptual alignment. To this end, we design a multi-tiered qualitative analysis based on investigations that include %lexically and syntactically
\major{lexical and syntactic} diversification of the explanations, low lexical similarity implications for conceptual divergence, consistency of explanations with the ethical theory selected by the model, moral vocabularies and normative traditions emerging across the corpus. To answer these, we combine computational techniques (TF-IDF similarity, LDA topic modeling, clustering, dimensionality reduction) with structured manual review. This dual approach balances scale and interpretability, enabling us to identify both aggregate trends and fine-grained misalignments.

\smallskip

\textit{Lexical Diversity and Similarity.} We first assess lexical similarity using TF-IDF vectorization and pairwise cosine similarity, computed both across all explanations globally and within each scenario. Despite a consistent prompt structure and fixed task format, we observe substantial variation in surface form. The average pairwise cosine similarity across scenarios is 0.11, with a min of 0.02 and a max of 0.17 (Figure~\ref{fig:similarityQ}). These low values indicate that models rarely repeat phrasings, even when agreeing on the same theory and judgment. This suggests that explanations are 
%not rigidly templated or memorized, but are 
generated with contextual variation. This result implies that LLMs are not simply giving fixed moral explanations but are capable of producing distinct, scenario-sensitive moral language. It also poses a methodological challenge; similarity metrics that rely on surface overlap will systematically underestimate conceptual agreement unless paraphrase-aware techniques are employed.

%\smallskip

\textit{Semantic Clustering and Model Positioning.} To understand how explanations vary semantically, we applied Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) to the TF-IDF vector space. These projections allow us to visualize clusters of models and identify potential outliers.
%
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figures/qualitative/plot_llm_clustering.png}
\vspace{-0.6cm}
\caption{PCA Clustering}
\label{fig:pcaclustering}
\end{figure}
\begin{figure*}[t]
\centering
\begin{minipage}{0.6\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/qualitative/plot_tsne_clusters.png}
  \caption{t-SNE Clustering}
  \label{fig:tsneclustering}
\end{minipage}%
\hfill
\begin{minipage}{0.34\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/qualitative/plot_similarity_per_question2.png}
  \vspace{-0.6cm}
  \caption{Similarity per question}
  \label{fig:similarityQ}
\end{minipage}
\vspace{0.5cm}
\end{figure*}
%
As shown in Figures~\ref{fig:pcaclustering} and~\ref{fig:tsneclustering}, most models occupy a dense central region, indicating that their explanations are semantically similar at the coarse level. A few models, notably COHERE and DEEPSEEK-R1, appear in peripheral regions. Manual inspection reveals that this divergence is primarily due to stylistic verbosity or recurrent syntactic structures, rather than shifts in ethical stance. We find no evidence that any model persistently aligns with a particular ethical theory in its language use alone. Instead, model positioning appears to reflect stylistic preferences rather than moral commitments. This further supports the hypothesis that surface diversity does not equate to conceptual inconsistency.
%
\begin{figure}[ht]
\centering
%\includegraphics[width=0.5\textwidth]{figures/qualitative/plot_llm_heatmap2.png}
%
%
%
% cropped out the legend to increase the labels font as requested by RevB, the numbers are in the boxes so the legend is not needed
%
%
%
\includegraphics[width=0.50\textwidth, clip, trim=0 0 88 0]{figures/qualitative/plot_llm_heatmap2.png}
\caption{Semantic similarity heatmap}
\label{fig:heatmap}
\end{figure}
%
The heatmap in Figure~\ref{fig:heatmap} confirms that there is no large cluster of highly similar models; most LLMs are only weakly related to each other in their use of language, specifically LLMs with the same family derivation.

\smallskip
\major{
\textit{Topic Modeling and Moral Vocabulary.} We applied Latent Dirichlet Allocation (LDA) to the corpus of model explanations to uncover recurrent moral themes and to support downstream, user-directed interpretation. We used a standard preprocessing pipeline (lower-casing, tokenization, stopword removal, lemmatization, and bigram detection), built a filtered dictionary and bag-of-words corpus, and trained LDA models for a range of topic counts \(k\). Model selection was guided by the coherence curve \(C_v\), which improved up to \(k=12\) and then showed negligible gains, indicating that twelve topics provide an adequate balance between semantic granularity and interpretability (Figure~\ref{fig:ldacoherence}). This choice is therefore empirically grounded rather than arbitrary. For each topic we inspected the highest-weight tokens and representative explanations nearest to the topic centroid to propose a short, human-readable descriptor. Crucially, this labeling is descriptive and domain/application-oriented: it illustrates how an analyst or practitioner can use our system to surface moral vocabulary and attach domain-specific meanings, rather than asserting a fixed ontology. We report some of the labeling applied to the actual clusters (Figure \ref{fig:ldatopics}), the complete labeling table, including the token-to-label mapping and per-topic exemplar explanations, is provided in the replication package.}
\begin{itemize}
    \item \major{1 - Harm and character responsibility (Tokens such as \textit{harm, protecting, character, behavior, good} indicate emphasis on avoiding harm through responsible conduct.)}
    \item \major{2 - Normative framing and intervention (Presence of \textit{deontological, virtue, privacy, safety, intervening} reflects meta-theoretical framing and protective action.)}
    \item \major{...}
    \item \major{12 - Justice, integrity, and solidarity (\textit{justice, rules, duties, integrity, solidarity, courage} signals rule-guided fairness and character strength.)}
\end{itemize}
\major{\noindent Importantly, no single topic dominates the corpus. Explanations often blend multiple topics within a single rationale, indicating flexible use of moral vocabulary across theories. This pluralism is expected in ethical discourse and is compatible with our goal: the system exposes structured moral themes and their lexical supports, while leaving the interpretive labeling to the analyst’s aims and domain constraints.}
%\textit{Topic Modeling and Moral Vocabulary.} To ensure that the topic modeling reflected meaningful and stable moral themes, we evaluated LDA coherence across varying numbers of topics. Coherence scores quantify the degree to which the most probable words within each topic co-occur in the corpus, and thus provide a proxy for semantic interpretability. As shown in Figure~\ref{fig:ldacoherence}, coherence increased monotonically up to twelve topics and then plateaued, suggesting that twelve topics capture the major explanatory structures without overfitting or fragmenting the semantic space.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{figures/qualitative/plot_coherence_scores.png}
\caption{LDA Coherence Score}
\label{fig:ldacoherence}
\end{figure}
%The selection of twelve topics is therefore grounded in empirical optimization, rather than arbitrary selection, and allows us to balance granularity with interpretability. Each topic, when qualitatively examined, corresponds to recognizable ethical constructs (e.g., obligation, utility, virtue, autonomy), reinforcing the validity of the LDA decomposition. This ensures that subsequent interpretation and alignment with ethical traditions are based on a semantically coherent foundation.
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/qualitative/plot_lda_topics_omitted.png}
\caption{\major{LDA topic excerpt. The full image is available in the replication package.}}
\vspace{0.5pt}
\label{fig:ldatopics}
\end{figure}
%To explore deeper conceptual content, we applied Latent Dirichlet Allocation (LDA) to the corpus of explanations, aiming to uncover latent moral themes and trace their relation to ethical traditions. The model was optimized for coherence, which stabilized at twelve topics. The resulting topics (Figure~\ref{fig:ldatopics}) correspond to recognizable moral constructs: duty, virtue, honesty, justice, social utility, obligation, empathy, and harm minimization. Importantly, no single topic dominates the corpus. Most explanations blend multiple topics within a single explanation, indicating that models draw flexibly from different ethical vocabularies depending on the scenario. For example, a model might cite “public interest” (utilitarian), “obligation to disclose” (deontological), and “honesty” (virtue ethics) in the same explanation. %This pluralism mirrors real-world moral reasoning and suggests that models are not rigidly mapping to predefined theory templates, but instead construct morally plausible arguments using a hybrid vocabulary.
%This pluralism reflects real-world moral reasoning and suggests that the models are not rigidly following predefined theoretical frameworks, but instead construct morally plausible arguments using a hybrid moral vocabulary.

\smallskip

\textit{Theory–Explanation Alignment.} A core requirement for ethical reasoning is that the explanation should be consistent with the moral theory being invoked. To assess this, we conducted a manual alignment check on a stratified sample of 180 responses (6 models × 10 scenarios × 3 ethical theories), manually excluding outliers. In over 90\% of cases, the explanation supported the selected theory in a coherent manner. For example, Deontological responses often cited duties, rules, or rights (e.g., “The action violates the duty of confidentiality.”), Utilitarian responses referenced consequences and well-being (e.g., “The outcome benefits more people.”), and Virtue-based explanations appealed to character or intention (e.g., “It reflects compassion and honesty.”). Misalignments, when \major{they} occurred, tended to arise in edge cases or procedurally complex scenarios. Occasionally, a model labeled a decision as “virtue ethics” but explained it in outcome-based terms. Mismatches were rare and not concentrated in specific models.

\smallskip

\textit{Conciseness and Structural Patterns.}
\begin{figure}[ht]
\centering
\includegraphics[width=0.53\textwidth]{figures/qualitative/plot_sentence_counts.png}
\vspace{-0.6cm}
\caption{Sentence counts}
\label{fig:sentencecounts}
\end{figure}
We analyzed sentence and word counts across all explanations to understand the structural economy of model explanations. Over 95\% of outputs were a single sentence. Mean word counts ranged from 7.7 to 21.2 tokens (Figures~\ref{fig:sentencecounts}–\ref{fig:wordscounts}). Some models (e.g., GHOST, DEEPSEEK) tended to be more verbose, but longer explanations did not correlate with stronger theory alignment or clarity.
\begin{figure}[ht]
\centering
\includegraphics[width=0.53\textwidth]{figures/qualitative/plot_word_counts.png}
\vspace{-0.6cm}
\caption{\major{Word} counts}
\label{fig:wordscounts}
\end{figure}
%
%
This brevity is notable; despite being concise, most explanations successfully reference relevant moral principles. The findings suggest that LLMs are able to generate ethical explanations that are both succinct and substantively meaningful\major{,} a valuable property for deployment in constrained SE contexts, such as UI prompts or trace logs.

\smallskip

\textit{Explanatory Convergence and Disagreement.}
\begin{figure}[ht]
\hspace*{-1.2em}
\includegraphics[width=0.525\textwidth]{figures/qualitative/wordcloud_all_llms.png}
\caption{Word cloud}
\label{fig:wordcloud}
\end{figure}
In high-consensus scenarios (e.g., helping a clerk, donating to Wikipedia), all models agreed both on judgment and theory, and their explanations while lexically distinct shared moral themes like civic virtue, altruism, or public benefit. Word cloud analysis (Figure~\ref{fig:wordcloud}) confirms recurring terms such as “respect”, “action”, “duty”, “virtue”, and “harm”. In low-agreement scenarios, such as those involving conflicting obligations or ambiguous responsibility, we observed both linguistic and conceptual spread. Some models emphasized personal integrity, others systemic outcomes. However, even here, the diversity appeared to reflect the ambiguity of the scenario rather than noise or error.

\smallskip

\textit{Implications for Ethical Profiling and SE Systems.}
These findings have direct implications for the use of LLMs in SE applications that require ethical reasoning or user profiling:

\begin{itemize}[left=0pt]
    \item \textit{Trust and Transparency.} The presence of coherent, interpretable explanations supports use cases requiring traceable decision-making (e.g., user-facing ethical explanations or compliance logging).
    \item \textit{Profile Construction.} The diversity and flexibility in moral vocabulary provide rich semantic data for generating dynamic ethical profiles based on user input.
    \item \textit{Model Assessment.} Surface similarity metrics may underestimate performance; systems should instead incorporate theory-alignment checks or conceptual similarity measures (e.g., based on semantic entailment).
    \item \textit{Ethical Memory.} Explanatory patterns may be used to build cumulative ethical profiles over time, enabling systems to adapt to evolving user values without retraining.
\end{itemize}


\textbf{Qualitative characteristics (RQ4).} From what emerges from our analysis, the LLM-generated explanations for ethical decisions are marked by high surface diversity and consistent moral coherence. Despite low lexical overlap across outputs, with average pairwise similarity scores around 0.11, the models consistently construct explanations that align with the selected ethical theory in over 90\% of sampled cases. %This indicates that models do not rely on fixed templates but instead generate context-sensitive explanations. 
In line with what has been argued so far, this also suggests that models operate beyond rigid theoretical frameworks, producing context-sensitive explanations.
Dimensionality reduction and clustering analyses (PCA, t-SNE) show that most models occupy a dense central region in semantic space, suggesting convergence at the conceptual level, even as their syntactic forms diverge. Outliers (e.g., COHERE, DEEPSEEK) diverge primarily due to stylistic rather than ethical differences. Topic modeling reveals that LLMs implicitly draw from a pluralistic moral vocabulary, spanning utilitarian, deontological, and virtue-based notions. Most explanations combine references to multiple moral principles within a single response, confirming the models' ability to construct hybrid, plausible explanations. %rather than mapping rigidly to single-theory templates. 
Structurally, over 95\% of explanations are a single sentence, and brevity does not preclude moral relevance. Even short explanations tend to highlight key ethical features (e.g., duty, harm, empathy), making them well-suited for constrained SE contexts. Finally, in high-consensus scenarios, lexical variation coexists with thematic convergence around shared moral terms (e.g., ``altruism'', ``civic duty'', ``harm''), while disagreement in complex cases reflects underlying ambiguity rather than interpretive noise. These findings support the use of LLMs for both traceable decision-making and dynamic ethical profiling in software engineering systems.


