%\pat{added}
\section{Quantitative Analysis}
\label{sec:quantitative}

%\subsection{Experimental Protocol and Metrics}

%\textbf{Experimental Protocol and Metrics.} 
For each scenario in the dataset, all 16 LLMs were queried independently using the shared prompt structure. Each response was parsed to extract the selected ethical theory, the binary acceptability judgment, and the free-text explanation. The same protocol was applied to a group of 3 human experts in ethics and applied philosophy to provide a comparative baseline. Agreement among models was quantified using two primary metrics:


\textit{Theory Consistency Rate (TCR).} \major{The share of models that select the modal theory for a scenario; it measures convergence of interpretive framing. TCR is introduced in this work as a deliberately simple, transparent modal-share indicator; it is not a gold-standard agreement coefficient.}


\textit{Binary Agreement Rate (BAR).} \major{The share of models that agree on the yes/no acceptability given their selected lens; it measures outcome-level consensus.}

\major{We report z-scores for both metrics and visualize thresholds to identify high-variance (ambiguous) scenarios.}

% \begin{itemize}
%     \item \textit{Theory Consistency Rate (TCR):} the percentage of models selecting the modal ethical theory for a given scenario, measuring convergence on moral framing.
%     \item \textit{Binary Agreement Rate (BAR):} the percentage of models agreeing on the binary moral acceptability (yes/no), capturing outcome-level consensus.
% \end{itemize}
To account for scale differences, both metrics are standardized via \textit{z-score} normalization ($z_{\mathrm{TCR}}$ and $z_{\mathrm{BAR}}$), where $\mu$ and $\sigma$ represent the mean and standard deviation of the respective metric across all scenarios:
\smallskip
\[
z_\text{TCR} = \frac{\text{TCR} - \mu_\text{TCR}}{\sigma_\text{TCR}}, \qquad
z_\text{BAR} = \frac{\text{BAR} - \mu_\text{BAR}}{\sigma_\text{BAR}}
\]

A combined agreement score per scenario is computed as:

\[
\text{Combined z-score} = \frac{z_\text{TCR} + z_\text{BAR}}{2}
\]

This provides a unified measure of inter-model consistency across both theoretical and acceptability dimensions. The same procedure was applied to the human experts. All parsing and scoring scripts are included in the replication package.
% required by metareview
\smallskip

\textbf{Ethical reasoning capacity (RQ1).}
LLMs %shows
\major{show} ethical reasoning capacities over %ethical-relevant
\major{ethically relevant} scenarios. With a 73.3\% average agreement on ethical theory and 86.7\% on acceptability, models show consistent normative interpretation under zero-shot conditions. Results shown in Figure~\ref{fig:sTCR_BAR_table} suggest that LLMs can produce structured, theory-informed outputs for moral scenarios, despite the lack of fine-tuning.

\smallskip

\textbf{LLMs agreement (RQ2).} 
Across all 30 scenarios, pairwise model agreement was 73.3\% for TCR and 86.7\% for BAR. Agreement varied across scenarios, with high divergence in ethically ambiguous situations especially those involving trade-offs between duties, rights, or risks.

\begin{figure}[ht]
\centering
\includegraphics[width=0.49\textwidth]{figures/quantitative/TCR-BAR+ZSCORES.PNG}
\caption{LLMs TCR and BAR results with Fleiss' Kappa agreement coloring and Z-Scores with threshold coloring.}
\label{fig:sTCR_BAR_table}
\end{figure}

\noindent
Figure~\ref{fig:sTCR_BAR_table} visualizes model agreement using interpretive thresholds inspired by Fleissâ€™ Kappa~\cite{falotico2015fleiss}. Green and yellow cells mark strong and fair agreement, while red signals interpretive divergence. This allows rapid identification of high-variance scenarios that may warrant escalation or human oversight. LLMs reach high consistency on moral acceptability and moderate-to-high consistency on ethical theory. Divergences occur in conceptually complex cases, confirming that model disagreements align with known areas of ethical ambiguity.

\smallskip

\textbf{LLMs vs. Human Experts (RQ3).}
To compare human and model behavior, we compute the combined z-score per scenario for both groups. 
%required by major revision
\major{In several cases (e.g., scenarios numbered 14, 15, 21, 27, 29 in the replication package), LLM agreement patterns parallel the degree of expert convergence on the same scenarios, whereas sustained divergences on both sides (e.g., scenarios 4, 9, 11, 13, 17, 23) indicate intrinsic interpretive ambiguity. We therefore interpret agreement as a signal of stability, and disagreement as a cue for triage, rather than as evidence of normative correctness. Expert TCR values (often around two-thirds) should not be read as poor performance but as human-level pluralism under minimal normative instruction. This variability is a feature of the task design: it exposes multiple legitimate framings and makes explicit where automated interpretation should escalate to humans.}
%In many cases, high agreement among LLMs mirrors high agreement among experts (e.g., scenarios numbered 14, 15, 21, 27, 29 in the replication package). Shared low scores (e.g., scenarios 4, 9, 11, 13, 17, 23) suggest persistent interpretive uncertainty. LLMs show comparable agreement to human experts across most scenarios. Convergences and divergences between groups align with scenario difficulty, indicating that model disagreement often reflects intrinsic ambiguity rather than random noise. Some scenarios, like scenario 3 %\mashal{maybe we refer to where this scenario 3 can be found, if you refer to the three bullets we have above, then we need to number the bullets, maybe?}, show higher model alignment than human consensus, and vice versa. These mismatches indicate both the strength and blind spots of LLM-based reasoning, and support human-in-the-loop configurations.


\smallskip

\textit{Single LLM vs. Ensemble.}
No single model was in perfect agreement with the modal ensemble across all scenarios. Several models exhibited idiosyncratic choices, suggesting that relying on a single LLM for ethically-sensitive decisions introduces variance and potential bias. Ensemble-based aggregation mitigates this by producing more robust and explainable outcomes, particularly in ethically ambiguous or high-stakes settings.

\smallskip

\textit{Implications for Software Engineering.}
The described capacity to identify scenarios with high or low model agreement is potentially actionable in SE pipelines. Scenarios with high agreement can be handled autonomously; those with low agreement can trigger alerts or escalate decisions for human review. Moreover, scenario-wise agreement scores can serve as confidence metrics to support ethical decision auditing, runtime triage in recommender or assistant systems, and automated filtering of morally unstable outputs. This sets the stage for embedding LLMs as modular ethical profilers providing scalable, explainable, and context-sensitive reasoning components in future software systems.

\smallskip

\begin{comment}

%we have a limitations paragraph in the discussion section

\textit{Limitations.}
Despite promising results, some limitations constrain this analysis: only three ethical theories are considered, excluding other frameworks (e.g., care ethics, relativism); models are prompted independently in zero-shot; no interaction, clarification, or user context is allowed; scenario phrasing and prompt structure may influence model behavior; prompt sensitivity was not systematically assessed; agreement metrics capture convergence, not correctness; systematic errors may go undetected. These limitations motivate not only the next section but also future work on theory extension, scenario design, and semantically robust evaluation tools.
\end{comment}