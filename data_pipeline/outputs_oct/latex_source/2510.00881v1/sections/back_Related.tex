\section{Related Work}
\label{sec:related}
\noindent\emph{\major{Ethics in autonomous and software-intensive systems.}}
The integration of ethical considerations into autonomous systems has been widely examined in software engineering. Prior work spans design-time approaches that encode ethics via codes of conduct, principles, and rules~\cite{suri2023software,alidoosti2021ethics,alidoosti2022incorporating,DBLP:journals/access/ShahinHNPSGW22,trailer2024ciniselli}, and verification-time approaches that formalize and check system decisions against ethical frameworks~\cite{jedlickova2024ensuring,dennis2016formal,cardoso2021implementing,inverardi2019ethics,de2024engineering,inverardi2022ethical,Machine_Ethics_in_Changing_Contexts:2021,karim2017ethical}. This body of research establishes both the need and the mechanisms for ensuring ethically compliant behavior in autonomous decision pipelines.

%\smallskip
\noindent\emph{\major{LLMs in software engineering and the need for ethical reasoning.}}
Concurrently, LLMs have been adopted across SE tasks such as code generation, bug detection, and documentation~\cite{hou2024large}. As these models are integrated into SE workflows, it becomes important to assess whether they exhibit ethical reasoning and how they might be leveraged in systems with ethical implications~\cite{DBLP:conf/emnlp/RaoKTAC23,han2022aligning}.

%\smallskip
\noindent\emph{\major{Model-centric evaluations of moral reasoning.}}
Han et al.~\cite{han2022aligning} evaluate LLM understanding of moral/ethical reasoning across five domains (justice, deontology, virtue ethics, utilitarianism, and commonsense morality). They fine-tune BERT-base, BERT-large, RoBERTa-large, and ALBERT-xxlarge on ETHICS datasets comprising over 13{,}000 scenarios, and then test the models’ ability to classify scenarios in line with the ethical theories used during fine-tuning. In contrast, the present work does not rely on fine-tuning; it evaluates whether pre-trained LLMs, in a zero-shot setup, display ethical reasoning.

%\smallskip
\noindent\emph{\major{Value identification in scholarly texts vs. ethically charged situations.}}
A complementary thread leverages LLMs to extract values and structure from scientific writing. For example,~\cite{ICSE2025PAPER} studies ChatGPT’s ability to identify human values from titles and abstracts of SE publications using Schwartz’s theory~\cite{schwartz2012overview}, followed by manual verification by humans. Related efforts show that LLMs can assist in extracting insights from scholarly texts, classifying publications, and mining metadata for literature reviews~\cite{hou2024large,alshami2023harnessing}. These studies focus on value detection in academic prose rather than on evaluating ethical concrete reasoning, ethically charged scenarios.

%\smallskip
\noindent\emph{\major{Reasoning with policies and learning moral rewards.}}
Rao et al.~\cite{DBLP:conf/emnlp/RaoKTAC23} argue against hard-coding specific moral values in LLMs and advocate for general ethical reasoning capacities that adapt to diverse contexts. They introduce in-context ethical policies defined at varying abstraction levels and grounded in deontology, virtue ethics, and consequentialism, reporting experiments across GPT-3, ChatGPT, and GPT-4 with GPT-4 showing stronger ethical reasoning. Tennant et al.~\cite{DBLP:conf/iclr/TennantHM25} incorporate intrinsic moral rewards, grounded in deontological and utilitarian theories, into reinforcement learning fine-tuning. Using the Iterated Prisoner’s Dilemma, they demonstrate that LLM agents can learn morally aligned strategies and even unlearn previously selfish behaviors. While these works shape model behavior through policies or moral rewards, the present study compares outputs across multiple models without additional tuning, treating ethical reasoning as a testing ground rather than a direct optimization target.

%\smallskip
\noindent\emph{\major{Positioning of the present study.}}
Prior research establishes design/verification-time mechanisms for ethics in autonomous systems, documents LLM utility in SE, and explores both fine-tuned moral reasoning and in-context ethical policy use. The contribution here is orthogonal: a zero-shot assessment of pre-trained LLMs’ ethical reasoning on ethically charged scenarios, contrasting with fine-tuned or policy-conditioned settings, and distinct from value-mining in academic text.



\begin{comment}
\section{Related Work}
\label{sec:related}


In recent years, the integration of ethical considerations into autonomous systems has been widely discussed within the software engineering community~\cite{suri2023software,alidoosti2021ethics,alidoosti2022incorporating,DBLP:journals/access/ShahinHNPSGW22,trailer2024ciniselli}. While some studies have proposed approaches for embedding ethics into autonomous systems during the design phase through code of conduct, principles, and rules, others have focused on formal verification of decisions taken by these systems based on ethical frameworks to ensure their ethical compliance~\cite{jedlickova2024ensuring,dennis2016formal,cardoso2021implementing,inverardi2019ethics,de2024engineering,inverardi2022ethical,Machine_Ethics_in_Changing_Contexts:2021,karim2017ethical}. Recently, LLMs have also begun to play a growing role in software engineering tasks including code generation, bug detection, and documentation~\cite{hou2024large}. As LLMs become integrated into software engineering workflows, it is crucial to understand whether they are capable of ethical reasoning, and whether they can be used to engineer systems that have ethical implications~\cite{DBLP:conf/emnlp/RaoKTAC23,han2022aligning}.

%A different perspective is presented in~\cite{MemonAFSI24,Mashal2024} where ethical profiles are generated by asking users to manually rank disposition for specific contexts e.g., how much precedence they would give to an injured person in hospital v/s at the airport in a scenario over a resource contention. The user ethical profiles are then embedded into the decision-making process of autonomous systems for ethical decision-making. However, as ethical preferences are context-dependent, it is impractical to ask users to rank the dispositions for every possible scenario.

Consequently, several studies have investigated whether LLMs possess ethical reasoning capabilities. The study in~\cite{han2022aligning} has proposed the approach where they evaluate multiple LLMs' understanding of moral and ethical reasoning across five domains, including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. The authors fine-tuned several models, including BERT-base, BERT-large, RoBERTa-large, and ALBERT-xxlarge, on ETHICS datasets comprising over 13,000 real-life scenarios. These models were then tested to evaluate their ability to classify scenarios in alignment with the ethical theories used during fine-tuning. In contrast, our approach does not rely on fine-tuning. Instead, we assess whether LLMs demonstrate ethical reasoning capabilities in their pre-trained state. This allows us to evaluate their understanding of ethical reasoning. The study in~\cite{ICSE2025PAPER} explores the automated reasoning abilities of ChatGPT, as the LLM to identify the presence of human values within the abstracts and titles of various publication in SE venues. The authors prompt ChatGPT to assess whether any of the human values defined by Schwartz’s theory~\cite{schwartz2012overview} are reflected in these texts. This automated analysis is then followed by a manual verification process conducted by human annotators. Similarly, other studies have also highlighted the effectiveness of LLMs in extracting valuable insights from scholarly texts as well as classifying publications and extracting their metadata for literature reviews~\cite{hou2024large,alshami2023harnessing}. Although these studies exploit LLMs to identify human values in academic writing, they do not assess LLM capabilities to identify human values when presented with ethically charged situations. In contrast, our work focuses on evaluating the ethical reasoning capabilities of LLMs in such situations. The study in~\cite{DBLP:conf/emnlp/RaoKTAC23} proposes a different perspective arguing that LLMs should not be hard coded with specific moral values, instead these models should be equipped with general ethical reasoning capabilities that allow them to adapt to diverse ethical contexts as the existence of multiple, often conflicting moral systems makes it difficult to align LLMs to a single theory. The authors introduce the in-context ethical policies framework, allowing LLMs to reason based on ethical principles provided at runtime. These policies are defined at varying levels of abstraction and based on different ethical theories, including deontology, virtue ethics, and consequentialism. The authors conduct the experiments using different models including GPT-3, ChatGPT, and GPT-4, across a set of moral dilemmas and present that GPT-4 exhibits better ethical reasoning capabilities compared to other models. Similarly, the study in~\cite{DBLP:conf/iclr/TennantHM25} introduces an approach that incorporates intrinsic moral rewards, grounded in Deontological and Utilitarian ethical theories, into the reinforcement learning fine-tuning process. Using the Iterated Prisoner’s Dilemma, they demonstrate that LLM agents can learn morally-aligned strategies and even unlearn previously selfish behaviors. However, while their focus is on fine-tuning LLM agents, our work focuses on comparing LLM outputs across models, using ethical reasoning as a testing ground rather than a direct modeling target.
\end{comment}