@inproceedings{gousios2014exploratory,
  title={An exploratory study of the pull-based software development model},
  author={Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
  booktitle={Proceedings of the 36th international conference on software engineering},
  pages={345--355},
  year={2014}
}

@inproceedings{kononenko2016code,
  title={Code review quality: How developers see it},
  author={Kononenko, Oleksii and Baysal, Olga and Godfrey, Michael W},
  booktitle={Proceedings of the 38th international conference on software engineering},
  pages={1028--1038},
  year={2016}
}

@inproceedings{bosu2013impact,
  title={Impact of peer code review on peer impression formation: A survey},
  author={Bosu, Amiangshu and Carver, Jeffrey C},
  booktitle={2013 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
  pages={133--142},
  year={2013},
  organization={IEEE}
}

@inproceedings{lu2023llama,
  title={LLaMA-Reviewer: Advancing code review automation with large language models through parameter-efficient fine-tuning},
  author={Lu, Junyi and Yu, Lei and Li, Xiaojia and Yang, Li and Zuo, Chun},
  booktitle={2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE)},
  pages={647--658},
  year={2023},
  organization={IEEE}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@inproceedings{bosu2015characteristics,
  title={Characteristics of useful code reviews: An empirical study at microsoft},
  author={Bosu, Amiangshu and Greiler, Michaela and Bird, Christian},
  booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
  pages={146--156},
  year={2015},
  organization={IEEE}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{luo2024semi,
  title={Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models},
  author={Luo, Xianzhen and Zhu, Qingfu and Zhang, Zhiming and Wang, Xu and Yang, Qing and Xu, Dongliang and Che, Wanxiang},
  journal={arXiv preprint arXiv:2403.00338},
  year={2024}
}

@inproceedings{sadowski2018modern,
  title={Modern code review: a case study at google},
  author={Sadowski, Caitlin and S{\"o}derberg, Emma and Church, Luke and Sipko, Michal and Bacchelli, Alberto},
  booktitle={Proceedings of the 40th international conference on software engineering: Software engineering in practice},
  pages={181--190},
  year={2018}
}

@inproceedings{shi2019automatic,
  title={Automatic code review by learning the revision of source code},
  author={Shi, Shu-Ting and Li, Ming and Lo, David and Thung, Ferdian and Huo, Xuan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={4910--4917},
  year={2019}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@inproceedings{hong2022commentfinder,
  title={Commentfinder: a simpler, faster, more accurate code review comments recommendation},
  author={Hong, Yang and Tantithamthavorn, Chakkrit and Thongtanunam, Patanamon and Aleti, Aldeida},
  booktitle={Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering},
  pages={507--519},
  year={2022}
}

@inproceedings{gupta2018intelligent,
  title={Intelligent code reviews using deep learning},
  author={Gupta, Anshul and Sundaresan, Neel},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’18) Deep Learning Day},
  year={2018}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@article{lozhkov2024starcoder,
  title={Starcoder 2 and the stack v2: The next generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@article{islam2024mapcoder,
  title={MapCoder: Multi-Agent Code Generation for Competitive Problem Solving},
  author={Islam, Md Ashraful and Ali, Mohammed Eunus and Parvez, Md Rizwan},
  journal={arXiv preprint arXiv:2405.11403},
  year={2024}
}

@inproceedings{wei2024magicoder,
  title={Magicoder: Empowering code generation with oss-instruct},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{steenhoek2023reinforcement,
  title={Reinforcement learning from automatic feedback for high-quality unit test generation},
  author={Steenhoek, Benjamin and Tufano, Michele and Sundaresan, Neel and Svyatkovskiy, Alexey},
  journal={arXiv preprint arXiv:2310.02368},
  year={2023}
}

@article{dou2024stepcoder,
  title={StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback},
  author={Dou, Shihan and Liu, Yan and Jia, Haoxiang and Xiong, Limao and Zhou, Enyu and Shan, Junjie and Huang, Caishuang and Shen, Wei and Fan, Xiaoran and Xi, Zhiheng and others},
  journal={arXiv preprint arXiv:2402.01391},
  year={2024}
}

@article{shen2023pangu,
  title={Pangu-coder2: Boosting large language models for code with ranking feedback},
  author={Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others},
  journal={arXiv preprint arXiv:2307.14936},
  year={2023}
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{tang2024understanding,
  title={Understanding the performance gap between online and offline alignment algorithms},
  author={Tang, Yunhao and Guo, Daniel Zhaohan and Zheng, Zeyu and Calandriello, Daniele and Cao, Yuan and Tarassov, Eugene and Munos, R{\'e}mi and Pires, Bernardo {\'A}vila and Valko, Michal and Cheng, Yong and others},
  journal={arXiv preprint arXiv:2405.08448},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{jelinek1977perplexity,
  title={Perplexity—a measure of the difficulty of speech recognition tasks},
  author={Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={62},
  number={S1},
  pages={S63--S63},
  year={1977},
  publisher={Acoustical Society of America}
}

@inproceedings{miaschi2021makes,
  title={What makes my model perplexed? a linguistic investigation on neural language models perplexity},
  author={Miaschi, Alessio and Brunato, Dominique and Dell’Orletta, Felice and Venturi, Giulia},
  booktitle={Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  pages={40--47},
  year={2021}
}

@article{yang2024harnessing,
  title={Harnessing the power of llms in practice: A survey on chatgpt and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={ACM Transactions on Knowledge Discovery from Data},
  volume={18},
  number={6},
  pages={1--32},
  year={2024},
  publisher={ACM New York, NY}
}

@article{pearson1900x,
  title={X. On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={50},
  number={302},
  pages={157--175},
  year={1900},
  publisher={Taylor \& Francis}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{biderman2024lora,
  title={Lora learns less and forgets less},
  author={Biderman, Dan and Ortiz, Jose Gonzalez and Portes, Jacob and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and others},
  journal={arXiv preprint arXiv:2405.09673},
  year={2024}
}

@article{plank2022problem,
  title={The'Problem'of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation},
  author={Plank, Barbara},
  journal={arXiv preprint arXiv:2211.02570},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{fagan1999design,
  title={Design and code inspections to reduce errors in program development},
  author={Fagan, Michael E},
  journal={IBM Systems Journal},
  volume={38},
  number={2.3},
  pages={258--287},
  year={1999},
  publisher={IBM}
}

@inproceedings{liu2024coachlm,
  title={CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning},
  author={Liu, Yilun and Tao, Shimin and Zhao, Xiaofeng and Zhu, Ming and Ma, Wenbing and Zhu, Junhao and Su, Chang and Hou, Yutai and Zhang, Miao and Zhang, Min and others},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)},
  pages={5184--5197},
  year={2024},
  organization={IEEE}
}

@article{rejeleene2024towards,
  title={Towards Trustable Language Models: Investigating Information Quality of Large Language Models},
  author={Rejeleene, Rick and Xu, Xiaowei and Talburt, John},
  journal={arXiv preprint arXiv:2401.13086},
  year={2024}
}

@article{silva2023repairllama,
  title={Repairllama: Efficient representations and fine-tuned adapters for program repair},
  author={Silva, Andr{\'e} and Fang, Sen and Monperrus, Martin},
  journal={arXiv preprint arXiv:2312.15698},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{rong2024distilling,
  title={Distilling Quality Enhancing Comments from Code Reviews to Underpin Reviewer Recommendation},
  author={Rong, Guoping and Yu, Yongda and Zhang, Yifan and Zhang, He and Shen, Haifeng and Shao, Dong and Kuang, Hongyu and Wang, Min and Wei, Zhao and Xu, Yong and others},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}

@article{mcaleese2024llm,
  title={LLM Critics Help Catch LLM Bugs},
  author={McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
  journal={arXiv preprint arXiv:2407.00215},
  year={2024}
}

@article{dziri2022origin,
  title={On the origin of hallucinations in conversational models: Is it the datasets or the models?},
  author={Dziri, Nouha and Milton, Sivan and Yu, Mo and Zaiane, Osmar and Reddy, Siva},
  journal={arXiv preprint arXiv:2204.07931},
  year={2022}
}

@article{hou2023large,
  title={Large language models for software engineering: A systematic literature review},
  author={Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  journal={arXiv preprint arXiv:2308.10620},
  year={2023}
}

@inproceedings{fan2023large,
  title={Large language models for software engineering: Survey and open problems},
  author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M},
  booktitle={2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)},
  pages={31--53},
  year={2023},
  organization={IEEE}
}

@inproceedings{li2022codereviewer,
  title={Automating code review activities by large-scale pre-training},
  author={Li, Zhiyu and Lu, Shuai and Guo, Daya and Duan, Nan and Jannu, Shailesh and Jenks, Grant and Majumder, Deep and Green, Jared and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1035--1047},
  year={2022}
}

@article{tufano2024code,
  title={Code review automation: strengths and weaknesses of the state of the art},
  author={Tufano, Rosalia and Dabi{\'c}, Ozren and Mastropaolo, Antonio and Ciniselli, Matteo and Bavota, Gabriele},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}

@article{shypula2023learning,
  title={Learning performance-improving code edits},
  author={Shypula, Alexander and Madaan, Aman and Zeng, Yimeng and Alon, Uri and Gardner, Jacob and Hashemi, Milad and Neubig, Graham and Ranganathan, Parthasarathy and Bastani, Osbert and Yazdanbakhsh, Amir},
  journal={arXiv preprint arXiv:2302.07867},
  year={2023}
}

@inproceedings{croft2023data,
  title={Data quality for software vulnerability datasets},
  author={Croft, Roland and Babar, M Ali and Kholoosi, M Mehdi},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={121--133},
  year={2023},
  organization={IEEE}
}

@inproceedings{tihanyi2023formai,
  title={The formai dataset: Generative ai in software security through the lens of formal verification},
  author={Tihanyi, Norbert and Bisztray, Tamas and Jain, Ridhi and Ferrag, Mohamed Amine and Cordeiro, Lucas C and Mavroeidis, Vasileios},
  booktitle={Proceedings of the 19th International Conference on Predictive Models and Data Analytics in Software Engineering},
  pages={33--43},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


@article{deepseek-coder,
  title={Deepseek-coder: When the large language model meets programming--the rise of code intelligence, 2024},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Yu and Li, YK and others},
  journal={URL https://arxiv. org/abs/2401.14196},
  volume={5},
  pages={19},
  year={2024}
}

@article{widyasari2023explaining,
  title={Explaining explanation: An empirical study on explanation in code reviews},
  author={Widyasari, Ratnadira and Zhang, Ting and Bouraffa, Abir and Maalej, Walid and Lo, David},
  journal={arXiv preprint arXiv:2311.09020},
  year={2023}
}

@article{li2022advance,
  title={On the advance of making language models better reasoners},
  author={Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2206.02336},
  volume={2},
  year={2022}
}

@article{arora2022ask,
  title={Ask me anything: A simple strategy for prompting language models},
  author={Arora, Simran and Narayan, Avanika and Chen, Mayee F and Orr, Laurel and Guha, Neel and Bhatia, Kush and Chami, Ines and Sala, Frederic and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2210.02441},
  year={2022}
}

@article{yu2024fine,
  title={Fine-tuning large language models to improve accuracy and comprehensibility of automated code review},
  author={Yu, Yongda and Rong, Guoping and Shen, Haifeng and Zhang, He and Shao, Dong and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong},
  journal={ACM transactions on software engineering and methodology},
  volume={34},
  number={1},
  pages={1--26},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{eysenbachmaximum,
  title={Maximum Entropy RL (Provably) Solves Some Robust RL Problems},
  author={Eysenbach, Benjamin and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{liu2023maximum,
  title={Maximum entropy heterogeneous-agent reinforcement learning},
  author={Liu, Jiarong and Zhong, Yifan and Hu, Siyi and Fu, Haobo and Fu, Qiang and Chang, Xiaojun and Yang, Yaodong},
  journal={arXiv preprint arXiv:2306.10715},
  year={2023}
}

@article{zhang2024logicode,
  title={Logicode: an llm-driven framework for logical anomaly detection},
  author={Zhang, Yiheng and Cao, Yunkang and Xu, Xiaohao and Shen, Weiming},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2024},
  publisher={IEEE}
}

@article{ning2024defining,
  title={Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents},
  author={Ning, Kaiwen and Chen, Jiachi and Zhang, Jingwen and Li, Wei and Wang, Zexu and Feng, Yuming and Zhang, Weizhe and Zheng, Zibin},
  journal={arXiv preprint arXiv:2412.18371},
  year={2024}
}

@article{dunivin2024scalable,
  title={Scalable qualitative coding with llms: Chain-of-thought reasoning matches human performance in some hermeneutic tasks},
  author={Dunivin, Zackary Okun},
  journal={arXiv preprint arXiv:2401.15170},
  year={2024}
}

@article{nong2024chain,
  title={Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities},
  author={Nong, Yu and Aldeen, Mohammed and Cheng, Long and Hu, Hongxin and Chen, Feng and Cai, Haipeng},
  journal={arXiv preprint arXiv:2402.17230},
  year={2024}
}

@article{yeo2502demystifying,
  title={Demystifying long chain-of-thought reasoning in llms, 2025},
  author={Yeo, Edward and Tong, Yuxuan and Niu, Morry and Neubig, Graham and Yue, Xiang},
  journal={URL https://arxiv. org/abs/2502.03373},
  year={2025}
}

@article{wang2024drt,
  title={Drt-o1: Optimized deep reasoning translation via long chain-of-thought},
  author={Wang, Jiaan and Meng, Fandong and Liang, Yunlong and Zhou, Jie},
  journal={arXiv e-prints},
  pages={arXiv--2412},
  year={2024}
}

@inproceedings{tran2019does,
  title={Does BLEU score work for code migration?},
  author={Tran, Ngoc and Tran, Hieu and Nguyen, Son and Nguyen, Hoan and Nguyen, Tien},
  booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
  pages={165--176},
  year={2019},
  organization={IEEE}
}

@article{kocmi2024navigating,
  title={Navigating the metrics maze: Reconciling score magnitudes and accuracies},
  author={Kocmi, Tom and Zouhar, Vil{\'e}m and Federmann, Christian and Post, Matt},
  journal={arXiv preprint arXiv:2401.06760},
  year={2024}
}

@article{zhao2023right,
  title={The right prompts for the job: Repair code-review defects with large language model},
  author={Zhao, Zelin and Xu, Zhaogui and Zhu, Jialong and Di, Peng and Yao, Yuan and Ma, Xiaoxing},
  journal={arXiv preprint arXiv:2312.17485},
  year={2023}
}

@article{guiasu1985principle,
  title={The principle of maximum entropy},
  author={Guiasu, Silviu and Shenitzer, Abe},
  journal={The mathematical intelligencer},
  volume={7},
  pages={42--48},
  year={1985},
  publisher={Springer-Verlag}
}

@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{qwq32b,
    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},
    url = {https://qwenlm.github.io/blog/qwq-32b/},
    author = {Qwen Team},
    month = {March},
    year = {2025}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@inproceedings{rezatofighi2019generalized,
  title={Generalized intersection over union: A metric and a loss for bounding box regression},
  author={Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={658--666},
  year={2019}
}

@article{li2024llmasajudge,
  title   = {From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge},
  author  = {Dawei Li and Bohan Jiang and Liangjie Huang and Alimohammad Beigi and Chengshuai Zhao and Zhen Tan and Amrita Bhattacharjee and Yuxuan Jiang and Canyu Chen and Tianhao Wu and Kai Shu and Lu Cheng and Huan Liu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.16594}
}

@article{gu2024survey,
  title={A survey on llm-as-a-judge},
  author={Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others},
  journal={arXiv preprint arXiv:2411.15594},
  year={2024}
}

@article{kvaalseth1989note,
  title={Note on Cohen's kappa},
  author={Kv{\aa}lseth, Tarald O},
  journal={Psychological reports},
  volume={65},
  number={1},
  pages={223--226},
  year={1989},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{li2025preference,
      title={Preference Leakage: A Contamination Problem in LLM-as-a-judge}, 
      author={Dawei Li and Renliang Sun and Yue Huang and Ming Zhong and Bohan Jiang and Jiawei Han and Xiangliang Zhang and Wei Wang and Huan Liu},
      year={2025},
      eprint={2502.01534},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.01534}, 
}

@article{jaynes1957information,
  title={Information theory and statistical mechanics},
  author={Jaynes, Edwin T},
  journal={Physical review},
  volume={106},
  number={4},
  pages={620},
  year={1957},
  publisher={APS}
}

@inproceedings{nigam1999using,
  title={Using maximum entropy for text classification},
  author={Nigam, Kamal and Lafferty, John and McCallum, Andrew},
  booktitle={IJCAI-99 workshop on machine learning for information filtering},
  volume={1},
  number={1},
  pages={61--67},
  year={1999},
  organization={Stockholom, Sweden}
}

@article{el2015arabic,
  title={Arabic text classification using maximum entropy},
  author={El-Halees, Alaa M},
  journal={IUG Journal of Natural Studies},
  volume={15},
  number={1},
  year={2015}
}

@inproceedings{och2002discriminative,
  title={Discriminative training and maximum entropy models for statistical machine translation},
  author={Och, Franz Josef and Ney, Hermann},
  booktitle={Proceedings of the 40th Annual meeting of the Association for Computational Linguistics},
  pages={295--302},
  year={2002}
}

@inproceedings{ittycheriah2005maximum,
  title={A maximum entropy word aligner for arabic-english machine translation},
  author={Ittycheriah, Abraham and Roukos, Salim},
  booktitle={Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  pages={89--96},
  year={2005}
}

@article{kaelbling1996reinforcement, 
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@article{arulkumaran2017deep,
  title={Deep reinforcement learning: A brief survey},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={26--38},
  year={2017},
  publisher={IEEE}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={Pmlr}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@article{qu2025survey,
  title={A survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond},
  author={Qu, Xiaoye and Li, Yafu and Su, Zhaochen and Sun, Weigao and Yan, Jianhao and Liu, Dongrui and Cui, Ganqu and Liu, Daizong and Liang, Shuxian and He, Junxian and others},
  journal={arXiv preprint arXiv:2503.21614},
  year={2025}
}

@article{zhang2025making,
  title={Making small language models efficient reasoners: Intervention, supervision, reinforcement},
  author={Zhang, Xuechen and Huang, Zijian and Ni, Chenshun and Xiong, Ziyang and Chen, Jiasi and Oymak, Samet},
  journal={arXiv preprint arXiv:2505.07961},
  year={2025}
}


@article{he2025can,
  title={Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Wang, Weixun and Bu, Xingyuan and Zhang, Ge and Peng, Zhongyuan and Zhang, Zhaoxiang and Zheng, Zhicheng and Su, Wenbo and others},
  journal={arXiv preprint arXiv:2502.19361},
  year={2025}
}

@ARTICLE{2025arXiv250517117S,
       author = {{Shani}, Chen and {Jurafsky}, Dan and {LeCun}, Yann and {Shwartz-Ziv}, Ravid},
        title = "{From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning}",
      journal = {arXiv e-prints},
     keywords = {Computation and Language, Artificial Intelligence, Information Theory},
         year = 2025,
        month = may,
          eid = {arXiv:2505.17117},
        pages = {arXiv:2505.17117},
          doi = {10.48550/arXiv.2505.17117},
archivePrefix = {arXiv},
       eprint = {2505.17117},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2025arXiv250517117S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{feng2023towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={70757--70798},
  year={2023}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@article{chen2025towards,
  title={Towards reasoning era: A survey of long chain-of-thought for reasoning large language models},
  author={Chen, Qiguang and Qin, Libo and Liu, Jinhao and Peng, Dengyun and Guan, Jiannan and Wang, Peng and Hu, Mengkang and Zhou, Yuhang and Gao, Te and Che, Wanxiang},
  journal={arXiv preprint arXiv:2503.09567},
  year={2025}
}

@article{xia2024beyond,
  title={Beyond chain-of-thought: A survey of chain-of-x paradigms for llms},
  author={Xia, Yu and Wang, Rui and Liu, Xu and Li, Mingyan and Yu, Tong and Chen, Xiang and McAuley, Julian and Li, Shuai},
  journal={arXiv preprint arXiv:2404.15676},
  year={2024}
}

@inproceedings{dao2023flashattention2,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{10.1145/3394486.3406703,
author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
title = {DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3406703},
doi = {10.1145/3394486.3406703},
abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record.The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology.DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {3505–3506},
numpages = {2},
keywords = {distributed deep learning, machine learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{li2022automating,
  title={Automating code review activities by large-scale pre-training},
  author={Li, Zhiyu and Lu, Shuai and Guo, Daya and Duan, Nan and Jannu, Shailesh and Jenks, Grant and Majumder, Deep and Green, Jared and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  booktitle={Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1035--1047},
  year={2022}
}

@inproceedings{tufano2021towards,
  title={Towards automating code review activities},
  author={Tufano, Rosalia and Pascarella, Luca and Tufano, Michele and Poshyvanyk, Denys and Bavota, Gabriele},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={163--174},
  year={2021},
  organization={IEEE}
}

@article{wu2025more,
  title={When more is less: Understanding chain-of-thought length in llms},
  author={Wu, Yuyang and Wang, Yifei and Ye, Ziyu and Du, Tianqi and Jegelka, Stefanie and Wang, Yisen},
  journal={arXiv preprint arXiv:2502.07266},
  year={2025}
}

@article{yeo2025demystifying,
  title={Demystifying long chain-of-thought reasoning in llms},
  author={Yeo, Edward and Tong, Yuxuan and Niu, Morry and Neubig, Graham and Yue, Xiang},
  journal={arXiv preprint arXiv:2502.03373},
  year={2025}
}

@article{chao2024maximum,
  title={Maximum entropy reinforcement learning via energy-based normalizing flow},
  author={Chao, Chen-Hao and Feng, Chien and Sun, Wei-Fang and Lee, Cheng-Kuang and See, Simon and Lee, Chun-Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={56136--56165},
  year={2024}
}

@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022}
}

@article{wang2025multimodal,
  title={Multimodal chain-of-thought reasoning: A comprehensive survey},
  author={Wang, Yaoting and Wu, Shengqiong and Zhang, Yuecheng and Yan, Shuicheng and Liu, Ziwei and Luo, Jiebo and Fei, Hao},
  journal={arXiv preprint arXiv:2503.12605},
  year={2025}
}

@inproceedings{rahman2016optimizing,
  title={Optimizing intersection-over-union in deep neural networks for image segmentation},
  author={Rahman, Md Atiqur and Wang, Yang},
  booktitle={International symposium on visual computing},
  pages={234--244},
  year={2016},
  organization={Springer}
}



@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{li2024llms,
  title={Llms-as-judges: a comprehensive survey on llm-based evaluation methods},
  author={Li, Haitao and Dong, Qian and Chen, Junjie and Su, Huixue and Zhou, Yujia and Ai, Qingyao and Ye, Ziyi and Liu, Yiqun},
  journal={arXiv preprint arXiv:2412.05579},
  year={2024}
}

@inproceedings{bosu2014identifying,
  title={Identifying the characteristics of vulnerable code changes: An empirical study},
  author={Bosu, Amiangshu and Carver, Jeffrey C and Hafiz, Munawar and Hilley, Patrick and Janni, Derek},
  booktitle={Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of software engineering},
  pages={257--268},
  year={2014}
}

@inproceedings{yang2023evacrc,
  title={Evacrc: Evaluating code review comments},
  author={Yang, Lanxin and Xu, Jinwei and Zhang, Yifan and Zhang, He and Bacchelli, Alberto},
  booktitle={Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={275--287},
  year={2023}
}

@article{rong2024distilling,
  title={Distilling quality enhancing comments from code reviews to underpin reviewer recommendation},
  author={Rong, Guoping and Yu, Yongda and Zhang, Yifan and Zhang, He and Shen, Haifeng and Shao, Dong and Kuang, Hongyu and Wang, Min and Wei, Zhao and Xu, Yong and others},
  journal={IEEE Transactions on Software Engineering},
  volume={50},
  number={7},
  pages={1658--1674},
  year={2024},
  publisher={IEEE}
}

@article{yu2024distilling,
  title={Distilling Desired Comments for Enhanced Code Review with Large Language Models},
  author={Yu, Yongda and Zhang, Lei and Rong, Guoping and Shen, Haifeng and Zhang, Jiahao and Yan, Haoxiang and Shi, Guohao and Shao, Dong and Pan, Ruiqi and Li, Yuan and others},
  journal={arXiv preprint arXiv:2412.20340},
  year={2024}
}

@article{chen2025understanding,
  title={Understanding Practitioners’ Expectations on Clear Code Review Comments},
  author={Chen, Junkai and Li, Zhenhao and Mao, Qiheng and Hu, Xing and Liu, Kui and Xia, Xin},
  journal={Proceedings of the ACM on Software Engineering},
  volume={2},
  number={ISSTA},
  pages={1257--1279},
  year={2025},
  publisher={ACM New York, NY, USA}
}

@article{sun2025bitsai,
  title={BitsAI-CR: Automated Code Review via LLM in Practice},
  author={Sun, Tao and Xu, Jian and Li, Yuanpeng and Yan, Zhao and Zhang, Ge and Xie, Lintao and Geng, Lu and Wang, Zheng and Chen, Yueyan and Lin, Qin and others},
  journal={arXiv preprint arXiv:2501.15134},
  year={2025}
}

@article{10.1145/3690635,
author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
title = {Structured Chain-of-Thought Prompting for Code Generation},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3690635},
doi = {10.1145/3690635},
abstract = {Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still cannot satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29\% Pass@1 in HumanEval. In this article, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures (i.e., sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs (i.e., gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct- ({) 1.3B, 6.7B, 33B (}) ) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79\% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {37},
numpages = {23},
keywords = {Code Generation, Large Language Models, Prompting Engineering}
}

@article{nong2024chain,
  title={Chain-of-thought prompting of large language models for discovering and fixing software vulnerabilities},
  author={Nong, Yu and Aldeen, Mohammed and Cheng, Long and Hu, Hongxin and Chen, Feng and Cai, Haipeng},
  journal={arXiv preprint arXiv:2402.17230},
  year={2024}
}

@article{lyu2025exploring,
  title={Exploring the limit of outcome reward for learning mathematical reasoning},
  author={Lyu, Chengqi and Gao, Songyang and Gu, Yuzhe and Zhang, Wenwei and Gao, Jianfei and Liu, Kuikun and Wang, Ziyi and Li, Shuaibin and Zhao, Qian and Huang, Haian and others},
  journal={arXiv preprint arXiv:2502.06781},
  year={2025}
}

@article{zhang2024improve,
  title={Improve vision language model chain-of-thought reasoning},
  author={Zhang, Ruohong and Zhang, Bowen and Li, Yanghao and Zhang, Haotian and Sun, Zhiqing and Gan, Zhe and Yang, Yinfei and Pang, Ruoming and Yang, Yiming},
  journal={arXiv preprint arXiv:2410.16198},
  year={2024}
}

@article{li2025small,
  title={Small models struggle to learn from strong reasoners},
  author={Li, Yuetai and Yue, Xiang and Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Lin, Bill Yuchen and Ramasubramanian, Bhaskar and Poovendran, Radha},
  journal={arXiv preprint arXiv:2502.12143},
  year={2025}
}