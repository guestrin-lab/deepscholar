
\section{Introduction}

Talking-face video underpins human–computer interaction, accessibility, content creation, and post-production. Practical systems must jointly reason about linguistic content, audio–visual timing, video editing to match a target speech track or reference motion, and facial expressions/actions. Classic synchronisation pipelines such as SyncNet and subsequent Transformer-based variants (e.g., VocaLiST) focus on detecting in-/out-of-sync pairs or removing stream lags, while recent audio–visual pretraining methods (e.g., AV-HuBERT) have shown strong transfer to lip reading and AV-ASR. Yet, these advances do not directly yield a \emph{token-level}, stream-synchronised representation that cleanly separates identity, speech-synchronised mouth motion, and other facial dynamics—capabilities that are essential for both analysis and controllable generation.

Most prior work tackles one facet at a time. Visual or video masked modeling (MAE/VideoMAE) excels at reconstruction but is modality-specific and does not enforce audio–visual alignment; contrastive AV learning improves correspondence but is typically framed as binary sync classification rather than a shared token space; and dubbing systems (e.g., Wav2Lip, LatentSync, MuseTalk) optimise for generation quality and lip accuracy but are not designed to provide a unified interface that can be driven interchangeably by audio or reference motion within a single model. As a result, existing solutions often rely on wider temporal windows or task-specific heads, and they lack a factorised, alignment-aware representation that transfers seamlessly across synchronisation, understanding, and dubbing. 

We introduce \name, a self-supervised pretraining framework tailored to talking-face video. \name learns three per-frame prompt tokens—\textbf{identity}, \textbf{vocal motion} (speech-synchronised dynamics), and \textbf{ambient motion} (audio-agnostic movements)—and couples masked visual reconstruction with a cross-modal contrastive objective that aligns vocal-motion tokens to temporally matched audio tokens in a shared embedding space. A two-view masking strategy (uniform vs.\ face-preserving) sources patch context and factor prompts, and a simple identity-shuffle regulariser further purifies the identity token. Decoding is performed in two passes per frame (video- and audio-conditioned) with shared weights, which provides symmetric supervision that tightens audio–visual stream synchronization at the token level. This factorised, aligned interface directly supports diverse downstream uses without task-specific architecture changes.

Beyond analysis, \name enables \emph{unified} visual dubbing: using a WanVACE-style DiT backbone for masked video inpainting, we inject either aligned audio tokens or vocal-motion tokens \emph{after} patch embedding via a lightweight AudioPack, so the same model admits no-difference audio- or video-driven control. This preserves identity and pose while re-synchronising the mouth region, and it naturally benefits from the shared token space learned during pretraining.

\textbf{Contributions.} (i) We propose a synchronization-aware, factorised pretraining scheme that yields three prompt tokens (identity, vocal motion, ambient motion) and a shared audio–visual token space. (ii) We introduce a two-view masking design and identity-shuffle regularisation tailored to talking faces. (iii) We show a unified dubbing interface that \emph{for the first time} enables no-difference audio- or video-driven control within a single model by injecting aligned control tokens into a WanVACE backbone. (iv) We demonstrate state-of-the-art performance across four distinct tasks—audio--visual stream synchronization, facial expression/action understanding, visual speech recognition, and visual dubbing—highlighting the effectiveness and generality of the approach.

\input{depds/fig_overview}
