\section{Conclusion}
We introduced \name, a self-supervised framework for talking-face video that couples masked visual reconstruction with contrastive audio–visual alignment and explicitly factorizes each frame into three prompt tokens—\textbf{identity}, \textbf{vocal motion}, and \textbf{ambient motion}. 
This design yields synchronization-aware, disentangled features that transfer directly to diverse applications. 
Across four tasks—audio–visual stream synchronization, facial understanding, visual speech recognition, and unified visual dubbing—\name achieves state-of-the-art performance, validating the effectiveness of this pretraining strategy.
