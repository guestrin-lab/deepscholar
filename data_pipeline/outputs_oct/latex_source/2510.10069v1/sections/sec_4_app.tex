\section{Downstream Adaptation}
\label{sec:downstream}

\input{depds/tab_video_dubbing}

We apply \name\ to four tasks:
\textbf{(i) audio--visual stream synchronization} — infer frame-level lag and synchrony by comparing aligned audio tokens $\mathbf{A}_t$ with vocal-motion tokens $\mathbf{z}^{\mathrm{voc}}_t$;
\textbf{(ii) facial understanding} — infer emotion and head/face actions from the vocal- and ambient-motion tokens;
\textbf{(iii) visual speech recognition (VSR)} — infer text from the vocal-motion tokens;
\textbf{(iv) visual dubbing} — edit a source face video to match target speech or a reference motion, supporting audio- or video-driven control within a single model.

\subsection{Audio--visual synchronisation}
\label{subsec:sync}
We estimate audio–video lag directly in the pretrained shared space without any extra head or training. 
Given per-frame audio tokens $\mathbf{A}_t$ and vocal-motion tokens $\mathbf{z}^{\mathrm{voc}}_s$, compute frame-wise cosine similarity
\[
s(t,s)=\langle \mathbf{A}_t,\mathbf{z}^{\mathrm{voc}}_s\rangle,
\]
and aggregate along temporal offsets
\[
S(\Delta)=\tfrac{1}{T-|\Delta|}\sum_{t=1}^{T-|\Delta|} s\!\big(t,\,t{+}\Delta\big),\qquad
\hat{\Delta}=\arg\max_{\Delta\in[-\Delta_{\max},\,\Delta_{\max}]} S(\Delta).
\]
The in-sync score is $S(0)$ and the estimated lag is $\hat{\Delta}$. 
All computations use the frozen tokens from pretraining; no correspondence MLP or additional supervision is introduced.


\subsection{Facial understanding: emotion and action}
\label{subsec:face-understand}
We use \name’s motion factors per frame and keep the prediction head minimal. 
For each frame $t$, we form a motion descriptor by averaging the two factors,
$\mathbf{u}_t=\tfrac{1}{2}\big(\mathbf{z}^{\mathrm{voc}}_t+\mathbf{z}^{\mathrm{amb}}_t\big)$, 
and apply a single linear classifier to obtain frame-level logits. 
Video-level logits are the mean of frame-level logits over the sampled frames; the final prediction is the softmax over the aggregated logits. 
This setup probes whether the representation cleanly separates speech-synchronized orofacial motion from audio-agnostic dynamics (e.g., blinks, head pose) while keeping the adaptation lightweight; datasets such as CelebV-HQ provide the emotion/action labels used in this setting.



\subsection{Lip reading (VSR)}
\label{subsec:vsr}
We use a visual-only sequence-to-sequence head on top of \name’s vocal-motion tokens. 
Let $\mathbf{V}=\{\mathbf{z}^{\mathrm{voc}}_t\}_{t=1}^{T'}$ be the input to a Conformer encoder, yielding hidden states $\mathbf{H}=\{\mathbf{h}_t\}_{t=1}^{T'}$. 
A Transformer decoder autoregressively predicts subword units with cross-attention to $\mathbf{H}$.

\textit{Losses.}
Let the target token sequence be $Y=\{y_u\}_{u=1}^{U}$ over vocabulary $\Sigma$ (blank $\varnothing$ for CTC). 
The CTC branch defines
\[
P_{\mathrm{CTC}}(Y\,|\,\mathbf{H})=\!\!\sum_{\pi\in\mathcal{B}^{-1}(Y)}\ \prod_{t=1}^{T'} p_{\mathrm{ctc}}(\pi_t\,|\,\mathbf{h}_t), 
\qquad
\mathcal{L}_{\mathrm{CTC}}=-\log P_{\mathrm{CTC}}(Y\,|\,\mathbf{H}),
\]
where $\mathcal{B}$ collapses repeats and removes blanks. 
The decoder branch is trained with teacher forcing:
\[
\mathcal{L}_{\mathrm{DEC}}
= -\sum_{u=1}^{U} \log p_{\mathrm{dec}}\!\left(y_u \,\middle|\, y_{<u},\, \mathbf{H}\right).
\]
The total objective is the standard hybrid form
\[
\mathcal{L}_{\mathrm{VSR}}
= \lambda_{\mathrm{CTC}}\ \mathcal{L}_{\mathrm{CTC}}
+ (1-\lambda_{\mathrm{CTC}})\ \mathcal{L}_{\mathrm{DEC}},
\]
with a tunable weight $\lambda_{\mathrm{CTC}}\in(0,1)$.


\subsection{Video dubbing}
\label{subsec:dubbing}
We cast dubbing as masked video inpainting on face crops within a VACE-style unified interface: task inputs (reference frames and a spatiotemporal mouth mask) are organized as a Video Condition Unit and processed by a Diffusion-Transformer backbone with Concept Decoupling and Context Adapters, following VACE.
Concretely, we instantiate the pipeline on WanVACE and keep its architecture, losses, and training schedule unchanged. WanVACE natively accepts multiple reference images (1–3 in the public release), exposed as a comma-separated list, which we leverage for identity control. 

\textit{Control injection.} Inspired by audio-conditioned injection in recent avatar generation, we adopt a minimal \emph{AudioPack} placed \emph{after} the VACE patch-embedding stage and \emph{before} the first DiT VACE block. Per-frame control tokens—either audio tokens $\mathbf{A}_t$ produced by \name\ or reference vocal-motion tokens $\mathbf{z}^{\mathrm{voc}}_t$—are temporally encoded and projected to the DiT width, then \emph{added} to the patch-embedded video token stream. No other WanVACE components are modified; switching $\mathbf{A}_t$ vs.\ $\mathbf{z}^{\mathrm{voc}}_t$ toggles audio-driven vs.\ video-driven control within the same model.

\textit{Long-form continuity via dual references.} To support arbitrarily long, coherent dubbing, we provide two references: (i) an \emph{identity} image (stable appearance prior), and (ii) the \emph{last frame of the previous segment} (temporal bridge). During training we randomly drop the continuity reference with 10\% probability, so the model can also initialize the \textit{first} segment without a history frame.

\textit{Masking strategy.} Following common lip-sync inpainting practice, we detect the facial region (chin to hairline) and mask the \emph{lower half} as the editable area; to accommodate jaw excursions (post-edit lip motion often increases mouth opening), the mask is expanded slightly downward beyond the chin. This aligns with widely used lower-face inpainting setups in Wav2Lip-style and diffusion-based lip-sync systems. 

\input{depds/tab_vsr}
