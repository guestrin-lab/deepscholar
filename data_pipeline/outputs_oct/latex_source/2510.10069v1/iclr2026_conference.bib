@inproceedings{l3net,
  title={Look, listen and learn},
  author={Arandjelovic, Relja and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={609--617},
  year={2017}
}

@article{avts,
  title={Learning representations from audio-visual spatial alignment},
  author={Morgado, Pedro and Li, Yi and Nvasconcelos, Nuno},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4733--4744},
  year={2020}
}

@inproceedings{syncnet,
  title={Out of time: automated lip sync in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Asian conference on computer vision},
  pages={251--263},
  year={2016},
  organization={Springer}
}

@article{vocalist,
  title={Vocalist: An audio-visual synchronisation model for lips and voices},
  author={Kadandale, Venkatesh S and Montesinos, Juan F and Haro, Gloria},
  journal={arXiv preprint arXiv:2204.02090},
  year={2022}
}

@inproceedings{avhubert,
  title={Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction},
  author={Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@inproceedings{mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@article{videomae,
  title={Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training},
  author={Tong, Zhan and Song, Yibing and Wang, Jue and Wang, Limin},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={10078--10093},
  year={2022}
}

@inproceedings{marlin,
  title={Marlin: Masked autoencoder for facial video representation learning},
  author={Cai, Zhixi and Ghosh, Shreya and Stefanov, Kalin and Dhall, Abhinav and Cai, Jianfei and Rezatofighi, Hamid and Haffari, Reza and Hayat, Munawar},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1493--1504},
  year={2023}
}

@inproceedings{mae_dfer,
  title={Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition},
  author={Sun, Licai and Lian, Zheng and Liu, Bin and Tao, Jianhua},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6110--6121},
  year={2023}
}

@inproceedings{wav2lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={484--492},
  year={2020}
}

@article{makelttalk,
  title={Makelttalk: speaker-aware talking-head animation},
  author={Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu},
  journal={ACM Transactions On Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--15},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sadtalker,
  title={Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8652--8661},
  year={2023}
}

@article{latentsync,
  title={LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision},
  author={Li, Chunyu and Zhang, Chao and Xu, Weikai and Lin, Jingyu and Xie, Jinghui and Feng, Weiguo and Peng, Bingyue and Chen, Cunjian and Xing, Weiwei},
  journal={arXiv preprint arXiv:2412.09262},
  year={2024}
}

@article{lipnet,
  title={Lipnet: End-to-end sentence-level lipreading},
  author={Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and De Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01599},
  year={2016}
}

@inproceedings{whisperer,
  title={Speech recognition models are strong lip-readers},
  author={Prajwal, KR and Afouras, Triantafyllos and Zisserman, Andrew},
  year={2024},
  booktitle={ISCA}
}

@inproceedings{faceformer,
  title={Faceformer: Speech-driven 3d facial animation with transformers},
  author={Fan, Yingruo and Lin, Zhaojiang and Saito, Jun and Wang, Wenping and Komura, Taku},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18770--18780},
  year={2022}
}

@article{wav2vec2,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{scl,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}

@article{dcl,
  title={Debiased contrastive learning},
  author={Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={8765--8775},
  year={2020}
}

@inproceedings{autoavsr,
  title={Auto-avsr: Audio-visual speech recognition with automatic labels},
  author={Ma, Pingchuan and Haliassos, Alexandros and Fernandez-Lopez, Adriana and Chen, Honglie and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{scsampler,
  title={Scsampler: Sampling salient clips from video for efficient action recognition},
  author={Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6232--6242},
  year={2019}
}

@inproceedings{celebvhq,
  title={CelebV-HQ: A large-scale video facial attributes dataset},
  author={Zhu, Hao and Wu, Wayne and Zhu, Wentao and Jiang, Liming and Tang, Siwei and Zhang, Li and Liu, Ziwei and Loy, Chen Change},
  booktitle={European conference on computer vision},
  pages={650--667},
  year={2022},
  organization={Springer}
}

@article{musetalk,
  title={MuseTalk: Real-Time High-Fidelity Video Dubbing via Spatio-Temporal Sampling},
  author={Zhang, Yue and Zhong, Zhizhou and Liu, Minhao and Chen, Zhaokang and Wu, Bin and Zeng, Yubin and Zhan, Chao and He, Yingjie and Huang, Junxin and Zhou, Wenjiang},
  journal={arXiv preprint arXiv:2410.10122},
  year={2024}
}

@inproceedings{avsyncnet,
  title={Audio-driven Talking Face Generation with Stabilized Synchronization Loss},
  author={Yaman, Dogucan and Eyiokur, Fevziye Irem and B{\"a}rmann, Leonard and Ekenel, Haz{\i}m Kemal and Waibel, Alexander},
  booktitle={European Conference on Computer Vision},
  pages={417--435},
  year={2024},
  organization={Springer}
}

@inproceedings{hallo3,
  title={Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer},
  author={Cui, Jiahao and Li, Hui and Zhan, Yun and Shang, Hanlin and Cheng, Kaihui and Ma, Yuqi and Mu, Shan and Zhou, Hang and Wang, Jingdong and Zhu, Siyu},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={21086--21095},
  year={2025}
}

@inproceedings{celebvtext,
  title={Celebv-text: A large-scale facial text-video dataset},
  author={Yu, Jianhui and Zhu, Hao and Jiang, Liming and Loy, Chen Change and Cai, Weidong and Wu, Wayne},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14805--14814},
  year={2023}
}

@inproceedings{mead,
  title={Mead: A large-scale audio-visual dataset for emotional talking-face generation},
  author={Wang, Kaisiyuan and Wu, Qianyi and Song, Linsen and Yang, Zhuoqian and Wu, Wayne and Qian, Chen and He, Ran and Qiao, Yu and Loy, Chen Change},
  booktitle={European conference on computer vision},
  pages={700--717},
  year={2020},
  organization={Springer}
}

@inproceedings{vfhq,
  title={Vfhq: A high-quality dataset and benchmark for video face super-resolution},
  author={Xie, Liangbin and Wang, Xintao and Zhang, Honglun and Dong, Chao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={657--666},
  year={2022}
}

@inproceedings{hdtf,
  title={Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3661--3670},
  year={2021}
}

@article{ravdness,
  title={The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
  author={Livingstone, Steven R and Russo, Frank A},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0196391},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{fid,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{fvd,
  title={Towards accurate generative models of video: A new metric \& challenges},
  author={Unterthiner, Thomas and Van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Raphael and Michalski, Marcin and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1812.01717},
  year={2018}
}

@article{vace,
  title={Vace: All-in-one video creation and editing},
  author={Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},
  journal={arXiv preprint arXiv:2503.07598},
  year={2025}
}

@article{perfectmatch,
  title={Perfect match: A simple method for learning representations for counterfactual inference with neural networks},
  author={Schwab, Patrick and Linhardt, Lorenz and Karlen, Walter},
  journal={arXiv preprint arXiv:1810.00656},
  year={2018}
}

@article{wan,
  title={Wan: Open and advanced large-scale video generative models},
  author={Wan, Team and Wang, Ang and Ai, Baole and Wen, Bin and Mao, Chaojie and Xie, Chen-Wei and Chen, Di and Yu, Feiwu and Zhao, Haiming and Yang, Jianxiao and others},
  journal={arXiv preprint arXiv:2503.20314},
  year={2025}
}

@article{omniavatar,
  title={OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation},
  author={Gan, Qijun and Yang, Ruizi and Zhu, Jianke and Xue, Shaofei and Hoi, Steven},
  journal={arXiv preprint arXiv:2506.18866},
  year={2025}
}

@inproceedings{dit,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4195--4205},
  year={2023}
}