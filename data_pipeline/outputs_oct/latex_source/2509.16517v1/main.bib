@article{liu2025culturevlm,
  title={Culture{VLM}: Characterizing and Improving Cultural Understanding of Vision-Language Models for over 100 Countries},
  author={Liu, Shudong and Jin, Yiqiao and Li, Cheng and Wong, Derek F and Wen, Qingsong and Sun, Lichao and Chen, Haipeng and Xie, Xing and Wang, Jindong},
  journal={arXiv preprint arXiv:2501.01282},
  year={2025}
  }

@misc{schneider2025gimmickgloballyinclusive,
      title={{GIMMICK} -- Globally Inclusive Multimodal Multitask Cultural Knowledge Benchmarking}, 
      author={Florian Schneider and Carolin Holtermann and Chris Biemann and Anne Lauscher},
      year={2025},
      eprint={2502.13766},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.13766}, 
}

@inproceedings{bhatia-etal-2024-local_GlobalRG,
    title = "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
    author = "Bhatia, Mehar  and
      Ravi, Sahithya  and
      Chinchure, Aditya  and
      Hwang, EunJeong  and
      Shwartz, Vered",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.385/",
    doi = "10.18653/v1/2024.emnlp-main.385",
    pages = "6763--6782",
    abstract = "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures {--} underscoring the necessity for enhancing multicultural understanding in vision-language models."
}

@inproceedings{yin2021broaden_GDVCR,
  title = {Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning},
  author = {Yin, Da and Li, Liunian Harold and Hu, Ziniu and Peng, Nanyun and Chang, Kai-Wei},
  booktitle = {EMNLP},
  year = {2021}
}

@inproceedings{
mogrovejo2024cvqa,
title={{CVQA}: Culturally-diverse Multilingual Visual Question Answering Benchmark},
author={David Orlando Romero Mogrovejo and Chenyang Lyu and Haryo Akbarianto Wibowo and Santiago G{\'o}ngora and Aishik Mandal and Sukannya Purkayastha and Jesus-German Ortiz-Barajas and Emilio Villa Cueva and Jinheon Baek and Soyeong Jeong and Injy Hamed and Zheng Xin Yong and Zheng Wei Lim and Paula M{\'o}nica Silva and Jocelyn Dunstan and M{\'e}lanie Jouitteau and David LE MEUR and Joan Nwatu and Ganzorig Batnasan and Munkh-Erdene Otgonbold and Munkhjargal Gochoo and Guido Ivetta and Luciana Benotti and Laura Alonso Alemany and Hern{\'a}n Maina and Jiahui Geng and Tiago Timponi Torrent and Frederico Belcavello and Marcelo Viridiano and Jan Christian Blaise Cruz and Dan John Velasco and Oana Ignat and Zara Burzo and Chenxi Whitehouse and Artem Abzaliev and Teresa Clifford and Gr{\'a}inne Caulfield and Teresa Lynn and Christian Salamea-Palacios and Vladimir Araujo and Yova Kementchedjhieva and Mihail Minkov Mihaylov and Israel Abebe Azime and Henok Biadglign Ademtew and Bontu Fufa Balcha and Naome A Etori and David Ifeoluwa Adelani and Rada Mihalcea and Atnafu Lambebo Tonja and Maria Camila Buitrago Cabrera and Gisela Vallejo and Holy Lovenia and Ruochen Zhang and Marcos Estecha-Garitagoitia and Mario Rodr{\'\i}guez-Cantelar and Toqeer Ehsan and Rendi Chevi and Muhammad Farid Adilazuarda and Ryandito Diandaru and Samuel Cahyawijaya and Fajri Koto and Tatsuki Kuribayashi and Haiyue Song and Aditya Nanda Kishore Khandavally and Thanmay Jayakumar and Raj Dabre and Mohamed Fazli Mohamed Imam and Kumaranage Ravindu Yasas Nagasinghe and Alina Dragonetti and Luis Fernando D'Haro and Olivier NIYOMUGISHA and Jay Gala and Pranjal A Chitale and Fauzan Farooqui and Thamar Solorio and Alham Fikri Aji},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=E18kRXTGmV}
}

@inproceedings{nayak-etal-2024-benchmarking_CulturalVQA,
    title = "Benchmarking Vision Language Models for Cultural Understanding",
    author = "Nayak, Shravan  and
      Jain, Kanishk  and
      Awal, Rabiul  and
      Reddy, Siva  and
      Steenkiste, Sjoerd Van  and
      Hendricks, Lisa Anne  and
      Stanczak, Karolina  and
      Agrawal, Aishwarya",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.329/",
    doi = "10.18653/v1/2024.emnlp-main.329",
    pages = "5769--5790",
    abstract = "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding - recognizing objects, attributes, and actions - rather than cultural comprehension. This study introduces CulturalVQA, a visual question-answering benchmark aimed at assessing VLM`s geo-diverse cultural understanding. We curate a diverse collection of 2,378 image-question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly weaker capabilities for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CulturalVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures."
}

@misc{tang2024mtvqa,
      title={{MTVQA}: Benchmarking Multilingual Text-Centric Visual Question Answering}, 
      author={Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and Yanjie Wang and Yuliang Liu and Hao Liu and Xiang Bai and Can Huang},
      year={2024},
      eprint={2405.11985},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.11985}, 
}

@inproceedings{2024_seavqa,
    title = "{SEA}-{VQA}: {S}outheast {A}sian Cultural Context Dataset For Visual Question Answering",
    author = "Urailertprasert, Norawit  and
      Limkonchotiwat, Peerat  and
      Suwajanakorn, Supasorn  and
      Nutanong, Sarana",
    editor = "Gu, Jing  and
      Fu, Tsu-Jui (Ray)  and
      Hudson, Drew  and
      Celikyilmaz, Asli  and
      Wang, William",
    booktitle = "Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.alvr-1.15/",
    doi = "10.18653/v1/2024.alvr-1.15",
    pages = "173--185",
    abstract = "Visual Question Answering (VQA) is a critical task that requires the simultaneous understanding of visual and textual information. While significant advancements have been made with multilingual datasets, these often lack cultural specificity, especially in the context of Southeast Asia (SEA). In this paper, we introduce SEA-VQA aiming to highlight the challenges and gaps in existing VQA models when confronted with culturally specific content. Our dataset includes images from eight SEA countries, curated from the UNESCO Cultural Heritage collection. Our evaluation, comparing GPT-4 and GEMINI models, demonstrates substantial performance drops on culture-centric questions compared to the A-OKVQA dataset, a commonsense and world-knowledge VQA benchmark comprising approximately 25,000 questions. Our findings underscore the importance of cultural diversity in VQA datasets and reveal substantial gaps in the ability of current VQA models to handle culturally rich contexts. SEA-VQA serves as a crucial benchmark for identifying these gaps and guiding future improvements in VQA systems."
}

@inproceedings{nikandrou_2025_crope,
    title = "{CROPE}: Evaluating In-Context Adaptation of Vision and Language Models to Culture-Specific Concepts",
    author = "Nikandrou, Malvina  and
      Pantazopoulos, Georgios  and
      Vitsakis, Nikolas  and
      Konstas, Ioannis  and
      Suglia, Alessandro",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.402/",
    pages = "7917--7936",
    ISBN = "979-8-89176-189-6"
}

 @article{Wang_CVLUE_2025, title={{CVLUE}: A New Benchmark Dataset for Chinese Vision-Language Understanding Evaluation}, volume={39}, DOI={10.1609/aaai.v39i8.32884}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wang, Yuxuan and Liu, Yijun and Yu, Fei and Huang, Chen and Li, Kexin and Wan, Zhiguo and Che, Wanxiang and Chen, Hongyang}, year={2025}, month=apr, pages={8196–8204} }

@ARTICLE{10214252,
  author={Zhang, Xi and Zhang, Feifei and Xu, Changsheng},
  journal={IEEE Transactions on Image Processing}, 
  title={Reducing Vision-Answer Biases for Multiple-Choice {VQA}}, 
  year={2023},
  volume={32},
  number={},
  pages={4621-4634},
  keywords={Visualization;Predictive models;Correlation;Videos;Dogs;Training;Annotations;Multiple-choice VQA;vision-answer bias;causal intervention;counterfactual interaction learning},
  doi={10.1109/TIP.2023.3302162}}

@inproceedings{wang-etal-2023-dataset,
    title = "Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond",
    author = "Wang, Zhecan  and
      Chen, Long  and
      You, Haoxuan  and
      Xu, Keyang  and
      He, Yicheng  and
      Li, Wenhao  and
      Codella, Noel  and
      Chang, Kai-Wei  and
      Chang, Shih-Fu",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.576/",
    doi = "10.18653/v1/2023.findings-emnlp.576",
    pages = "8598--8617",
    abstract = "Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is Unbalanced Matching bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is Distractor Similarity bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentiation. Extensive experiments demonstrate the effectiveness of ADS and ICT in consistently improving model performance across different benchmarks, even in domain-shifted scenarios."
}

 @article{Zhang_Zhang_Xu_2024, address={Los Alamitos, CA, USA}, title={{NExT-OOD}: Overcoming Dual Multiple-Choice {VQA} Biases}, volume={46}, ISSN={1939-3539}, DOI={10.1109/TPAMI.2023.3269429}, number={04}, journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence}, publisher={IEEE Computer Society}, author={Zhang, Xi and Zhang, Feifei and Xu, Changsheng}, year={2024}, month=apr, pages={1913–1931} }

@inproceedings{fu-etal-2023-generate,
    title = "Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge",
    author = "Fu, Xingyu  and
      Zhang, Sheng  and
      Kwon, Gukyeong  and
      Perera, Pramuditha  and
      Zhu, Henghui  and
      Zhang, Yuhao  and
      Li, Alexander Hanbo  and
      Wang, William Yang  and
      Wang, Zhiguo  and
      Castelli, Vittorio  and
      Ng, Patrick  and
      Roth, Dan  and
      Xiang, Bing",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.147/",
    doi = "10.18653/v1/2023.findings-acl.147",
    pages = "2333--2346",
    abstract = "The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias {--} the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality {--} only models using GPT-3 can achieve the best result. To address the aforementioned challenges, we propose RASO: a new VQA pipeline that deploys a generate-then-select strategy guided by world knowledge for the first time. Rather than following the de facto standard to train a multi-modal model that directly generates the VQA answer, {\{}pasted macro {\textquoteleft}MODEL'{\}}name first adopts PLM to generate all the possible answers, and then trains a lightweight answer selection model for the correct answer. As proved in our analysis, RASO expands the knowledge coverage from in-domain training data by a large margin. We provide extensive experimentation and show the effectiveness of our pipeline by advancing the state-of-the-art by 4.1{\%} on OK-VQA, without additional computation cost."
}

 @article{Liu_Fan_Zhou_Xu_2023, title={Be flexible! learn to debias by sampling and prompting for robust visual question answering}, volume={60}, ISSN={0306-4573}, DOI={https://doi.org/10.1016/j.ipm.2023.103296}, abstractNote={Recent studies point out that VQA models tend to rely on the language prior in the training data to answer the questions, which prevents the VQA model from generalization on the out-of-distribution test data. To address this problem, approaches are designed to reduce the language distribution prior effect by constructing negative image–question pairs, while they cannot provide the proper visual reason for answering the question. In this paper, we present a new debiasing framework for VQA by Learning to Sample paired image–question and Prompt for given question (LSP). Specifically, we construct the negative image–question pairs with certain sampling rate to prevent the model from overly relying on the visual shortcut content. Notably, question types provide a strong hint for answering the questions. We utilize question type to constrain the sampling process for negative question–image pairs, and further learn the question type-guided prompt for better question comprehension. Extensive experiments on two public benchmarks, VQA-CP v2 and VQA v2, demonstrate that our model achieves new state-of-the-art results in overall accuracy, i.e., 61.95% and 65.26%.}, number={3}, journal={Information Processing \& Management}, author={Liu, Jin and Fan, ChongFeng and Zhou, Fengyu and Xu, Huijuan}, year={2023}, pages={103296} }

@inproceedings{thapliyal-etal-2022-crossmodal3600,
    title = "Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset",
    author = "Thapliyal, Ashish V.  and
      Pont Tuset, Jordi  and
      Chen, Xi  and
      Soricut, Radu",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.45/",
    doi = "10.18653/v1/2022.emnlp-main.45",
    pages = "715--729",
    abstract = "Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics."
}

@INPROCEEDINGS{IPAS_mosaic_2025,
  author={Burda-Lassen, Olena and Chadha, Aman and Goswami, Shashank and Jain, Vinija},
  booktitle={2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS)}, 
  title={How Culturally Aware Are Vision-Language Models?}, 
  year={2025},
  volume={CFP2540Z-ART},
  number={},
  pages={1-6},
  keywords={Humanities;Accuracy;Sensitivity;Image processing;Symbols;Data models;Cultural differences;Artificial intelligence;Vision-Language Models;Image Captioning;Evaluation of Vision-Language Models;Culture-Specific Items},
  doi={10.1109/IPAS63548.2025.10924504}}

@misc{bhalerao2025multiagentmultimodalmodelsmulticultural_mosaig,
      title={Multi-Agent Multimodal Models for Multicultural Text to Image Generation}, 
      author={Parth Bhalerao and Mounika Yalamarty and Brian Trinh and Oana Ignat},
      year={2025},
      eprint={2502.15972},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.15972}, 
}

@misc{cahyawijaya2025crowdsourcecrawlgeneratecreating_SEA-VL,
      title={Crowdsource, Crawl, or Generate? Creating {SEA-VL}, a Multicultural Vision-Language Dataset for Southeast Asia}, 
      author={Samuel Cahyawijaya and Holy Lovenia and Joel Ruben Antony Moniz and Tack Hwa Wong and Mohammad Rifqi Farhansyah and Thant Thiri Maung and Frederikus Hudi and David Anugraha and Muhammad Ravi Shulthan Habibi and Muhammad Reza Qorib and Amit Agarwal and Joseph Marvin Imperial and Hitesh Laxmichand Patel and Vicky Feliren and Bahrul Ilmi Nasution and Manuel Antonio Rufino and Genta Indra Winata and Rian Adam Rajagede and Carlos Rafael Catalan and Mohamed Fazli Imam and Priyaranjan Pattnayak and Salsabila Zahirah Pranida and Kevin Pratama and Yeshil Bangera and Adisai Na-Thalang and Patricia Nicole Monderin and Yueqi Song and Christian Simon and Lynnette Hui Xian Ng and Richardy Lobo' Sapan and Taki Hasan Rafi and Bin Wang and Supryadi and Kanyakorn Veerakanjana and Piyalitt Ittichaiwong and Matthew Theodore Roque and Karissa Vincentio and Takdanai Kreangphet and Phakphum Artkaew and Kadek Hendrawan Palgunadi and Yanzhi Yu and Rochana Prih Hastuti and William Nixon and Mithil Bangera and Adrian Xuan Wei Lim and Aye Hninn Khine and Hanif Muhammad Zhafran and Teddy Ferdinan and Audra Aurora Izzani and Ayushman Singh and Evan and Jauza Akbar Krito and Michael Anugraha and Fenal Ashokbhai Ilasariya and Haochen Li and John Amadeo Daniswara and Filbert Aurelian Tjiaranata and Eryawan Presma Yulianrifat and Can Udomcharoenchaikit and Fadil Risdian Ansori and Mahardika Krisna Ihsani and Giang Nguyen and Anab Maulana Barik and Dan John Velasco and Rifo Ahmad Genadi and Saptarshi Saha and Chengwei Wei and Isaiah Flores and Kenneth Ko Han Chen and Anjela Gail Santos and Wan Shen Lim and Kaung Si Phyo and Tim Santos and Meisyarah Dwiastuti and Jiayun Luo and Jan Christian Blaise Cruz and Ming Shan Hee and Ikhlasul Akmal Hanif and M. Alif Al Hakim and Muhammad Rizky Sya'ban and Kun Kerdthaisong and Lester James V. Miranda and Fajri Koto and Tirana Noor Fatyanosa and Alham Fikri Aji and Jostin Jerico Rosal and Jun Kevin and Robert Wijaya and Onno P. Kampman and Ruochen Zhang and Börje F. Karlsson and Peerat Limkonchotiwat},
      year={2025},
      eprint={2503.07920},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.07920}, 
}

@inproceedings{liu-etal-2021-visually_MaRVL,
    title = "Visually Grounded Reasoning across Languages and Cultures",
    author = "Liu, Fangyu  and
      Bugliarello, Emanuele  and
      Ponti, Edoardo Maria  and
      Reddy, Siva  and
      Collier, Nigel  and
      Elliott, Desmond",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.818",
    pages = "10467--10485"
}

@inproceedings{heritage1,
booktitle = {Eurographics Workshop on Graphics and Cultural Heritage},
editor = {Bucciero, Alberto and Fanini, Bruno and Graf, Holger and Pescarin, Sofia and Rizvic, Selma},
title = {{AI Based Image Segmentation of Cultural Heritage Objects used for Multi-View Stereo 3D Reconstructions}},
author = {Kutlu, Hasan and Brucker, Felix and Kallendrusch, Ben and Santos, Pedro and Fellner, Dieter W.},
year = {2023},
publisher = {The Eurographics Association},
ISSN = {2312-6124},
ISBN = {978-3-03868-217-2},
DOI = {10.2312/gch.20231160}
}

@article{heritage2,
title = {Adaptive segmentation of traditional cultural pattern based on superpixel Log-Euclidean Gaussian metric},
journal = {Applied Soft Computing},
volume = {97},
pages = {106828},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106828},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620307663},
author = {Xiaogang Hou and Haiying Zhao and Yan Ma and Wei Zhou},
keywords = {Image segmentation, Superpixel, Log-Euclidean Gaussian, Manifold, Spectral clustering}
}

@INPROCEEDINGS{heritage_notredame,
  author={Réby, Kévin and Guilhelm, Anaïs and De Luca, Livio},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Semantic Segmentation using Foundation Models for Cultural Heritage: an Experimental Study on Notre-Dame de Paris}, 
  year={2023},
  volume={},
  number={},
  pages={1681-1689},
  keywords={Computer vision;Vocabulary;Grounding;Computational modeling;Semantic segmentation;Conferences;Data models;Deep learning;Foundation models;Cultural Heritage;Image segmentation;Notre Dame de Paris},
  doi={10.1109/ICCVW60793.2023.00184}}

@article{heritage_museum,
author = {Mazzamuto*, Michele and Ragusa*, Francesco and Furnari*, Antonino and Farinella*, Giovanni Maria},
title = {Learning to Detect Attended Objects in Cultural Sites with Gaze Signals and Weak Object Supervision},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3647999},
doi = {10.1145/3647999},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {35},
numpages = {21},
keywords = {Cultural sites, wearable devices, gaze, object detection}
}

@Article{heritage_siena,
AUTHOR = {Bassier, M. and Mazzacca, G. and Battisti, R. and Malek, S. and Remondino, F.},
TITLE = {COMBINING IMAGE AND POINT CLOUD SEGMENTATION TO IMPROVE HERITAGE UNDERSTANDING},
JOURNAL = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
VOLUME = {XLVIII-2/W4-2024},
YEAR = {2024},
PAGES = {49--56},
URL = {https://isprs-archives.copernicus.org/articles/XLVIII-2-W4-2024/49/2024/},
DOI = {10.5194/isprs-archives-XLVIII-2-W4-2024-49-2024}
}

@INPROCEEDINGS {heritage_damage,
author = { Ivanova, Daniela and Aversa, Marco and Henderson, Paul and Williamson, John },
booktitle = { 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) },
title = {{ ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage }},
year = {2025},
volume = {},
ISSN = {},
pages = {7439-7449},
keywords = {Training;Image segmentation;Taxonomy;Machine learning;Media;Benchmark testing;Transformers;Image restoration;Reliability;Textiles},
doi = {10.1109/WACV61041.2025.00723},
url = {https://doi.ieeecomputersociety.org/10.1109/WACV61041.2025.00723},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =mar}

@INPROCEEDINGS{segment_culture1,
  author={Hou, Xiaogang and Zhao, Haiying and Zhou, Xiangbing and Liang, Xiaoyue and Yang, Jiaxing},
  booktitle={2024 International Conference on Culture-Oriented Science \& Technology (CoST)}, 
  title={Traditional Pattern Segmentation Algorithm Based on Semantic Prediction and Region Learning*}, 
  year={2024},
  volume={},
  number={},
  pages={218-223},
  keywords={Training;Image segmentation;Annotations;Heuristic algorithms;Semantic segmentation;Semantics;Prediction algorithms;Semantic segmentation;Category activation map;Region growing;Seed region expansion;Traditional pattern segmentation},
  doi={10.1109/CoST64302.2024.00050}}

@INPROCEEDINGS{segment_culture2,
  author={Garcia Alvarado, Jose Alfredo and Erich, Floris and Motoda, Tomohiro and Mustafa, Abdullah and Domae, Yukiyasu and Ramirez-Alpizar, Ixchel},
  booktitle={2025 IEEE/SICE International Symposium on System Integration (SII)}, 
  title={Foundation Models need to be culturally fine-tuned}, 
  year={2025},
  volume={},
  number={},
  pages={381-385},
  keywords={Location awareness;Image segmentation;Adaptation models;Image recognition;Foundation models;System integration;Transformers;Cultural differences;Robots;Guidelines},
  doi={10.1109/SII59315.2025.10871084}}

@inproceedings{li-etal-2024-foodieqa,
    title = "{F}oodie{QA}: A Multimodal Dataset for Fine-Grained Understanding of {C}hinese Food Culture",
    author = "Li, Wenyan  and
      Zhang, Crystina  and
      Li, Jiaang  and
      Peng, Qiwei  and
      Tang, Raphael  and
      Zhou, Li  and
      Zhang, Weijia  and
      Hu, Guimin  and
      Yuan, Yifei  and
      S{\o}gaard, Anders  and
      Hershcovich, Daniel  and
      Elliott, Desmond",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    year = "2024",
    url = "https://aclanthology.org/2024.emnlp-main.1063",
    }

@MISC{make-sense-ai,
   author = {Piotr Skalski},
   title = {{Make Sense}},
   howpublished = "\url{https://github.com/SkalskiP/make-sense/}",
   year = {2019},
}

@INPROCEEDINGS{7780909,
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Visual7{W}: Grounded Question Answering in Images}, 
  year={2016},
  volume={},
  number={},
  pages={4995-5004},
  keywords={Visualization;Grounding;Knowledge discovery;Data collection;Genomics;Bioinformatics;Cognition},
  doi={10.1109/CVPR.2016.540}}

@inproceedings{bai-etal-2025-power_MosAIC,
    title = "The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning",
    author = "Bai, Longju  and
      Borah, Angana  and
      Ignat, Oana  and
      Mihalcea, Rada",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.152/",
    pages = "2970--2993",
    ISBN = "979-8-89176-189-6"
}

@software{ChatGPT_software,
  author       = {OpenAI},
  title        = {ChatGPT},
  year         = {2014}
}

@inproceedings{idefix2,
    title={What matters when building vision-language models?},
    author={Hugo Lauren{\c{c}}on and Leo Tronchon and Matthieu Cord and Victor Sanh},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=dtvJF1Vy2i}
}

@article{li2025llavaonevision,
title={{LL}a{VA}-{O}ne{V}ision: Easy Visual Task Transfer},
author={Bo Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Peiyuan Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2025},
url={https://openreview.net/forum?id=zKv8qULV6n},
note={}
}

@article{chen2024expanding,
    title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
    author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
    journal={arXiv preprint arXiv:2412.05271},
    year={2024}
  }

@article{llama32,
  publtype={informal},
  author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurélien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Rozière and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Grégoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and et al.},
  title={The {L}lama 3 Herd of Models},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2407.21783},
  url={https://doi.org/10.48550/arXiv.2407.21783}
}

@inproceedings{dai2023instructblip,
title={Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=vvoWPYqZJA}
}

@article{yao2024minicpm,
  title={{MiniCPM-V}: A {GPT-4V} Level {MLLM} on Your Phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{qwen25,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@misc{corr_kviscuit,
      title={Evaluating Visual and Cultural Interpretation: The {K-V}iscuit Benchmark with Human-{VLM} Collaboration}, 
      author={Yujin Baek and ChaeHun Park and Jaeseok Kim and Yu-Jung Heo and Du-Seong Chang and Jaegul Choo},
      year={2024},
      eprint={2406.16469},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16469}, 
}

@InProceedings{archaeo_2023_ICCV,
    author    = {Enayati, Aref and Palmieri, Luca and Vascon, Sebastiano and Pelillo, Marcello and Aslan, Sinem},
    title     = {Semantic Motif Segmentation of Archaeological Fresco Fragments},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2023},
    pages     = {1717-1725}
}

@misc{satar2023exploitingsemanticrolecontextualized,
      title={Exploiting Semantic Role Contextualized Video Features for Multi-Instance Text-Video Retrieval EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2022}, 
      author={Burak Satar and Hongyuan Zhu and Hanwang Zhang and Joo Hwee Lim},
      year={2023},
      eprint={2206.14381},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.14381}, 
}

@ARTICLE{Damen2022RESCALING,
           title={Rescaling Egocentric Vision: Collection, Pipeline and Challenges for EPIC-KITCHENS-100},
           author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Furnari, Antonino 
           and Ma, Jian and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
           journal   = {International Journal of Computer Vision (IJCV)},
           year      = {2022},
           volume = {130},
           pages = {33–55},
           Url       = {https://doi.org/10.1007/s11263-021-01531-2}
} 

@article{Ilaslan_Köksal_Lin_Satar_Shou_Xu_2025, title={VG-TVP: Multimodal Procedural Planning via Visually Grounded Text-Video Prompting}, volume={39}, url={https://ojs.aaai.org/index.php/AAAI/article/view/32406}, DOI={10.1609/aaai.v39i4.32406}, abstractNote={Large Language Model (LLM)-based agents have shown promise in procedural tasks, but the potential of multimodal instructions augmented by texts and videos to assist users remains under-explored. To address this gap, we propose the Visually Grounded Text-Video Prompting (VG-TVP) method which is a novel LLM-empowered Multimodal Procedural Planning (MPP) framework. It generates cohesive text and video procedural plans given a specified high-level objective. The main challenges are achieving textual and visual informativeness, temporal coherence, and accuracy in procedural plans. VG-TVP leverages the zero-shot reasoning capability of LLMs, the video-to-text generation ability of the video captioning models, and the text-to-video generation ability of diffusion models. VG-TVP improves the interaction between modalities by proposing a novel Fusion of Captioning (FoC) method and using Text-to-Video Bridge (T2V-B) and Video-to-Text Bridge (V2T-B). They allow LLMs to guide the generation of visually-grounded text plans and textual-grounded video plans. To address the scarcity of datasets suitable for MPP, we have curated a new dataset called Daily-Life Task Procedural Plans (Daily-PP). We conduct comprehensive experiments and benchmarks to evaluate human preferences (regarding textual and visual informativeness, temporal coherence, and plan accuracy). Our VG-TVP method outperforms unimodal baselines on the Daily-PP dataset.}, number={4}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ilaslan, Muhammet Furkan and Köksal, Ali and Lin, Kevin Qinghong and Satar, Burak and Shou, Mike Zheng and Xu, Qianli}, year={2025}, month={Apr.}, pages={3886-3894} }