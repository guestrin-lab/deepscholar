\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Baek et~al.(2024)Baek, Park, Kim, Heo, Chang, and Choo}]{corr_kviscuit}
Yujin Baek, ChaeHun Park, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, and Jaegul Choo. 2024.
\newblock \href {https://arxiv.org/abs/2406.16469} {Evaluating visual and cultural interpretation: The {K-V}iscuit benchmark with human-{VLM} collaboration}.
\newblock \emph{Preprint}, arXiv:2406.16469.

\bibitem[{Bai et~al.(2025)Bai, Borah, Ignat, and Mihalcea}]{bai-etal-2025-power_MosAIC}
Longju Bai, Angana Borah, Oana Ignat, and Rada Mihalcea. 2025.
\newblock \href {https://aclanthology.org/2025.naacl-long.152/} {The power of many: Multi-agent multimodal models for cultural image captioning}.
\newblock In \emph{Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 2970--2993, Albuquerque, New Mexico. Association for Computational Linguistics.

\bibitem[{Bhalerao et~al.(2025)Bhalerao, Yalamarty, Trinh, and Ignat}]{bhalerao2025multiagentmultimodalmodelsmulticultural_mosaig}
Parth Bhalerao, Mounika Yalamarty, Brian Trinh, and Oana Ignat. 2025.
\newblock \href {https://arxiv.org/abs/2502.15972} {Multi-agent multimodal models for multicultural text to image generation}.
\newblock \emph{Preprint}, arXiv:2502.15972.

\bibitem[{Bhatia et~al.(2024)Bhatia, Ravi, Chinchure, Hwang, and Shwartz}]{bhatia-etal-2024-local_GlobalRG}
Mehar Bhatia, Sahithya Ravi, Aditya Chinchure, EunJeong Hwang, and Vered Shwartz. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.emnlp-main.385} {From local concepts to universals: Evaluating the multicultural understanding of vision-language models}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 6763--6782, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Burda-Lassen et~al.(2025)Burda-Lassen, Chadha, Goswami, and Jain}]{IPAS_mosaic_2025}
Olena Burda-Lassen, Aman Chadha, Shashank Goswami, and Vinija Jain. 2025.
\newblock \href {https://doi.org/10.1109/IPAS63548.2025.10924504} {How culturally aware are vision-language models?}
\newblock In \emph{2025 IEEE 6th International Conference on Image Processing, Applications and Systems (IPAS)}, volume CFP2540Z-ART, pages 1--6.

\bibitem[{Cahyawijaya et~al.(2025)Cahyawijaya, Lovenia, Moniz, Wong, Farhansyah, Maung, Hudi, Anugraha, Habibi, Qorib, Agarwal, Imperial, Patel, Feliren, Nasution, Rufino, Winata, Rajagede, Catalan, Imam, Pattnayak, Pranida, Pratama, Bangera, Na-Thalang, Monderin, Song, Simon, Ng, Sapan, Rafi, Wang, Supryadi, Veerakanjana, Ittichaiwong, Roque, Vincentio, Kreangphet, Artkaew, Palgunadi, Yu, Hastuti, Nixon, Bangera, Lim, Khine, Zhafran, Ferdinan, Izzani, Singh, Evan, Krito, Anugraha, Ilasariya, Li, Daniswara, Tjiaranata, Yulianrifat, Udomcharoenchaikit, Ansori, Ihsani, Nguyen, Barik, Velasco, Genadi, Saha, Wei, Flores, Chen, Santos, Lim, Phyo, Santos, Dwiastuti, Luo, Cruz, Hee, Hanif, Hakim, Sya'ban, Kerdthaisong, Miranda, Koto, Fatyanosa, Aji, Rosal, Kevin, Wijaya, Kampman, Zhang, Karlsson, and Limkonchotiwat}]{cahyawijaya2025crowdsourcecrawlgeneratecreating_SEA-VL}
Samuel Cahyawijaya, Holy Lovenia, Joel Ruben~Antony Moniz, Tack~Hwa Wong, Mohammad~Rifqi Farhansyah, Thant~Thiri Maung, Frederikus Hudi, David Anugraha, Muhammad Ravi~Shulthan Habibi, Muhammad~Reza Qorib, Amit Agarwal, Joseph~Marvin Imperial, Hitesh~Laxmichand Patel, Vicky Feliren, Bahrul~Ilmi Nasution, Manuel~Antonio Rufino, Genta~Indra Winata, Rian~Adam Rajagede, Carlos~Rafael Catalan, and 73 others. 2025.
\newblock \href {https://arxiv.org/abs/2503.07920} {Crowdsource, crawl, or generate? creating {SEA-VL}, a multicultural vision-language dataset for southeast asia}.
\newblock \emph{Preprint}, arXiv:2503.07920.

\bibitem[{Chen et~al.(2024)Chen, Wang, Cao, Liu, Gao, Cui, Zhu, Ye, Tian, Liu et~al.}]{chen2024expanding}
Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024.
\newblock Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling.
\newblock \emph{arXiv preprint arXiv:2412.05271}.

\bibitem[{Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi}]{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.
\newblock \href {https://openreview.net/forum?id=vvoWPYqZJA} {Instruct{BLIP}: Towards general-purpose vision-language models with instruction tuning}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Damen et~al.(2022)Damen, Doughty, Farinella, Furnari, Ma, Kazakos, Moltisanti, Munro, Perrett, Price, and Wray}]{Damen2022RESCALING}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. 2022.
\newblock \href {https://doi.org/10.1007/s11263-021-01531-2} {Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 130:33–55.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan, Goyal, Hartshorn, Yang, Mitra, Sravankumar, Korenev, Hinsvark, Rao, Zhang, Rodriguez, Gregerson, Spataru, Rozière, Biron, Tang, Chern, Caucheteux, Nayak, Bi, Marra, McConnell, Keller, Touret, Wu, Wong, Ferrer, Nikolaidis, Allonsius, Song, Pintz, Livshits, Esiobu, Choudhary, Mahajan, Garcia-Olano, Perino, Hupkes, Lakomkin, AlBadawy, Lobanova, Dinan, Smith, Radenovic, Zhang, Synnaeve, Lee, Anderson, Nail, Mialon, Pang, Cucurell, Nguyen, Korevaar, Xu, Touvron, Zarov, Ibarra, Kloumann, Misra, Evtimov, Copet, Lee, Geffert, Vranes, Park, Mahadeokar, Shah, van~der Linde, Billock, Hong, Lee, Fu, Chi, Huang, Liu, Wang, Yu, Bitton, Spisak, Park, Rocca, Johnstun, Saxe, Jia, Alwala, Upasani, Plawiak, Li, Heafield, Stone, and et~al.}]{llama32}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 82 others. 2024.
\newblock \href {https://doi.org/10.48550/arXiv.2407.21783} {The {L}lama 3 herd of models}.
\newblock \emph{CoRR}, abs/2407.21783.

\bibitem[{Fu et~al.(2023)Fu, Zhang, Kwon, Perera, Zhu, Zhang, Li, Wang, Wang, Castelli, Ng, Roth, and Xiang}]{fu-etal-2023-generate}
Xingyu Fu, Sheng Zhang, Gukyeong Kwon, Pramuditha Perera, Henghui Zhu, Yuhao Zhang, Alexander~Hanbo Li, William~Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Dan Roth, and Bing Xiang. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-acl.147} {Generate then select: Open-ended visual question answering guided by world knowledge}.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 2333--2346, Toronto, Canada. Association for Computational Linguistics.

\bibitem[{Ilaslan et~al.(2025)Ilaslan, Köksal, Lin, Satar, Shou, and Xu}]{Ilaslan_Köksal_Lin_Satar_Shou_Xu_2025}
Muhammet~Furkan Ilaslan, Ali Köksal, Kevin~Qinghong Lin, Burak Satar, Mike~Zheng Shou, and Qianli Xu. 2025.
\newblock \href {https://doi.org/10.1609/aaai.v39i4.32406} {Vg-tvp: Multimodal procedural planning via visually grounded text-video prompting}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 39(4):3886--3894.

\bibitem[{Lauren{\c{c}}on et~al.(2024)Lauren{\c{c}}on, Tronchon, Cord, and Sanh}]{idefix2}
Hugo Lauren{\c{c}}on, Leo Tronchon, Matthieu Cord, and Victor Sanh. 2024.
\newblock \href {https://openreview.net/forum?id=dtvJF1Vy2i} {What matters when building vision-language models?}
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[{Li et~al.(2025)Li, Zhang, Guo, Zhang, Li, Zhang, Zhang, Zhang, Li, Liu, and Li}]{li2025llavaonevision}
Bo~Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2025.
\newblock \href {https://openreview.net/forum?id=zKv8qULV6n} {{LL}a{VA}-{O}ne{V}ision: Easy visual task transfer}.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Li et~al.(2024)Li, Zhang, Li, Peng, Tang, Zhou, Zhang, Hu, Yuan, S{\o}gaard, Hershcovich, and Elliott}]{li-etal-2024-foodieqa}
Wenyan Li, Crystina Zhang, Jiaang Li, Qiwei Peng, Raphael Tang, Li~Zhou, Weijia Zhang, Guimin Hu, Yifei Yuan, Anders S{\o}gaard, Daniel Hershcovich, and Desmond Elliott. 2024.
\newblock \href {https://aclanthology.org/2024.emnlp-main.1063} {{F}oodie{QA}: A multimodal dataset for fine-grained understanding of {C}hinese food culture}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}.

\bibitem[{Liu et~al.(2021)Liu, Bugliarello, Ponti, Reddy, Collier, and Elliott}]{liu-etal-2021-visually_MaRVL}
Fangyu Liu, Emanuele Bugliarello, Edoardo~Maria Ponti, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021.
\newblock \href {https://aclanthology.org/2021.emnlp-main.818} {Visually grounded reasoning across languages and cultures}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 10467--10485, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Liu et~al.(2023)Liu, Fan, Zhou, and Xu}]{Liu_Fan_Zhou_Xu_2023}
Jin Liu, ChongFeng Fan, Fengyu Zhou, and Huijuan Xu. 2023.
\newblock \href {https://doi.org/10.1016/j.ipm.2023.103296} {Be flexible! learn to debias by sampling and prompting for robust visual question answering}.
\newblock \emph{Information Processing \& Management}, 60(3):103296.

\bibitem[{Liu et~al.(2025)Liu, Jin, Li, Wong, Wen, Sun, Chen, Xie, and Wang}]{liu2025culturevlm}
Shudong Liu, Yiqiao Jin, Cheng Li, Derek~F Wong, Qingsong Wen, Lichao Sun, Haipeng Chen, Xing Xie, and Jindong Wang. 2025.
\newblock Culture{VLM}: Characterizing and improving cultural understanding of vision-language models for over 100 countries.
\newblock \emph{arXiv preprint arXiv:2501.01282}.

\bibitem[{Mogrovejo et~al.(2024)Mogrovejo, Lyu, Wibowo, G{\'o}ngora, Mandal, Purkayastha, Ortiz-Barajas, Cueva, Baek, Jeong, Hamed, Yong, Lim, Silva, Dunstan, Jouitteau, MEUR, Nwatu, Batnasan, Otgonbold, Gochoo, Ivetta, Benotti, Alemany, Maina, Geng, Torrent, Belcavello, Viridiano, Cruz, Velasco, Ignat, Burzo, Whitehouse, Abzaliev, Clifford, Caulfield, Lynn, Salamea-Palacios, Araujo, Kementchedjhieva, Mihaylov, Azime, Ademtew, Balcha, Etori, Adelani, Mihalcea, Tonja, Cabrera, Vallejo, Lovenia, Zhang, Estecha-Garitagoitia, Rodr{\'\i}guez-Cantelar, Ehsan, Chevi, Adilazuarda, Diandaru, Cahyawijaya, Koto, Kuribayashi, Song, Khandavally, Jayakumar, Dabre, Imam, Nagasinghe, Dragonetti, D'Haro, NIYOMUGISHA, Gala, Chitale, Farooqui, Solorio, and Aji}]{mogrovejo2024cvqa}
David Orlando~Romero Mogrovejo, Chenyang Lyu, Haryo~Akbarianto Wibowo, Santiago G{\'o}ngora, Aishik Mandal, Sukannya Purkayastha, Jesus-German Ortiz-Barajas, Emilio~Villa Cueva, Jinheon Baek, Soyeong Jeong, Injy Hamed, Zheng~Xin Yong, Zheng~Wei Lim, Paula~M{\'o}nica Silva, Jocelyn Dunstan, M{\'e}lanie Jouitteau, David~LE MEUR, Joan Nwatu, Ganzorig Batnasan, and 57 others. 2024.
\newblock \href {https://openreview.net/forum?id=E18kRXTGmV} {{CVQA}: Culturally-diverse multilingual visual question answering benchmark}.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}.

\bibitem[{Nayak et~al.(2024)Nayak, Jain, Awal, Reddy, Steenkiste, Hendricks, Stanczak, and Agrawal}]{nayak-etal-2024-benchmarking_CulturalVQA}
Shravan Nayak, Kanishk Jain, Rabiul Awal, Siva Reddy, Sjoerd~Van Steenkiste, Lisa~Anne Hendricks, Karolina Stanczak, and Aishwarya Agrawal. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.emnlp-main.329} {Benchmarking vision language models for cultural understanding}.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 5769--5790, Miami, Florida, USA. Association for Computational Linguistics.

\bibitem[{Nikandrou et~al.(2025)Nikandrou, Pantazopoulos, Vitsakis, Konstas, and Suglia}]{nikandrou_2025_crope}
Malvina Nikandrou, Georgios Pantazopoulos, Nikolas Vitsakis, Ioannis Konstas, and Alessandro Suglia. 2025.
\newblock \href {https://aclanthology.org/2025.naacl-long.402/} {{CROPE}: Evaluating in-context adaptation of vision and language models to culture-specific concepts}.
\newblock In \emph{Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 7917--7936, Albuquerque, New Mexico. Association for Computational Linguistics.

\bibitem[{OpenAI(2014)}]{ChatGPT_software}
OpenAI. 2014.
\newblock Chatgpt.

\bibitem[{Satar et~al.(2023)Satar, Zhu, Zhang, and Lim}]{satar2023exploitingsemanticrolecontextualized}
Burak Satar, Hongyuan Zhu, Hanwang Zhang, and Joo~Hwee Lim. 2023.
\newblock \href {https://arxiv.org/abs/2206.14381} {Exploiting semantic role contextualized video features for multi-instance text-video retrieval epic-kitchens-100 multi-instance retrieval challenge 2022}.
\newblock \emph{Preprint}, arXiv:2206.14381.

\bibitem[{Schneider et~al.(2025)Schneider, Holtermann, Biemann, and Lauscher}]{schneider2025gimmickgloballyinclusive}
Florian Schneider, Carolin Holtermann, Chris Biemann, and Anne Lauscher. 2025.
\newblock \href {https://arxiv.org/abs/2502.13766} {{GIMMICK} -- globally inclusive multimodal multitask cultural knowledge benchmarking}.
\newblock \emph{Preprint}, arXiv:2502.13766.

\bibitem[{Skalski(2019)}]{make-sense-ai}
Piotr Skalski. 2019.
\newblock {Make Sense}.
\newblock \url{https://github.com/SkalskiP/make-sense/}.

\bibitem[{Tang et~al.(2024)Tang, Liu, Ye, Lu, Wei, Lin, Li, Mahmood, Feng, Zhao, Wang, Liu, Liu, Bai, and Huang}]{tang2024mtvqa}
Jingqun Tang, Qi~Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz~Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. 2024.
\newblock \href {https://arxiv.org/abs/2405.11985} {{MTVQA}: Benchmarking multilingual text-centric visual question answering}.
\newblock \emph{Preprint}, arXiv:2405.11985.

\bibitem[{Thapliyal et~al.(2022)Thapliyal, Pont~Tuset, Chen, and Soricut}]{thapliyal-etal-2022-crossmodal3600}
Ashish~V. Thapliyal, Jordi Pont~Tuset, Xi~Chen, and Radu Soricut. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.emnlp-main.45} {Crossmodal-3600: A massively multilingual multimodal evaluation dataset}.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 715--729, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.

\bibitem[{Urailertprasert et~al.(2024)Urailertprasert, Limkonchotiwat, Suwajanakorn, and Nutanong}]{2024_seavqa}
Norawit Urailertprasert, Peerat Limkonchotiwat, Supasorn Suwajanakorn, and Sarana Nutanong. 2024.
\newblock \href {https://doi.org/10.18653/v1/2024.alvr-1.15} {{SEA}-{VQA}: {S}outheast {A}sian cultural context dataset for visual question answering}.
\newblock In \emph{Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR)}, pages 173--185, Bangkok, Thailand. Association for Computational Linguistics.

\bibitem[{Wang et~al.(2025)Wang, Liu, Yu, Huang, Li, Wan, Che, and Chen}]{Wang_CVLUE_2025}
Yuxuan Wang, Yijun Liu, Fei Yu, Chen Huang, Kexin Li, Zhiguo Wan, Wanxiang Che, and Hongyang Chen. 2025.
\newblock \href {https://doi.org/10.1609/aaai.v39i8.32884} {{CVLUE}: A new benchmark dataset for chinese vision-language understanding evaluation}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 39(8):8196–8204.

\bibitem[{Wang et~al.(2023)Wang, Chen, You, Xu, He, Li, Codella, Chang, and Chang}]{wang-etal-2023-dataset}
Zhecan Wang, Long Chen, Haoxuan You, Keyang Xu, Yicheng He, Wenhao Li, Noel Codella, Kai-Wei Chang, and Shih-Fu Chang. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-emnlp.576} {Dataset bias mitigation in multiple-choice visual question answering and beyond}.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 8598--8617, Singapore. Association for Computational Linguistics.

\bibitem[{Yang et~al.(2024)Yang, Yang, Zhang, Hui, Zheng, Yu, Li, Liu, Huang, Wei, Lin, Yang, Tu, Zhang, Yang, Yang, Zhou, Lin, Dang, Lu, Bao, Yang, Yu, Li, Xue, Zhang, Zhu, Men, Lin, Li, Xia, Ren, Ren, Fan, Su, Zhang, Wan, Liu, Cui, Zhang, and Qiu}]{qwen25}
An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024.
\newblock Qwen2.5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}.

\bibitem[{Yao et~al.(2024)Yao, Yu, Zhang, Wang, Cui, Zhu, Cai, Li, Zhao, He et~al.}]{yao2024minicpm}
Yuan Yao, Tianyu Yu, Ao~Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, and 1 others. 2024.
\newblock {MiniCPM-V}: A {GPT-4V} level {MLLM} on your phone.
\newblock \emph{arXiv preprint arXiv:2408.01800}.

\bibitem[{Yin et~al.(2021)Yin, Li, Hu, Peng, and Chang}]{yin2021broaden_GDVCR}
Da~Yin, Liunian~Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. 2021.
\newblock Broaden the vision: Geo-diverse visual commonsense reasoning.
\newblock In \emph{EMNLP}.

\bibitem[{Zhang et~al.(2023)Zhang, Zhang, and Xu}]{10214252}
Xi~Zhang, Feifei Zhang, and Changsheng Xu. 2023.
\newblock \href {https://doi.org/10.1109/TIP.2023.3302162} {Reducing vision-answer biases for multiple-choice {VQA}}.
\newblock \emph{IEEE Transactions on Image Processing}, 32:4621--4634.

\bibitem[{Zhang et~al.(2024)Zhang, Zhang, and Xu}]{Zhang_Zhang_Xu_2024}
Xi~Zhang, Feifei Zhang, and Changsheng Xu. 2024.
\newblock \href {https://doi.org/10.1109/TPAMI.2023.3269429} {{NExT-OOD}: Overcoming dual multiple-choice {VQA} biases}.
\newblock \emph{IEEE Transactions on Pattern Analysis \& Machine Intelligence}, 46(04):1913–1931.

\bibitem[{Zhu et~al.(2016)Zhu, Groth, Bernstein, and Fei-Fei}]{7780909}
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li~Fei-Fei. 2016.
\newblock \href {https://doi.org/10.1109/CVPR.2016.540} {Visual7{W}: Grounded question answering in images}.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 4995--5004.

\end{thebibliography}
