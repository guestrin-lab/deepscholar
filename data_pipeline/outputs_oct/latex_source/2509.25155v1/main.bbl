% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{chung2025long}
\BIBentryALTinterwordspacing
Y.~Chung, G.~T. Kakkar, Y.~Gan, B.~Milne, and F.~Ozcan, ``Is long context all you need? leveraging llm's extended context for nl2sql,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2501.12372}
\BIBentrySTDinterwordspacing

\bibitem{qian2024long}
\BIBentryALTinterwordspacing
H.~Qian, Z.~Liu, P.~Zhang, K.~Mao, Y.~Zhou, X.~Chen, and Z.~Dou, ``Are long-llms a necessity for long-context tasks?'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.15318}
\BIBentrySTDinterwordspacing

\bibitem{dao2022flash}
\BIBentryALTinterwordspacing
T.~Dao, D.~Y. Fu, S.~Ermon, A.~Rudra, and C.~Ré, ``Flashattention: Fast and memory-efficient exact attention with io-awareness,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.14135}
\BIBentrySTDinterwordspacing

\bibitem{gu2024mamba}
\BIBentryALTinterwordspacing
A.~Gu and T.~Dao, ``Mamba: Linear-time sequence modeling with selective state spaces,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2312.00752}
\BIBentrySTDinterwordspacing

\bibitem{dao2024ssd}
\BIBentryALTinterwordspacing
T.~Dao and A.~Gu, ``Transformers are ssms: Generalized models and efficient algorithms through structured state space duality,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.21060}
\BIBentrySTDinterwordspacing

\bibitem{lieber2024jambahybridtransformermambalanguage}
\BIBentryALTinterwordspacing
O.~Lieber, B.~Lenz, H.~Bata, G.~Cohen, J.~Osin, I.~Dalmedigos, E.~Safahi, S.~Meirom, Y.~Belinkov, S.~Shalev-Shwartz, O.~Abend, R.~Alon, T.~Asida, A.~Bergman, R.~Glozman, M.~Gokhman, A.~Manevich, N.~Ratner, N.~Rozen, E.~Shwartz, M.~Zusman, and Y.~Shoham, ``Jamba: A hybrid transformer-mamba language model,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2403.19887}
\BIBentrySTDinterwordspacing

\bibitem{dong2024hymbahybridheadarchitecturesmall}
\BIBentryALTinterwordspacing
X.~Dong, Y.~Fu, S.~Diao, W.~Byeon, Z.~Chen, A.~S. Mahabaleshwarkar, S.-Y. Liu, M.~V. Keirsbilck, M.-H. Chen, Y.~Suhara, Y.~Lin, J.~Kautz, and P.~Molchanov, ``Hymba: A hybrid-head architecture for small language models,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2411.13676}
\BIBentrySTDinterwordspacing

\bibitem{sun2023retentivenetworksuccessortransformer}
\BIBentryALTinterwordspacing
Y.~Sun, L.~Dong, S.~Huang, S.~Ma, Y.~Xia, J.~Xue, J.~Wang, and F.~Wei, ``Retentive network: A successor to transformer for large language models,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2307.08621}
\BIBentrySTDinterwordspacing

\bibitem{poli2023hyena}
\BIBentryALTinterwordspacing
M.~Poli, S.~Massaroli, E.~Nguyen, D.~Y. Fu, T.~Dao, S.~Baccus, Y.~Bengio, S.~Ermon, and C.~Ré, ``Hyena hierarchy: Towards larger convolutional language models,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2302.10866}
\BIBentrySTDinterwordspacing

\bibitem{peng2023rwkv}
\BIBentryALTinterwordspacing
B.~Peng, E.~Alcaide, Q.~Anthony, A.~Albalak, S.~Arcadinho, S.~Biderman, H.~Cao, X.~Cheng, M.~Chung, M.~Grella, K.~K. GV, X.~He, H.~Hou, J.~Lin, P.~Kazienko, J.~Kocon, J.~Kong, B.~Koptyra, H.~Lau, K.~S.~I. Mantri, F.~Mom, A.~Saito, G.~Song, X.~Tang, B.~Wang, J.~S. Wind, S.~Wozniak, R.~Zhang, Z.~Zhang, Q.~Zhao, P.~Zhou, Q.~Zhou, J.~Zhu, and R.-J. Zhu, ``Rwkv: Reinventing rnns for the transformer era,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.13048}
\BIBentrySTDinterwordspacing

\bibitem{qin2023toeplitzneuralnetworksequence}
\BIBentryALTinterwordspacing
Z.~Qin, X.~Han, W.~Sun, B.~He, D.~Li, D.~Li, Y.~Dai, L.~Kong, and Y.~Zhong, ``Toeplitz neural network for sequence modeling,'' 2023. [Online]. Available: \url{https://arxiv.org/abs/2305.04749}
\BIBentrySTDinterwordspacing

\bibitem{tran2024fouriermixedwindowattentionaccelerating}
\BIBentryALTinterwordspacing
N.~T. Tran and J.~Xin, ``Fourier-mixed window attention: Accelerating informer for long sequence time-series forecasting,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2307.00493}
\BIBentrySTDinterwordspacing

\bibitem{zhang2024edgeshardefficientllminference}
\BIBentryALTinterwordspacing
M.~Zhang, J.~Cao, X.~Shen, and Z.~Cui, ``Edgeshard: Efficient llm inference via collaborative edge computing,'' 2024. [Online]. Available: \url{https://arxiv.org/abs/2405.14371}
\BIBentrySTDinterwordspacing

\bibitem{10.1145/3700410.3702126}
Z.~Wang, J.~Yang, X.~Qian, S.~Xing, X.~Jiang, C.~Lv, and S.~Zhang, ``Mnn-llm: A generic inference engine for fast large language model deployment on mobile devices,'' in \emph{Proceedings of the 6th ACM International Conference on Multimedia in Asia Workshops}, ser. MMAsia '24 Workshops.\hskip 1em plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing Machinery, 2024.

\bibitem{llamacpp}
\BIBentryALTinterwordspacing
Llama.cpp. [Online]. Available: \url{https://github.com/ggerganov/llama.cpp}
\BIBentrySTDinterwordspacing

\bibitem{haris2024designing}
J.~Haris, R.~Saha, W.~Hu, and J.~Cano, ``Designing efficient llm accelerators for edge devices,'' \emph{arXiv preprint arXiv:2408.00462}, 2024.

\bibitem{xu2025fast}
D.~Xu, H.~Zhang, L.~Yang, R.~Liu, G.~Huang, M.~Xu, and X.~Liu, ``Fast on-device llm inference with npus,'' in \emph{Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1}, 2025, pp. 445--462.

\bibitem{zhu2025edge}
Y.~Zhu and H.~Lu, ``Edge-side npu inference optimization: Adaptation research of multimodal large models on qualcomm platforms,'' \emph{Intelligent Data Analysis}, p. 1088467X251342172, 2025.

\bibitem{das2025xamba}
\BIBentryALTinterwordspacing
A.~Das, A.~Raha, S.~Kundu, S.~K. Ghosh, D.~Mathaikutty, and V.~Raghunathan, ``Xamba: Enabling efficient state space models on resource-constrained neural processing units,'' 2025. [Online]. Available: \url{https://arxiv.org/abs/2502.06924}
\BIBentrySTDinterwordspacing

\bibitem{aalishah2025mambalitesr}
R.~Aalishah, M.~Navardi, and T.~Mohsenin, ``Mambalitesr: Image super-resolution with low-rank mamba using knowledge distillation,'' in \emph{2025 26th International Symposium on Quality Electronic Design (ISQED)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2025, pp. 1--8.

\end{thebibliography}
