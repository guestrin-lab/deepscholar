@misc{zhang2024dovetail,
      title={Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference}, 
      author={Libo Zhang and Zhaoning Zhang and Baizhou Xu and Songzhu Mei and Dongsheng Li},
      year={2024},
      eprint={2412.18934},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18934}, 
}
@misc{jayanth2024benchmarking,
      title={Benchmarking Edge AI Platforms for High-Performance ML Inference}, 
      author={Rakshith Jayanth and Neelesh Gupta and Viktor Prasanna},
      year={2024},
      eprint={2409.14803},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.14803}, 
}
@misc{das2025xamba,
      title={XAMBA: Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units}, 
      author={Arghadip Das and Arnab Raha and Shamik Kundu and Soumendu Kumar Ghosh and Deepak Mathaikutty and Vijay Raghunathan},
      year={2025},
      eprint={2502.06924},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.06924}, 
}
@misc{dao2024ssd,
      title={Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality}, 
      author={Tri Dao and Albert Gu},
      year={2024},
      eprint={2405.21060},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.21060}, 
}
@misc{gu2024mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@misc{chung2025long,
      title={Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL}, 
      author={Yeounoh Chung and Gaurav T. Kakkar and Yu Gan and Brenton Milne and Fatma Ozcan},
      year={2025},
      eprint={2501.12372},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      doi={https://doi.org/10.14778/3742728.3742761},
      url={https://arxiv.org/abs/2501.12372}, 
}

@misc{qian2024long,
      title={Are Long-LLMs A Necessity For Long-Context Tasks?}, 
      author={Hongjin Qian and Zheng Liu and Peitian Zhang and Kelong Mao and Yujia Zhou and Xu Chen and Zhicheng Dou},
      year={2024},
      eprint={2405.15318},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.15318}, 
}
@misc{dao2022flash,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.14135}, 
}

@online{llamacpp,
    title = {Llama.cpp},
    url = {https://github.com/ggerganov/llama.cpp},
    urldate      = {11/3/2024}
}

@misc{yu2024edgellmenablingefficientlarge,
      title={EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting}, 
      author={Zhongzhi Yu and Zheng Wang and Yuhan Li and Haoran You and Ruijie Gao and Xiaoya Zhou and Sreenidhi Reedy Bommu and Yang Katie Zhao and Yingyan Celine Lin},
      year={2024},
      eprint={2406.15758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.15758}, 
}

@misc{zhang2024edgeshardefficientllminference,
      title={EdgeShard: Efficient LLM Inference via Collaborative Edge Computing}, 
      author={Mingjin Zhang and Jiannong Cao and Xiaoming Shen and Zeyang Cui},
      year={2024},
      eprint={2405.14371},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2405.14371}, 
}

@inproceedings{10.1145/3700410.3702126, author = {Wang, Zhaode and Yang, Jingbang and Qian, Xinyu and Xing, Shiwen and Jiang, Xiaotang and Lv, Chengfei and Zhang, Shengyu}, title = {MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices}, year = {2024}, isbn = {9798400713149}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {}, doi = {}, abstract = {Large language models (LLMs) have demonstrated exceptional performance across a variety of tasks. However, their substantial scale leads to significant computational resource consumption during inference, resulting in high costs. Consequently, edge device inference presents a promising solution. The primary challenges of edge inference include memory usage and inference speed. This paper introduces MNN-LLM, a framework specifically designed to accelerate the deployment of large language models on mobile devices. MNN-LLM addresses the runtime characteristics of LLMs through model quantization and DRAM-Flash hybrid storage, effectively reducing memory usage. It rearranges weights and inputs based on mobile CPU instruction sets and GPU characteristics while employing strategies such as multicore load balancing, mixed-precision floating-point operations, and geometric computations to enhance performance. Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current mainstream LLM-specific frameworks.}, booktitle = {Proceedings of the 6th ACM International Conference on Multimedia in Asia Workshops}, articleno = {11}, numpages = {7}, location = { }, series = {MMAsia '24 Workshops} }

@article{haris2024designing,
  title={Designing efficient LLM accelerators for edge devices},
  author={Haris, Jude and Saha, Rappy and Hu, Wenhao and Cano, Jos{\'e}},
  journal={arXiv preprint arXiv:2408.00462},
  year={2024}
}

@inproceedings{shen2025sparse,
  title={Sparse Learning for State Space Models on Mobile},
  author={Shen, Xuan and Zheng, Hangyu and Gong, Yifan and Kong, Zhenglun and Yang, Changdi and Zhan, Zheng and Wu, Yushu and Lin, Xue and Wang, Yanzhi and Zhao, Pu and others},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025}
}

@article{wang2025fastmamba,
  title={FastMamba: A High-Speed and Efficient Mamba Accelerator on FPGA with Accurate Quantization},
  author={Wang, Aotao and Shao, Haikuo and Ma, Shaobo and Wang, Zhongfeng},
  journal={arXiv preprint arXiv:2505.18975},
  year={2025}
}

@misc{qin2023toeplitzneuralnetworksequence,
      title={Toeplitz Neural Network for Sequence Modeling}, 
      author={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},
      year={2023},
      eprint={2305.04749},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.04749}, 
}

@misc{tran2024fouriermixedwindowattentionaccelerating,
      title={Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence Time-Series Forecasting}, 
      author={Nhat Thanh Tran and Jack Xin},
      year={2024},
      eprint={2307.00493},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.00493}, 
}

@misc{sun2023retentivenetworksuccessortransformer,
      title={Retentive Network: A Successor to Transformer for Large Language Models}, 
      author={Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
      year={2023},
      eprint={2307.08621},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.08621}, 
}

@misc{lieber2024jambahybridtransformermambalanguage,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}

@misc{dong2024hymbahybridheadarchitecturesmall,
      title={Hymba: A Hybrid-head Architecture for Small Language Models}, 
      author={Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},
      year={2024},
      eprint={2411.13676},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.13676}, 
}





@article{zhu2025edge,
  title={Edge-side NPU inference optimization: Adaptation research of multimodal large models on qualcomm platforms},
  author={Zhu, Yajie and Lu, Hongtao},
  journal={Intelligent Data Analysis},
  pages={1088467X251342172},
  year={2025},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{xu2025fast,
  title={Fast on-device LLM inference with npus},
  author={Xu, Daliang and Zhang, Hao and Yang, Liming and Liu, Ruiqi and Huang, Gang and Xu, Mengwei and Liu, Xuanzhe},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={445--462},
  year={2025}
}

@inproceedings{aalishah2025mambalitesr,
  title={MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge Distillation},
  author={Aalishah, Romina and Navardi, Mozhgan and Mohsenin, Tinoosh},
  booktitle={2025 26th International Symposium on Quality Electronic Design (ISQED)},
  pages={1--8},
  year={2025},
  organization={IEEE}
}

@misc{de2024griffinmixinggatedlinear,
      title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}, 
      author={Soham De and Samuel L. Smith and Anushan Fernando and Aleksandar Botev and George Cristian-Muraru and Albert Gu and Ruba Haroun and Leonard Berrada and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and Arnaud Doucet and David Budden and Yee Whye Teh and Razvan Pascanu and Nando De Freitas and Caglar Gulcehre},
      year={2024},
      eprint={2402.19427},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.19427}, 
}


@misc{poli2023hyena,
      title={Hyena Hierarchy: Towards Larger Convolutional Language Models}, 
      author={Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
      year={2023},
      eprint={2302.10866},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.10866}, 
}


@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13048}, 
}