\section{Related Work}
\label{sec:background}
This section reviews prior research on machine learning for molecular representations, multimodal language models in chemistry, and reasoning capabilities in LLMs, which are foundational to our proposed framework for training reasoning LLMs tailored to molecular property prediction.

\paragraph{Machine Learning for Molecular Representation.}
GNNs have evolved as a dominant paradigm for molecular graph representation, progressing from early convolutional \citep{moleculargraph,schnet} and message-passing \citep{mpnn} frameworks to sophisticated 3D-aware models \citep{mpnn, rgcl, simsgt,unimol}, enabling robust applications in property prediction, virtual screening, and drug discovery \citep{gnn-chemistry-applications}.
In parallel, specialized molecular language models have reframed molecular structures as textual sequences such as SMILES strings\citep{smiles}, with models like MolecularGPT \citep{moleculargpt} and BioT5-Plus \citep{biot5-plus} supporting few-shot adaptation and multi-task learning for diverse chemical and biological tasks \citep{scilitllm, reactxt, molca}. 

\paragraph{Multimodal Language Models for Chemistry.}
The emerging trend of multimodal LLMs in chemistry integrates diverse data typesâ€”such as SMILES strings and molecular graphs to address unimodal limitations, as seen in foundational molecular-text models \citep{mol-llm, chemvlm, molca}, instruction-tuned assistants \citep{instructmol}, and tool-augmented systems \citep{chemcrow}, enhancing robustness in property prediction \citep{molt5}, molecular design \citep{mol-llm}, and synthesis planning \citep{relm, reactxt}. However, these models still lack the capability to provide chemical reasoning for their predictions.

\paragraph{Reasoning in Large Language Models.}
Reasoning capabilities have demonstrated remarkable efficacy in commercial LLMs, particularly through chain-of-thought processes as exemplified in OpenAI's o1 series and other advanced models \citep{cot,openai-o1,gemini,claude}.
Training these abilities leverages RL techniques, from Proximal Policy Optimization \citep{ppo} in RL from Human Feedback (RLHF) \citep{rlhf} for preference alignment, to efficient extensions like Group Relative Policy Optimization (GRPO) \citep{grpo} with outcome-based rewards and Reinforcement Learning with Verifiable Rewards (RLVR) for one-shot verifiable steps, improving generalization on complex tasks \citep{rlvr}.
These RL advancements motivate our adaptation for chemical-specific reasoning in the field of molecular property prediction.