\appendix

\section{Appendix}

\subsection{Overview}

\cref{sec:suppl_implementation} provides additional implementation details of our method C$^3$Editor, as in Sec. 4.1 in \textit{Main Paper}. \cref{sec:suppl_qualitative} provides additional qualitative results of our method C$^3$Editor, as in Sec. 4.2 in \textit{Main Paper}. \cref{sec:suppl_ablation} provides the ablation study of view propagation in our method C$^3$Editor, as in Sec. 4.4 in \textit{Main Paper}. \cref{sec:suppl_gt} provides the visualization of ground truth fitting in our method C$^3$Editor, as in Sec. 5 in \textit{Main Paper}. \cref{sec:suppl_prompt} provides the prompt library used in our method C$^3$Editor, as in Sec. 4.2 in \textit{Main Paper}. The videos in the \textit{Suppl} folder demonstrate our editing demo. The editing prompt is: turn him into a clown, and the selected GT view is Frame 40.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth,trim=0 1em 0 0,clip]{figure/suppl_gt.pdf}
    \caption{2D editing results without view propagation.}
    \label{fig:suppl_gt_view_prop}
\end{figure}

\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth,trim=0 1em 0 0,clip]{figure/suppl_gt_fit.pdf}
    \caption{2D editing results of the GT view during intra-GT prior fitting}
    \label{fig:suppl_fitting}
\end{figure*}

\subsection{Implementation Details}
\label{sec:suppl_implementation}

As detailed in Sec. 4.1 of the \textit{Main Paper}, our approach builds upon the advanced 2D-lifting-based 3D Gaussian Splatting (GS) Editing framework, GaussianEditor~\cite{chen2024gaussianeditor}. Specifically, we adopt 3D Gaussian Splatting~\cite{kerbl20233d} as the underlying 3D representation and leverage Instruct-Pix2Pix~\cite{brooks2023instructpix2pix}, a state-of-the-art diffusion-based 2D editing model. All experiments were executed on a single NVIDIA RTX A6000 GPU, with the fine-tuning process requiring only 1 minute in total.

We evaluate our method using the MipNeRF-360~\cite{barron2022mip} and Instruct-NeRF-to-NeRF~\cite{haque2023instruct} datasets. The MipNeRF-360 dataset provides 360-degree views of various 3D scenes, while the Instruct-NeRF-to-NeRF dataset consists of diverse 3D editing scenarios. To assess performance, we utilize CLIP-Scores~\cite{taited2023CLIPScore} (both image-text and image-image) and Fr√©chet Inception Distance (FID)~\cite{heusel2017gans, Seitzer2020FID}. CLIP-Score (image-text) evaluates the alignment between 3D edited outputs and the provided editing text, while CLIP-Score (image-image) measures consistency across 2D views generated during the editing process. Higher CLIP-Scores indicate better editing quality and greater view consistency. FID, computed between the original rendered images and the edited results, serves as a metric for image quality, where lower scores signify superior results. For the image-image CLIP-score across $n$ views, we compute the similarity between view $i$ and view $i+1$ and take the average as the score for the given setting (view $n$ is computed with view $0$). For the image-text CLIP-score, we directly compute the similarity between each image and the text, then average the scores across all views under the given setting to obtain the final score. Details of the prompt library are provided in \cref{sec:suppl_prompt}.

\subsection{Additional Qualitative Results}
\label{sec:suppl_qualitative}

We provide additional qualitative results of our method C$^3$Editor on the MipNeRF-360~\cite{barron2022mip} and Instruct-NeRF-to-NeRF~\cite{haque2023instruct} datasets. As shown in \cref{fig:suppl_gt_qual} and \cref{fig:suppl_gt_qual_2}, our method successfully achieves controllable consistency in 2D models for 3D editing. The results demonstrate that our method can generate high-quality 3D editing results with controllable consistency across multiple views. The edited results are consistent with the provided editing text, the chosen GT edited image, and maintain high-quality image generation across different views. The results also show that our method can handle diverse editing scenarios, such as changing the color of objects, adding new objects, and modifying object shapes. The qualitative results further demonstrate the effectiveness of our method in achieving controllable consistency in 2D models for 3D editing.

\subsection{Ablation of View Propagation}
\label{sec:suppl_ablation}

We demonstrate the importance of view propagation in our method C$^3$Editor. As shown in \cref{fig:suppl_gt_view_prop}, we show the results of our method without view propagation. Without view propagation, the edited results exhibit inconsistencies across different views; only views near the GT view are consistent with the GT edited image. In contrast, views far from the GT view exhibit significant inconsistencies with the GT edited image. This may be caused by the limited generalization ability of the 2D editing model. Our method with view propagation can effectively address this issue by propagating the editing information across different views, achieving controllable consistency in 2D models for 3D editing, as shown in Fig.6 in \textit{Main Paper}.


\subsection{Visualization of GT Fitting}
\label{sec:suppl_gt}

We provide the visualization of ground truth fitting in our method C$^3$Editor. As shown in \cref{fig:suppl_fitting}, we visualize the 2D editing results of the GT view during intra-GT prior fitting in Sec. 3.3 in \textit{Main Paper}. The results demonstrate that our method can effectively fit the ground truth edited image, achieving controllable consistency in 2D models for 3D editing. The results further demonstrate the effectiveness of our method in achieving controllable consistency in 2D models for 3D editing.


\subsection{Prompt Library}
\label{sec:suppl_prompt}

``Give him a cowboy hat", ``Give him a mustache", ``Make him bald" ``Turn him into a clown", ``As a bronze bust", ``Turn him into Albert Einstein", ``Turn his face into a skull", ``Turn him into a Modigliani painting", ``Turn him into Batman", ``Turn him into Hulk", ``Turn him into an old lady", ``make it snowy", ``make the bike on fire", ``make the bench on fire", ``make the road snowy", ``make bike colorful", ``make the bike red and blue", ``make the bench red and blue", ``make the front wheel red and the rare wheel blue", ``Customize the bench with a galaxy theme", ``Add fallen autumn leaves", ``turn it into a marble table", ``Transform the table surface to white ceramic with blue patterns", ``Apply a gradient effect on the table", ``make it look like Van Gogh's painting", ``Add glowing lights around the bonsai branches", ``Add a few golden flowers", ``Make it look like it's covered in snow", ``Replace the flowers with glowing lanterns", ``Turn the bonsai flowers into red maple leaves", ``Change the color of the bulldozer to bright blue"


\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth,trim=0 8em 0 5em,clip]{figure/suppl_qualitative.pdf}
    \caption{More qualitative results of our method C$^3$Editor.}
    \label{fig:suppl_gt_qual}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.95\linewidth,trim=0 8em 0 5em,clip]{figure/suppl_qualitative_2.pdf}
    \caption{More qualitative results of our method C$^3$Editor.}
    \label{fig:suppl_gt_qual_2}
\end{figure*}