\vspace{-10pt}
\section{Experiment: Sudoku Puzzle} \label{appendix:examples_sudoku}
\vspace{-10pt}


We provide detailed description of training data generation for the Sudoku puzzle experiment. The data generation process simulates a backtracking-based solving algorithm and records the intermediate states as supervised training trajectories for AP-MDM.

\paragraph{State Representation}
The $9 \times 9$ Sudoku grid is represented as a sequence of 324 tokens, where each cell is encoded using 3 consecutive tokens: $(\text{value}, \text{color}, \text{marker})$. The vocabulary consists of 32 tokens including: EMPTY (unfilled cell), MASK (unknown position to be predicted), digits 1-9 (filled values), WHITE (default color indicating no branch), 15 branch COLOR tokens (tracking different branching paths), NORMAL (standard state), SKULL (failed branch point), BRANCH (active branch starting position), and SEPARATOR (structural delimiter between cells). The value token can be EMPTY (for unfilled cells), a digit 1-9 (for filled cells), or MASK (for positions being predicted). The color token tracks which branching decision path the cell belongs to, using WHITE for non-branched cells and one of 15 COLOR tokens for cells within branches. The marker token indicates the cell's computational status: NORMAL for standard cells, SKULL for positions that caused backtracking, and BRANCH for branch starting points.

The solving algorithm consists of several atomic operations, each translated into state-transition tuples for AP-MDM training:

\textbf{Assign and Branch:} The two most fundamental operations in Sudoku solving are deterministic assignment and branch creation, both implemented through combinations of $\remask$ and $\unmask$ operations. For Assign, when deterministically filling a cell with value $v$, we generate a 2-step transition: first, the three tokens at the target position are remasked, converting them to $(\mask, \mask, \mask)$; second, these masks are unmasked to the target values $(v, c, \text{NORMAL})$ where $c$ is the appropriate color token determined by the current branch context. For Branch, when creating a branch at position $(r, c)$ with candidate value $v$ and branch identifier $b$, we similarly generate a 2-step transition: first, remask the cell tokens to $(\mask, \mask, \mask)$; second, unmask to $(v, \text{COLOR}_b, \text{BRANCH})$ to mark this cell as a branch starting point with the corresponding branch color. 

Concrete examples visualizing these two operations are shown in \Cref{fig:sudoku_assign_branch}.

\textbf{Contradiction Marking, Backtrack, and Recovery:} When the solver detects a contradiction (a cell with no valid candidates), before backtracking, we mark this contradicting position through a 2-step transition: first remask the cell to $(\mask, \mask, \mask)$, then unmask to $(\text{EMPTY}, \text{COLOR}_b, \text{SKULL})$ to visually indicate the contradiction location. When backtracking from a failed branch, multiple cells need to be cleared through a 2-step transition that handles both ordinary cells and the branch starting point differently: in the first step, ordinary cells are remasked in all three token positions while the branch starting point only has its marker token remasked; in the second step, ordinary cells are unmasked to $(\text{EMPTY}, \text{WHITE}, \text{NORMAL})$ while the branch starting point undergoes simultaneous remask and unmask operations with the first two positions remasked to $(\mask, \mask)$ and the third position unmasked to store the failed value. After backtracking, when filling the failed position with a new value, we generate a 2-step transition to convert from the SKULL state: the first step unmasks the value and color while remasking the marker to $(\mask, \mask, v_{\text{old}}) \to (v_{\text{new}}, c, \mask)$, and the second step unmasks the marker to either NORMAL or BRANCH depending on whether this creates a new branch. Concrete examples visualizing these operations are shown in \Cref{fig:sudoku_other_ops}.


\paragraph{Training Data Statistics}
Following this generation process, each Sudoku puzzle produces 1421.3 state-transition tuples (i.e. from $\mathbf x_t$ to $\mathbf x_{t+1}$) on average depending on the puzzle difficulty and the number of backtracking steps required. For our experiments, we generated training data from 100 hard Sudoku puzzles, each yielding 25,022.6 training state-transition tuples on average.

\begin{figure}[t]
	\centering
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_595.pdf}}
	
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_596.pdf}}
	\caption{Demonstration of value assignment.}
	\label{fig:sudoku_assign_branch}
\end{figure} 

\begin{figure}[t]
	\centering
    \ContinuedFloat
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_611.pdf}}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_612.pdf}}
	\caption{Demonstration of backtracking and recovery from failure operations (Part 1). (Continued on next page)}
	\label{fig:sudoku_other_ops}
\end{figure} 

\begin{figure}[p]
	\ContinuedFloat
	\centering
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_613.pdf}}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_614.pdf}}
	\caption{(Continued) Part 2.}
\end{figure} 

\begin{figure}[p]
	\ContinuedFloat
	\centering
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_615.pdf}}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/sample_616.pdf}}
	\caption{(Continued) Part 3.}
\end{figure} 

\clearpage


\section{Experiment: Graph Generation} \label{appendix:examples_graph}

We consider a graph editing task that requires computing the minimum edge set to disconnect two specified nodes. Formally, given a directed graph $G = (V, E)$ with unit-capacity edges and two designated nodes $s, t \in V$ (source and target), the task is to generate a modified graph $G' = (V, E')$ where $E' \subseteq E$ such that there exists no path from $s$ to $t$ in $G'$, and $|E \setminus E'|$ (the number of removed edges) is minimized. This is equivalent to computing the minimum $s$-$t$ cut, which by the max-flow min-cut theorem equals the maximum flow from $s$ to $t$. The generation process involves iteratively finding augmenting paths, modifying the graph structure by reversing edge directions, and tracking intermediate states until no more augmenting paths exist. The final output is the graph with min-cut edges removed, effectively disconnecting $s$ and $t$.

We provide detailed description of training data generation for this graph editing task based on the Edmonds-Karp algorithm. The data generation process simulates the BFS-based augmenting path search and records the intermediate algorithmic states as supervised training trajectories for AP-MDM.

\paragraph{State Representation}
A directed graph with $n$ nodes and $m$ edges is represented as a token sequence with three main components: prompt (source and target nodes), graph data (edge list with features), and node data (node list with features). Each edge is encoded with its endpoints $(u, v)$ and two feature slots tracking the edge's directional availability: slot1 represents forward direction availability (initially FB for forward-backward) and slot2 represents reverse direction availability (initially MASK indicating unavailable). Each node is encoded with its ID and two features: level (BFS layer, initially INF for unvisited or LVL0 for source) and parent (parent node in BFS tree, initially NIL). The vocabulary includes structural tokens (PROMPT, SRC, TGT, GRAPH, NODES, parentheses), edge feature tokens (FB, MASK), node feature tokens (LVL0-LVL9, INF, NIL, PAR), node IDs (0-299), and termination tokens (EOA, EOS).

\paragraph{Atomic Operations}
The Edmonds-Karp algorithm is decomposed into atomic operations, each translated into state-transition tuples for AP-MDM training. The algorithm consists of four phases:

\textbf{Feature Expansion:} Before the algorithm begins, edge and node feature slots must be initialized through a 3-step process following a expansion-then-unmask paradigm. First, MASK tokens are inserted after each edge's second node and after each node's ID ($\inser$ operation). Second, another MASK is inserted while simultaneously unmasking the first MASK to FB for edges and to the appropriate level for nodes ($\inser$ + $\unmask$ operations). Third, the second MASK is unmasked to MASK for edges and to NIL for nodes ($\unmask$ operation). See \Cref{fig:graph_feature_expansion}.

\textbf{Breadth-First Search:} The breadth-first search proceeds by discovering nodes layer by layer. When a new node is discovered through an edge, we generate a 2-step transition: first, remask the node's level and parent features to MASK ($\remask$ operation); second, unmask these MASKs to the new level and parent ID ($\unmask$ operation). This continues until either the target node is reached (proceed to augmentation) or no new nodes can be discovered (algorithm terminates). See \Cref{fig:graph_bfs}.

\textbf{Path Augmentation:} When an augmenting path from source to target is found, we generate a 2-step transition to flip edges along the path and reset node features. First, for each edge on the path, swap its slot1 and slot2 values (reversing the direction), and simultaneously remask all node features to MASK ($\remask$ operation). Second, unmask all node MASKs back to their initial values: INF for non-source nodes and LVL0 for the source node, with all parents set to NIL ($\unmask$ operation). After augmentation, the algorithm returns to BFS phase to search for the next augmenting path. See \Cref{fig:graph_augmentation}.

\textbf{Termination and Structural Editing:} When no more augmenting paths exist, the final BFS identifies two disjoint sets $S$ and $T$ where $S$ contains the source and $T$ contains the target. The min-cut edges are those directed from nodes in $S$ to nodes in $T$ in the original graph, and these edges must be removed to disconnect source and target. This is accomplished through a 3-step process. First, remask all tokens representing the min-cut edges to MASK ($\remask$ operation). Second, delete these edges while simultaneously expanding a MASK token after EOA ($\delete$ + $\inser$ operations). Third, unmask the expanded MASK to EOS ($\unmask$ operation), marking algorithm completion. See \Cref{fig:graph_termination}.

% Following this generation process, each graph instance produces state-transition tuples proportional to the number of augmenting paths and the BFS steps required to find them. Our training set includes graphs with varying structures, densities, and node IDs, providing diverse algorithmic scenarios for the model to learn from.

\paragraph{ARM Baseline}
For comparison with AP-MDM, we train an autoregressive baseline that learns to generate the complete solving trajectory as a single sequence. The ARM baseline takes the initial graph state as input and must generate the full sequence of operations needed to solve the task. To enable this, we convert the AP-MDM training data into an ARM-compatible format by representing each operation as a triplet $(p, o, v)$ where $p$ is the position index (POSE0-POSE399), $o$ is the operation type (REMASK, UNMASK, INSERT, DELETE), and $v$ is the value (or NONE for operations without values). The sequence format is: $[\text{initial state } \mathbf{x}_0]$ STEP $[\text{operations}_1]$ STEP $[\text{operations}_2]$ STEP $\cdots$ ANSWER $[\text{final state } \mathbf{x}_{\text{final}}]$, where each STEP separates consecutive state transitions and ANSWER marks the beginning of the final output. For ARM training, we use the standard next-token prediction objective with teacher forcing, where the model learns to autoregressively generate the entire operation sequence given the initial state.


\begin{table}[t]
	\centering
\caption{Sequence length statistics for graphs of different sizes. AP-MDM sequences contain state-transition tuples, while ARM sequences enumerate all operations explicitly.}
\label{tab:graph_sequence_stats}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|cc|cc}
\toprule
\multirow{2}{*}{\textbf{\# Nodes}} & \multirow{2}{*}{\textbf{\# Edges}} & \multicolumn{2}{c|}{\textbf{AP-MDM}} & \multicolumn{2}{c}{\textbf{ARM}} \\
& & \textbf{Avg. Seq. Length} & \textbf{Max Seq. Length} & \textbf{Avg. Seq. Length} & \textbf{Max Seq. Length} \\
\midrule
4 & 12 & 56 & 65 & 932 & 932 \\
5 & 17 & 81 & 88 & 1,375 & 1,403 \\
6 & 23 & 114 & 129 & 2,083 & 2,140 \\
7 & 29 & 148 & 170 & 2,687 & 2,874 \\
8 & 36 & 189 & 217 & 3,529 & 3,865 \\
9 & 43 & 236 & 252 & 5,586 & 6,597 \\
10 & 50 & 270 & 305 & 4,915 & 5,392 \\
\bottomrule
\end{tabular}
}
\end{table} 


As shown in \Cref{tab:graph_sequence_stats}, the sequence length grows with graph size for both AP-MDM and ARM, but ARM requires significantly longer sequences due to the explicit operation enumeration. 

\begin{figure}[t]
	\centering
	\setcounter{subfigure}{0}
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_0.pdf}}
	
	\vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_1.pdf}}
	
	\vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_2.pdf}}
	\caption{Feature Expansion phase in graph generation, showing the initialization process for edge and node feature slots using $\inser$ and $\unmask$ operations.}
	\label{fig:graph_feature_expansion}
\end{figure} 

\begin{figure}[t]
	\centering
	\setcounter{subfigure}{0}
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_3.pdf}}
	
	\vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_4.pdf}}
	\caption{Parallelized BFS phase in graph generation, showing layer-by-layer node discovery with parallel processing using $\remask$ and $\unmask$ operations.}
	\label{fig:graph_bfs}
\end{figure}

\begin{figure}[t]
	\centering
	\setcounter{subfigure}{0}
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_5.pdf}}
	
	\vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_6.pdf}}
	\caption{Path Augmentation phase in graph generation, showing edge reversal and node feature reset after finding an augmenting path using $\remask$ and $\unmask$ operations.}
	\label{fig:graph_augmentation}
\end{figure}

\begin{figure}[t]
	\centering
	\setcounter{subfigure}{0}
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_29.pdf}}
	
	\vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_30.pdf}}
    \vspace{0.2cm}
	
	\subfigure[]{\includegraphics[width=1.0\textwidth]{figs/graph_31.pdf}}
	\caption{Termination and Structural Editing phase, showing the deletion of min-cut edges and algorithm completion using $\remask$ and $\delete$ operations.}
	\label{fig:graph_termination}
\end{figure}

\section{Experiment: Parity}  \label{appendix:examples_parity}


The parity task requires determining whether a binary sequence contains an even or odd number of 1s. Formally, given an input sequence $\mathbf{x} = (x_1, x_2, \ldots, x_n) \in \{0, 1\}^n$, the task is to compute $\bigoplus_{i=1}^n x_i$ (XOR of all bits), outputting 0 for even parity and 1 for odd parity. 

\paragraph{Algorithm and Data Generation}
For AP-MDM training, we implement an elimination algorithm that mimics how humans naturally solve parity: repeatedly remove pairs of identical elements until only the result remains. The vocabulary consists of 5 tokens: BOS (sequence start), EOS (sequence end), MASK, digit 0, and digit 1. The elimination process follows these rules: when encountering 0s in the sequence, convert them to MASK; when encountering a pair of 1s, convert both to MASK; then delete the MASK tokens. This process repeats until only BOS remains (for even parity) or BOS followed by a single 1 remains (for odd parity). Each elimination step generates a state-transition tuple: converting tokens to MASK is a $\remask$ operation, and removing MASKs is a $\delete$ operation.

\paragraph{Data}
The training data consists of only 4 instances that cover all possible elimination patterns. Each sample is a single-step state transition demonstrating one atomic operation. The test set contains 1,000 randomly generated binary sequences with variying lengths. The test sequences have approximately equal distribution of even and odd parities.

\paragraph{ARM Baseline}
For the ARM baseline, we train autoregressive models with chain-of-thought reasoning where the model generates intermediate cumulative XOR values at each position before outputting the final result. The sequence structure is: BOS $[x_1 \, x_2 \, \cdots \, x_n]$ EOP $[s_1 \, s_2 \, \cdots \, s_n \, \text{result}]$ EOS, where $s_i = \bigoplus_{j=1}^i x_j$ represents the cumulative XOR up to position $i$, and result is True (for odd parity) or False (for even parity). Only the content after EOP is used for computing the training loss. We train ARM models with up to 10K training instances at various fixed lengths (e.g., length 2, 10, 50, 100) and evaluate their ability to generalize to longer unseen lengths. 






