@inproceedings{austin2021structured,
  title={Structured Denoising Diffusion Models in Discrete State-Spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@misc{nie2025large,
      title={Large Language Diffusion Models}, 
      author={Shen Nie and Fengqi Zhu and Zebin You and Xiaolu Zhang and Jingyang Ou and Jun Hu and Jun Zhou and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
      year={2025},
      eprint={2502.09992},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{lou2024discrete,
  title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
  author={Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  year={2024},
  url={https://arxiv.org/abs/2310.16834}
}

@inproceedings{zhang2024consistency,
    title={Consistency Large Language Models},
    author={Zhang, Chen-Lin and Zhang, Zirui and Temgoua, Lancelot and Li, Yuhui and Song, Yang},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=9d0126a3-2}
}

@inproceedings{sahoo2025diffusion,
  title={The Diffusion Duality},
  author={Sahoo, Subham Sekhar and Deschenaux, Justin and Gokaslan, Aaron and Wang, Guanghan and Chiu, Justin and Kuleshov, Volodymyr},
  booktitle={International Conference on Machine Learning},
  year={2025},
  url={https://arxiv.org/abs/2506.10892}
}

@misc{kim2025train,
      title={Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions}, 
      author={Jaeyeon Kim and Kulin Shah and Vasilis Kontonis and Sham Kakade and Sitan Chen},
      year={2025},
      eprint={2502.06768},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deng2025bagel,
  title = {Emerging Properties in Unified Multimodal Pretraining},
  author = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},
  journal = {arXiv preprint arXiv:2505.14683},
  year = {2025}
}

@misc{sahoo2025esotericlanguagemodels,
      title={Esoteric Language Models}, 
      author={Subham Sekhar Sahoo and Zhihan Yang and Yash Akhauri and Johnna Liu and Deepansha Singh and Zhoujun Cheng and Zhengzhong Liu and Eric Xing and John Thickstun and Arash Vahdat},
      year={2025},
      eprint={2506.01928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.01928}, 
}

@article{zhu2025llada,
    title={LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models},
    author={Zhu, Fengqi and Wang, Rongzhen and Nie, Shen and Zhang, Xiaolu and Wu, Chunwei and Hu, Jun and Zhou, Jun and Chen, Jianfei and Lin, Yankai and Wen, Ji-Rong and others},
    journal={arXiv preprint arXiv:2505.19223},
    year={2025}
}

@inproceedings{liu2025think,
  title={Think While You Generate: Discrete Diffusion with Planned Denoising},
  author={Sulin Liu and Juno Nam and Andrew Campbell and Hannes St{\"a}rk and Yilun Xu and Tommi Jaakkola and Rafael G{\'o}mez-Bombarelli},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=MJNywBdSDy}
}

@misc{peng2025path,
      title={Path Planning for Masked Diffusion Model Sampling}, 
      author={Fred Zhangzhi Peng and Zachary Bezemek and Sawan Patel and Jarrid Rector-Brooks and Sherwood Yao and Avishek Joey Bose and Alexander Tong and Pranam Chatterjee},
      year={2025},
      eprint={2502.03540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{huang2025reinforcing,
      title={Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models}, 
      author={Zemin Huang and Zhiyang Chen and Zijun Wang and Tiancheng Li and Guo-Jun Qi},
      year={2025},
      eprint={2505.10446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2025fastdllm,
      title={Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding}, 
      author={Chengyue Wu and Hao Zhang and Shuchen Xue and Zhijian Liu and Shizhe Diao and Ligeng Zhu and Ping Luo and Song Han and Enze Xie},
      year={2025},
      eprint={2505.22618},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2025fudokidiscreteflowbasedunified,
    title={FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities}, 
    author={Jin Wang and Yao Lai and Aoxue Li and Shifeng Zhang and Jiacheng Sun and Ning Kang and Chengyue Wu and Zhenguo Li and Ping Luo},
    year={2025},
    eprint={2505.20147},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2505.20147}
}

@inproceedings{LiuICLR25,
  author    = {Liu, Anji and Broadrick, Oliver and Niepert, Mathias and Van den Broeck, Guy},
  title     = {Discrete Copula Diffusion},
  booktitle = {Proceedings of the 13th International Conference on Learning Representations (ICLR)},
  month     = {apr},
  year      = {2025},
  url       = "https://arxiv.org/pdf/2410.01949",
  keywords  = {conference,selective}
}

@misc{vonrutte2025generalized,
      title={Generalized Interpolating Discrete Diffusion}, 
      author={Dimitri von Rütte and Janis Fluri and Yuhui Ding and Antonio Orvieto and Bernhard Schölkopf and Thomas Hofmann},
      year={2025},
      eprint={2503.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{shi2024simplified,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K.},
  booktitle={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}

@inproceedings{arriola2025interpolating,
  title={Interpolating Autoregressive and Discrete Denoising Diffusion Language Models},
  author={Marianne Arriola and Aaron Gokaslan and Justin T Chiu and Jiaqi Han and Zhihan Yang and Zhixuan Qi and Subham Sekhar Sahoo and Volodymyr Kuleshov},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=tyEyYT267x}
}

@misc{shi2025muddit,
      title={Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model}, 
      author={Qingyu Shi and Jinbin Bai and Zhuoran Zhao and Wenhao Chai and Kaidong Yu and Jianzong Wu and Shuangyong Song and Yunhai Tong and Xiangtai Li and Xuelong Li and Shuicheng Yan},
      year={2025},
      eprint={2505.23606},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nie2025scaling,
      title={Scaling up Masked Diffusion Models on Text}, 
      author={Shen Nie and Fengqi Zhu and Chao Du and Tianyu Pang and Qian Liu and Guangtao Zeng and Min Lin and Chongxuan Li},
      year={2025},
      eprint={2410.18514},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ou2025your,
      title={Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data}, 
      author={Jingyang Ou and Shen Nie and Kaiwen Xue and Fengqi Zhu and Jiacheng Sun and Zhenguo Li and Chongxuan Li},
      year={2025},
      eprint={2406.03736},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{swerdlow2025unidisc,
        title = {Unified Multimodal Discrete Diffusion},
        author = {Swerdlow, Alexander and Prabhudesai, Mihir and Gandhi, Siddharth and Pathak, Deepak and Fragkiadaki, Katerina},
        journal = {arXiv preprint arXiv:2503.20853},
        year = {2025},
        doi = {10.48550/arXiv.2503.20853},
}

@misc{li2025lavida,
      title={LaViDa: A Large Diffusion Language Model for Multimodal Understanding}, 
      author={Shufan Li and Konstantinos Kallidromitis and Hritik Bansal and Akash Gokul and Yusuke Kato and Kazuki Kozuka and Jason Kuen and Zhe Lin and Kai-Wei Chang and Aditya Grover},
      year={2025},
      eprint={2505.16839},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{you2025lladav,
      title={LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning}, 
      author={Zebin You and Shen Nie and Xiaolu Zhang and Jun Hu and Jun Zhou and Zhiwu Lu and Ji-Rong Wen and Chongxuan Li},
      year={2025},
      eprint={2505.16933},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhao2025d1,
  title={d1: Scaling reasoning in diffusion large language models via reinforcement learning},
  author={Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya},
  journal={arXiv preprint arXiv:2504.12216},
  year={2025}
}

@misc{yang2025mmada,
      title={MMaDA: Multimodal Large Diffusion Language Models}, 
      author={Ling Yang and Ye Tian and Bowen Li and Xinchen Zhang and Ke Shen and Yunhai Tong and Mengdi Wang},
      year={2025},
      eprint={2505.15809},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yu2025dimple,
      title={Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding}, 
      author={Runpeng Yu and Xinyin Ma and Xinchao Wang},
      year={2025},
      eprint={2505.16990},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ma2025dkvcache,
      title={dKV-Cache: The Cache for Diffusion Language Models}, 
      author={Xinyin Ma and Runpeng Yu and Gongfan Fang and Xinchao Wang},
      year={2025},
      eprint={2505.15781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2025dllmcache,
      title={dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching}, 
      author={Zhiyuan Liu and Yicun Yang and Yaojie Zhang and Junjie Chen and Chang Zou and Qingyuan Wei and Shaobo Wang and Linfeng Zhang},
      year={2025},
      eprint={2506.06295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{sahoo2024simple,
    title={Simple and Effective Masked Diffusion Language Models},
    author={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Mariano Marroquin and Justin T Chiu and Alexander M Rush and Volodymyr Kuleshov},
    booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
    year={2024},
    url={https://openreview.net/forum?id=L4uaAR4ArM}
}

@misc{yin2024theoretical,
      title={A Theoretical Perspective for Speculative Decoding Algorithm}, 
      author={Ming Yin and Minshuo Chen and Kaixuan Huang and Mengdi Wang},
      year={2024},
      eprint={2411.00841},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{xu2025energy,
  title={Energy-Based Diffusion Language Models for Text Generation},
  author={Minkai Xu and Tomas Geffner and Karsten Kreis and Weili Nie and Yilun Xu and Jure Leskovec and Stefano Ermon and Arash Vahdat},
  booktitle={International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=sL2F9YCMXf}
}



@inproceedings{gong2023diffuseq,
  title={DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Li, Lin},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019},
  month={Jun},
  publisher={Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/N19-1423},
}

@inproceedings{chang2022maskgit,
    title={MaskGIT: Masked Generative Image Transformer},
    author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={11335--11345},
    year={2022}
}

@article{li2025block,
  title={Block Diffusion: Interpolating between Autoregressive and Diffusion Language Models},
  author={Li, Han and Yang, Chenghao and Ma, Jianxin and Wang, Peng-Shuai and Liu, Yang},
  journal={arXiv preprint arXiv:2405.19932},
  year={2024}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4195--4205},
  year={2023}
}

@article{ewer2024entp,
  title={Entp: Encoder-only next token prediction},
  author={Ewer, Ethan and Chae, Daewon and Zeng, Thomas and Kim, Jinkyu and Lee, Kangwook},
  journal={arXiv preprint arXiv:2410.01600},
  year={2024}
}

@article{chen2024theoretical,
  title={Theoretical limitations of multi-layer transformer},
  author={Chen, Lijie and Peng, Binghui and Wu, Hongxun},
  journal={arXiv preprint arXiv:2412.02975},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{besta2024graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17682--17690},
  year={2024}
}

@inproceedings{drozdov2022compositional,
  title={Compositional semantic parsing with large language models},
  author={Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{khot2022decomposed,
  title={Decomposed prompting: A modular approach for solving complex tasks},
  author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2210.02406},
  year={2022}
}

@article{long2023large,
  title={Large language model guided tree-of-thought},
  author={Long, Jieyi},
  journal={arXiv preprint arXiv:2305.08291},
  year={2023}
}

@article{suzgun2024meta,
  title={Meta-prompting: Enhancing language models with task-agnostic scaffolding},
  author={Suzgun, Mirac and Kalai, Adam Tauman},
  journal={arXiv preprint arXiv:2401.12954},
  year={2024}
}

@article{sel2023algorithm,
  title={Algorithm of thoughts: Enhancing exploration of ideas in large language models},
  author={Sel, Bilgehan and Al-Tawaha, Ahmad and Khattar, Vanshaj and Jia, Ruoxi and Jin, Ming},
  journal={arXiv preprint arXiv:2308.10379},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}

@article{zelikman2022star,
  title={Star: Bootstrapping reasoning with reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15476--15488},
  year={2022}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}

@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{fu2024lazyllm,
  title={Lazyllm: Dynamic token pruning for efficient long context llm inference},
  author={Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
  journal={arXiv preprint arXiv:2407.14057},
  year={2024}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={arXiv preprint arXiv:2404.14469},
  year={2024}
}

@article{nawrot2024dynamic,
  title={Dynamic memory compression: Retrofitting llms for accelerated inference},
  author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
  journal={arXiv preprint arXiv:2403.09636},
  year={2024}
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@article{jiang2023llmlingua,
  title={Llmlingua: Compressing prompts for accelerated inference of large language models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  journal={arXiv preprint arXiv:2310.05736},
  year={2023}
}

@article{perez2021attention,
  title={Attention is turing-complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@article{merrill2023expresssive,
  title={The expresssive power of transformers with chain of thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{qiu2024ask,
  title={Ask, and it shall be given: Turing completeness of prompting},
  author={Qiu, Ruizhong and Xu, Zhe and Bao, Wenxuan and Tong, Hanghang},
  journal={arXiv preprint arXiv:2411.01992},
  year={2024}
}

@article{strobl2024formal,
  title={What formal languages can transformers express? a survey},
  author={Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={543--561},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{nowak2024representational,
  title={On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning},
  author={Nowak, Franz and Svete, Anej and Butoi, Alexandra and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2406.14197},
  year={2024}
}

@article{garrison2024memory,
  title={Memory makes computation universal, remember?},
  author={Garrison, Erik},
  journal={arXiv preprint arXiv:2412.17794},
  year={2024}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@article{lindner2024tracr,
  title={Tracr: Compiled transformers as a laboratory for interpretability},
  author={Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{friedman2024learning,
  title={Learning transformer programs},
  author={Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{liu2023code,
  title={Code execution with pre-trained language models},
  author={Liu, Chenxiao and Lu, Shuai and Chen, Weizhu and Jiang, Daxin and Svyatkovskiy, Alexey and Fu, Shengyu and Sundaresan, Neel and Duan, Nan},
  journal={arXiv preprint arXiv:2305.05383},
  year={2023}
}

@article{zhang2024transformer,
  title={Transformer-based models are not yet perfect at learning to emulate structural recursion},
  author={Zhang, Dylan and Tigges, Curt and Zhang, Zory and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
  journal={arXiv preprint arXiv:2401.12947},
  year={2024}
}

@book{baader1998term,
  title={Term rewriting and all that},
  author={Baader, Franz and Nipkow, Tobias},
  year={1998},
  publisher={Cambridge university press}
}

@article{selman1996generating,
  title={Generating hard satisfiability problems},
  author={Selman, Bart and Mitchell, David G and Levesque, Hector J},
  journal={Artificial intelligence},
  volume={81},
  number={1-2},
  pages={17--29},
  year={1996},
  publisher={Elsevier}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{prosser1993hybrid,
  title={Hybrid algorithms for the constraint satisfaction problem},
  author={Prosser, Patrick},
  journal={Computational intelligence},
  volume={9},
  number={3},
  pages={268--299},
  year={1993},
  publisher={Wiley Online Library}
}

@article{dziri2024faith,
  title={Faith and fate: Limits of transformers on compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{waswani2017attention,
  title={Attention is all you need},
  author={Waswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A and Kaiser, L and Polosukhin, I},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@book{o1985equational,
  title={Equational logic as a programming language},
  author={O'Donnell, Michael J},
  year={1985},
  publisher={Springer}
}

@book{wos1992automated,
  title={Automated reasoning introduction and applications},
  author={Wos, Larry and Overbeek, Ross and Lusk, Ewing and Boyle, Jim},
  year={1992},
  publisher={McGraw-Hill, Inc.}
}

@article{li2024chain,
  title={Chain of thought empowers transformers to solve inherently serial problems},
  author={Li, Zhiyuan and Liu, Hong and Zhou, Denny and Ma, Tengyu},
  journal={arXiv preprint arXiv:2402.12875},
  year={2024}
}

@article{feng2024towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{wang2024augmenting,
  title={Augmenting language models with long-term memory},
  author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{barcelo2023logical,
  title={Logical languages accepted by transformer encoders with hard attention},
  author={Barcel{\'o}, Pablo and Kozachinskiy, Alexander and Lin, Anthony Widjaja and Podolskii, Vladimir},
  journal={arXiv preprint arXiv:2310.03817},
  year={2023}
}

@article{merrill2022saturated,
  title={Saturated transformers are constant-depth threshold circuits},
  author={Merrill, William and Sabharwal, Ashish and Smith, Noah A},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={843--856},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{ramachandran2017searching,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@misc{openai2024reasoning,
  author       = {OpenAI},
  title        = {Learning to Reason with LLMs},
  year         = {2024},
  month        = {September},
  url          = {https://openai.com/index/learning-to-reason-with-llms/}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}


@article{yang2024counting,
  title={Counting like transformers: Compiling temporal counting logic into softmax transformers},
  author={Yang, Andy and Chiang, David},
  journal={arXiv preprint arXiv:2404.04393},
  year={2024}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{stockmeyer1973word,
  title={Word problems requiring exponential time (preliminary report)},
  author={Stockmeyer, Larry J and Meyer, Albert R},
  booktitle={Proceedings of the fifth annual ACM symposium on Theory of computing},
  pages={1--9},
  year={1973}
}

@article{impagliazzo2001complexity,
  title={On the complexity of k-SAT},
  author={Impagliazzo, Russell and Paturi, Ramamohan},
  journal={Journal of Computer and System Sciences},
  volume={62},
  number={2},
  pages={367--375},
  year={2001},
  publisher={Elsevier}
}

@article{yang2025pencil,
  title={Pencil: Long thoughts with short memory},
  author={Yang, Chenxiao and Srebro, Nathan and McAllester, David and Li, Zhiyuan},
  journal={arXiv preprint arXiv:2503.14337},
  year={2025}
}

@inproceedings{fortune1978parallelism,
  title={Parallelism in random access machines},
  author={Fortune, Steven and Wyllie, James},
  booktitle={Proceedings of the tenth annual ACM symposium on Theory of computing},
  pages={114--118},
  year={1978}
}

@book{jaja1992parallel,
  title={Parallel algorithms},
  author={J{\'a}J{\'a}, Joseph},
  year={1992}
}

@article{vishkin1982log,
  title={An O (log n) parallel connectivity algorithm},
  author={Vishkin, Y Shiloachand U and Shiloach, Y},
  journal={J. algorithms},
  volume={3},
  pages={57--67},
  year={1982}
}

@article{barrington1990uniformity,
  title={On uniformity within NC1},
  author={Barrington, David A Mix and Immerman, Neil and Straubing, Howard},
  journal={Journal of Computer and System Sciences},
  volume={41},
  number={3},
  pages={274--306},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{csanky1975fast,
  title={Fast parallel matrix inversion algorithms},
  author={Csanky, Laszlo},
  booktitle={16th Annual Symposium on Foundations of Computer Science (sfcs 1975)},
  pages={11--12},
  year={1975},
  organization={IEEE}
}

@article{valiant1974general,
  title={General context-free recognition in less than cubic time},
  author={Valiant, Leslie},
  year={1974},
  publisher={Carnegie Mellon University}
}

@article{zhu2025reasoning,
  title={Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought},
  author={Zhu, Hanlin and Hao, Shibo and Hu, Zhiting and Jiao, Jiantao and Russell, Stuart and Tian, Yuandong},
  journal={arXiv preprint arXiv:2505.12514},
  year={2025}
}

@article{xue2025any,
  title={Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture},
  author={Xue, Shuchen and Xie, Tianyu and Hu, Tianyang and Feng, Zijin and Sun, Jiacheng and Kawaguchi, Kenji and Li, Zhenguo and Ma, Zhi-Ming},
  journal={arXiv preprint arXiv:2506.19935},
  year={2025}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{geiping2025scaling,
  title={Scaling up test-time compute with latent reasoning: A recurrent depth approach},
  author={Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},
  journal={arXiv preprint arXiv:2502.05171},
  year={2025}
}

@inproceedings{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  pages={11398--11442},
  year={2023},
  organization={PMLR}
}

@article{arriola2025block,
  title={Block diffusion: Interpolating between autoregressive and diffusion language models},
  author={Arriola, Marianne and Gokaslan, Aaron and Chiu, Justin T and Yang, Zhihan and Qi, Zhixuan and Han, Jiaqi and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2503.09573},
  year={2025}
}

@article{wang2025remasking,
  title={Remasking discrete diffusion models with inference-time scaling},
  author={Wang, Guanghan and Schiff, Yair and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2503.00307},
  year={2025}
}

@article{havasi2025edit,
  title={Edit Flows: Flow Matching with Edit Operations},
  author={Havasi, Marton and Karrer, Brian and Gat, Itai and Chen, Ricky TQ},
  journal={arXiv preprint arXiv:2506.09018},
  year={2025}
}

@article{von2025generalized,
  title={Generalized interpolating discrete diffusion},
  author={von R{\"u}tte, Dimitri and Fluri, Janis and Ding, Yuhui and Orvieto, Antonio and Sch{\"o}lkopf, Bernhard and Hofmann, Thomas},
  journal={arXiv preprint arXiv:2503.04482},
  year={2025}
}

@misc{wu2025dreamon,
  title={DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-Size Canvas},
  author={Wu, Zirui and Zheng, Lin and Xie, Zhihui and Ye, Jiacheng and Gao, Jiahui and Feng, Yansong and Li, Zhenguo and W., Victoria and Zhou, Guorui and Kong, Lingpeng},
  year={2025},
  url={https://hkunlp.github.io/blog/2025/dreamon/},
  note={Blog post}
}

@article{yato2003complexity,
  title={Complexity and completeness of finding another solution and its application to puzzles},
  author={Yato, Takayuki and Seta, Takahiro},
  journal={IEICE transactions on fundamentals of electronics, communications and computer sciences},
  volume={86},
  number={5},
  pages={1052--1060},
  year={2003},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={pmlr}
}

@article{kingma2021variational,
  title={Variational diffusion models},
  author={Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21696--21707},
  year={2021}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={12454--12465},
  year={2021}
}

@article{vignac2022digress,
  title={Digress: Discrete denoising diffusion for graph generation},
  author={Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  journal={arXiv preprint arXiv:2209.14734},
  year={2022}
}

@article{gruver2023protein,
  title={Protein design with guided discrete diffusion},
  author={Gruver, Nate and Stanton, Samuel and Frey, Nathan and Rudner, Tim GJ and Hotzel, Isidro and Lafrance-Vanasse, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={12489--12517},
  year={2023}
}

@article{sun2023difusco,
  title={Difusco: Graph-based diffusion solvers for combinatorial optimization},
  author={Sun, Zhiqing and Yang, Yiming},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={3706--3731},
  year={2023}
}

@article{inceptionlabs2025mercury,
  title={Mercury: Ultra-Fast Language Models Based on Diffusion},
  author={Labs, Inception and Khanna, Samar and Kharbanda, Siddhant and Li, Shufan and Varma, Harshit and Wang, Eric and Birnbaum, Sawyer and Luo, Ziyang and Miraoui, Yanis and Palrecha, Akash and others},
  journal={arXiv preprint arXiv:2506.17298},
  year={2025}
}

@misc{deepmind2025gemini,
  author = {DeepMind},
  title = {Gemini diffusion},
  year = {2025},
  url = {https://deepmind.google/models/gemini-diffusion/}
}

@article{ye2025dream,
  title={Dream 7B: Diffusion Large Language Models},
  author={Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2508.15487},
  year={2025}
}

@article{kim2025any,
  title={Any-Order Flexible Length Masked Diffusion},
  author={Kim, Jaeyeon and Cheuk-Kit, Lee and Domingo-Enrich, Carles and Du, Yilun and Kakade, Sham and Ngotiaoco, Timothy and Chen, Sitan and Albergo, Michael},
  journal={arXiv preprint arXiv:2509.01025},
  year={2025}
}

@article{lee2025genmol,
  title={Genmol: A drug discovery generalist with discrete diffusion},
  author={Lee, Seul and Kreis, Karsten and Veccham, Srimukh Prasad and Liu, Meng and Reidenbach, Danny and Peng, Yuxing and Paliwal, Saee and Nie, Weili and Vahdat, Arash},
  journal={arXiv preprint arXiv:2501.06158},
  year={2025}
}


@article{shannon1951prediction,
  title={Prediction and entropy of printed English},
  author={Shannon, Claude E},
  journal={Bell system technical journal},
  volume={30},
  number={1},
  pages={50--64},
  year={1951},
  publisher={Wiley Online Library}
}

@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  volume={1},
  number={2},
  pages={270--280},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bachmann2024pitfalls,
  title={The pitfalls of next-token prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2403.06963},
  year={2024}
}

@inproceedings{lecun2023large,
  title={Do large language models need sensory grounding for meaning and understanding},
  author={LeCun, Yann},
  booktitle={Workshop on Philosophy of Deep Learning, NYU Center for Mind, Brain, and Consciousness and the Columbia Center for Science and Society},
  year={2023}
}

@article{malach2023auto,
  title={Auto-regressive next-token predictors are universal learners},
  author={Malach, Eran},
  journal={arXiv preprint arXiv:2309.06979},
  year={2023}
}

@article{berglund2023reversal,
  title={The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}

@article{nagarajan2025roll,
  title={Roll the dice \& look before you leap: Going beyond the creative limits of next-token prediction},
  author={Nagarajan, Vaishnavh and Wu, Chen Henry and Ding, Charles and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2504.15266},
  year={2025}
}

@article{gong2025diffucoder,
  title={DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation},
  author={Gong, Shansan and Zhang, Ruixiang and Zheng, Huangjie and Gu, Jiatao and Jaitly, Navdeep and Kong, Lingpeng and Zhang, Yizhe},
  journal={arXiv preprint arXiv:2506.20639},
  year={2025}
}

@article{wang2023scientific,
  title={Scientific discovery in the age of artificial intelligence},
  author={Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={47--60},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@misc{bommasani2022opportunitiesrisksfoundationmodels,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2108.07258}, 
}

@article{joshi2025theory,
  title={A theory of learning with autoregressive chain of thought},
  author={Joshi, Nirmit and Vardi, Gal and Block, Adam and Goel, Surbhi and Li, Zhiyuan and Misiakiewicz, Theodor and Srebro, Nathan},
  journal={arXiv preprint arXiv:2503.07932},
  year={2025}
}

@book{greenlaw1995limits,
  title={Limits to parallel computation: P-completeness theory},
  author={Greenlaw, Raymond and Hoover, H James and Ruzzo, Walter L},
  year={1995},
  publisher={Oxford university press}
}

@article{merrill2025exact,
  title={Exact Expressive Power of Transformers with Padding},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2505.18948},
  year={2025}
}

@article{shah2024causal,
  title={Causal language modeling can elicit search and reasoning capabilities on logic puzzles},
  author={Shah, Kulin and Dikkala, Nishanth and Wang, Xin and Panigrahy, Rina},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={56674--56702},
  year={2024}
}

@article{head1987formal,
  title={Formal language theory and DNA: an analysis of the generative capacity of specific recombinant behaviors},
  author={Head, Tom},
  journal={Bulletin of mathematical biology},
  volume={49},
  number={6},
  pages={737--759},
  year={1987},
  publisher={Springer}
}

@article{head1998splicing,
  title={Splicing languages generated with one sided context},
  author={Head, Thomas},
  journal={Computing With Bio-molecules: Theory and Experiments},
  pages={269--282},
  year={1998},
  publisher={Citeseer}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@article{merrill2024illusion,
  title={The illusion of state in state-space models},
  author={Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2404.08819},
  year={2024}
}

@inproceedings{kari2012deciding,
  title={Deciding whether a regular language is generated by a splicing system},
  author={Kari, Lila and Kopecki, Steffen},
  booktitle={International Workshop on DNA-Based Computers},
  pages={98--109},
  year={2012},
  organization={Springer}
}

@article{puaun1996splicing,
  title={On the splicing operation},
  author={P{\u{a}}un, Gheorghe},
  journal={Discrete Applied Mathematics},
  volume={70},
  number={1},
  pages={57--79},
  year={1996},
  publisher={Elsevier}
}

@article{ehrenfeucht1983regularity,
  title={On regularity of context-free languages},
  author={Ehrenfeucht, Andrzej and Haussler, David and Rozenberg, Grzegorz},
  journal={Theoretical Computer Science},
  volume={27},
  number={3},
  pages={311--332},
  year={1983},
  publisher={Elsevier}
}

@article{mcnaughton1988church,
  title={Church-Rosser Thue systems and formal languages},
  author={McNaughton, Robert and Narendran, Paliath and Otto, Friedrich},
  journal={Journal of the ACM (JACM)},
  volume={35},
  number={2},
  pages={324--344},
  year={1988},
  publisher={ACM New York, NY, USA}
}

@inproceedings{allender2006grid,
  title={Grid graph reachability problems},
  author={Allender, Eric and Chakraborty, Tanmoy and Barrington, David A Mix and Datta, Samir and Roy, Sambuddha},
  booktitle={21st Annual IEEE Conference on Computational Complexity (CCC'06)},
  pages={15--27},
  year={2006},
  organization={IEEE}
}

@book{robinson1993parallel,
  title={Parallel algorithms for group word problems},
  author={Robinson, David Hill},
  year={1993},
  publisher={University of California, San Diego}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}