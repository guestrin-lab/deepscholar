\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Allender et~al.(2006)Allender, Chakraborty, Barrington, Datta, and Roy]{allender2006grid}
Eric Allender, Tanmoy Chakraborty, David A~Mix Barrington, Samir Datta, and Sambuddha Roy.
\newblock Grid graph reachability problems.
\newblock In \emph{21st Annual IEEE Conference on Computational Complexity (CCC'06)}, pp.\  15--27. IEEE, 2006.

\bibitem[Arora \& Barak(2009)Arora and Barak]{arora2009computational}
Sanjeev Arora and Boaz Barak.
\newblock \emph{Computational complexity: a modern approach}.
\newblock Cambridge University Press, 2009.

\bibitem[Arriola et~al.(2025)Arriola, Gokaslan, Chiu, Yang, Qi, Han, Sahoo, and Kuleshov]{arriola2025block}
Marianne Arriola, Aaron Gokaslan, Justin~T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham~Sekhar Sahoo, and Volodymyr Kuleshov.
\newblock Block diffusion: Interpolating between autoregressive and diffusion language models.
\newblock \emph{arXiv preprint arXiv:2503.09573}, 2025.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and van~den Berg]{austin2021structured}
Jacob Austin, Daniel~D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  17981--17993, 2021.

\bibitem[Bachmann \& Nagarajan(2024)Bachmann and Nagarajan]{bachmann2024pitfalls}
Gregor Bachmann and Vaishnavh Nagarajan.
\newblock The pitfalls of next-token prediction.
\newblock \emph{arXiv preprint arXiv:2403.06963}, 2024.

\bibitem[Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{berglund2023reversal}
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans.
\newblock The reversal curse: Llms trained on" a is b" fail to learn" b is a".
\newblock \emph{arXiv preprint arXiv:2309.12288}, 2023.

\bibitem[Bommasani et~al.(2022)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card, Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya, Durmus, Ermon, Etchemendy, Ethayarajh, Fei-Fei, Finn, Gale, Gillespie, Goel, Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu, Huang, Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab, Koh, Krass, Krishna, Kuditipudi, Kumar, Ladhak, Lee, Lee, Leskovec, Levent, Li, Li, Ma, Malik, Manning, Mirchandani, Mitchell, Munyikwa, Nair, Narayan, Narayanan, Newman, Nie, Niebles, Nilforoshan, Nyarko, Ogut, Orr, Papadimitriou, Park, Piech, Portelance, Potts, Raghunathan, Reich, Ren, Rong, Roohani, Ruiz, Ryan, Ré, Sadigh, Sagawa, Santhanam, Shih, Srinivasan, Tamkin, Taori, Thomas, Tramèr, Wang, Wang, Wu, Wu, Wu, Xie, Yasunaga, You, Zaharia, Zhang, Zhang, Zhang, Zhang, Zheng, Zhou, and Liang]{bommasani2022opportunitiesrisksfoundationmodels}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared~Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li~Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang~Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang~Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher~D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan~Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon~Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin~W. Thomas, Florian Tramèr, Rose~E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang~Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang.
\newblock On the opportunities and risks of foundation models, 2022.
\newblock URL \url{https://arxiv.org/abs/2108.07258}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chang et~al.(2022)Chang, Zhang, Jiang, Liu, and Freeman]{chang2022maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T. Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  11335--11345, 2022.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pp.\  933--941. PMLR, 2017.

\bibitem[DeepMind(2025)]{deepmind2025gemini}
DeepMind.
\newblock Gemini diffusion, 2025.
\newblock URL \url{https://deepmind.google/models/gemini-diffusion/}.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Ewer et~al.(2024)Ewer, Chae, Zeng, Kim, and Lee]{ewer2024entp}
Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, and Kangwook Lee.
\newblock Entp: Encoder-only next token prediction.
\newblock \emph{arXiv preprint arXiv:2410.01600}, 2024.

\bibitem[Feng et~al.(2024)Feng, Zhang, Gu, Ye, He, and Wang]{feng2024towards}
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fortune \& Wyllie(1978)Fortune and Wyllie]{fortune1978parallelism}
Steven Fortune and James Wyllie.
\newblock Parallelism in random access machines.
\newblock In \emph{Proceedings of the tenth annual ACM symposium on Theory of computing}, pp.\  114--118, 1978.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11398--11442. PMLR, 2023.

\bibitem[Greenlaw et~al.(1995)Greenlaw, Hoover, and Ruzzo]{greenlaw1995limits}
Raymond Greenlaw, H~James Hoover, and Walter~L Ruzzo.
\newblock \emph{Limits to parallel computation: P-completeness theory}.
\newblock Oxford university press, 1995.

\bibitem[Gruver et~al.(2023)Gruver, Stanton, Frey, Rudner, Hotzel, Lafrance-Vanasse, Rajpal, Cho, and Wilson]{gruver2023protein}
Nate Gruver, Samuel Stanton, Nathan Frey, Tim~GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew~G Wilson.
\newblock Protein design with guided discrete diffusion.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 12489--12517, 2023.

\bibitem[Havasi et~al.(2025)Havasi, Karrer, Gat, and Chen]{havasi2025edit}
Marton Havasi, Brian Karrer, Itai Gat, and Ricky~TQ Chen.
\newblock Edit flows: Flow matching with edit operations.
\newblock \emph{arXiv preprint arXiv:2506.09018}, 2025.

\bibitem[Head(1998)]{head1998splicing}
Thomas Head.
\newblock Splicing languages generated with one sided context.
\newblock \emph{Computing With Bio-molecules: Theory and Experiments}, pp.\  269--282, 1998.

\bibitem[Head(1987)]{head1987formal}
Tom Head.
\newblock Formal language theory and dna: an analysis of the generative capacity of specific recombinant behaviors.
\newblock \emph{Bulletin of mathematical biology}, 49\penalty0 (6):\penalty0 737--759, 1987.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forr{\'e}, and Welling]{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max Welling.
\newblock Argmax flows and multinomial diffusion: Learning categorical distributions.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 12454--12465, 2021.

\bibitem[J{\'a}J{\'a}(1992)]{jaja1992parallel}
Joseph J{\'a}J{\'a}.
\newblock \emph{Parallel algorithms}.
\newblock 1992.

\bibitem[Kari \& Kopecki(2012)Kari and Kopecki]{kari2012deciding}
Lila Kari and Steffen Kopecki.
\newblock Deciding whether a regular language is generated by a splicing system.
\newblock In \emph{International Workshop on DNA-Based Computers}, pp.\  98--109. Springer, 2012.

\bibitem[Kim et~al.(2025{\natexlab{a}})Kim, Cheuk-Kit, Domingo-Enrich, Du, Kakade, Ngotiaoco, Chen, and Albergo]{kim2025any}
Jaeyeon Kim, Lee Cheuk-Kit, Carles Domingo-Enrich, Yilun Du, Sham Kakade, Timothy Ngotiaoco, Sitan Chen, and Michael Albergo.
\newblock Any-order flexible length masked diffusion.
\newblock \emph{arXiv preprint arXiv:2509.01025}, 2025{\natexlab{a}}.

\bibitem[Kim et~al.(2025{\natexlab{b}})Kim, Shah, Kontonis, Kakade, and Chen]{kim2025train}
Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen.
\newblock Train for the worst, plan for the best: Understanding token ordering in masked diffusions, 2025{\natexlab{b}}.

\bibitem[Labs et~al.(2025)Labs, Khanna, Kharbanda, Li, Varma, Wang, Birnbaum, Luo, Miraoui, Palrecha, et~al.]{inceptionlabs2025mercury}
Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et~al.
\newblock Mercury: Ultra-fast language models based on diffusion.
\newblock \emph{arXiv preprint arXiv:2506.17298}, 2025.

\bibitem[LeCun(2023)]{lecun2023large}
Yann LeCun.
\newblock Do large language models need sensory grounding for meaning and understanding.
\newblock In \emph{Workshop on Philosophy of Deep Learning, NYU Center for Mind, Brain, and Consciousness and the Columbia Center for Science and Society}, 2023.

\bibitem[Lee et~al.(2025)Lee, Kreis, Veccham, Liu, Reidenbach, Peng, Paliwal, Nie, and Vahdat]{lee2025genmol}
Seul Lee, Karsten Kreis, Srimukh~Prasad Veccham, Meng Liu, Danny Reidenbach, Yuxing Peng, Saee Paliwal, Weili Nie, and Arash Vahdat.
\newblock Genmol: A drug discovery generalist with discrete diffusion.
\newblock \emph{arXiv preprint arXiv:2501.06158}, 2025.

\bibitem[Li et~al.(2024)Li, Liu, Zhou, and Ma]{li2024chain}
Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma.
\newblock Chain of thought empowers transformers to solve inherently serial problems.
\newblock \emph{arXiv preprint arXiv:2402.12875}, 2024.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Lou et~al.(2024)Lou, Meng, and Ermon]{lou2024discrete}
Aaron Lou, Chenlin Meng, and Stefano Ermon.
\newblock Discrete diffusion modeling by estimating the ratios of the data distribution.
\newblock In \emph{International Conference on Machine Learning}, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.16834}.

\bibitem[Malach(2023)]{malach2023auto}
Eran Malach.
\newblock Auto-regressive next-token predictors are universal learners.
\newblock \emph{arXiv preprint arXiv:2309.06979}, 2023.

\bibitem[Merrill \& Sabharwal(2024)Merrill and Sabharwal]{merrill2023expresssive}
William Merrill and Ashish Sabharwal.
\newblock The expresssive power of transformers with chain of thought.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Merrill \& Sabharwal(2025)Merrill and Sabharwal]{merrill2025exact}
William Merrill and Ashish Sabharwal.
\newblock Exact expressive power of transformers with padding.
\newblock \emph{arXiv preprint arXiv:2505.18948}, 2025.

\bibitem[Merrill et~al.(2022)Merrill, Sabharwal, and Smith]{merrill2022saturated}
William Merrill, Ashish Sabharwal, and Noah~A Smith.
\newblock Saturated transformers are constant-depth threshold circuits.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 843--856, 2022.

\bibitem[Merrill et~al.(2024)Merrill, Petty, and Sabharwal]{merrill2024illusion}
William Merrill, Jackson Petty, and Ashish Sabharwal.
\newblock The illusion of state in state-space models.
\newblock \emph{arXiv preprint arXiv:2404.08819}, 2024.

\bibitem[Nagarajan et~al.(2025)Nagarajan, Wu, Ding, and Raghunathan]{nagarajan2025roll}
Vaishnavh Nagarajan, Chen~Henry Wu, Charles Ding, and Aditi Raghunathan.
\newblock Roll the dice \& look before you leap: Going beyond the creative limits of next-token prediction.
\newblock \emph{arXiv preprint arXiv:2504.15266}, 2025.

\bibitem[Nie et~al.(2025)Nie, Zhu, You, Zhang, Ou, Hu, Zhou, Lin, Wen, and Li]{nie2025large}
Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li.
\newblock Large language diffusion models, 2025.

\bibitem[P{\u{a}}un(1996)]{puaun1996splicing}
Gheorghe P{\u{a}}un.
\newblock On the splicing operation.
\newblock \emph{Discrete Applied Mathematics}, 70\penalty0 (1):\penalty0 57--79, 1996.

\bibitem[Peng et~al.(2025)Peng, Bezemek, Patel, Rector-Brooks, Yao, Bose, Tong, and Chatterjee]{peng2025path}
Fred~Zhangzhi Peng, Zachary Bezemek, Sawan Patel, Jarrid Rector-Brooks, Sherwood Yao, Avishek~Joey Bose, Alexander Tong, and Pranam Chatterjee.
\newblock Path planning for masked diffusion model sampling, 2025.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Robinson(1993)]{robinson1993parallel}
David~Hill Robinson.
\newblock \emph{Parallel algorithms for group word problems}.
\newblock University of California, San Diego, 1993.

\bibitem[Sahoo et~al.(2024)Sahoo, Arriola, Schiff, Gokaslan, Marroquin, Chiu, Rush, and Kuleshov]{sahoo2024simple}
Subham~Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar~Mariano Marroquin, Justin~T Chiu, Alexander~M Rush, and Volodymyr Kuleshov.
\newblock Simple and effective masked diffusion language models.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock URL \url{https://openreview.net/forum?id=L4uaAR4ArM}.

\bibitem[Sahoo et~al.(2025)Sahoo, Deschenaux, Gokaslan, Wang, Chiu, and Kuleshov]{sahoo2025diffusion}
Subham~Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov.
\newblock The diffusion duality.
\newblock In \emph{International Conference on Machine Learning}, 2025.
\newblock URL \url{https://arxiv.org/abs/2506.10892}.

\bibitem[Shah et~al.(2024)Shah, Dikkala, Wang, and Panigrahy]{shah2024causal}
Kulin Shah, Nishanth Dikkala, Xin Wang, and Rina Panigrahy.
\newblock Causal language modeling can elicit search and reasoning capabilities on logic puzzles.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 56674--56702, 2024.

\bibitem[Shannon(1951)]{shannon1951prediction}
Claude~E Shannon.
\newblock Prediction and entropy of printed english.
\newblock \emph{Bell system technical journal}, 30\penalty0 (1):\penalty0 50--64, 1951.

\bibitem[Shi et~al.(2024)Shi, Han, Wang, Doucet, and Titsias]{shi2024simplified}
Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis~K. Titsias.
\newblock Simplified and generalized masked diffusion for discrete data.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~37, 2024.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International conference on machine learning}, pp.\  2256--2265. pmlr, 2015.

\bibitem[Song et~al.(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, 2020.

\bibitem[Strobl et~al.(2024)Strobl, Merrill, Weiss, Chiang, and Angluin]{strobl2024formal}
Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin.
\newblock What formal languages can transformers express? a survey.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 543--561, 2024.

\bibitem[Sun \& Yang(2023)Sun and Yang]{sun2023difusco}
Zhiqing Sun and Yiming Yang.
\newblock Difusco: Graph-based diffusion solvers for combinatorial optimization.
\newblock \emph{Advances in neural information processing systems}, 36:\penalty0 3706--3731, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\  5998--6008, 2017.

\bibitem[Vignac et~al.(2022)Vignac, Krawczuk, Siraudin, Wang, Cevher, and Frossard]{vignac2022digress}
Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard.
\newblock Digress: Discrete denoising diffusion for graph generation.
\newblock \emph{arXiv preprint arXiv:2209.14734}, 2022.

\bibitem[von R{\"u}tte et~al.(2025)von R{\"u}tte, Fluri, Ding, Orvieto, Sch{\"o}lkopf, and Hofmann]{von2025generalized}
Dimitri von R{\"u}tte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Sch{\"o}lkopf, and Thomas Hofmann.
\newblock Generalized interpolating discrete diffusion.
\newblock \emph{arXiv preprint arXiv:2503.04482}, 2025.

\bibitem[von Rütte et~al.(2025)von Rütte, Fluri, Ding, Orvieto, Schölkopf, and Hofmann]{vonrutte2025generalized}
Dimitri von Rütte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Schölkopf, and Thomas Hofmann.
\newblock Generalized interpolating discrete diffusion, 2025.

\bibitem[Wang et~al.(2025)Wang, Schiff, Sahoo, and Kuleshov]{wang2025remasking}
Guanghan Wang, Yair Schiff, Subham~Sekhar Sahoo, and Volodymyr Kuleshov.
\newblock Remasking discrete diffusion models with inference-time scaling.
\newblock \emph{arXiv preprint arXiv:2503.00307}, 2025.

\bibitem[Wang et~al.(2023)Wang, Fu, Du, Gao, Huang, Liu, Chandak, Liu, Van~Katwyk, Deac, et~al.]{wang2023scientific}
Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van~Katwyk, Andreea Deac, et~al.
\newblock Scientific discovery in the age of artificial intelligence.
\newblock \emph{Nature}, 620\penalty0 (7972):\penalty0 47--60, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Weiss et~al.(2021)Weiss, Goldberg, and Yahav]{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock Thinking like transformers.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11080--11090. PMLR, 2021.

\bibitem[Wu et~al.(2025)Wu, Zheng, Xie, Ye, Gao, Feng, Li, W., Zhou, and Kong]{wu2025dreamon}
Zirui Wu, Lin Zheng, Zhihui Xie, Jiacheng Ye, Jiahui Gao, Yansong Feng, Zhenguo Li, Victoria W., Guorui Zhou, and Lingpeng Kong.
\newblock Dreamon: Diffusion language models for code infilling beyond fixed-size canvas, 2025.
\newblock URL \url{https://hkunlp.github.io/blog/2025/dreamon/}.
\newblock Blog post.

\bibitem[Xue et~al.(2025)Xue, Xie, Hu, Feng, Sun, Kawaguchi, Li, and Ma]{xue2025any}
Shuchen Xue, Tianyu Xie, Tianyang Hu, Zijin Feng, Jiacheng Sun, Kenji Kawaguchi, Zhenguo Li, and Zhi-Ming Ma.
\newblock Any-order gpt as masked diffusion model: Decoupling formulation and architecture.
\newblock \emph{arXiv preprint arXiv:2506.19935}, 2025.

\bibitem[Yang \& Chiang(2024)Yang and Chiang]{yang2024counting}
Andy Yang and David Chiang.
\newblock Counting like transformers: Compiling temporal counting logic into softmax transformers.
\newblock \emph{arXiv preprint arXiv:2404.04393}, 2024.

\bibitem[Yang et~al.(2025)Yang, Srebro, McAllester, and Li]{yang2025pencil}
Chenxiao Yang, Nathan Srebro, David McAllester, and Zhiyuan Li.
\newblock Pencil: Long thoughts with short memory.
\newblock \emph{arXiv preprint arXiv:2503.14337}, 2025.

\bibitem[Ye et~al.(2025)Ye, Xie, Zheng, Gao, Wu, Jiang, Li, and Kong]{ye2025dream}
Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.
\newblock Dream 7b: Diffusion large language models.
\newblock \emph{arXiv preprint arXiv:2508.15487}, 2025.

\bibitem[Zhu et~al.(2025)Zhu, Hao, Hu, Jiao, Russell, and Tian]{zhu2025reasoning}
Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, and Yuandong Tian.
\newblock Reasoning by superposition: A theoretical perspective on chain of continuous thought.
\newblock \emph{arXiv preprint arXiv:2505.12514}, 2025.

\end{thebibliography}
