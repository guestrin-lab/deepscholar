\section{Parallel Random Access Machine} \label{appendix:pram}

The Random Access Machine (RAM)~\citep{arora2009computational} serves as the foundational theoretical model for sequential computation, featuring a single processor that can access any memory location in unit time regardless of addressâ€”hence ``random access", along with a finite set of registers and basic arithmetic/logical operations. This contrasts with models like Turing machines where memory access is sequential. The RAM's key strength lies in its realistic abstraction of modern computers: it captures the essential computational primitives (arithmetic, memory access, conditional branching) while abstracting away hardware details, making it ideal for algorithm analysis.

The Parallel Random Access Machine (PRAM)~\citep{fortune1978parallelism,jaja1992parallel} extends this familiar RAM model to parallel computation by allowing $P(n)$ processors to operate synchronously on shared memory with $\mathcal{O}(\log n)$-bit word size.~\footnote{The $\mathcal{O}(\log n)$-bit word size choice ensures that pointer arithmetic, indexing, and basic integer operations on polynomially bounded values are unit-time, matching the standard RAM assumptions and avoiding artificial speedups due to unrealistically wide words.} Each processor in PRAM maintains its own program counter and unique identifier, enabling conditional branching and coordinated computation. The model operates in discrete synchronous time steps where all active processors execute simultaneously, inheriting RAM's unit-cost random access property while adding the complexity of concurrent memory operations. 

\paragraph{PRAM Variants} PRAM has several variants, which differ in their memory access discipline, forming a hierarchy with precise complexity relationships. Let $\EREW$, $\CREW$, $\CRCWCommon$, $\CRCWArbitrary$ and $\CRCWPriority$ denote the classes of problems solvable in polynomial parallel time with polynomially many processors under each model, listed in order of increasing expressivity:
\begin{itemize}[leftmargin=1.2em,topsep=2pt,itemsep=2pt]
    \item \textbf{\EREW} (Exclusive Read, Exclusive Write): No concurrent access to any memory cell. Most restrictive but captures essential parallelism.
    \item \textbf{\CREW} (Concurrent Read, Exclusive Write): Multiple processors may read the same cell simultaneously. Enables broadcast in $\mathcal{O}(1)$ time vs. $\Theta(\log n)$ in EREW.
    \item \textbf{\CRCWCommon}: Concurrent writes allowed only if all writers agree on the value. Boolean OR computable in $\mathcal{O}(1)$ time.
    \item \textbf{\CRCWArbitrary}: Any concurrent writer may succeed; the choice is made arbitrarily (often modeled as random selection).
    \item \textbf{\CRCWPriority}: Concurrent writes resolved by processor priority with various schemes (e.g., minimum/maximum index, sum of conflicting values).
\end{itemize}
Crucially, any algorithm in a stronger model can be simulated in a weaker model with at most $\mathcal{O}(\log n)$ parallel time overhead~\citep{jaja1992parallel}. This polylogarithmic separation appears in basic primitives, broadcast requires $\Theta(\log n)$ rounds in $\EREW$ but $\mathcal{O}(1)$ in $\CREW$, yet the models remain polynomially equivalent for most complexity-theoretic purposes. \textbf{We adopt the $\CREW$ model throughout this paper}, where different processors are not allowed to write to the same memory cell simultaneously.

PRAM, as an idealized abstraction of shared-memory multiprocessor systems, enables precise analysis of parallel algorithms and gives rise to parallel complexity classes such as \textsf{NC}~\citep{arora2009computational} (problems solvable in polylogarithmic parallel time using polynomially many processors). For example, PRAM can simulate algorithms on trees, linear arrays, meshes, and hypercubes without loss of parallel time, while reverse simulation costs at most $\mathcal{O}(\log^2 P(n))$ overhead; Boolean circuits of depth $D$ can be simulated on $\CREW$ in $\mathcal{O}(D)$ time, making PRAM a natural model for measuring parallel time complexity in theory.

Below, we provide a more formal definition that will be used in proofs.

\subsection{Definition and Execution Process of Word-RAM}

We formalize the standard word-RAM that matches a single-processor PRAM (i.e., $P(n)=1$). Throughout, let the input length be $n$ and fix the word size $w(n)=\Theta(\log n)$.

\paragraph{Word Size, Universe, and Addresses.}
Let the word universe be $\mathbb{U}=\{0,1,\dots,2^{w}-1\}$ with arithmetic modulo $2^{w}$
(two's-complement semantics). The address space is $\mathcal{A}=\{0,1,\dots,S(n)-1\}$ for some
$S(n)\le n^{\mathcal{O}(1)}$. Memory is a mapping $M:\mathcal{A}\to\mathbb{U}$, \emph{zero-initialized}.

Let $a(n)=\lceil\log_2 S(n)\rceil$ denote the address width. We adopt the transdichotomous condition:
\begin{equation}
w(n)\ \ge\ \max\{\lceil\log_2 S(n)\rceil,\ \lceil\log_2 P(n)\rceil\}
\qquad\text{and}\qquad w(n)=\Theta(\log n).
\end{equation}
This ensures every address and processor ID fits in one word, enabling well-typed register-indirect addressing and processor identification.

\paragraph{Instruction Set and Semantics.}
The machine operates with register names $\Reg=\{0,1,\ldots,k\}$ (for constant $k$), register file $R\in\mathbb{U}^{k+1}$, immediate constants $\Imm\subset\mathbb{Z}$ (a fixed finite set independent of $n$ and $w$), and label identifiers $\Lab$ for jump targets.\footnote{A label is a human-readable name for a program location (instruction index) serving as a jump/branch target.} We assume a constant-size register file with $|\Reg| \ge 2$ in the proof (any constant $\ge 2$ suffices up to constant factors). Programs define a partial label table $\mathrm{addr}:\Lab\rightharpoonup\{0,\ldots,\ell\}$ mapping each declared label to its instruction index (injective). 

The instruction alphabet $\Instr$ consists of the following parameterized forms ($r,s\in\Reg$, $c\in\Imm$, $L\in\Lab$):
\[
\begin{aligned}
\Instr \;=\;&
\{\ \mathtt{LOAD}\ r,\,[s],\ \mathtt{STORE}\ [s],\,r,\ \mathtt{LOADI}\ r,\,c\ \}\ \\
&\cup\ \{\ \mathtt{ADD}\ r,\,s,\ \mathtt{SUB}\ r,\,s,\ \mathtt{AND}\ r,\,s,\ \mathtt{XOR}\ r,\,s,\ \mathtt{SHL}\ r,\,s,\ \mathtt{SHR}\ r,\,s\ \}\ \\
&\cup\ \{\ \mathtt{BRZ}\ r,\,L,\ \mathtt{JMP}\ L,\ \mathtt{HALT}\ \}.
\end{aligned}
\]
Unbracketed registers $r,s$ denote their word values $R_r,R_s\in\mathbb{U}$. The bracketed form $[s]$ denotes register-indirect addressing: $\texttt{LOAD } r,[s]$ reads $M[R_s]$ into $R_r$, and $\texttt{STORE } [s],r$ writes $R_r$ to $M[R_s]$. Bracketed operands are only allowed in $\mathtt{LOAD}$/$\mathtt{STORE}$; nested or arithmetic addressing (e.g., $[[s]]$, $[r{+}c]$) is not part of this ISA. If $R_s\notin\mathcal{A}$, execution traps. Immediates are loaded as $c\bmod 2^w$.


The semantics of the instructions are as follows. We write $\sigma\to\sigma'$ for one execution step. Unless a jump changes it, set
$\mathrm{pc}\gets \mathrm{pc}+1$ where $\mathrm{pc}\in\{0,\ldots,\ell\}\cup\{\mathtt{HALT}\}$ is the program counter. Let $\oplus$ and $\wedge$ denote bitwise XOR and AND;
let $\ll$ and $\gg$ denote logical shifts; all arithmetic is modulo $2^w$. 

\begin{itemize}
\item $\mathtt{LOAD}\ r,\,[s]$:\;
      $a\gets R_s$; $R_r\gets M[a]$.

\item $\mathtt{STORE}\ [s],\,r$:\;
      $a\gets R_s$; $M[a]\gets R_r$.

\item $\mathtt{LOADI}\ r,\,c$:\;
      $R_r\gets c \bmod 2^w$.

\item $\mathtt{ADD}\ r,\,s$ / $\mathtt{SUB}\ r,\,s$:\;
      $R_r\gets (R_r \pm R_s)\bmod 2^w$.

\item $\mathtt{AND}\ r,\,s$ / $\mathtt{XOR}\ r,\,s$:\;
      $R_r\gets R_r \wedge R_s$ \; / \; $R_r\gets R_r \oplus R_s$.

\item $\mathtt{SHL}\ r,\,s$ or $\mathtt{SHR}\ r,\,s$:\;
      $h\gets R_s \bmod w$; \; $\mathtt{SHL}$: $R_r\gets (R_r \ll h)\bmod 2^w$;\;
      $\mathtt{SHR}$: $R_r\gets \lfloor R_r/2^h \rfloor$ (logical right shift, zero fill).

\item $\mathtt{BRZ}\ r,\,L$:\;
      If $R_r=0$ then $\mathrm{pc}\gets \mathrm{addr}(L)$ else (no change to $\mathrm{pc}$ beyond $+1$).

\item $\mathtt{JMP}\ L$:\;
      $\mathrm{pc}\gets \mathrm{addr}(L)$.

\item $\mathrm{pc}\gets \mathtt{HALT}$ and execution stops.
\end{itemize}

Intuitively, $\mathtt{LOAD}$ and $\mathtt{STORE}$ handle memory access through register-indirect addressing, $\mathtt{LOADI}$ loads immediate constants, $\mathtt{ADD}$/$\mathtt{SUB}$ perform modular arithmetic, $\mathtt{AND}$/$\mathtt{XOR}$ enable bitwise operations, $\mathtt{SHL}$/$\mathtt{SHR}$ provide bit shifts.  $\mathtt{BRZ}$ (branch 
if zero) enables conditional branching, $\mathtt{JMP}$ provides unconditional jumps, and $\mathtt{HALT}$ terminates execution.

\paragraph{Programs and Configurations.}
A \emph{program} is a pair $\mathcal{P}=(I_0,\ldots,I_\ell,\mathrm{addr})$ with $I_i\in\Instr$ and a partial label table $\mathrm{addr}:\Lab\rightharpoonup\{0,\ldots,\ell\}$ mapping each declared label to its
instruction index (injective). The program is
\emph{well formed} if whenever some $I_i$ equals $\mathtt{JMP}\ L$ or $\mathtt{BRZ}\ r,L$, then
$L\in\operatorname{dom}(\mathrm{addr})$. Code is immutable during execution and independent of~$n$ (and thus the RAM model considered here is uniform). A \emph{configuration} is $\sigma=(\mathrm{pc},R,M)$ where $\mathrm{pc}\in\{0,\ldots,\ell\}\cup\{\mathtt{HALT}\}$ is the program counter, $R\in\mathbb{U}^{k+1}$ is the register file, and $M:\mathcal{A}\to\mathbb{U}$ is memory. Input occupies $M[0..n-1]$; output is read from a designated location upon termination.

\paragraph{Initialization.}
Given an input instance of length $n$, initialization proceeds as follows:
\begin{enumerate}[leftmargin=1.2em, itemsep=1pt, topsep=2pt]
\item Build the label table $\mathrm{addr}$ from the loaded code and check well-formedness (every label operand in the code must appear exactly once as a declared label).
\item Zero-initialize memory $M$ and write the input into a designated block (e.g., $M[0..n-1]$) using the agreed-upon encoding.
\item Set all registers to zero: $R_i\gets 0$ for $i\in\{0,\ldots,k\}$.
\item Set the program counter to the first instruction: $\mathrm{pc}\gets 0$.
\end{enumerate}
The choice $w\ge \lceil\log_2 S(n)\rceil$ ensures that register-indirect addressing is well-typed: a bracketed operand $[s]$ uses $R_s$ as an address in $\mathcal{A}$.

\paragraph{Execution Cycle.}
While $\mathrm{pc}\neq\mathtt{HALT}$ and no trap occurs, the machine advances in discrete steps. Each successful step costs one time unit. Let $I_{\mathrm{pc}}$ denote the instruction at index $\mathrm{pc}$. Each step follows the fetch-decode-execute-commit cycle:

\begin{enumerate}[leftmargin=1.2em, itemsep=1pt, topsep=2pt]
\item \textbf{Fetch}: Read the current instruction $I \leftarrow I_{\mathrm{pc}}$. If $\mathrm{pc}\notin\{0,\ldots,\ell\}$, the run is invalid and we define this as a trap.

\item \textbf{Decode and read operands}: Parse the opcode and operands of $I$ without changing the machine state. Unbracketed registers $r,s$ denote their current word values $R_r,R_s\in\mathbb{U}$ (used as data). A bracketed operand $[s]$ denotes the candidate address $a\gets R_s$. An immediate $c\in\Imm$ is interpreted as $c\bmod 2^w$. A label $L$ resolves to $\mathrm{addr}(L)$ (guaranteed by well-formedness). No writes occur in this phase.

\item \textbf{Execute}: Apply the instruction semantics of $I$ to compute a finite \emph{write-set} $W$ (register and/or memory locations with their new values) and the \emph{next program counter} $\mathrm{pc}_{\text{next}}$. For memory-referencing instructions, a bracketed operand $[s]$ is valid only if $a=R_s\in\mathcal{A}$; otherwise a trap occurs. By default $\mathrm{pc}_{\text{next}}=\mathrm{pc}+1$, except for jumps/branches/halting which set it to $\mathrm{addr}(L)$ (or $\mathtt{HALT}$).

\item \textbf{Commit (writeback)}: Atomically apply the writes in $W$ to $(R,M)$ and then set $\mathrm{pc}\gets \mathrm{pc}_{\text{next}}$. Atomicity means all effects of the step become visible only at the end of the step.

\item \textbf{Cost and continuation}: If no trap occurred, charge one unit of time for this step and proceed to the next; otherwise the run aborts (abnormal termination), and only successfully committed steps are counted.
\end{enumerate}

\paragraph{Termination and Complexity.}
Execution halts when $\mathrm{pc}=\mathtt{HALT}$. The algorithm's output is read from the designated output location(s) in memory (or registers) as specified by the program. Under the assumptions above and for well-formed programs with legal memory accesses, the step relation is deterministic and yields a unique next state at each iteration. The \emph{time complexity} of an algorithm is the number of executed instructions before halting. A \emph{trap} aborts the run immediately (abnormal termination); only successfully committed steps are counted in time.

The RAM model defined here is polynomially equivalent to
bit-complexity RAM (a $\Theta(\log n)$ factor separates their running times) and to richer word-RAMs
that add $\mathtt{MUL}/\mathtt{DIV}/\mathtt{POPCNT}/\mathtt{CLZ}$ (whose presence typically improves
only by constant or $\log\log n$ factors).



\begin{algorithm}[t]
   \caption{Single-Processor Execution (Word-RAM semantics with PID init)}
   \label{alg:single-processor}
   \emph{Note.} In PRAM, $\mathtt{STORE}$ generates a pending write committed at the end of the round under the $\CREW$ rule. In the single-processor case, the store can be applied immediately.
   \begin{algorithmic}[1]
   \Require Program $\mathcal{P}=(I_0,\dots,I_\ell,\mathrm{addr})$, shared memory $M:\mathcal{A}\!\to\!\mathbb{U}$, word size $w$, processor id $\mathsf{pid}$, processor budget $P(n)$ (optional)
   \State \textbf{Init:} $pc \gets 0$;\quad $R[j]\gets 0$ for all $j$;\quad $R[0]\gets \mathsf{pid}$;\quad \textbf{optional: }$R[1]\gets P(n)\bmod 2^w$
   \While{$pc \neq \mathtt{HALT}$}
     \State $I \gets I_{pc}$ \Comment{fetch}
     \State $pc_{\text{next}} \gets pc + 1$ \Comment{default fall-through}
     \If{$I$ is \texttt{LOAD} $r,[s]$} \Comment{decode}
        \State $a \gets R[s]$
        \If{$a \notin \mathcal{A}$} \State \textbf{trap} \EndIf
        \State $R[r] \gets M[a]$ \Comment{execute}
     \ElsIf{$I$ is \texttt{STORE} $[s],r$}
        \State $a \gets R[s]$
        \If{$a \notin \mathcal{A}$} \State \textbf{trap} \EndIf
        \State $M[a] \gets R[r]$ \Comment{in PRAM semantics, this is a write event to be committed this round}
     \ElsIf{$I$ is \texttt{LOADI} $r,c$}
        \State $R[r] \gets c \bmod 2^w$
     \ElsIf{$I$ is \texttt{ADD} $r,s$}
        \State $R[r] \gets (R[r] + R[s]) \bmod 2^w$
     \ElsIf{$I$ is \texttt{SUB} $r,s$}
        \State $R[r] \gets (R[r] - R[s]) \bmod 2^w$
     \ElsIf{$I$ is \texttt{AND} $r,s$}
        \State $R[r] \gets R[r] \wedge R[s]$ \Comment{bitwise AND}
     \ElsIf{$I$ is \texttt{XOR} $r,s$}
        \State $R[r] \gets R[r] \oplus R[s]$ \Comment{bitwise XOR}
     \ElsIf{$I$ is \texttt{SHL} $r,s$}
        \State $h \gets R[s] \bmod w$
        \State $R[r] \gets (R[r] \ll h)\bmod 2^w$
     \ElsIf{$I$ is \texttt{SHR} $r,s$}
        \State $h \gets R[s] \bmod w$
        \State $R[r] \gets \lfloor R[r]/2^h \rfloor$ \Comment{logical right shift, zero-fill}
     \ElsIf{$I$ is \texttt{BRZ} $r,L$}
        \If{$R[r] = 0$} \State $pc_{\text{next}} \gets \mathrm{addr}(L)$ \EndIf
     \ElsIf{$I$ is \texttt{JMP} $L$}
        \State $pc_{\text{next}} \gets \mathrm{addr}(L)$
     \ElsIf{$I$ is \texttt{HALT}$\;$}
        \State $pc_{\text{next}} \gets \mathtt{HALT}$
     \Else
        \State \textbf{trap} \Comment{unknown opcode or malformed operands}
     \EndIf
     \State $pc \gets pc_{\text{next}}$ \Comment{commit PC; regs/memory updated in each branch above}
   \EndWhile
   \end{algorithmic}
   \end{algorithm}
   
   




\subsection{Extension to CREW PRAM}

We extend the Word-RAM defined above to a parallel machine with a processor-budget function $P:\mathbb{N}\to\mathbb{N}$ (typically $P(n)\le n^{\mathcal{O}(1)}$). All word-size/address-width assumptions, the instruction alphabet $\Instr$, the immediate-set restriction, and the \emph{single-processor} instruction semantics are exactly as in the Word-RAM subsection.

\paragraph{Processors and Shared State.}
Processors are indexed by $i\in\{0,\ldots,P(n)-1\}$. Each processor has its own program counter and register file; memory is shared:
\[
\Sigma \;=\; \bigl( (\pc_0,\ldots,\pc_{P(n)-1}),\ (R^0,\ldots,R^{P(n)-1}),\ M \bigr),
\]
where $\pc_i\in\{0,\ldots,\ell\}\cup\{\mathtt{HALT}\}$ and $R^i=(R^i_0,\ldots,R^i_k)\in\mathbb{U}^{k+1}$. All processors run the same program $\mathcal{P}=(I_0,\ldots,I_\ell,\addr)$.

\paragraph{Initialization (with Processor IDs).}
At time $t=0$:
\begin{enumerate}[leftmargin=1.2em, itemsep=1pt, topsep=2pt]
\item Build $\addr$ and check well-formedness (as in Word-RAM); zero-initialize $M$ and write the input block.
\item For each $i\in\{0,\ldots,P(n)-1\}$, set $\pc_i\gets 0$ and clear registers; then write \emph{processor-local identifiers:} \underline{$R^i_0\gets i$} and, if $P(n)\le 2^{w(n)}$, optionally \underline{$R^i_1\gets P(n)\bmod 2^{w(n)}$}. All other $R^i_j\gets 0$.
\end{enumerate}
These two words are provided so that processors can branch, partition work, and self-disable if unused.

\paragraph{Concurrent-Access Policy (CREW).}
Multiple processors may \emph{read} the same address in the same round; \emph{writes must be exclusive}: if two or more writes target the same address in a round, the run traps (abnormal termination).

\paragraph{Round Semantics (Referencing the Word-RAM Step).}
Each active processor executes exactly one instruction using the single-processor Word-RAM step semantics; the only new aspects are (i) simultaneous execution by many processors and (ii) end-of-round memory commit subject to the $\CREW$ policy. Execution proceeds in synchronous rounds $t=0,1,2,\ldots$ with state $\Sigma_t=((\pc_i^t)_i,(R^{i,t})_i,M^t)$. In round $t$, each active processor $i$ with $\pc_i^t\in\{0,\ldots,\ell\}$ executes instruction $I_{\pc_i^t}$ on its local snapshot $(\pc=\pc_i^t,\ R=R^{i,t})$ and shared memory $M^t$. After all processors compute their local effects, the round commits: register writebacks $R^{i,t}\to R^{i,t+1}$ (independently), then memory writes to $M^{t+1}$ under $\CREW$ constraints, finally program counter updates $\pc_i^{t+1}$.

\paragraph{Termination and Cost Measures.}
The parallel run terminates when $\pc_i^t=\mathtt{HALT}$ for all $i$ (or traps on an invalid access/conflict). One round costs one unit of \emph{parallel time}. The \emph{work} is the total number of executed instructions $W(n)=\sum_t |\{\,i:\pc_i^t\in\{0,\ldots,\ell\}\,\}|$, and the \emph{span} is $T_\infty(n)$ (the critical-path length). For $P(n)$ processors the Brent bound holds~\citep{jaja1992parallel}:
\[
T_{P(n)}(n)\ \le\ \Big\lceil \frac{W(n)}{P(n)} \Big\rceil + T_\infty(n).
\]
A single-processor run ($P(n)=1$) coincides with the Word-RAM model.

\paragraph{Remarks (on $P(n)$, Processor IDs, and Unused Processors).}
(1) \emph{Uniformity:} the code $\mathcal{P}$ and the immediate set $\Imm$ are independent of $n$; only the hardware parameters $w(n)$, $S(n)$, and $P(n)$ scale with input size. (2) \emph{Processor IDs:} the values $i$ and $P(n)$ are provided via initialization registers ($R^i_0$ and optionally $R^i_1$) for branching and work partitioning; programs may copy/overwrite them. (3) \emph{Unused processors:} if an algorithm needs only $m(n)\le P(n)$ processors, each processor executes a short self-filter based on $i$ (e.g., \texttt{if $i\ge m(n)$ then HALT}), or computes its assigned block; processors with empty assignment halt in $O(1)$ rounds, which does not affect the asymptotic parallel time.

The algorithm for a single-processor in PRAM is shown in \Cref{alg:single-processor}.
