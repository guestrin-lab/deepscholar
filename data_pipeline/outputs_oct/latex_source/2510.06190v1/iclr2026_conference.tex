\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[colorlinks,
            linkcolor=OliveGreen,
            anchorcolor=OliveGreen,
            citecolor=OliveGreen,
            urlcolor=OliveGreen
            ]{hyperref}
\usepackage{url}

% https://github.com/st--/annotate-equations
\usepackage{annotate-equations}
\usepackage[skins,listings,most]{tcolorbox}
\renewcommand{\eqnannotationfont}{\bfseries\small}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table,dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{floatrow}
\usepackage{colortbl}
\usepackage{amsmath,amscd,amsbsy,amsfonts,latexsym,url,bm,amsthm,amssymb,dsfont}
\usepackage{wrapfig}
% \usepackage[vlined,boxed,ruled]{algorithm2e}  % Commented out to avoid conflict with algorithm+algpseudocode
\usepackage{caption}
\usepackage{fvextra}
\usepackage{csquotes}
\usepackage{natbib}
% \setcitestyle{numbers,square}
\usepackage{enumitem}

\usepackage{cleveref}

\usepackage{listings}
\usepackage{mdframed}
\usepackage[colorinlistoftodos,prependcaption]{todonotes}
\usepackage{xargs}
\usepackage{BOONDOX-uprscr}
\usepackage{thmtools}
\usepackage{lipsum}    % 生成占位文字
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}  % For better algorithm support
\usepackage{float}         % For better float control

% Define circled numbers with proper alignment
\newcommand{\circled}[1]{\raisebox{0.15ex}{\textbf{\textcircled{\raisebox{-0.15ex}{\small #1}}}}}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}


\newcommand{\red}[1]{\textcolor{BrickRed}{#1}}
\newcommand{\blue}[1]{\textcolor{NavyBlue}{#1}}
\newcommand{\purple}[1]{\textcolor{DarkOrchid}{#1}}
\newcommand{\green}[1]{\textcolor{OliveGreen}{#1}}
\newcommand{\yellow}[1]{\textcolor{Dandelion}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}

% \definecolor{gray}{HTML}{7A7A7A}
% \definecolor{red}{rgb}{1.        , 0.17254902, 0.1372549 }
% \definecolor{purple}{rgb}{0.68980392, 0.40803922, 0.70921569 }
% \definecolor{blue}{rgb}{0.05555556, 0.43398693, 1.   }

\usepackage{enumitem}
\setlist[itemize,1]{leftmargin=10pt, labelindent=5pt, itemsep=3pt, parsep=3pt}
\setlist[enumerate,1]{leftmargin=10pt, labelindent=5pt, itemsep=3pt, parsep=3pt}
\setlist[itemize,2]{leftmargin=15pt, itemsep=3pt, parsep=3pt}
\setlist[enumerate,2]{leftmargin=15pt, itemsep=3pt, parsep=3pt}

% Define macros for unmask and remask with bold and colors (math mode only)
\newcommand{\f}{f}
\newcommand{\g}{\phi}
\newcommand{\nxt}{\pi}


% Define macro for mask token
\newcommand{\mask}{\textsf{M}}

\newcommand{\unmask}{\purple{\textbf{unmask}}}
\newcommand{\remask}{\blue{\textbf{remask}}}
\newcommand{\delete}{\red{\textbf{delete}}}
\newcommand{\inser}{\yellow{\textbf{insert}}}


\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\newcommand{\overbar}[1]{\mkern 1.3mu\overline{\mkern-1.3mu#1\mkern-1.3mu}\mkern 1.3mu}
\newcommand{\undebar}[1]{\mkern 1.3mu\underline{\mkern-1.3mu#1\mkern-1.3mu}\mkern 1.3mu}

\newcommand*{\ldblbrace}{\{\mskip-5mu\{}
\newcommand*{\rdblbrace}{\}\mskip-5mu\}}

\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}

\newcommandx{\cmt}[2][1=Comment]{\vspace{2pt}\todo[inline,backgroundcolor=black!5]{\textit{(#1)} \;#2}}

\newcommand{\chenxiao}[1]{{\blue{[Chenxiao: ``#1'']}}}
\newcommand{\zhiyuan}[1]{{\purple{[Zhiyuan: ``#1'']}}}
\newcommand{\david}[1]{{\red{[David: ``#1'']}}}


\crefname{section}{$\mathsection$}{$\mathsection\mathsection$}
\Crefname{section}{$\mathsection$}{$\mathsection\mathsection$}

\title{On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond}

% \title{On the Surprising Power to Generate Unnaturally}

% \title{Rethinking What are Natural Ways to Generate}

% \title{Generation to the Limit}

% \title{Unmasking Masked Diffusion: Towards Generation Processes beyond Natural Language}

% \title{In Search of A Universal Generation Process: Masked Diffusion and Beyond}

% \title{In Search of A Universal Generation Process: On Computational Power of Masked Diffusion / from Auto-Regressive to Masked Diffusion}

% \title{Should Programs Be Written Line by Line}

% \title{In Search of Universal Generation Process}

% \title{How Things Should Be Generated: A Theory of Masked Diffusion }

\iclrfinalcopy

\author{
  Chenxiao Yang$^\dagger$~~~~Cai Zhou$^\S$~~~~David Wipf~~~~Zhiyuan Li$^\dagger$\\
  $\dagger$ Toyota Technological Institute at Chicago~~~~$\S$ Massachusetts Institute of Technology\\
    \small\texttt{\{chenxiao,zhiyuanli\}@ttic.edu}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
This paper formally studies generation processes, including auto-regressive next-token prediction and masked diffusion, that abstract beyond architectural specifics. At this level of abstraction, we quantify their benefits and limitations through measurable criteria such as computational hardness and learnability. In particular, we demonstrate that allowing generation to proceed beyond autoregression and current masked diffusion, with capabilities to rewrite and length-variable edit, can bring significant theoretical and empirical advantages, with important implications for frontier LLMs that aspire to tackle increasingly hard problems and work universally across domains beyond natural language, such as coding and science.\footnote{Code is available at \url{https://github.com/chr26195/AP-MDM}.}
\end{abstract}



% \vspace{-5pt}
\section{Introduction}
% \vspace{-5pt}


% What power can being natural bring? 
The underlying generation process of almost everything in nature follows a unidirectional arrow of time. Perhaps most representative of all, spoken language is produced through a sequential process where each word builds upon preceding context in causal temporal order. This generic inductive bias has been encoded into \emph{Auto-Regressive Models (ARM)}~\citep{shannon1951prediction}, through next-token generation. Despite its simplicity, ARM when scaled through training on vast corpora, has produced remarkably powerful models capable of general-purpose task completion and reasoning, like GPT~\citep{radford2018improving,radford2019language,brown2020language,achiam2023gpt}. 

Yet reality might be more convoluted. Humans, when tackling challenging tasks, naturally undergo a non-sequential process of searching for solutions, evaluating and refining them, backtracking when needed, and iterating until answers are found. Such complexity is not fully captured by current ARM. While it is debatable if human intelligence fundamentally follows this left-to-right process~\citep{lecun2023large,malach2023auto,bachmann2024pitfalls,berglund2023reversal,nagarajan2025roll}, ARM appears increasingly ill-suited when we venture beyond natural language.

For example, code generation must subject to global constraints like balanced parentheses and well-typedness. Maintaining validity at each intermediate step makes transitions from one state to another easier, thus naturally involving updates such as inserting functions, adding branches, or changing input types. In biology, many domains remain largely beyond the reach of current LLMs, as molecular structures such as proteins and genes are combinatorial objects that can be modeled as graphs, trees, or strings that satisfy physical constraints. Their generation proceeds most naturally through structure-aware edits, e.g., swapping protein domains, inserting binding motifs into sequence graphs, or recombining DNA/RNA segments~\citep{wang2023scientific}.



% ~\citep{bommasani2022opportunitiesrisksfoundationmodels}
Given the long-standing pursuit of building foundation models powerful enough to handle increasingly complex reasoning tasks and general enough to work across diverse domains beyond natural language, it becomes important and timely to rethink generation process itself, as a mechanism separate from architectural specifics, by formally asking:

\vspace*{-0.1cm}
\begin{center}
\textit{How do we formally compare various ways to generate, and what opportunities may lie beyond next-token generation?}
\end{center}
\vspace*{-0.1cm}

Recent work suggests that next-token generation is not the only viable path. \emph{Masked Diffusion Models (MDM)}~\citep{hoogeboom2021argmax,austin2021structured,lou2024discrete,sahoo2024simple,shi2024simplified} offer a compelling alternative  procedure that, instead of causally generating tokens one by one, permits any-order generation and produces multiple tokens in parallel, with recent large-scale intantiations~\citep{deepmind2025gemini,inceptionlabs2025mercury,nie2025large,ye2025dream} showing comparable performance with AR-based LLMs. Interestingly, besides faster decoding (up to $10\times$ speedups), MDM's generation process brings empirical improvements on some order-sensitive tasks such as reversed-order poem completion~\citep{nie2025large,berglund2023reversal} and Sudoku puzzles~\citep{kim2025train,shah2024causal}. This motivates us to formally study it and compare with ARM.
% Therefore, we investigate whether such new generation processes bring advantages in terms of the scope of problems they enable solving. This seems to capture 

Perhaps counterintuitively, we find \textbf{while MDM is indeed more powerful than ARM in terms of parallelism and efficiency for simple tasks, the benefits of seemingly greater flexibility are surprisingly limited.} Like ARM~\citep{merrill2023expresssive,feng2024towards,li2024chain}, MDM also achieves Turing-completeness, but does so more efficiently with optimal parallel time complexity {(\Cref{thm:main_mdm})}, thus enabling \emph{exponential} speedups for simple parallelizable problems. However, for harder reasoning tasks, MDM faces similar fundamental limitations as ARM: both struggle with problems requiring backtracking and rewriting capabilities, and cannot handle them given realistic space resources {(\Cref{thm:main_constrained})}. Moreover, when controlling for other factors including degree of parallelism and architecture, any-order generation itself does not expand what ARM can already handle {(\Cref{thm:any_order})}, since any computation performed by MDM can be reorganized into left-to-right order to align with the underlying arrow of time. Therefore we ask: 

\vspace*{-0.1cm}
\begin{center}
\textit{What are provably more powerful ways to generate?}
\end{center}
\vspace*{-0.1cm}


\begin{figure}[t]
	\centering
    % \vspace{-10pt}
	\includegraphics[width=\textwidth]{figs/intro.pdf}
    \vspace{-20pt}
	\caption{Comparison between autoregressive generation, any-order generation (standard MDM) and any-process generation (our MDM). \vspace{-10pt}}
    \label{fig:ap_mdm_compare}
\end{figure} 

As an initial step, we propose \textbf{Any-Process Generation}, inspired by natural generative mechanisms found across domains. It extends standard MDM beyond its existing $\unmask$ capability with three additional operations (see \Cref{fig:ap_mdm_compare}): $\remask$ (converting decoded tokens back to masks), $\inser$ (adding new mask tokens at any position), and $\delete$ (removing mask tokens), all learned end-to-end from data without architectural changes. Freed from conventional physics-inspired diffusion frameworks, any-process generation removes unnecessary restrictions on mask ratios, decoding steps, sequence lengths and stopping criteria, enabling structural editing and test-time scaling. With these modifications, we show that MDM brings significant promise with both encouraging theoretical and empirical results as follows.

\textbf{Scalability to Hard Problems:}~ The capability to rewrite and backtrack breaks the non-erasable limitations of ARM and standard MDM, enabling our model to achive both optimal parallel time and space complexity (\Cref{thm:main_apmdm}), thus solving many $\textsf{NP}$-hard problems with polynomial space through test-time scaling, i.e. an exponential improvement from $\textsf{P}$ achieved by ARM and standard MDM. Empirically, on Sudoku puzzles (\Cref{fig:sudoku_example}), our model achieves 99.28\% accuracy using only 100 training instances, outperforming ARM (87.18\%) and any-order MDM (89.49\%) with $5\times$ parameters trained on 1.8M instances, which is orders of magnitude more.

\textbf{Structure-Aware Generation:}~ The flexibility to rewrite, insert and delete tokens enables structure-aware generation processes that resist sequential construction, such as gene splicing in biology (\Cref{fig:dna_example}) and 2D graph generation (\Cref{fig:graph_example}). To quantify such benefits, we prove ARM cannot even generate matched parentheses (two-sided Dyck-k language) for arbitrary lengths, i.e. one of the most basic skills for code generation, while our model can (\Cref{fig:dyck_example}, \Cref{thm:apmdm_simulation}). Empirical results on a graph editing / generation task show that our approach maintains perfect accuracy for increasingly larger graphs, while ARM performance degrades significantly as graph size increases.

\textbf{Learning and (OOD) Generalization:}~ Our approach enables learning previously-impossible simpler algorithms that significantly improve learning and generalization. For parity checking (\Cref{fig:parity_example}), our model achieves 100\% generalization to arbitrary lengths after training on only length-2 sequences, while even the latest GPT models struggle on this embarrassingly simple task.

Finally, envisioning a future with access to data of the underlying generation processes of objects we wish to generate, such as code revisions, math proof drafts, or molecular formation processes, any-process MDM is theoretically and empirically more suitable than ARM (\Cref{thm:apmdm_simulation2}). 

% A comprehensive discussion of related works is in \Cref{appendix:related}.


\begin{figure}[t]
    % \vspace{-10pt}
	\centering
	{\setlength{\subfigcapskip}{-5pt}
	\subfigure[\textbf{Sudoku} (NP-Complete Problem). Scaling up inference-time computes to solve significantly harder problems by allowing rewrites and backtracking using the $\remask$ operation (\Cref{sec:advantage_1}).]{\includegraphics[width=\textwidth]{figs/sudoku.pdf}\label{fig:sudoku_example}}
	
    \vspace{-3pt}
	\subfigure[\textbf{Coding} (Matched Parentheses / Dyck-$k$). Generating any-sized two-sided Dyck-$k$ is impossible for ARM, while our model can easily do so with the $\inser$ operation (\Cref{sec:advantage_2}).]{\includegraphics[width=\textwidth]{figs/dyck.pdf}\label{fig:dyck_example}}

    \vspace{-3pt}
	\subfigure[\textbf{Parity} (Counting 1s). Length generalizing parity and counting problems are enabled by learning a simple elimination algorithm with the $\remask$ and $\delete$ operations (\Cref{sec:advantage_3}).]{\includegraphics[width=\textwidth]{figs/parity.pdf}\label{fig:parity_example}}
	
    \vspace{-3pt}
	\subfigure[\textbf{DNA Recombination} (Splicing System). An example in science / biology where DNA segments are spliced and pasted using combinations of operations, which is hard for ARM (\Cref{sec:advantage_2}).]{\includegraphics[width=\textwidth]{figs/DNA.pdf}\label{fig:dna_example}}

    \vspace{-3pt}
	\subfigure[\textbf{Graph Editing.} Editing combinatorial structures where feature / structural evolution and parallel computation are naturally integrated (\Cref{sec:advantage_3}). Using ARM to simulate is hard (\Cref{sec:advantage_4}).]{\includegraphics[width=\textwidth]{figs/graph.pdf}\label{fig:graph_example}}
	}
	\vspace{-10pt}
	\caption{Examples of any-process generation for different tasks.\vspace{-10pt}}
	\label{fig:all_examples}
\end{figure} 



\vspace{-5pt}
\section{Preliminary} \label{sec:preliminary}
\vspace{-5pt}

\textbf{Auto-Regressive Model (ARM)}~~~ Let $\Sigma$ be a finite-sized vocabulary and $\nxt: \Sigma^* \rightarrow \Sigma$ be a next-token predictor, which maps a sequence $\mathbf{x} = (x_1, x_2, \cdots, x_n) \in \Sigma^n$ to a token $x_{n+1} \in \Sigma$. An autoregressive model (ARM) is defined based on a sequence-to-sequence mapping $\f: \Sigma^* \rightarrow \Sigma^*$, concatenating input sequence $\mathbf{x}$ and the next token $\nxt(\mathbf{x})$, i.e. $f(\mathbf{x}) = (\mathbf{x}, \nxt(\mathbf{x}))$. ARM formulates generation as an iterative process by repeatedly applying $\f$ to the current sequence. In practice, $\f$ is typically parameterized by a Transformer~\citep{vaswani2017attention} with causal attention and learnable parameters $\theta$. The notion of ARM here also aligns with \emph{Chain-of-Thought (CoT)}~\citep{wei2022chain} in many other works and we will use them interchangeably throughout this paper.

\textbf{Masked Diffusion Model (MDM)}~~~ Let $\bar{\Sigma} = \Sigma \cup \{\mask\}$ be the extended vocabulary where $\mask$ is an absorbing mask token~\citep{austin2021structured}. Consider sequences $\mathbf{x}_t = (x_{t,1}, x_{t,2}, \ldots, x_{t,S}) \in \bar{\Sigma}^S$ indexed by time $t\in[T]$, where $S$ is the maximum context length, $T$ is the number of decoding steps, $\mathbf{x}_0 = \{\mask\}^S$ is the fully masked sequence and $\mathbf{x}_T \in \Sigma^S$ is the target clean sequence.\footnote{Unlike convention in diffusion model where larger $t$ denotes earlier inference steps, we use $t$ following an intuitive feed-forward ordering during inference, i.e. the focus of this paper.} A masked diffusion model (MDM)~\citep{lou2024discrete,sahoo2024simple,shi2024simplified} also relies on a sequence-to-sequence mapping $\f: \bar{\Sigma}^S \rightarrow \bar{\Sigma}^S$ with $\mathbf{x}_{t+1} = \f(\mathbf{x}_t)$, formulating generation as an iterative process by repeatedly applying $\f$ to progressively unmasks tokens from the all-mask state.

Among many MDM variants, we consider the following standard design choices from recent large language diffusion models~\citep{nie2025large}: \textbf{1)} linear noise schedule with $S = P \cdot T$ for integer $P$, where each step reveals exactly $P$ tokens; \textbf{2)} confidence-based adaptive decoding~\citep{chang2022maskgit} rather than random token selection; \textbf{3)} encoder-only Transformer architecture without timestep embedding; \textbf{4)} conditional generation where input prompt $\mathbf{x}$ of length $n$ is a prefix of $\mathbf{x}_0$, with $n$ calculated within context length $S$, aligned with reasoning problem setup for ARM. Detailed MDM introduction and encoder-only Transformer definition are in \Cref{appendix:mdm} and \Cref{appendix:encoder}, respectively.

\vspace{-5pt}
\section{A Theory of Masked Diffusion} \label{sec:theory}
\vspace{-5pt}

The generation process in MDM is unique in two different ways: it generates multiple tokens in parallel and permits any-order generation. We now investigate whether and how exactly these properties, in their own right, translate into concrete advantages.


\vspace{-3pt}
\subsection{Power of Parallelism} \label{sec:parallelism}
\vspace{-3pt}

Prior work~\citep{merrill2023expresssive,feng2024towards,li2024chain} has shown that ARM with sufficiently many intermediate steps is Turing-complete and thus can solve any computable problem. Analogous to the role of intermediate steps in ARM, two governing resources determine MDM's power: \textbf{1)} number of decoding (denoising) steps $T(n)$, and \textbf{2)} maximum context length $S(n)$ (equivalently, the maximum number of tokens available to decode). 

\begin{definition}[\MDM]
Let $\MDM(S(n), T(n))$ be the class of decision problems solvable by MDM (\Cref{sec:preliminary}) with maximum context length $S(n)$ and at most $T(n)$ decoding steps, using some constant depth and $\log(n)$ embedding size encoder-only Transformer. Also, let $\MDM(S(n))  = \bigcup\limits_{T(n)} \MDM(S(n), T(n))$.
\end{definition}
\vspace{-10pt}


To formally characterize MDM's expressivity in relation to $T(n)$ and $S(n)$, we establish a connection with the canonical parallel computation model called \emph{Parallel Random Access Machine (PRAM)}~\citep{fortune1978parallelism,jaja1992parallel}, which is the RAM model extended to multiple processors executing over shared memory. See detailed introduction and a formal definition of the variant we use in \Cref{appendix:pram}.

\begin{definition}[\PRAM]
    Let $P(n)$ be the number of processors budget, and $w(n)=\Theta(\log n)$ the word size. Define $\PRAM(P(n),T(n))$ as the class of decision problems solvable by a \emph{uniform} CREW PRAM (see \Cref{appendix:pram} for CREW specification) using at most $P(n)$ processors in at most $T(n)$ parallel time.
\end{definition}

\begin{theorem}[MDM Simulation of PRAM, Informal] \label{thm:main_mdm}
    For any PRAM program that runs on input $\mathbf{x} \in \Sigma^n$  in at most $T(n)$ parallel time with $P(n)$ maximum processors, there exists an MDM on input $\mathbf{x}$, padded to $S(n) = \mathcal O(P(n) \cdot T(n))$, that matches the PRAM output in $\mathcal O(T(n))$ decoding steps, i.e. $\PRAM(P(n),T(n)) \subseteq \MDM(\mathcal O(P(n) \cdot T(n)), \mathcal O(T(n)))$. See formal statement in \Cref{thm:main_mdm_formal}.
\end{theorem}
\vspace{-3pt}

\textbf{This demonstrates that MDM can simulate any PRAM algorithm with optimal parallel time complexity, thereby it is not only Turing-complete as ARM already achieves, but can also solve problems significantly faster with parallelization, something ARM cannot offer.} The speedup can be \emph{exponential} compared to ARM's serial time complexity: for efficiently parallelizable problems in $\textsf{NC}$~\citep{arora2009computational},\footnote{$\textsf{NC}$ is the complexity class for efficiently parallelizable problems, those that are solvable in $\text{polylog}(n)$ time using $\text{poly}(n)$ processors; $\textsf{NC} \subseteq \textsf{P}$ and it is open whether $\textsf{NC} = \textsf{P}$~\citep{greenlaw1995limits}. PRAM is the canonical model for this notion as a Turing machine is for \textsf{P}.} graph connectivity can be solved in $\mathcal{O}(\log n)$ decoding steps versus ARM's linear complexity, and context-free languages including Dyck-k require only $\mathcal{O}(\log^2 n)$ steps. These tasks have been demonstrated hard or inefficient for ARM in previous literature~\citep{strobl2024formal,zhu2025reasoning}.


\vspace{-3pt}
\subsection{(Un)Scalability to Hard Tasks} \label{sec:inherently_hard}
\vspace{-3pt}


While noteworthy, the computational power described above comes with a non-negligible cost: solving a problem requires context length $S(n)$ to scale as $\mathcal{O}(T(n) \cdot P(n))$ (the total parallel work), a quantity at least as large as the serial time complexity (with $P(n) = 1$), per Brent's Theorem~\citep{jaja1992parallel}. Particularly, in resource-constrained regimes, we have:

\begin{theorem} \label{thm:main_constrained}
$\MDM(S(n)) \subseteq \PRAM(1, \tilde{\mathcal{O}}(S^3(n)))$, where logarithmic factors are hidden in $\tilde{\mathcal{O}}$.
\end{theorem}
\vspace{-3pt}

In other words, MDM with context length $S(n)$ cannot solve problems requiring more than $\tilde{\mathcal{O}}(S^3(n))$ serial time. This limitation is also shared by ARM~\citep{yang2025pencil}.

\textbf{This implies MDM is inherently not scalable to solving hard reasoning or generation tasks}: for problems beyond $\textsf{P}$ (e.g., $\textsf{NP}$-hard problems), this would require superpolynomial context length (under standard complexity assumptions), practically intractable in terms of both memory and per-step FLOPs. The root cause lies in MDM's irreversible token generation: once decoded, those positions cannot be reused or rewritten. As reflected in the construction of \Cref{thm:main_mdm}, each memory write must be permanently stored as tokens, forcing space to scale with computation time.


In contrast, human reasoning on hard problems naturally involves  continuous revision, exploration of alternative paths, and correction of mistakes before reaching final conclusions. Generation tasks are no different: for instance, generating planar graphs (drawable on planes without edge crossings) with minimum splitting numbers is NP-complete and naturally involves iteratively adding nodes, checking planarity constraints, and backtracking when violations occur. Such process has not been captured by either ARM or MDM.



\vspace{-3pt}
\subsection{(Limited) Power of Any-Order Generation}
\vspace{-3pt}

Any-order generation seems to offer extra flexibility over auto-regression, but does it truly translates into computational advantages? To attribute gains to any-order generation itself, we control for orthogonal factors differentiating ARM and MDM: \textbf{1)} the number of tokens generated per step, and \textbf{2)} the backbone architecture (decoder v.s. encoder). The former has already been shown to confer stronger parallelism to MDM (\Cref{sec:parallelism}); the latter provides internal parallelization benefits~\citep{ewer2024entp} and improved expressiveness through padding with dummy  tokens (i.e. $\mask$)~\citep{merrill2025exact}. 

Therefore, we fix MDM to emit exactly one token per step and ARM to use the encoder‑backbone with mask tokens padding the sequence to the same length (called \emph{Masked-ARM}), or equivalently:

\begin{definition}[Masked-ARM] \label{def:masked_arm}
A Masked-ARM is defined as an MDM (\Cref{sec:preliminary}) that is forced to decode in left-to-right order and one token per step.
\end{definition}
\vspace{-3pt}

\textbf{Perhaps counterintuitively, we show that the computational benefits from any-order generation are rather limited, by itself not enabling what Masked-ARM cannot already solve}:

\begin{theorem}[Left-to-Right v.s. Any-Order, Informal] \label{thm:any_order}
For any AO-MDM with context length $S(n)$ decoding one token per step, there exists a Masked-ARM with length $\mathcal{O}(S(n))$ and extra constant layers, that can produce the same generation process for any given input $\mathbf x$, by explicitly specifying both where to write (position) and what to write (token). See formal statement in \Cref{lemma:aomdm_simulation}.
\end{theorem}
\vspace{-3pt}

Simulating any-order generation with autoregressive models is not hard because the attention mechanism is good at fetching information from any position, and re-organizing it internally in the correct order to perform the same computation. While Masked-ARM need not replicate MDM's exact final sequence, an additional post-processing step can align their outputs without affecting the theoretical conclusion. There are also some empirical evidences showing the effectiveness of ARM simulating any-order~\citep{xue2025any}.

But not all intricacies inherent in natural generation processes can be easily sequentialized. Coding for example (as well as many natural scientific processes alike), involves anywhere editing where a new valid state depends upon previous valid states that may not be contained in the final sequence.  And even when described in left-to-right temporal order, reproducing the state requires more than simple re-organization, which attention is already provably good at. Hence such a complex process is not captured by ARM or MDM as currently instantiated.



\textbf{Remark}~~ We note that MDM's observed advantages in practice may lie in discovering an optimal order~\citep{kim2025train} from data, where left-to-right ordering need not exactly correspond to the optimal temporal generation order, though computationally equivalent (\Cref{thm:any_order}).




\vspace{-5pt}
\section{{Any-Process Generation}} \label{sec:any_process}
\vspace{-5pt}


We now introduce {\emph{Any-Process Generation}}, a more powerful generation paradigm that extends the any-order masked diffusion from \Cref{sec:preliminary} (referred to as AO-MDM hereafter) by removing various restrictions to capture natural processes not present in existing generation strategies.

\textbf{A General Formulation}~~~ Let $f_\theta: \bar{\Sigma}^* \rightarrow \bar{\Sigma}^* \times \Sigma^* \times \C^*$ be a function, by default parameterized by the same Transformer architecture, which on input $\mathbf{x}_k \in \bar{\Sigma}^*$ returns the triple $(\mathbf{x}_k, \mathbf{y}_k, \ctrlvec_k)$ with $\mathbf{y}_k \in \Sigma^{|\mathbf{x}_k|}$ and $\ctrlvec_k \in \C^{|\mathbf{x}_k|}$, where $\mathbf{x}_k$ may contain one or more $\mask$ tokens (a masked sequence) while $\mathbf{y}_k$ is mask-free; $\mathbf{y}_k$ and $\ctrlvec_k$ have the same length as the input. Core to the design of this generation process is a parameter-free (and optionally non-deterministic) transition function $g: \bar{\Sigma}^* \times \Sigma^* \times \C^* \rightarrow \bar{\Sigma}^*$, which takes $(\mathbf{x}_k, \mathbf{y}_k, \ctrlvec_k)$ as input to produce the next sequence $\mathbf{x}_{k+1} \in \bar{\Sigma}^*$ that can differ in length from $\mathbf{x}_k$. The inclusion of input $\mathbf{x}_k$ itself in the output of $f_\theta(\mathbf{x}_k)$ ensures that $g$ has access to which positions are masks initially. Overall, generation is formulated as the iterative application of $f_\theta$ and $g$ until some stopping criterion is met:
\begin{align}
\mathbf{x}_{t+1} = g\big(f_\theta(\mathbf{x}_t)\big), \quad\text{and hence }\; \mathbf{x}_t = (g \circ f_\theta)^t(\mathbf{x}_0) = {g \circ f_\theta \circ g \circ f_\theta \circ \cdots \circ g \circ f_\theta}(\mathbf{x}_0).
\end{align}
Notably, unlike vanilla MDM, this framework imposes no restrictions on mask ratios at any given time 
step, therefore each decoding step can unmask an arbitrary number of tokens, and 
$\mathbf{x}_0$ can be the input prompt $\mathbf{x}$ directly with no initial mask, as in ARM. 
This framework also does not limit the maximum number of decoding steps $T$, the stopping 
criterion is flexible and need not to be a fully unmasked sequence, as in ARM. We dub this class of models as \emph{Any-Process MDM} (AP-MDM) and will detail a specific instantiation. It is not difficult to see AO-MDM is a special case of AP-MDM.

\vspace{-3pt}
\subsection{An Instantiation with $\unmask$, $\remask$, $\inser$, and $\delete$ Operations}
\vspace{-3pt}

Define $\C = \{0,1\}^3$. For each position $i$ and time step $t$, the per-position control is a 3-bit vector $\ctrl_{t,i} = (\ctrl_{t,i}[1],\,\ctrl_{t,i}[2],\,\ctrl_{t,i}[3]) \in \C$ reserved for different purposes that will be detailed below. Correspondingly, we write $\ctrlvec_t = (\ctrl_{t,1},\ldots,\ctrl_{t,|\mathbf{x}_t|}) \in \C^{|\mathbf{x}_t|}$. 

\textbf{Capability: Rewrite via Remask}~~~ We use the first bit of the per-position control  $\ctrl_{t,i}[1] \in \{0,1\}$ to control remasking (and whether to unmask) and define $\forall y \in {\Sigma}$:
\begin{align}
    \remask_{x_{t,i}, \ctrl_{t,i}}(y) = \begin{cases}
\mask & \text{if } \ctrl_{t,i}[1] = 1 \\
y & \text{if } x_{t,i} = \mask \text{ and } \ctrl_{t,i}[1] = 0 \\
x_{t,i} & \text{otherwise}
\end{cases}
\end{align}
In other words, when $\ctrl_{t,i}[1] = 1$, position $i$ is a mask after decoding regardless of its previous state; otherwise, standard unmasking follows as usual. This operation enables self-correction and \emph{test-time scaling}, allowing models to scale computation exponentially in $S(n)$ before state repetition occurs. Additionally, since the remasking signal can be learned from data, models can adaptively determine both decoding order and parallelization degree at each step.


\textbf{Capability: Length-Variable Edit via Insert / Delete}~~~ We use the second and third bits of the per-position control $\ctrl_{t,i}$ to govern insertion and deletion, respectively. Define $\forall y \in \bar{\Sigma} \cup \{\epsilon\}$:
\begin{align}
\inser_{\ctrl_{t,i}}(y) = \begin{cases}
(y, \mask) & \text{if } \ctrl_{t,i}[2] = 1 \\
y & \text{otherwise}
\end{cases},~ \delete_{x_{t,i}, \ctrl_{t,i}}(y) = \begin{cases}
    \epsilon & \text{if } x_{t,i} = \mask \text{ and } \ctrl_{t,i}[3] = 1 \\
    y & \text{otherwise}
    \end{cases}
\end{align}
where $\epsilon$ denotes the empty string.
In other words, insert adds a mask token after position $i$ when $\ctrl_{t,i}[2] = 1$, and delete removes position $i$ when it was originally a mask token ($x_{t,i} = \mask$) and $\ctrl_{t,i}[3] = 1$. These operations enable dynamic sequence length adjustment based on problem complexity, with the insert operation allowing sequence length to grow exponentially as \emph{each mask token can spawn additional masks}. Furthermore, the delete operation provides computational efficiency by freeing space during stages that require less extensive computation, reducing overall FLOPs waste. This mechanism can work orthogonally with semi-autoregression~\citep{arriola2025block} that expands space at the end of the sequence.


\textbf{Composition}~~~ To summarize, each decoding step applies the three operations coordinate-wise and concatenates the resulting segments:
\begin{align} \label{eq:apmdm_implementation}
\begin{split}
    &f_\theta(\mathbf{x}_t) = (\mathbf{x}_t, \mathbf{y}_t, \ctrlvec_t), \quad g(\mathbf{x}_t, \mathbf{y}_t, \ctrlvec_t) = (s_{t,1}, s_{t,2}, \ldots, s_{t,|\mathbf{x}_t|}) = \mathbf{x}_{t+1} \\
    &\text{where } s_{t,i} = \inser_{\ctrl_{t,i}} \circ \delete_{x_{t,i}, \ctrl_{t,i}} \circ \remask_{x_{t,i}, \ctrl_{t,i}}(y_{t,i}), \quad \forall i \in [|\mathbf{x}_t|]
\end{split}
\end{align}

An algorithmic description of any-process generation with these operations is given in \Cref{alg:apmdm_sampling}, \Cref{appendix:algorithm}. And the stopping criterion can be flexibly defined, e.g. one can use generation of an EOS token (as in ARM) or convergence to a repeated sequence (loop occurs). Architecturally, implementing these capabilities requires no changes to the Transformer structure, only three additional logit dimensions are needed for producing control signals, i.e. three extra linear heads (details in \Cref{appendix:apmdm_architecture}).


\textbf{Pre-Training / Fine-Tuning / Data Availability}~~~ All three operations can be trained end-to-end from text corpora using self-supervised objectives, preserving MDM's scalability to large-scale training (details in \Cref{appendix:training}). The minimal architectural changes also enable direct fine-tuning from existing large diffusion models, possibly using supervised data where the underlying generation process is explicitly constructed (details in \Cref{appendix:training2}). This training flexibility separates our approach from alternatives like looped Transformers~\citep{dehghani2018universal,giannou2023looped}, which are expressive but notoriously difficult to train due to lack of intermediate supervision. 


\textbf{Design Considerations}~~~ The proposed three operations all revolve around the mask token $\mask$, leveraging existing MDM's strong unmasking capability while only adding modular extensions that are easier to pretrain or fine-tune than learning harder operations such as inversion of uniform noise~\citep{sahoo2025diffusion}. Moreover, while the definitions of $g$ and $\ctrlvec_t$ in (\ref{eq:apmdm_implementation}) suffice for achieving theoretical benefits detailed later, they are not necessary conditions; other designs are possible.

We note that the idea of remasking and editing have been individually explored in some prior/concurrent works~\citep{wang2025remasking,peng2025path,von2025generalized,havasi2025edit,wu2025dreamon}. However, there has not been a systematic study of guidance principles for yielding provable computational benefits and their practical implications. See discussions in \Cref{appendix:related}. 





% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=1\textwidth]{figs/tasks.pdf}
% 	\caption{This is a figure.} \label{fig:sudoku_example}
% \end{figure} 

\vspace{-5pt}
\section{The Power of Any-Process Generation}
\vspace{-5pt}

We now show how any-process generation circumvents various difficulties that current ARM and MDM encounter when handling tasks across different domains and complexities.

% , providing both theoretical and empirical support.

\vspace{-3pt} 
\subsection{Universally Efficient Computation} \label{sec:advantage_1}
\vspace{-3pt}

\begin{tcolorbox}[
    enhanced,
    colback=Gray!3!white,
    colframe=Gray,
    arc=4mm,
    boxrule=1pt,
    left=4pt,
    right=4pt,
    top=4pt,
    bottom=4pt,
    drop shadow={opacity=0.15, xshift=1.5pt, yshift=-1.5pt},
    fonttitle=\bfseries,
    coltitle=Gray,
    before skip=8pt,
    after skip=8pt
]
\textit{\textbf{Benefit 1}: Scalability to significantly harder problems through rewriting and backtracking.}
\end{tcolorbox} 

As discussed in \Cref{sec:inherently_hard}, inherently hard problems (e.g. many NP-hard tasks) typically do not admit sequential processes but require iterative ``search–verify–revise" loops, which hold across various domains: from theorem proving, solving code challenges to synthesis of complex structures in nature. The pathological way current generation paradigms let discardable tokens accumulate indefinitely creates scaling barriers, where space explodes and each step incurs ever-increasing computational cost (\Cref{thm:main_constrained}). We now demonstrate how AP-MDM resolves this:


\begin{theorem}[AP-MDM Simulation of PRAM, Informal] \label{thm:main_apmdm}
    For any PRAM program that runs in at most $T(n)$ parallel time, $P(n)$ processors and $S(n)$ memory usage, there exists an AP-MDM that matches PRAM output on any given input with $\mathcal{O}(S(n))$ context length and $\mathcal{O}(T(n))$ decoding steps. See formal statement in \Cref{thm:main_apmdm_formal}.
\end{theorem}\vspace{-3pt}

By comparison, standard MDM requires space scaling with the total work $\mathcal{O}(P(n) \cdot T(n))$ (\Cref{thm:main_mdm}), whereas AP-MDM requires only the actual space needed, achieving both optimal parallel time and space complexity (\Cref{thm:main_apmdm}). This implies AP-MDM not only retains the efficiency of parallelization, but also dramatically expands the range of solvable problems.  In particular, given polynomial context length, AP-MDM can solve problems in \textsf{PSPACE} rather than just \textsf{P}, which is an exponential improvement that makes many \textsf{NP}-hard problems solvable with test-time scaling. 



\textbf{Experiment: Sudoku Puzzles}~~ We conduct experiments on Sudoku puzzles, i.e. an \textsf{NP}-complete problem when generalized to $n^2 \times n^2$ grids, requiring both the capability of any-order generation, and the capability to rewrite. As illustrated in \Cref{fig:sudoku_example}, AP-MDM can use the $\remask$ (and standard $\unmask$) operations to choose the easiest position to fill first, and also erase failed branches and try alternative assignments, effectively scaling computates to solve harder instances. 


We follow the experimental setup from~\citep{kim2025train} but with a key difference: while the original work used 1.8M training puzzles, we use only 100 (moderately hard) instances for training AP-MDM. Despite this significantly reduced dataset, our approach achieves near-perfect accuracy (99.28\%) on most Sudoku puzzles, outperforming both AO-MDM and ARM that use substantially more samples and larger model sizes, as shown in \Cref{fig:sudoku_combined}. Any-process generation is sample more efficient because if the model is allowed to conduct more computes and more steps to solve the same problem, each step would become easier to learn. Orthogonally, we find from the training dynamics in \Cref{fig:sudoku_combined} that the model quickly learns to identify and fill the easiest positions (unmasking loss drops rapidly), while learning the order (which position to fill next) proves more challenging.



\begin{figure}[t!]
    % \vspace{-15pt}
    \centering
    % \vspace{-0.5cm}
    \begin{minipage}[t]{0.5\linewidth}
        \vspace{0pt}
        \centering
        \resizebox{\textwidth}{!}{%
        \setlength{\tabcolsep}{1mm}{%
        \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Method} & \textbf{\# Samples} & \textbf{\# Param} & \textbf{Accuracy} \\
        \midrule
        ARM {\small(w/o ordering)} & 1.8M & 42M & 9.73\% \\
        ARM {\small(with ordering)} & 1.8M & 42M & 87.18\% \\
        \midrule
        AO-MDM {\small(vanilla)} & 1.8M & 6M & 6.88\% \\
        AO-MDM {\tiny(Top-K probability)}  & 1.8M & 6M & 18.51\% \\
        AO-MDM {\tiny(Top-K prob. margin)} & 1.8M & 6M & 89.49\% \\
        \midrule
        AP-MDM & 100 & 1.2M & 99.28\% \\
        \bottomrule
        \end{tabular}}}\\[10pt]
        {\textbf{(a)} Comparison of accuracy on Sudoku.}
        \label{tab:sudoku_accuracy}
    \end{minipage}
    \hspace{10pt}
    \begin{minipage}[t]{0.42\linewidth}
        \vspace{0pt}
        \centering
        \includegraphics[width=\linewidth]{figs/sudoku_curve.pdf}
        \\[-6pt]
        {\textbf{(b)} Convergence of losses.}
        \label{fig:sudoku_process}
    \end{minipage}
    \vspace{-5pt}
    \caption{Experimental results on Sudoku puzzles. Results of ARM and AO-MDM are taken from ~\citet{kim2025train}. Losses are defined in \Cref{appendix:training}.\vspace{-12pt}}
    \label{fig:sudoku_combined}
\end{figure}



\vspace{-3pt}
\subsection{Structural Generation: Examples in Coding and Science} \label{sec:advantage_2}
\vspace{-3pt}


\begin{tcolorbox}[
    enhanced,
    colback=Gray!3!white,
    colframe=Gray,
    arc=4mm,
    boxrule=1pt,
    left=4pt,
    right=4pt,
    top=4pt,
    bottom=4pt,
    drop shadow={opacity=0.15, xshift=1.5pt, yshift=-1.5pt},
    fonttitle=\bfseries,
    coltitle=Gray,
    before skip=8pt,
    after skip=8pt
]
\textit{\textbf{Benefit 2}: Generating (or reasoning over) complex structured objects that evolve non-sequentially, common across domains beyond natural language (e.g. coding, science).}
\end{tcolorbox} 


When the evolving object involves some complex structures (e.g. trees, graphs, strings with constraints) that do not inherently build up linearly, forcing the generation into a sequential procedure can introduce unnecessary computational difficulties. Such scenarios are especially common across domains beyond natural language (e.g. coding, biology), which current LLMs struggle with.

\textbf{Example 1: Coding}~~~ Programs generally require satisfying global constraints like syntax and semantics at every intermediate state during development, since building each state upon the previous valid one is easier than jumping directly to the final solution. To illustrate this, consider the basic task of generating matched parentheses (the Dyck-$k$ language with $k$ types of parentheses), as illustrated in \Cref{fig:dyck_example}. Natural generation involves inserting parentheses anywhere without breaking balance constraints, while left-to-right generation requires global foresight and constant validity checking during generation, provably impossible for Transformers at scale. Particularly, we consider a variant called the two-sided Dyck-k (Definition in \Cref{appendix:proof_apmdm_simulation}):

\begin{theorem}[Generating Two-Sided Dyck-$k$, Informal] \label{thm:apmdm_simulation}
For any $k \ge 2$, there exists a stochastic AP-MDM whose generation distribution has support exactly equal to the two-sided Dyck-$k$ language, i.e., a string has positive generation probability if and only if it is in the language. Conversely, for any fixed ARM, there exists a length threshold beyond which the ARM cannot guarantee positive probability for all strings in two-sided Dyck-$k$. See formal statement in \Cref{thm:apmdm_simulation_formal}.
\end{theorem}
% \vspace{-3pt}


This result holds because generating Dyck language for arbitrary length is as hard as recognizing it, i.e., deciding if the current sequence has matched parentheses, which Transformer restricted in $\textsf{TC}^0$ expressivity cannot do. The vanilla MDM faces similar difficult as it has to predetermine the number of tokens between each matched parentheses. AP-MDM fundamentally circumvents this difficulty.



\textbf{Example 2: Science / Biology}~~~ Consider linear splicing~\citep{head1987formal,puaun1996splicing}, which is DNA recombination abstracted (and perhaps over-simplified) as cutting two strings and cross-pasting their halves, as illustrated in \Cref{fig:dna_example}. Iterating such rules from a finite seed set generates a splicing language, and any regular language with a marker added to the left side can be generated by such a system~\citep{head1998splicing,kari2012deciding}, while regular language has been proven impossible for constant-depth Transformers~\citep{liu2022transformers,li2024chain}. 



\textbf{Experiment: Graph Generation}~ To illustrate the power of generating complex structures by editing, we consider a graph generation task.  Given a directed graph and a prompt specifying source and target nodes $s$ and $t$, the model is required to generate a modified graph that disconnects $s$ and $t$ by removing the minimum number of edges. This is equivalent to finding the min-cut. Efficient algorithms for generation typically involve iterative editing: \textbf{1)} Use BFS to find a path from $s$ to $t$; \textbf{2)} Augment this path and modify the graph structure; \textbf{3)} Repeat until $s$ and $t$ are disconnected, then remove the min-cut edges. Any-process generation is naturally suited for such graph editing tasks, leveraging $\inser$/$\delete$/$\remask$ operations for adaptive structural and feature modifications and MDM's parallel capabilities, as illustrated in \Cref{fig:graph_example}. As shown in \Cref{fig:parity_graph}, our model achieves almost perfect accuracy for increasingly larger graphs. Meanwhile, when we train ARM to simulate the same process, they fail to perform well as graph size increases.

\vspace{-3pt}
\subsection{Learning and OOD Generalization} \label{sec:advantage_3}
\vspace{-3pt}




\begin{tcolorbox}[
    enhanced,
    colback=Gray!3!white,
    colframe=Gray,
    arc=4mm,
    boxrule=1pt,
    left=4pt,
    right=4pt,
    top=4pt,
    bottom=4pt,
    drop shadow={opacity=0.15, xshift=1.5pt, yshift=-1.5pt},
    fonttitle=\bfseries,
    coltitle=Gray,
    before skip=8pt,
    after skip=8pt
]
\textit{\textbf{Benefit 3}: Enabling the use of simpler algorithms to solve problems, thereby {improving sample efficiency and (out-of-distribution) generalization}.}

\end{tcolorbox} 


% The benefit could go beyond computational arguments discussed so far.

\textbf{Experiment: Parity}~~~ Given a binary sequence $\mathbf{x} \in \{0, 1\}^n$, parity involves determining if there are an even or odd number of 1s. This task is conceptually trivial but embarrassingly difficult for LLMs. Intuitively, the difficulty arises because ARM is forced to attend the entire input and learn a position-invariant target function, which is hard on training sequences with finite-length. With any-process generation, the model circumvents this difficulty by learning a simple elimination algorithm: examine the first two tokens, $\delete$ all 0s if the pair contains any 0 or $\delete$ the pair if both are 1s, then repeat until all are processed (\Cref{fig:parity_example}). The answer is true if any 1s remain. This mimics how humans solves the problem, a simple length-generalizable approach only possible with deletion. 
% See details in \Cref{appendix:exp_parity}.

As shown in \Cref{fig:parity_graph}, our model achieves 100\% accuracy on \textbf{any length} after training on only $n=2$ length sequences with a tiny model ($\sim$200 parameters), while ARM with orders of magnitude more parameters and samples fails to generalize beyond training lengths.


\begin{figure}[t!]
    % \vspace{-8pt}
    \centering
    \begin{minipage}[t]{0.48\linewidth}
        \vspace{0pt}
	\centering
        \resizebox{\textwidth}{!}{%
        \setlength{\tabcolsep}{2.5mm}{%
        \begin{tabular}{@{}c|c|c@{}}
        \toprule
        \textbf{Graph Size (\# Nodes)} & \textbf{ARM Acc.} & \textbf{AP-MDM Acc.} \\
        \midrule
        \textbf{4} & 90.32\% & 100\% \\
        \textbf{5} & 43.04\% & 100\% \\
        \textbf{6} & 0.30\% & 100\% \\
        \textbf{7} & N/A & 100\% \\
        \textbf{8} & N/A & 99.99\% \\
        \textbf{9} & N/A & 99.97\% \\
        \textbf{10} & N/A & 99.92\% \\
        \bottomrule
        \end{tabular}}}\\[8pt]
        {\textbf{(a)} Graph generation via editing.}
    \end{minipage}
    \hspace{10pt}
    \begin{minipage}[t]{0.41\linewidth}
        \vspace{0pt}
        \centering
        \includegraphics[width=\linewidth]{figs/parity_generalization.pdf}\\[-6pt]
        {\textbf{(b)} Length generalization on parity.}
    \end{minipage}
    \vspace{-8pt}
    \caption{Graph generation and parity task results.\vspace{-15pt}}
    \label{fig:parity_graph}
\end{figure} 


%\vspace{-3pt}
\subsection{Hardness of Being Simulated} \label{sec:advantage_4}
%\vspace{-3pt}

\begin{tcolorbox}[
    enhanced,
    colback=Gray!3!white,
    colframe=Gray,
    arc=4mm,
    boxrule=1pt,
    left=4pt,
    right=4pt,
    top=4pt,
    bottom=4pt,
    drop shadow={opacity=0.15, xshift=1.5pt, yshift=-1.5pt},
    fonttitle=\bfseries,
    coltitle=Gray,
    before skip=8pt,
    after skip=8pt
]
\textit{\textbf{Benefit 4}: If in the future we have access to data of underlying generation processes (e.g. revision history of code, articles, math proof drafts, molecular formation processes), any-process MDM is more suitable than ARM for practical training.}
\end{tcolorbox} 

Besides scalability to harder tasks (\Cref{sec:advantage_1}) and universality across domains (\Cref{sec:advantage_2}), a crucial question remains: suppose given access to datasets containing revision histories of the objects we wish to generate, would AP-MDM be the most appropriate model for such data and large-scale training? To answer this, we consider ARM as a competitor as it is Turing-complete, and equally expressive as AO-MDM (\Cref{thm:any_order}) when controlled for orthogonal factors.

We next show ARM is inherently unsuitable for training on editing datasets in two ways.  \textbf{Firstly}, unlike any-order generation (\Cref{thm:any_order}), AP-MDM's editing operations is hard to be simulated by ARM by explicitly specifying editing operations applied at each decoding step; particularly

\begin{theorem}[Hardness of Simulating AP-MDM, Informal] \label{thm:apmdm_simulation2}
There exists a constant-depth AP-MDM, such that no constant-depth ARM can simulate the generation process of that AP-MDM using a sequence of triplets, i.e., what operation to use ($\unmask$, $\remask$, $\inser$, $\delete$), where to apply the operation  (position) and what to write for the unmask operation (token), on any input $\mathbf x$, under some complexity hardness assumptions in \Cref{appendix:proof_apmdm_simulation2}. See formal statement in \Cref{thm:apmdm_simulation2_formal}.
\end{theorem}
%\vspace{-3pt}

Empirically, we show that representing our generation process using triplets described above for ARM simulation indeed becomes increasingly difficult to train as sequence length grows, as demonstrated in the graph generation task in \Cref{sec:advantage_2}.

\textbf{Secondly}, if we disregard the resource constraints from \Cref{sec:advantage_1} and \Cref{sec:advantage_2}, simulation becomes possible through additional intermediate steps, but this could require highly contrived trajectories that defeat the purpose of practical training, e.g. periodically summarize the current state, or using more than constant tokens to represent each application of an operation.

\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}

This paper provides formal analysis of generation processes and shows, provably and empirically, that moving beyond standard autoregression and current masked diffusion yields more powerful models. These results suggest concrete design principles for frontier LLMs, pointing to training and decoding schemes that scale to increasingly hard tasks and generalize across domains such as code and the sciences. See further contextualization with respect to related work in \Cref{appendix:related}. 




\bibliography{ref}
\bibliographystyle{iclr2026_conference}

\clearpage
\appendix




\tableofcontents
\clearpage


\section{Related Work} \label{appendix:related}

Masked diffusion models~\citep{hoogeboom2021argmax,austin2021structured,lou2024discrete,sahoo2024simple,shi2024simplified} extend continuous diffusion models~\citep{sohl2015deep,ho2020denoising,song2020score} to discrete data. Early work applied these models to specialized domains such as graph generation~\citep{vignac2022digress,sun2023difusco}, protein design~\citep{gruver2023protein}, and drug discovery~\citep{lee2025genmol}, where non-sequential generation provides natural advantages. The field has evolved with recent commercial-scale language models like Gemini Diffusion~\citep{deepmind2025gemini} and Mercury~\citep{inceptionlabs2025mercury}, which demonstrate competitive performance on language generation, reasoning, and coding tasks. This suggests that MDMs can serve as viable alternatives to the autoregressive models that currently dominate LLMs. Against this background, this paper investigates the fundamental computational differences between generation paradigms and explores whether more powerful generation methods exist.

Several works have explored extensions to standard MDM through mechanisms that enable rewriting and editing~\citep{vonrutte2025generalized,wang2025remasking,peng2025path,havasi2025edit,wu2025dreamon,kim2025any}, which relate to our any-process generation framework. \citet{wang2025remasking} introduces random remasking during inference, though this capability is not learned from data. \citet{lou2024discrete,vonrutte2025generalized,sahoo2025diffusion} propose adding uniform noise in the forward process rather than using masks, with models learning to revert them in the backward process, but this approach generally underperforms since modifying tokens directly appears more difficult than unmasking. \citet{peng2025path} introduces path planning to control generation, though the planner is not trained end-to-end with the base model. Current with ours: \citet{havasi2025edit} introduces edit operations to flow matching frameworks but faces similar limitations as uniform noise approaches; \citet{kim2025any} introduces to insert tokens at any 
position while \citet{wu2025dreamon} proposes expansion 
and delete, but these capabilities per se are 
insufficient for handling hard reasoning tasks as 
discussed in \Cref{sec:theory}. 


\section{Background: Masked Diffusion Model} \label{appendix:mdm}

We introduce the preliminaries of diffusion language models or masked diffusion models (MDM), following the notation established in \Cref{sec:preliminary}. Let $\bar{\Sigma} = \Sigma \cup \{\mask\}$ be the extended vocabulary where $\mask$ is the mask special token. Consider sequences $\mathbf{x}_t = (x_{t,1}, x_{t,2}, \ldots, x_{t,S}) \in \bar{\Sigma}^S$ indexed by time $t\in[T]$, where $S$ is the maximum context length, $T$ is the number of decoding steps, $\mathbf{x}_0 = \{\mask\}^S$ is the fully masked sequence and $\mathbf{x}_T \in \Sigma^S$ is the target clean sequence.

\paragraph{Forward Noising Process}
The forward noising process constructs training data by generating noisy versions $\mathbf{x}_t$ from clean sequences $\mathbf{x}_T$. Unlike the discrete inference steps in \Cref{sec:preliminary}, training uses continuous time $t \in [0, T]$ with larger $t$ denoting later denoising steps. MDM employs the \emph{absorbing mask kernel}~\citep{austin2021structured} where the signal ratio $\alpha_t = t/T$ represents the marginal probability that a token remains unmasked. Since $\alpha_t$ increases monotonically with $t$ (i.e., $\alpha_s < \alpha_t$ for $s < t$), later timesteps preserve more original tokens, consistent with our convention where $t=0$ is fully masked and $t=T$ is clean. At each position $i$, tokens either stay unchanged or become $\mask$, and once masked, they ``absorb'' into this state. The marginal distribution is:
\begin{align}
q(x_{t,i} \mid x_{T,i}) = \operatorname{Cat}(x_{t,i}; \alpha_t \mathbf{e}_{x_{T,i}} + (1-\alpha_t) \mathbf{e}_{\mask})
\label{eq:forward_marginal}
\end{align}
where $\mathbf{e}_v$ denotes the one-hot vector for token $v \in \bar{\Sigma}$. To obtain noised sequence $\mathbf{x}_t$ from $\mathbf{x}_T$, we compute the masking probability $1-\alpha_t$ and mask each position independently with this probability. 

\paragraph{Training Objective} 
The reverse process aims to recover $\mathbf{x}_T$ from $\mathbf{x}_0$. MDM parameterizes $p_\theta(\mathbf{x}_T \mid \mathbf{x}_t, t)$ to predict the clean data directly (but as mentioned in \Cref{sec:preliminary}, many recent large-scale MDMs omit explicit timestep embeddings). For the absorbing mask kernel, the true posterior $q(\mathbf{x}_s \mid \mathbf{x}_t, \mathbf{x}_T)$ for $s < t$ has an analytical form. For each position $i$:
\begin{align}
q(x_{s,i} \mid x_{t,i}, x_{T,i}) = \begin{cases}
\text{Cat}(x_{s,i}; \mathbf{e}_{x_{t,i}}), & \text{if } x_{t,i} \neq \mask \\
\text{Cat}\left(x_{s,i}; \frac{(1-\alpha_s)\mathbf{e}_{\mask} + (\alpha_t-\alpha_s)\mathbf{e}_{x_{T,i}}}{1-\alpha_s}\right), & \text{if } x_{t,i} = \mask
\end{cases}
\label{eq:true_posterior}
\end{align}
This means that, if position $i$ is not masked at time $t$, it remains unchanged at time $s$; if position $i$ is masked at time $t$, it transitions probabilistically between the original token and mask. The training objective is derived from the variational lower bound. For the absorbing mask kernel, it simplifies to:
\begin{align}
\mathcal{L}_{\text{CE}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}(0,T), \mathbf{x}_T \sim p_{\text{data}}, \mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_T)} \left[ -\frac{1}{|\{i: x_{t,i} = \mask\}|} \sum_{i: x_{t,i} = \mask} \log p_\theta(x_{T,i} \mid \mathbf{x}_t, t) \right]
\label{eq:ce_loss}
\end{align}
The loss is computed only on masked positions and averaged over the number of masked tokens, making this equivalent to conditional masked language modeling with proper normalization. Here, $t$ is sampled uniformly from the continuous interval $[0, T]$ during training, $\mathbf{x}_T$ is sampled from the data distribution, and $\mathbf{x}_t$ is obtained by applying the forward noising process.

\input{method.tex}

\input{exp.tex}

\input{pram.tex}

\input{tf_arc.tex}

\input{fasp.tex}   

\input{proof_sec3.tex}

\input{proof_sec4.tex}

\input{examples.tex}


\end{document}

