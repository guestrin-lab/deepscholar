\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:experiments_setup}

\begin{table*}[htbp!]
    \centering
    \caption{Retrieval performance comparison across baseline models. Results are reported at retrieval depths \(k = 5, 10, 15\), where \(k\) denotes the number of retrieved passages. The best score for each metric is shown in \textbf{bold}, and the second-best is \underline{underlined}.}
    \label{tab:ragas_baselines}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{1}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1 Score}} & \multicolumn{3}{c}{\textbf{Relevancy}}  \\
        & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\)\\
        \midrule
        General RAG  & 0.37 & 0.37 & 0.30 & 0.24 & 0.26 & 0.28 & 0.29 & 0.30 & 0.29 & \underline{0.40} & \underline{0.43} & \underline{0.47} \\
        Self-RAG     & 0.74 & 0.60 & 0.55 & 0.27 & 0.28 & 0.31 & 0.39 & 0.38 & 0.40 & 0.30 & 0.31 & 0.33 \\
        LightRAG     & \textbf{0.88} & \underline{0.85} & \underline{0.85} & 0.39 & 0.42 & 0.47 & 0.54 & 0.56 & 0.60 & 0.38 & 0.37 & 0.39\\
        GraphRAG & \textbf{0.88} & \textbf{0.89} & \textbf{0.87} & \underline{0.56} & \underline{0.55} & \underline{0.55} & \underline{0.67} & \underline{0.66} & \underline{0.66} & 0.17 & 0.16 & 0.17 \\
        RAPTOR       & 0.69 & 0.65 & 0.62 & 0.11 & 0.14 & 0.22 & 0.19 & 0.23 & 0.32 & 0.38 & 0.41 & 0.45\\
        \textbf{FinGEAR} & \underline{0.79} & 0.76 & 0.72 & \textbf{0.61} & \textbf{0.62} & \textbf{0.65} & \textbf{0.69} & \textbf{0.68} & \textbf{0.68} & \textbf{0.50} & \textbf{0.64} & \textbf{0.62}\\
        \bottomrule
    \end{tabular}
    }
\end{table*}


\paragraph{Baselines.}
We benchmark against \textit{General RAG}~\cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, \textit{Self-RAG}~\cite{selfrag2023}, \textit{LightRAG}~\cite{guo2024lightragsimplefastretrievalaugmented}, \textit{GraphRAG}~\cite{edge2024localglobalgraphrag}, and \textit{RAPTOR}~\cite{raptor}.
LightRAG is run in its “mix” mode (keyword+dense with local neighborhood expansion).
GraphRAG follows the official local–global community summary traversal (community construction and inference-time community propagation).
Unless noted, baselines use default settings with \texttt{text-embedding-ada-002} for dense similarity.
(Full baseline configurations are summarized in Appendix~\ref{appendix:baseline_configurations}.)

\paragraph{Reader model.}
All results use \texttt{GPT-4o-mini} as a fixed reader with default settings; only the retriever varies across systems. It was chosen for being a strong, cost- and latency-efficient reader at the time of this study. Crucially, the reader choice is independent of the retrieval pipeline and algorithmic design as our conclusions target retrieval quality.

\paragraph{Model and index settings (promoted from appendices).}
\textbf{BM25:} Lucene; \(k_1{=}1.5\), \(b{=}0.75\); tokenizer: PyStemmer (eng). \\
\textbf{Dense embeddings:} \texttt{BAAI/bge-base-en-v1.5} (\(d{=}768\)); fine-tuned FinRAD \& FinQA variants with MultipleNegatives + Matryoshka; learning rate \(2{\times}10^{-5}\); batch size 32. Empirical gains from the embedding fine-tuning are summarized in Table~\ref{tab:embedding_finetuning}.\\
\textbf{Cross-encoder reranker:} \texttt{BAAI/bge-reranker- large} (default inference settings). \\
\textbf{UMAP+GMM:} UMAP dim{=}10 (cosine); GMM max components{=}50; threshold{=}0.1; max tree depth{=}2 (fan-out rationale from Item sizes). \\
\textbf{Chunking:} \(\sim\)2{,}000 tokens with 100-token overlap.


\paragraph{Retrieval Pipeline.}
We use \textbf{BM25s}~\cite{bm25s} for sparse matching and \textbf{FinLang} domain-specific sentence embeddings~\cite{finlang2025finance} for dense similarity. The embeddings are fine-tuned on FinQA (question–passage) and FinRAD (lexicon–sentence) objectives (Appendix~\ref{appendix:embedding_finetuning}). Candidate financial terms are extracted with spaCy’s \texttt{PhraseMatcher} to drive FLAM’s term clustering and Item weighting. Unless noted otherwise, all experiments use this BM25s+FinLang configuration; hierarchical indexing and reranking are detailed in Section~\ref{sec:methodology}.

\paragraph{Retrieval Settings.}
Retrieval performance is reported at depths \(k = 5, 10, 15\), reflecting trade-offs between retrieval quantity and contextual precision. All models are evaluated using identical traversal logic and input constraints to ensure comparability. Additional configuration details, including chunk size, overlap settings, and node budget, are reported in Appendix~\ref{appendix:token_statistics}, Table~\ref{tab:token_statistics}.

\paragraph{Ablation Settings.} 
To assess the contribution of FinGEAR’s core components, we conduct a series of ablation studies. We begin by disabling individual modules, the \textit{Summary Tree}, \textit{Question Tree}, and \textit{FLAM}, to measure their impact on retrieval F1 and Relevancy. We then perform multi-component ablations by jointly removing pairs of modules to analyze interaction effects between structural and lexical guidance (Appendix~\ref{appendix:multi_ablation}).
In addition, we evaluate alternative lexicon weighting strategies within the FLAM module, including \textit{Relative Frequency}, \textit{Exponential Scaling}, and \textit{Softmax Weighting}. These modules affect Item prioritization during retrieval. Ablation results are presented in Section~\ref{sec:results}.

\subsection{Results}
\label{sec:results}

\begin{table*}[htbp!]
    \centering
    \caption{Ablation study on the impact of removing individual components from FinGEAR. 
    We evaluate the effect of disabling the Summary Tree, Question Tree, FLAM modules and Reranker. 
    Results are reported at retrieval depths \(k = 5, 10, 15\) and averaged across metrics. 
    \textbf{Bold} indicates the best-performing configuration.}
    \label{tab:ablation_single_component}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{1}{*}{\textbf{Ablation Setting}} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1 Score}} & \multicolumn{3}{c}{\textbf{Relevancy}}\\
        & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) \\
        \midrule
        \textbf{Full FinGEAR}     & \textbf{0.79} & \textbf{0.76} & \textbf{0.72} & \textbf{0.61} & \textbf{0.62} & \textbf{0.65} & \textbf{0.69} & \textbf{0.68} & \textbf{0.68} & \textbf{0.50} & \textbf{0.64} & \textbf{0.62} \\
        No Summary Tree           & 0.41 & 0.61 & 0.58 & 0.33 & 0.36 & 0.40 & 0.37 & 0.46 & 0.47 & 0.29 & 0.57 & 0.63 \\
        No Question Tree          & 0.43 & 0.62 & 0.57 & 0.35 & 0.37 & 0.40 & 0.39 & 0.47 & 0.47 & 0.29 & 0.56 & 0.63 \\
        No FLAM Module            & 0.42 & 0.61 & 0.58 & 0.34 & 0.35 & 0.42 & 0.38 & 0.44 & 0.49 & 0.29 & 0.58 & 0.63 \\
        No Reranker            & 0.51 & 0.44 & 0.36 & 0.37 & 0.30 & 0.31 & 0.43 & 0.36 & 0.34 & 0.45 & 0.55 & 0.58 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

We evaluate FinGEAR’s performance on four core retrieval metrics—\textit{precision}, \textit{recall}, \textit{F1 score}, and \textit{relevancy}—alongside ablation studies and lexicon-weighting variants. While FinGEAR is retrieval-first, we also report \textit{answer accuracy} to assess how improved retrieval supports downstream tasks.

\vspace{-0.5em}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{System} & \(k=5\) & \(k=10\) & \(k=15\) \\
\midrule
General RAG & 29.8\% & 30.3\% & 30.5\% \\
Self-RAG & 28.7\% & 29.8\% & 27.4\% \\
LightRAG & 35.7\% & 58.8\% & 36.5\% \\
GraphRAG & 28.4\% & 29.1\% & 29.4\% \\ %Global 26.7\%
RAPTOR & 34.0\% & 20.9\% & 37.3\% \\
\textbf{FinGEAR (Ours)} & \textbf{49.1\%} & \textbf{49.7\%} & \textbf{50.0\%} \\
\bottomrule
\end{tabular}
} 
\caption{Final answer accuracy with GPT-4o-mini as reader. Accuracy is reported for each retrieval depth.}
\label{tab:answer_accuracy}
\end{table}
\vspace{1em}

\subsubsection{Retrieval Performance Across Baselines}

Table~\ref{tab:ragas_baselines} compares FinGEAR with five baselines: General RAG, Self-RAG, LightRAG, GraphRAG, and RAPTOR. Across all retrieval depths and evaluation metrics, including precision, recall, F1 score, and relevancy, FinGEAR consistently outperforms the baselines, demonstrating its effectiveness for structure-aware retrieval over disclosures. 
% While LightRAG reports the highest isolated \textit{precision} (e.g., 0.88 at \(k=5\)), this comes at the expense of recall and F1 score, reflecting a narrower retrieval scope. In contrast, FinGEAR achieves strong precision alongside substantially higher recall and F1, offering more balanced and effective retrieval across complex, full-length financial filings.\footnote{In LightRAG, each retrieval combines entities, relations, and vector text, leading to precision inflation but requiring 15$\times$ the runtime of FinGEAR per document.}

\textit{Recall.} FinGEAR achieves the highest recall at every depth, reaching 0.65 at \(k{=}15\), ahead of LightRAG (0.47), Self-RAG (0.31), and General RAG (0.28). RAPTOR lags behind, highlighting limited adaptability to the standardized section hierarchy and terminology of 10-Ks. Strong recall indicates that FinGEAR reliably recovers the reasoning spans dispersed across long documents.

\textit{F1.} FinGEAR also leads on F1 at all depths, reflecting a balanced trade-off between precision and coverage. The combination of FLAM-guided Item selection and dual-tree traversal yields candidate sets that are both relevant and diverse—important in high-stakes financial analysis.

\textit{Relevancy.} FinGEAR delivers the most semantically aligned results at all depths, peaking at 0.64. Retrieved passages are not only topically related but also faithful to the specific intent of the query. By contrast, General RAG and RAPTOR return more diffuse or off-target content.

\textit{LightRAG.} LightRAG attains the highest precision in isolation (e.g., 0.88 at \(k{=}5\)), but this comes with reduced recall and F1, reflecting a narrower candidate set after entity/relation hops. FinGEAR maintains competitive precision while substantially improving recall and F1, yielding a more balanced profile for long, heterogeneous filings.\footnote{In LightRAG, each retrieval combines entities, relations, and vector text, which can inflate precision but requires approximately fifteen times the runtime of FinGEAR per document.}

\textit{GraphRAG.} GraphRAG shows high precision and strong recall (0.56/0.55/0.55), yielding second-best F1 (0.67/0.66/0.66). However, its relevancy is lowest (0.17/0.16/0.17), and its final answer accuracy is also low (Table~\ref{tab:answer_accuracy}: 28.4–29.4\%). In our setting, FinGEAR achieves higher F1 and much higher relevancy at all depths, aligning with its stronger downstream accuracy.

\begin{table*}[htbp!]
    \centering
    \caption{
    Ablation study of lexicon weighting strategies in FLAM. 
    We compare three methods for computing weights from financial terminology: Relative Frequency, Logarithmic Weighting, and Softmax Weighting. 
    Scores are reported at retrieval depths \(k = 5, 10, 15\) and averaged across all metrics. 
    \textbf{Bold} indicates the best-performing configuration.
    }
    \label{tab:flam_weighting}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{1}{*}{\textbf{Weighting Strategy}} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1 Score}} & \multicolumn{3}{c}{\textbf{Relevancy}}\\
        & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) \\
        \midrule
        \textbf{Relative Frequency} & \textbf{0.79} & \textbf{0.76} & \textbf{0.72} & \textbf{0.61} & \textbf{0.62} & \textbf{0.65} & \textbf{0.69} & \textbf{0.68} & \textbf{0.68} & \textbf{0.50} & \textbf{0.64} & 0.62  \\
        Logarithmic Weighting         & 0.70 & 0.66 & 0.63 & 0.47 & 0.45 & 0.45 & 0.56 & 0.53 & 0.52 & \textbf{0.50} & 0.60 & 0.60 \\
        Softmax Weighting           & 0.70 & 0.68 & 0.66 & 0.47 & 0.44 & 0.44 & 0.56 & 0.53 & 0.53 & 0.48 & 0.59 & \textbf{0.63}\\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\subsubsection{Evaluating QA as a Downstream Task: Answer Accuracy}
To assess downstream utility, we measure final answer accuracy with the fixed reader introduced above. FinGEAR achieves the strongest accuracy at \(k{=}5\) and \(k{=}15\) (49.1\% and 50.0\%), and remains competitive at \(k{=}10\) (49.7\%), where LightRAG shows a one-off spike (58.8\%) but degrades at the other depths (35.7\% at \(k{=}5\), 36.5\% at \(k{=}15\)). General RAG and GraphRAG hover near 30\% across depths, and RAPTOR is unstable, dropping from 34.0\% at \(k{=}5\) to 20.9\% at \(k{=}10\), then rebounding to 37.3\% at \(k{=}15\). Overall, FinGEAR’s accuracy is consistently high and increases with \(k\), mirroring its recall/F1 trends and indicating that it surfaces numerically faithful, context-grounded evidence for fact-sensitive financial QA.

\subsubsection{Analysis of Retrieval Depths}

FinGEAR maintains strong retrieval quality across all evaluated depths. At \(k=5\), it achieves a high precision of 0.79 and F1 score of 0.69, indicating strong early-stage retrieval performance. As retrieval depth increases, recall steadily improves, reaching 0.65 at \(k=15\), while precision and F1 remain consistently high. This stability reflects FinGEAR’s ability to scale retrieval scope without sacrificing semantic accuracy or contextual fit.

Relevancy scores follow a similar pattern—peaking at 0.64 at \(k=10\) and holding at 0.62 at \(k=15\), showing that even as the system retrieves more passages, it continues to surface content that is contextually faithful to the query. These trends suggest that FinGEAR balances depth and alignment effectively, making it suitable for long-context applications where both coverage and interpretability are critical.

In contrast, baselines exhibit more pronounced trade-offs. LightRAG shows strong precision at shallow depths but plateaus in recall and F1. Self-RAG and RAPTOR struggle to maintain performance as retrieval depth increases, reflecting limited adaptability to structured disclosures. FinGEAR’s consistent gains across all depths demonstrate its robustness and domain alignment in retrieval from complex financial documents.

\subsubsection{Ablation Study}
\label{sec:ablation_study}

We now report the outcomes of the ablations defined in Section~\ref{sec:experiments_setup}. We conduct single-component ablations: (1) removing the \textit{Summary Tree}, which supports hierarchical sparse abstraction; (2) removing the \textit{Question Tree}, which enables dense query alignment; and (3) disabling the \textit{FLAM module}, which provides lexicon-driven domain targeting. As shown in Table~\ref{tab:ablation_single_component}, all components are critical to retrieval quality—particularly at lower depths (\(k = 5\)), where removing the Summary Tree alone reduces F1 from 0.69 to 0.37. These results confirm that FinGEAR’s performance arises from the interplay of structural, semantic, and domain-specific signals, rather than any isolated component.

We further assess the impact of lexicon weighting strategies within FLAM. Table~\ref{tab:flam_weighting} compares three approaches: \textit{Relative Frequency}, \textit{Logarithmic Weighting}, and \textit{Softmax Weighting}. Relative Frequency consistently delivers the strongest performance, with an F1 of 0.69 and relevancy scores of 0.50, 0.64, and 0.62 at \(k=5\), \(k=10\), and \(k=15\), respectively. Logarithmic Weighting downweights frequent but meaningful terms, reducing recall to 0.45 at \(k=15\), while Softmax Weighting slightly improves relevancy at \(k=15\) (0.63) but lowers F1 by over-concentrating on dominant keywords. These results affirm Relative Frequency as the most robust and interpretable default strategy within FLAM.

Multi-component ablations in Appendix~\ref{appendix:multi_ablation} (Table~\ref{tab:multi_ablation_results}) reveal compounded degradation when two modules are removed. In particular, eliminating both FLAM and the Question Tree results in the steepest drops in F1 and relevancy, underscoring their complementary roles in domain grounding and query sensitivity. Removing FLAM also increases variance across depths, highlighting its stabilizing role and the complementarity of the three modules. These findings reinforce that FinGEAR’s effectiveness and stability stem from the coordinated integration of all three core modules.

\paragraph{Question-type breakdown.}
\label{sec:dataset_ablation}
We also report performance by \emph{Numerical vs.\ Categorical} and \emph{Simple vs.\ Complex (multi-hop)} questions; full results are in Appendix~\ref{appendix:question_type_ablation} (Table~\ref{tab:dataset_ablation_results}). At \(k{=}10\), FinGEAR attains F1 \(=\) 0.68 (Numerical) vs.\ 0.81 (Categorical), and 0.70 (Simple) vs.\ 0.67 (Complex). 

Two patterns are observed. (1) \textbf{Categorical} questions achieve higher F1, likely because answers are stated directly (e.g., presence/absence) and require less aggregation; by contrast, \textit{relevancy} is higher on \textbf{Numerical} queries (0.64 vs.\ 0.48 at \(k{=}10\)), reflecting tighter alignment around figures and units. (2) \textbf{Complex} (multi-hop) questions trail \textbf{Simple} ones in F1 yet show comparable or higher \textit{relevancy} at larger \(k\) (e.g., 0.65 at \(k{=}10\), 0.67 at \(k{=}15\)), indicating that dual-tree traversal helps surface distributed evidence; the remaining F1 gap is consistent with broader evidence requirements.

Overall, this breakdown shows that FinQA contains diverse question types, such as discrete (categorical) and quantitative (numerical), single-step and multi-hop, providing a comprehensive testbed for retrieval. It also indicates that FinGEAR’s gains are not confined to a particular question class; improvements hold across categories, suggesting the method is broadly applicable.