\section{Potential Risks}
\label{appendix:potential_risks}

FinGEAR restricts retrieval strictly to 10-K filings, thereby reducing the risk of hallucinated content in high-stakes financial applications. Nonetheless, two primary risks remain.

First, the system may occasionally fail to retrieve the most relevant disclosure due to semantic drift, ambiguous query interpretation, or traversal limitations. While this challenge is not unique to automated systems—human readers are similarly affected—it underscores the importance of improving query-to-structure alignment.

Second, although FinGEAR effectively retrieves relevant context, it does not constrain the generation behavior of downstream language models. As a result, models may still produce incorrect or speculative reasoning based on retrieved evidence, potentially leading to confirmation bias if users do not critically assess generated answers.

FinGEAR is intended to serve as a retrieval-first component within broader financial QA workflows. Its outputs should be interpreted in conjunction with the retrieved content, and human verification is recommended to ensure decision-making accuracy in professional settings.

\section{Implementation Details}
\label{appendix:implementation_details}

\paragraph{Baselines.} All baseline systems are based on publicly available implementations. General RAG\footnote{\url{https://python.langchain.com/docs/tutorials/rag/}} and Self-RAG\footnote{\url{https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/}} are based on the LangChain implementations. RAPTOR\footnote{\url{https://github.com/parthsarthi03/raptor/tree/master}} and LightRAG\footnote{\url{https://github.com/HKUDS/LightRAG}} use official GitHub repositories.

\paragraph{API Costs.} FinGEAR uses external APIs for LLM access, while financial embeddings are computed locally without cost. Using \texttt{gpt-4o-mini}, the estimated one-time cost to construct FinGEAR’s index for a 127-page 10-K filing (76,114 words) is approximately \$0.11: \$0.057 for Summary Tree construction and \$0.029 for Question Tree construction. Answering a FinQA query costs approximately \$0.00048 (Top-5 retrieval), \$0.00076 (Top-10), and \$0.00100 (Top-15).

\section{Baseline Configurations}
\label{appendix:baseline_configurations}

All baseline models are evaluated under consistent retrieval settings with Top-$k \in \{5,10,15\}$. Each baseline retrieves from the same pool of recovered full 10-K filings (Section~\ref{sec:dataset_metrics}) or uniformly segmented chunks, without curated summaries. All systems—including FinGEAR and baselines—use the same reader model (\texttt{gpt-4o-mini}) for answer generation to isolate retrieval effects.

\paragraph{General RAG.}
Vanilla BM25 + dense retrieval (default LangChain recipe).\footnote{\url{https://python.langchain.com/docs/tutorials/rag/}} Dense encoder: \texttt{text-embedding-ada-002} unless noted.

\paragraph{Self-RAG.}
Self-evaluation guided retrieval using the LangGraph implementation.\footnote{\url{https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/}} Default settings for critique and selection.

\paragraph{RAPTOR.}
Hierarchical summarization and tree retrieval using the official repository.\footnote{\url{https://github.com/parthsarthi03/raptor/tree/master}} Default depth policy; flat chunking inputs; no domain priors.

\paragraph{LightRAG.}
Official implementation in “mix” mode (keyword + dense with lightweight neighborhood expansion).\footnote{\url{https://github.com/HKUDS/LightRAG}} Default neighborhood expansion; entity/relation indexing as released.

\paragraph{GraphRAG.} We use the official local--global community summary traversal with default community construction and inference-time community propagation settings.

\section{Embedding Fine-Tuning Details}
\label{appendix:embedding_finetuning}

FinGEAR fine-tunes sentence embeddings for two key financial retrieval tasks using the base \texttt{finance-embeddings-investopedia} model from FinLang~\cite{finlang2025finance}:

\noindent\textbf{Lexicon-based Fine-Tuning:} Trained on FinRAD~\cite{ghosh2021finrad} (public) by pairing financial lexicons with disclosure sentences that contain or exclude them. This optimizes embedding alignment for lexicon-based indexing.\\
\textbf{Hierarchical Tree Fine-Tuning:} Trained on FinQA~\cite{chen2021finqa} using question-passage pairs to improve semantic clustering and navigation within the Summary and Question Trees.

\paragraph{Evaluation Metrics.} We use cosine-based Pearson and Spearman correlations for \textit{FinRAD}, measuring embedding similarity between financial terms and disclosure sentences. For \textit{FinQA}, we apply NDCG@10 to assess ranked retrieval performance per query.

\paragraph{Loss Functions.} We use Matryoshka Representation Learning~\cite{henderson2023matryoshka} (dim=768) and \texttt{MultipleNegativesRankingLoss} to support semantically rich representations across both clustering and ranking stages.
\paragraph{Hardware.} All embedding are fine-tuned (FinRAD and FinQA) on a machine with a NVIDIA A100-SXM4-80GB GPU. Training details can be found in Section~\ref{appendix:runtime}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset} & \textbf{Metric} & \textbf{Baseline} & \textbf{Fine-Tuned} \\
\midrule
\multirow{2}{*}{FinRAD} 
& Pearson Cosine & 0.1082 & \textbf{0.4063} \\
& Spearman Cosine & 0.0904 & \textbf{0.3655} \\
\midrule
FinQA & NDCG@10 (Cosine) & 0.0282 & \textbf{0.2902} \\
\bottomrule
\end{tabular}
}
\caption{Embedding evaluation before and after fine-tuning on FinRAD and FinQA. Metrics reflect task-specific objectives: similarity alignment for FinRAD, and ranked retrieval quality for FinQA.}
\label{tab:embedding_finetuning}
\end{table}

% FinQa Fine Tuning
% {'train_runtime': 1971.5281, 'train_samples_per_second': 158.532, 'train_steps_per_second': 0.304, 'train_loss': 4.5503160778681435, 'epoch': 46.16}
% 6251 rows (train), 1147 rows (test)

% Finrad Fine Tuning
% {'train_runtime': 3607.1604, 'train_samples_per_second': 239.801, 'train_steps_per_second': 0.457, 'train_loss': 26.40469178170869, 'epoch': 49.98}
% 17300 rows (train), 5190 rows (test) <- uniformly sampled from each document

% Preprocessing Time (10 documents)
% +-----------------+-------+--------+--------+
% |      Method     |  min  |  mean  |  max   |
% +-----------------+-------+--------+--------+
% | Keyword Mapping | 53.49 | 248.87 | 434.56 |
% +-----------------+-------+--------+--------+

% +--------------+-------------+-----------------------------+
% |    Method    | per_company | total_timing (10 companies) |
% +--------------+-------------+-----------------------------+
% | Keyword Tree |     7.57    |            75.68            |
% +--------------+-------------+-----------------------------+

% +---------------+--------+--------+--------+
% |     Method    |  min   |  mean  |  max   |
% +---------------+--------+--------+--------+
% |  Summary Tree | 221.08 | 340.98 | 440.16 |
% | Question Tree | 22.15  | 67.26  | 162.48 |
% |     Total     | 243.23 | 408.24 | 602.64 |
% +---------------+--------+--------+--------+

\section{FLAM Clustering Configuration}
\label{appendix:clustering_config}

The FLAM (Financial Lexicon-Aware Mapping) module utilizes a clustering pipeline to group semantically related financial terms, facilitating more robust and lexicon-guided retrieval. 
The configuration follows three key stages:
(1) Embedding Encoding is performed using sentence embeddings that have been fine-tuned on financial QA datasets to capture domain-specific semantic similarity;
(2) Dimensionality Reduction is applied via UMAP, reducing embeddings to a 10-dimensional space using cosine similarity as the distance metric to preserve neighborhood structure;
(3) Clustering is conducted using Gaussian Mixture Models (GMM) with a maximum of 50 clusters and a convergence threshold set to 0.1, enabling soft assignments of terms into semantically coherent groups.

\section{Token-Level Statistics}
\label{appendix:token_statistics}

To balance retrieval efficiency with financial context fidelity, FinGEAR applies a fixed-length chunking strategy prior to indexing. Each chunk contains approximately 2,000 tokens with a 100-token overlap to maintain contextual continuity, which is same across all baseline. Tree-based indexing is then applied to support structure-aware retrieval. 

“Tokens per summary node” refers to the average length of high-level disclosure summaries used in the Summary Tree, while “tokens per page node” refers to the full-length segments prior to hierarchical expansion. For latency and deployment details, refer to Appendix~\ref{appendix:deployment}.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Statistic} & \textbf{Min} & \textbf{Mean} & \textbf{Max} \\
\midrule
Tree depth (max) & 2 & 3.14 & 4 \\
Tree depth (mean) & 1.22 & 1.43 & 1.80 \\
Total node count & 27 & 254.86 & 1322 \\
Internal node count & 0 & 17.84 & 85 \\
Leaf node count & 27 & 237.02 & 1237 \\
Page count & 84 & 168.89 & 555 \\
Document count & 184 & 1342.77 & 7078 \\
Tokens per document & 291.98 & 331.03 & 382.23 \\
Tokens per summary node & 634.72 & 657.78 & 687.46 \\
Tokens per query & 16.16 & 17.27 & 18.60 \\
Tokens per page node & 598.71 & 2129.56 & 7021.38 \\
\bottomrule
\end{tabular}
}
\caption{Token-level and structural statistics across the FinGEAR indexing pipeline.}
\label{tab:token_statistics}
\end{table}

Under this configuration, the total retrieval volume scales linearly with depth \(k\) as:
\[
\text{Total tokens retrieved} \approx 2000 \times k.
\]

% +---------------------+--------+---------+---------+
% |         Stat        |  min   |   mean  |   max   |
% +---------------------+--------+---------+---------+
% |      depth_max      |   2    |   3.14  |    4    |
% |      depth_mean     |  1.22  |   1.43  |   1.8   |
% |      node_count     |   27   |  254.86 |   1322  |
% | node_internal_count |   0    |  17.84  |    85   |
% |   node_leaf_count   |   27   |  237.02 |   1237  |
% |      page_count     |   84   |  168.89 |   555   |
% |    document_count   |  184   | 1342.77 |   7078  |
% |  token_per_document | 291.98 |  331.03 |  382.23 |
% |  token_per_summary  | 634.72 |  657.78 |  687.46 |
% |  token_per_question | 16.16  |  17.27  |   18.6  |
% |    token_per_page   | 598.71 | 2129.56 | 7021.38 |
% +---------------------+--------+---------+---------+


\section{Practical Deployment Details}
\label{appendix:deployment}

\subsection{System Integration}

FinGEAR is designed as a modular retrieval layer that integrates into RAG pipelines with minimal configuration effort. It exposes a unified retriever interface supporting both sparse and dense retrieval strategies, along with hybrid reranking. These strategies are fully configurable by depth k, retrieval type, and domain specificity.

The system supports hierarchical retrieval via two independent traversal mechanisms: the Summary Tree, which provides disclosure-level navigation based on financial document structure, and the Question Tree, which guides retrieval using query semantics through sub-question expansion. This separation enables controlled traversal of financial disclosures at varying levels of abstraction.

FinGEAR is compatible with vector store backends such as FAISS for indexing and searching over domain-tuned embeddings, and its reranking modules are cross-encoder based for contextual precision. While optimized for structured regulatory disclosures like 10-K filings, its traversal and retrieval logic can be extended to semi-structured corpora—such as earnings calls or ESG reports—by replacing FLAM with unsupervised segmentation or alternative indexing methods. The system optionally integrates with FAISS for dense indexing and LangChain for pipeline orchestration, though these components are modular and replaceable.

\begin{table*}[h]
\centering
\small
\begin{tabular}{p{3.2cm} p{4.0cm} p{4.0cm} p{3.6cm}}
\toprule
\textbf{Query} & \textbf{Successful Retrieval} & \textbf{Failure Case} & \textbf{Interpretation} \\
\midrule
What is the debt-to-equity ratio in 2022? & “Consolidated Balance Sheets” showing total debt and equity. & “Liquidity and Capital Resources” discussing debt repayment but omitting equity. & Partial match—missed denominator component due to narrative emphasis. \\
\addlinespace
What was the YoY revenue growth for Q4? & “Results of Operations” section with quarterly revenue breakdown. & “Management Overview” with general trends but no figures. & High-level commentary lacks granularity—numeric context lost. \\
\addlinespace
How much did R\&D expenses increase year-over-year? & “Operating Expenses” table with line items for R\&D across periods. & “Business Overview” noting investment in innovation but not amounts. & Lexical match on “R\&D” misaligned with quantitative query intent. \\
\addlinespace
What percentage of revenue came from the US? & “Segment Reporting” with revenue by geography. & “Market Strategy” outlining international expansion plans. & Structural mismatch: retrieved future projections instead of current data. \\
\addlinespace
What is the gross margin for 2021? & “Income Statement” showing gross profit and revenue. & “Risk Factors” discussing cost pressures without figures. & Terminology overlap led to non-numerical section—filtered out post-reranking. \\
\bottomrule
\end{tabular}
\caption{Examples of FinGEAR’s retrieval outcomes for representative FinQA-style queries. Failure cases illustrate interpretable mismatches from lexical ambiguity, structural misalignment, or contextual drift—often mitigated by reranking.}
\label{tab:error_analysis}
\end{table*}

\begin{table*}[htbp!]
    \centering
    \caption{Multi-component ablation results. Disabling pairs of modules leads to compounded performance degradation, underscoring the synergistic design of FinGEAR and the importance of coordinated retrieval.}
    \label{tab:multi_ablation_results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{1}{*}{\textbf{Ablation Setting}} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1 Score}} & \multicolumn{3}{c}{\textbf{Relevancy}}\\
        & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) \\
        \midrule
        \textbf{Full FinGEAR}     & \textbf{0.79} & \textbf{0.76} & \textbf{0.72} & \textbf{0.61} & \textbf{0.62} & \textbf{0.65} & \textbf{0.69} & \textbf{0.68} & \textbf{0.68} & \textbf{0.50} & \textbf{0.64} & \textbf{0.62} \\
        No FLAM + No Summary Tree           & 0.48 & 0.59 & 0.51 & 0.40 & 0.30 & 0.33 & 0.44 & 0.40 & 0.40 & 0.25 & 0.34 & 0.34 \\
        No FLAM + No Question Tree          & 0.48 & 0.58 & 0.52 & 0.40 & 0.31 & 0.36 & 0.44 & 0.40 & 0.43 & 0.26 & 0.34 & 0.34 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}


\subsection{Latency Statistics}

\begin{table}[h]
\centering
\small
\begin{tabular}{lc}
\toprule
Metric & Value (seconds) \\
\midrule
Retrieval Latency ($k=5$) & 12.05 \\
Retrieval Latency ($k=10$) & 18.28 \\
Retrieval Latency ($k=15$) & 22.42 \\
\midrule
Average Retrieval Latency & 17.58\textsuperscript{(a)} \\
\bottomrule
\end{tabular}
\caption{Latency statistics for FinGEAR deployment.\newline
\textsuperscript{(a)} Includes tree traversal, FLAM scoring, and reranking averaged across depths.}
\label{tab:latency_stats}
\end{table}

All evaluations were conducted on a MacBook Pro with an M3 Max chip and 64GB of RAM. FinGEAR demonstrates practical deployability on standard enterprise hardware, maintaining tractable inference latency suitable for real-world financial workflows.


\section{Error Analysis Examples}
\label{appendix:error_examples}

Table~\ref{tab:error_analysis} presents paired examples of successful and imperfect retrievals from FinQA-aligned questions, illustrating how FinGEAR handles both precise matching and interpretable failure cases. For example, in response to the question \textit{“What is the company’s debt-to-equity ratio?”}, FinGEAR successfully surfaces the “Balance Sheet” section disclosing the necessary financial figures. In contrast, a failure case retrieves a “Capital Management” section that is conceptually related but lacks numeric content—highlighting a mismatch between strategic language and quantitative requirements.

Similarly, for \textit{“What litigation is the company currently facing?”}, the system initially retrieves generic “Risk Factors” content due to shared legal phrasing, but the reranker recovers the correct “Legal Proceedings” section, demonstrating partial recovery. In other cases, such as the misretrieval of a “Commodity Risk” section when asked about foreign exchange exposure, errors stem from keyword ambiguity across structurally distinct topics.

These examples underscore the role of FinGEAR’s hybrid architecture in surfacing relevant content while also providing interpretable signals for tracing failure modes. The system’s transparency allows users and developers to adjust lexicon weights, reranking sensitivity, or structural traversal logic—enabling more robust deployment and auditability in financial applications.

\section{Multi-Component Ablation Study}
\label{appendix:multi_ablation}

To evaluate the interdependence of FinGEAR’s core modules, we conduct a multi-component ablation study by disabling pairs among \textit{FLAM}, the \textit{Summary Tree}, and the \textit{Question Tree}. This setup allows us to assess how retrieval quality degrades under partial configurations and whether FinGEAR’s performance stems from isolated components or coordinated design.

\paragraph{Findings.}  
Disabling multiple modules results in substantial, compounded performance drops across all retrieval metrics, consistently observed at depths \(k = 5, 10, 15\). The largest declines in F1 score and relevancy occur when both \textit{FLAM} and the \textit{Question Tree} are removed, revealing their complementary roles in guiding retrieval semantically and contextually. When both \textit{FLAM} and the \textit{Summary Tree} are ablated, precision and recall drop sharply, indicating that lexical anchoring and hierarchical structuring are jointly essential for balancing coverage and specificity. Furthermore, these ablations introduce instability across depths—particularly in precision and recall—suggesting that without FLAM’s lexicon-targeted control, the traversal process becomes overly uniform and sensitive to document variation. This leads to inconsistent candidate selection and noisier retrieval. These results confirm that FinGEAR’s performance emerges from the coordinated integration of financial mapping, semantic indexing, and query-aware guidance—rather than from any individual module in isolation.

\section{Question-Type Ablation}
\label{appendix:question_type_ablation}

To support analysis in Section \ref{sec:dataset_ablation}, we provide the full per-question-type retrieval results in Table~\ref{tab:dataset_ablation_results}.

\begin{table*}[htbp!]
    \centering
    \caption{Categorical = yes/no answers, Numerical = numeric answers, Simple = one reasoning step, Complex = multiple reasoning steps}
    \label{tab:dataset_ablation_results}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
        \toprule
        \multirow{1}{*}{\textbf{Question Type}} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1 Score}} & \multicolumn{3}{c}{\textbf{Relevancy}}\\
        & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) & \(k=5\) & \(k=10\) & \(k=15\) \\
        \midrule
        Numerical & 0.79 & 0.76 & 0.71 & 0.61 & 0.62 & 0.64 & 0.69 & 0.68 & 0.68 & 0.50 & 0.64 & 0.63 \\
        Categorical & 0.83 & 0.92 & 0.87 & 0.71 & 0.72 & 0.82 & 0.77 & 0.81 & 0.86 & 0.46 & 0.48 & 0.49 \\
        \midrule
        Simple & 0.79 & 0.76 & 0.73 & 0.62 & 0.64 & 0.67 & 0.70 & 0.70 & 0.70 & 0.51 & 0.62 & 0.58 \\
        Complex & 0.78 & 0.76 & 0.72 & 0.59 & 0.59 & 0.61 & 0.67 & 0.67 & 0.66 & 0.48 & 0.65 & 0.67 \\
        \bottomrule
    \end{tabular}
    }
\end{table*}

\section{Runtime and Preprocessing Analysis}
\label{appendix:runtime}

\paragraph{Embedding Fine-Tuning.}
FinGEAR employs dense retrieval powered by sentence embeddings fine-tuned on financial data. We train two embedding models: one on FinQA for question–answer alignment, and one on FinRAD for lexicon-level semantic proximity. Both models are trained for 50 epochs. FinQA training (6{,}251 train / 1{,}147 test examples) completes in approximately 1{,}972 seconds, while FinRAD training (17{,}300 train / 5{,}190 test, sampled across filings) takes roughly 3{,}607 seconds. Training is performed on a single NVIDIA A100-SXM4-80GB GPU. These embeddings are applied throughout the retrieval pipeline, including document chunking, hierarchical traversal, and candidate scoring.

\paragraph{Preprocessing Pipeline.}
The full preprocessing workflow comprises: (1) keyword mapping, (2) keyword tree construction, and (3) hierarchical clustering for the Summary and Question Trees.

\textbf{Keyword Mapping}, implemented using spaCy’s \texttt{PhraseMatcher}, identifies relevant domain terms and runs between 53.5 and 434.6 seconds per document (mean: 248.9 seconds).  
\textbf{Keyword Tree Construction} is fast, requiring an average of 7.6 seconds per company.  
\textbf{Hierarchical Clustering and Summarization}, including UMAP-based dimensionality reduction and GMM clustering, builds the Summary and Question Trees. Across 10 filings, total runtime ranges from 243.2 to 602.6 seconds per document, with a mean of 408.2 seconds—primarily driven by the Summary Tree’s iterative summarization.

\paragraph{Deployment Considerations.}
All preprocessing steps are one-time operations per document and can be cached for repeated use. FinGEAR supports dual-tree traversal and modular re-indexing, making it scalable for real-world financial workloads. Runtime remains stable across filings of similar size, and preprocessing cost is amortized across multiple downstream queries.

\section{Glossary of Terms and Notation}
\label{appendix:glossary}
This appendix consolidates the terms, symbols, and procedure names used throughout the paper for quick reference. It denotes definitions for the FinGEAR components (e.g., traversal logic, and scoring functions). Unless stated otherwise, scalars are in italics (e.g., \(k\), \(w_i\)), vectors are in bold, and Item labels follow SEC notation (Item 1, Item 1A, Item 7). For mechanics in the pipeline, see Sections~\ref{sec:methodology} and~\ref{sec:in_retrieval}; the table below serves as a concise lookup.
\begin{table*}[h]
\centering
\small
\begin{tabular}{p{3.0cm} p{11.2cm}}
\toprule
\textbf{Term / Symbol} & \textbf{Definition} \\
\midrule
UMAP & Dimensionality reduction used before clustering; preserves local neighborhoods for tree construction. We use cosine distance with output dimension 10 (see Section~\ref{sec:experiments_setup}). \\
GMM & Gaussian Mixture Models used for soft clustering over UMAP embeddings to form internal tree nodes; max components 50; convergence threshold 0.1. \\
FLAM & \emph{Financial Lexicon-Aware Mapping}. Clusters financial terms corpus-wide and computes Item weights to guide global allocation (Section~\ref{sec:in_retrieval}). Default weighting: Relative Frequency. \\
Summary Tree & Content hierarchy within an Item. Internal nodes are summaries of clustered chunks; leaves are original text chunks (Section~\ref{sec:structure_extraction}). \\
Question Tree & Mirrors the Summary Tree topology; nodes store LLM-generated sub-questions embedded in the same space as user queries; leaves reference the same chunk IDs (Section~\ref{sec:structure_extraction}). \\
$k$ & Total retrieval budget per query (Top-$k$ evaluation). \\
$k^\ast$ & Per-Item budget after FLAM weighting; $\sum_i k^\ast_i = k$ (Section~\ref{sec:in_retrieval}). \\
Semantic traversal & Top-down navigation of a tree: score parent’s children, select the best, and descend until leaves or max depth (2). \\
Hybrid scoring & Within-Item node scoring that combines BM25 over summaries (sparse) and cosine similarity over embeddings (dense). \\
Reranking (Stage 1) & Cross-tree reranking that jointly scores candidates from Summary and Question Trees using \texttt{BAAI/bge-reranker-large}. \\
Reranking (Stage 2) & Cross-Item reranking over the pooled top spans from all Items to prioritize globally informative answers. \\
\bottomrule
\end{tabular}
\caption{Glossary of key components, symbols, and procedures used in FinGEAR.}
\end{table*}

\section{Prompt Design}
\label{appendix:prompts}

All prompts are used in zero-shot mode unless otherwise stated. Structured examples may be incorporated in future versions to enhance control and reproducibility. Here are three specialized prompts:

\subsection{Summarization Prompt}

\noindent\fbox{%
\parbox{\columnwidth}{%
\small
\texttt{The content provided below is a subset of a 10-K filing. The 10-K report is a comprehensive document outlining the company's financial performance, including revenue, expenses, and profits. Your task is to generate a detailed summary using only the provided content, without embellishment. Summarize main topics, key insights (5–7), and unusual observations (1–2). Use clear paragraphs and Markdown headings.}
}
}

\subsection{Title Generation Prompt}

\noindent\fbox{%
\parbox{\columnwidth}{%
\small
\texttt{Generate a title for a subsection of a 10-K report based on the provided summary.}
}
}

\subsection{Question Generation Prompt}

\noindent\fbox{%
\parbox{\columnwidth}{%
\small
\texttt{Suppose you are a financial analyst. Generate \{num\_questions\} questions based on the provided summary, focusing on financial aspects and factual details.}
}
}