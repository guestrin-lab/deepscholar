\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Alberg(1970)}]{alberg1970sec}
Tom~A Alberg. 1970.
\newblock Sec disclosure requirements for corporations.
\newblock \emph{Bus. Law.}, 26:1223.

\bibitem[{Asai et~al.(2023)Asai, Wu, Wang, Sil, and Hajishirzi}]{selfrag2023}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
\newblock \href {https://arxiv.org/abs/2310.11511} {Self-rag: Learning to retrieve, generate, and critique through self-reflection}.
\newblock \emph{Preprint}, arXiv:2310.11511.

\bibitem[{Chen et~al.(2024)Chen, Gao, Song, and Tan}]{chen2024hiqa}
Xinyue Chen, Pengyu Gao, Jiangjiang Song, and Xiaoyang Tan. 2024.
\newblock Hiqa: A hierarchical contextual augmentation rag for massive documents qa.
\newblock \emph{arXiv preprint arXiv:2402.01767}.

\bibitem[{Chen et~al.(2021)Chen, Chen, Smiley, Shah, Borova, Langdon, Moussa, Beane, Huang, Routledge et~al.}]{chen2021finqa}
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et~al. 2021.
\newblock Finqa: A dataset of numerical reasoning over financial data.
\newblock \emph{arXiv preprint arXiv:2109.00122}.

\bibitem[{Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov}]{dai2019transformerxlattentivelanguagemodels}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan Salakhutdinov. 2019.
\newblock \href {https://arxiv.org/abs/1901.02860} {Transformer-xl: Attentive language models beyond a fixed-length context}.
\newblock \emph{arXiv preprint arXiv:1901.02860}.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Edge et~al.(2024)Edge, Trinh, Cheng, Bradley, Chao, Mody, Truitt, and Larson}]{edge2024localglobalgraphrag}
Darren Edge, Ha~Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024.
\newblock \href {https://arxiv.org/abs/2404.16130} {From local to global: A graph rag approach to query-focused summarization}.
\newblock \emph{Preprint}, arXiv:2404.16130.

\bibitem[{Es et~al.(2023)Es, James, Espinosa-Anke, and Schockaert}]{shahul2023ragas}
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023.
\newblock \href {https://arxiv.org/abs/2309.15217} {Ragas: Automated evaluation of retrieval augmented generation}.

\bibitem[{Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli}]{fan-etal-2019-eli5}
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1346} {{ELI5}: Long form question answering}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 3558--3567. Association for Computational Linguistics.

\bibitem[{FinLang(2025)}]{finlang2025finance}
FinLang. 2025.
\newblock Finance embeddings investopedia.
\newblock \url{https://huggingface.co/FinLang/finance-embeddings-investopedia}.
\newblock Accessed: 2025-02-14.

\bibitem[{Ghosh et~al.(2021)Ghosh, Sengupta, Naskar, and Singh}]{ghosh2021finrad}
Sohom Ghosh, Shovon Sengupta, Sudip Naskar, and Sunny~Kumar Singh. 2021.
\newblock \href {https://aclanthology.org/2021.icon-main.81} {{F}in{R}ead: A transfer learning based tool to assess readability of definitions of financial terms}.
\newblock In \emph{Proceedings of the 18th International Conference on Natural Language Processing (ICON)}, pages 658--659, National Institute of Technology Silchar, Silchar, India. NLP Association of India (NLPAI).

\bibitem[{Guo et~al.(2024)Guo, Xia, Yu, Ao, and Huang}]{guo2024lightragsimplefastretrievalaugmented}
Zirui Guo, Lianghao Xia, Yanhua Yu, Tu~Ao, and Chao Huang. 2024.
\newblock \href {https://arxiv.org/abs/2410.05779} {Lightrag: Simple and fast retrieval-augmented generation}.
\newblock \emph{Preprint}, arXiv:2410.05779.

\bibitem[{Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang}]{guu2020retrievalaugmented}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020.
\newblock Realm: retrieval-augmented language model pre-training.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org.

\bibitem[{Henderson et~al.(2023)Henderson, Gilmer, Agarwal, Bouchard, Belanger, Tomar, and Dauphin}]{henderson2023matryoshka}
Peter Henderson, Justin Gilmer, Rishabh Agarwal, Guillaume Bouchard, David Belanger, Gaurav Tomar, and Yann Dauphin. 2023.
\newblock Matryoshka representation learning.
\newblock \emph{arXiv preprint arXiv:2305.17137}.

\bibitem[{Jeong et~al.(2024)Jeong, Baek, Cho, Hwang, and Park}]{adaptive2023}
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung~Ju Hwang, and Jong~C. Park. 2024.
\newblock \href {https://arxiv.org/abs/2403.14403} {Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity}.
\newblock \emph{Preprint}, arXiv:2403.14403.

\bibitem[{Lewis et~al.(2021)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, Küttler, Lewis, tau Yih, Rocktäschel, Riedel, and Kiela}]{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.
\newblock \href {https://arxiv.org/abs/2005.11401} {Retrieval-augmented generation for knowledge-intensive nlp tasks}.
\newblock \emph{Preprint}, arXiv:2005.11401.

\bibitem[{Li et~al.(2024)Li, Cao, Yu, Javaji, Deng, He, Jiang, Zhu, Subbalakshmi, Xiong et~al.}]{li2024investorbench}
Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar~Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, et~al. 2024.
\newblock Investorbench: A benchmark for financial decision-making tasks with llm-based agent.
\newblock \emph{arXiv preprint arXiv:2412.18174}.

\bibitem[{Li et~al.(2023)Li, Gao, Wu, and Vasarhelyi}]{li2023extracting}
Huaxia Li, Haoyun Gao, Chengzhang Wu, and Miklos~A. Vasarhelyi. 2023.
\newblock Extracting financial data from unstructured sources: Leveraging large language models.
\newblock \emph{Forthcoming in the Journal of Information Systems}.
\newblock Available at SSRN: \url{https://ssrn.com/abstract=4567607} or \url{http://dx.doi.org/10.2139/ssrn.4567607}.

\bibitem[{Liu et~al.(2023)Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang}]{liu2023lostmiddlelanguagemodels}
Nelson~F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.
\newblock \href {https://arxiv.org/abs/2307.03172} {Lost in the middle: How language models use long contexts}.
\newblock \emph{arXiv preprint arXiv:2307.03172}.

\bibitem[{Lù(2024)}]{bm25s}
Xing~Han Lù. 2024.
\newblock \href {https://arxiv.org/abs/2407.03618} {Bm25s: Orders of magnitude faster lexical search via eager sparse scoring}.
\newblock \emph{Preprint}, arXiv:2407.03618.

\bibitem[{Maynez et~al.(2020)Maynez, Narayan, Bohnet, and McDonald}]{maynez2020faithfulnessfactualityabstractivesummarization}
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020.
\newblock \href {https://arxiv.org/abs/2005.00661} {On faithfulness and factuality in abstractive summarization}.
\newblock \emph{arXiv preprint arXiv:2005.00661}.

\bibitem[{Ram et~al.(2023)Ram, Levine, Dalmedigos, Muhlgay, Shashua, Leyton-Brown, and Shoham}]{ram2023}
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
\newblock \href {https://arxiv.org/abs/2302.00083} {In-context retrieval-augmented language models}.
\newblock \emph{Preprint}, arXiv:2302.00083.

\bibitem[{Reddy et~al.(2024)Reddy, Koncel-Kedziorski, Lai, Krumdick, Lovering, and Tanner}]{docfinQA}
Varshini Reddy, Rik Koncel-Kedziorski, Viet~Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024.
\newblock \href {https://arxiv.org/abs/2401.06915} {Docfinqa: A long-context financial reasoning dataset}.
\newblock \emph{Preprint}, arXiv:2401.06915.

\bibitem[{Sarthi et~al.(2024)Sarthi, Abdullah, Tuli, Khanna, Goldie, and Manning}]{raptor}
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher~D. Manning. 2024.
\newblock Raptor: Recursive abstractive processing for tree-organized retrieval.
\newblock In \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[{Singh(2017)}]{singh2017replaceretrievekeywordsdocuments}
Vikash Singh. 2017.
\newblock \href {https://arxiv.org/abs/1711.00046} {Replace or retrieve keywords in documents at scale}.
\newblock \emph{Preprint}, arXiv:1711.00046.

\bibitem[{Wang et~al.(2024)Wang, Cohen, and Ma}]{wang2024modeling}
Mengyu Wang, Shay~B Cohen, and Tiejun Ma. 2024.
\newblock Modeling news interactions and influence for financial market prediction.
\newblock \emph{arXiv preprint arXiv:2410.10614}.

\bibitem[{Wang and Ma(2024)}]{wang2024mana}
Mengyu Wang and Tiejun Ma. 2024.
\newblock Mana-net: Mitigating aggregated sentiment homogenization with news weighting for enhanced market prediction.
\newblock In \emph{Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}, pages 2379--2389.

\bibitem[{Wu et~al.(2023)Wu, Irsoy, Lu, Dabravolski, Dredze, Gehrmann, Kambadur, Rosenberg, and Mann}]{wu2023bloomberggpt}
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.
\newblock \href {https://arxiv.org/abs/2303.17564} {Bloomberggpt: A large language model for finance}.
\newblock \emph{Preprint}, arXiv:2303.17564.

\bibitem[{Yang et~al.(2023{\natexlab{a}})Yang, Liu, and Wang}]{yang2023fingpt}
Hongyang Yang, Xiao-Yang Liu, and Christina~Dan Wang. 2023{\natexlab{a}}.
\newblock Fingpt: Open-source financial large language models.
\newblock \emph{arXiv preprint arXiv:2306.06031}.

\bibitem[{Yang et~al.(2023{\natexlab{b}})Yang, Liu, Li, Sun, and Xie}]{yang-etal-2023-longtriever}
Junhan Yang, Zheng Liu, Chaozhuo Li, Guangzhong Sun, and Xing Xie. 2023{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.223} {Longtriever: a pre-trained long text encoder for dense document retrieval}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 3655--3665. Association for Computational Linguistics.

\bibitem[{Yu et~al.(2024)Yu, Xu, and Akkiraju}]{yu2024defense}
Tan Yu, Anbang Xu, and Rama Akkiraju. 2024.
\newblock \href {https://arxiv.org/abs/2409.01666} {In defense of rag in the era of long-context language models}.
\newblock \emph{Preprint}, arXiv:2409.01666.

\bibitem[{Yuan et~al.(2021)}]{finbertqa}
Bit Yuan et~al. 2021.
\newblock Finbert-qa: Financial question answering with pre-trained bert language models.
\newblock \url{https://github.com/yuanbit/FinBERT-QA}.
\newblock Accessed: 2025-04-29.

\bibitem[{Zhang et~al.(2023)Zhang, Yang, Zhou, Babar, and Liu}]{zhang2023financialsentiment}
Boyu Zhang, Hongyang Yang, Tianyu Zhou, Ali Babar, and Xiao-Yang Liu. 2023.
\newblock \href {https://arxiv.org/abs/2310.04027} {Enhancing financial sentiment analysis via retrieval augmented large language models}.
\newblock \emph{Preprint}, arXiv:2310.04027.

\bibitem[{Zhong et~al.(2025)Zhong, Liu, Cui, Zhang, and Qin}]{zhong2025mixofgranularityoptimizechunkinggranularity}
Zijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang, and Zengchang Qin. 2025.
\newblock \href {https://arxiv.org/abs/2406.00456} {Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation}.
\newblock \emph{arXiv preprint arXiv:2406.00456}.

\bibitem[{Zhu et~al.(2021)Zhu, Lei, Huang, Wang, Zhang, Lv, Feng, and Chua}]{zhu2021tatqaquestionansweringbenchmark}
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021.
\newblock \href {https://arxiv.org/abs/2105.07624} {Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance}.
\newblock \emph{arXiv preprint arXiv:2105.07624}.

\end{thebibliography}
