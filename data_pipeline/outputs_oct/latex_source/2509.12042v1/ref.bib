@inproceedings{finrad,
    title = "{F}in{RAD}: Financial Readability Assessment Dataset - 13,000+ Definitions of Financial Terms for Measuring Readability",
    author = "Ghosh, Sohom  and
      Sengupta, Shovon  and
      Naskar, Sudip  and
      Singh, Sunny Kumar",
    editor = "El-Haj, Mahmoud  and
      Rayson, Paul  and
      Zmandar, Nadhem",
    booktitle = "Proceedings of the 4th Financial Narrative Processing Workshop @LREC2022",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.fnp-1.1/"
}

@misc{asai2023selfraglearningretrievegenerate,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11511}, 
}

@misc{jeong2024adaptiverag,
      title={Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity}, 
      author={Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung Ju Hwang and Jong C. Park},
      year={2024},
      eprint={2403.14403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.14403}, 
}
@inproceedings{Singh_2024, series={ICAIF ’24},
   title={FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven Question Answering Pipeline},
   url={http://dx.doi.org/10.1145/3677052.3698682},
   DOI={10.1145/3677052.3698682},
   booktitle={Proceedings of the 5th ACM International Conference on AI in Finance},
   publisher={ACM},
   author={Singh, Kuldeep and Kaur, Simerjot and Smiley, Charese},
   year={2024},
   month=nov, pages={266–273},
   collection={ICAIF ’24} }

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year={2023},
  month={02},
  doi={10.48550/arXiv.2302.13971},
  url={https://doi.org/10.48550/arXiv.2302.13971}
}

@article{llama3herdmodels,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{yang2023fingpt,
    title={Fingpt: Open-source financial large language models},
    author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},
    journal={arXiv preprint arXiv:2306.06031},
    year={2023}
}
@article{alberg1970sec,
  title={SEC Disclosure Requirements for Corporations},
  author={Alberg, Tom A},
  journal={Bus. Law.},
  volume={26},
  pages={1223},
  year={1970},
  publisher={HeinOnline}
}
@article{li2023extracting,
  author    = {Li, Huaxia and Gao, Haoyun and Wu, Chengzhang and Vasarhelyi, Miklos A.},
  title     = {Extracting Financial Data from Unstructured Sources: Leveraging Large Language Models},
  journal   = {Forthcoming in the Journal of Information Systems},
  year      = {2023},
  month     = {September 6},
  note      = {Available at SSRN: \url{https://ssrn.com/abstract=4567607} or \url{http://dx.doi.org/10.2139/ssrn.4567607}}
}
@misc{kim2024financialstatementanalysislarge,
      title={Financial Statement Analysis with Large Language Models}, 
      author={Alex Kim and Maximilian Muhn and Valeri Nikolaev},
      year={2024},
      eprint={2407.17866},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST},
      url={https://arxiv.org/abs/2407.17866}, 
}
@misc{bauguess2018machine,
  author    = {Bauguess, Scott W.},
  title     = {The Role of Machine Readability in an AI World},
  year      = {2018},
  month     = {May 3},
  howpublished = {SEC Speeches and Statements},
  note      = {Available at: \url{https://www.sec.gov/news/speech/speech-bauguess-050318}}
}
@misc{balaguer2024ragvsfinetuningpipelines,
      title={RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture}, 
      author={Angels Balaguer and Vinamra Benara and Renato Luiz de Freitas Cunha and Roberto de M. Estevão Filho and Todd Hendry and Daniel Holstein and Jennifer Marsman and Nick Mecklenburg and Sara Malvar and Leonardo O. Nunes and Rafael Padilha and Morris Sharp and Bruno Silva and Swati Sharma and Vijay Aski and Ranveer Chandra},
      year={2024},
      eprint={2401.08406},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.08406}
}
@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{shi2023largelanguagemodelseasily,
      title={Large Language Models Can Be Easily Distracted by Irrelevant Context}, 
      author={Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed Chi and Nathanael Schärli and Denny Zhou},
      year={2023},
      eprint={2302.00093},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.00093}, 
}
@misc{longllmlingua,
      title={LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression}, 
      author={Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
      year={2024},
      eprint={2310.06839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06839}, 
}
@misc{araci2019finbert,
    author = {Araci, D.},
    title = {FinBERT: Financial Sentiment Analysis with Pre-Trained Language Models},
    howpublished = {University of Amsterdam. Available online},
    year = {2019},
    url = {https://arxiv.org/pdf/1908.10063.pdf}
}

@inproceedings{wang2024mana,
  title={Mana-net: Mitigating aggregated sentiment homogenization with news weighting for enhanced market prediction},
  author={Wang, Mengyu and Ma, Tiejun},
  booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages={2379--2389},
  year={2024}
}

@inproceedings{guu2020retrievalaugmented, author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei}, title = {REALM: retrieval-augmented language model pre-training}, year = {2020}, publisher = {JMLR.org}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, articleno = {368}, numpages = {10}, series = {ICML'20} }

@misc{ram2023,
      title={In-Context Retrieval-Augmented Language Models}, 
      author={Ori Ram and Yoav Levine and Itay Dalmedigos and Dor Muhlgay and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2302.00083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.00083}, 
}

@misc{guo2024lightragsimplefastretrievalaugmented,
      title={LightRAG: Simple and Fast Retrieval-Augmented Generation}, 
      author={Zirui Guo and Lianghao Xia and Yanhua Yu and Tu Ao and Chao Huang},
      year={2024},
      eprint={2410.05779},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.05779}, 
}
@misc{adaptive2023,
      title={Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity}, 
      author={Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung Ju Hwang and Jong C. Park},
      year={2024},
      eprint={2403.14403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.14403}, 
}

@misc{selfrag2023,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11511}, 
}

@article{wang2024modeling,
  title={Modeling News Interactions and Influence for Financial Market Prediction},
  author={Wang, Mengyu and Cohen, Shay B and Ma, Tiejun},
  journal={arXiv preprint arXiv:2410.10614},
  year={2024}
}

@misc{yu2024defense,
      title={In Defense of RAG in the Era of Long-Context Language Models}, 
      author={Tan Yu and Anbang Xu and Rama Akkiraju},
      year={2024},
      eprint={2409.01666},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.01666}, 
}
@misc{xu2024retrieval,
      title={Retrieval meets Long Context Large Language Models}, 
      author={Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
      year={2024},
      eprint={2310.03025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03025}, 
}

@misc{zhang2023financialsentiment,
      title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models}, 
      author={Boyu Zhang and Hongyang Yang and Tianyu Zhou and Ali Babar and Xiao-Yang Liu},
      year={2023},
      eprint={2310.04027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04027}, 
}
@inproceedings{ouyang2022training,
    title={Training language models to follow instructions with human feedback},
    author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
    booktitle={Advances in Neural Information Processing Systems},
    volume={35},
    pages={27730--27744},
    year={2022}
}

@misc{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Le Scao, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and El Sayed, William},
  year={2023},
  eprint={2310.06825},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{touvron2023llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year={2023},
  eprint={2307.09288},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}


@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year={2022},
  journal={Journal Name Here},
  volume={Volume Number Here},
  number={Issue Number Here},
  pages={Page Numbers Here},
  publisher={Publisher Name Here}
}
@article{henderson2023matryoshka,
  title={Matryoshka Representation Learning},
  author={Henderson, Peter and Gilmer, Justin and Agarwal, Rishabh and Bouchard, Guillaume and Belanger, David and Tomar, Gaurav and Dauphin, Yann},
  journal={arXiv preprint arXiv:2305.17137},
  year={2023}
}
@inproceedings{kannan2017smartreply,
  title={Smart Reply: Automated Response Suggestion for Email},
  author={Kannan, Anjuli and Kurach, Karol and Ravi, Sujith and Kaufman, Tobias and Tomkins, Andrew and Miklos, Balint and Corrado, Greg and Krichene, Walid and Anil, Rohan and Singh, Patrick and others},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={955--964},
  year={2017},
  organization={ACM}
}
@article{gao2021scaling,
  title={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2101.06983},
  year={2021}
}
@online{durmus2024persuasion,
author = {Esin Durmus and Liane Lovitt and Alex Tamkin and Stuart Ritchie and Jack Clark and Deep Ganguli},
title = {Measuring the Persuasiveness of Language Models},
date = {2024-04-09},
year = {2024},
url = {https://www.anthropic.com/news/measuring-model-persuasiveness},
}

@article{zhou2023comprehensive,
  title={A comprehensive survey on pretrained foundation models: A history from BERT to ChatGPT},
  author={Zhou, C. and Li, Q. and Li, C. and Yu, J. and Liu, Y. and Wang, G. and Zhang, K. and Ji, C. and Yan, Q. and He, L. and others},
  journal={arXiv preprint arXiv:2302.09419},
  year={2023}
}

@misc{liu2022pretrain,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, P. and Yuan, W. and Fu, J. and Jiang, Z. and Hayashi, H. and Neubig, G.},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  year={2022}
}

@misc{han2021pretrained,
  title={Pre-trained models: Past, present and future},
  author={Han, X. and Zhang, Z. and Ding, N. and Gu, Y. and Liu, X. and Huo, Y. and Qiu, J. and Yao, Y. and Zhang, A. and Zhang, L. and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021}
}

@misc{openai2024gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and others},
  year={2024},
  eprint={2303.08774},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{qiu2020pretrained,
  title={Pre-trained models for natural language processing: A survey},
  author={Qiu, X. and Sun, T. and Xu, Y. and Shao, Y. and Dai, N. and Huang, X.},
  journal={CoRR},
  volume={abs/2003.08271},
  year={2020}
}

@inproceedings{HybridRAG,
  author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and others},
  title = {HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction},
  booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
  year = {2024},
  url = {https://doi.org/10.1145/3677052.3698671},
  doi = {10.1145/3677052.3698671}
}

@misc{rankRAG,
  title = {RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs},
  author = {Yu, Yue and Ping, Wei and Liu, Zihan and others},
  year = {2024},
  url = {https://arxiv.org/abs/2407.02485}
}

@misc{LoRAL,
  title = {LoRA-Contextualizing Adaptation of Large Multimodal Models for Long Document Understanding},
  author = {Chen, Jian and Zhang, Ruiyi and Zhou, Yufan and others},
  year = {2024},
  url = {https://arxiv.org/abs/2411.01106}
}

@inproceedings{FinQAPT,
  title = {FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven Question Answering Pipeline},
  author = {Singh, Kuldeep and Kaur, Simerjot and Smiley, Charese},
  booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
  year = {2024},
  url = {http://dx.doi.org/10.1145/3677052.3698682},
  doi = {10.1145/3677052.3698682}
}

@inproceedings{brown2020,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{gemmateam2024gemma,
      title={Gemma: Explorations in Multimodal Domains},
      author={Gemma Team and Rohan Anil and others},
      year={2024},
      eprint={2312.11806},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{geminiteam2024gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models},
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and others},
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and others},
  year={2022},
  eprint={2204.02311},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{almazrouei2023falcon,
      title={The Falcon Series of Open Language Models}, 
      author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
      year={2023},
      eprint={2311.16867},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@techreport{llama2024,
  author       = "Llama team",
  title        = "The Llama 3 Herd of Models",
  institution  = "Meta",
  year         = 2024,
  note         = "A detailed contributor list can be found in the appendix of this paper."
}
@article{maitrey2015mapreduce,
  author = {Seema Maitrey and C.K. Jha},
  title = {MapReduce: Simplified Data Analysis of Big Data},
  journal = {Procedia Computer Science},
  year = {2015},
  volume = {57},
  pages = {563-571},
  keywords = {Big Data, Data Mining, Parallelization Techniques, HDFS, MapReduce, Hadoop},
  doi = {10.1016/j.procs.2015.07.392},
  issn = {1877-0509},
  note = {3rd International Conference on Recent Trends in Computing 2015 (ICRTC-2015)},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050915019213}
}
@article{hashem2016mapreduce,
  author = {Hashem, Ibrahim and Anuar, Nor and Gani, Abdullah and Yaqoob, Ibrar and Xia, Feng and Khan, Samee},
  year = {2016},
  month = {April},
  title = {MapReduce: Review and Open Challenges},
  volume = {109},
  journal = {Scientometrics},
  doi = {10.1007/s11192-016-1945-y}
}

@article{touvron2022,
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Vincent and others},
  title = {Llama: Open and Efficient Foundation Language Models},
  journal = {arXiv preprint arXiv:2302.13971},
  year = {2022}
}

@inproceedings{FINRED,
author = {Sharma, Soumya and Nayak, Tapas and Bose, Arusarka and Meena, Ajay Kumar and Dasgupta, Koustuv and Ganguly, Niloy and Goyal, Pawan},
title = {FinRED: A Dataset for Relation Extraction in Financial Domain},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524637},
doi = {10.1145/3487553.3524637},
abstract = {Relation extraction models trained on a source domain cannot be applied on a different target domain due to the mismatch between relation sets. In the current literature, there is no extensive open-source relation extraction dataset specific to the finance domain. In this paper, we release FinRED, a relation extraction dataset curated from financial news and earning call transcripts containing relations from the finance domain. FinRED has been created by mapping Wikidata triplets using distance supervision method. We manually annotate the test data to ensure proper evaluation. We also experiment with various state-of-the-art relation extraction models on this dataset to create the benchmark. We see a significant drop in their performance on FinRED compared to the general relation extraction datasets which tells that we need better models for financial relation extraction.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {595–597},
numpages = {3},
keywords = {financial relation extraction, financial information extraction, financial dataset},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{fiqa-sa,
author = {Maia, Macedo and Handschuh, Siegfried and Freitas, Andr\'{e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
title = {WWW'18 Open Challenge: Financial Opinion Mining and Question Answering},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3192301},
doi = {10.1145/3184558.3192301},
abstract = {The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The finance domain, with its reliance on the interpretation of multiple unstructured and structured data sources and its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1941–1942},
numpages = {2},
keywords = {financial domain, opinion mining, question answering},
location = {Lyon, France},
series = {WWW '18}
}


@article{zhang_finbertmrc_2023,
	title = {{FinBERT}–{MRC}: {Financial} {Named} {Entity} {Recognition} {Using} {BERT} {Under} the {Machine} {Reading} {Comprehension} {Paradigm}},
	volume = {55},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-023-11266-5},
	doi = {10.1007/s11063-023-11266-5},
	abstract = {Financial named entity recognition (FinNER) is a challenging task in the field of financial text information extraction, which aims to extract a large amount of financial knowledge from unstructured texts. It is widely accepted to use the sequence tagging framework to implement the FinNER tasks. However, such sequence tagging models cannot fully take advantage of the semantic information in the texts. Instead, we formulate the FinNER task as a machine reading comprehension (MRC) problem and propose a new model termed FinBERT–MRC. This formulation introduces significant prior information by utilizing well-designed queries, and extracts the start index and end index of the target entities without decoding modules such as conditional random fields (CRFs). We conduct experiments on a publicly available Chinese financial dataset ChFinAnn and a real-world business dataset AdminPunish. FinBERT–MRC achieves avehoffmanne \$\$\{F\}\_\{1\}\$\$scores of 92.78\% and 96.80\% on two datasets, respectively, with avehoffmanne \$\$\{F\}\_\{1\}\$\$gains + 3.75\% and + 0.68\% over some sequence tagging models including BiLSTM–CRF, BiGRU–CRF, BiLSTM–CNN–CRF, FinBERT–Tagger, and FinBERT–CRF. The source code is available at https://github.com/zyz0000/FinBERT-MRC.},
	number = {6},
	journal = {Neural Processing Letters},
	author = {Zhang, Yuzhe and Zhang, Hong},
	month = dec,
	year = {2023},
	pages = {7393--7413},
}

@misc{finbertqa,
  author = {Yuan, Bit and others},
  title = {FinBERT-QA: Financial Question Answering with Pre-trained BERT Language Models},
  howpublished = {\url{https://github.com/yuanbit/FinBERT-QA}},
  year = {2021},
  note = {Accessed: 2025-04-29}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{liu2023lostmiddlelanguagemodels,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={arXiv preprint arXiv:2307.03172},
  year={2023},
  url={https://arxiv.org/abs/2307.03172}
}

@article{dai2019transformerxlattentivelanguagemodels,
  title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019},
  url={https://arxiv.org/abs/1901.02860}
}

@article{zhong2025mixofgranularityoptimizechunkinggranularity,
  title={Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation},
  author={Zhong, Zijie and Liu, Hanwen and Cui, Xiaoya and Zhang, Xiaofan and Qin, Zengchang},
  journal={arXiv preprint arXiv:2406.00456},
  year={2025},
  url={https://arxiv.org/abs/2406.00456}
}

@inproceedings{yang-etal-2023-longtriever,
  title={Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval},
  author={Yang, Junhan and Liu, Zheng and Li, Chaozhuo and Sun, Guangzhong and Xie, Xing},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={3655--3665},
  year={2023},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2023.emnlp-main.223/},
  doi={10.18653/v1/2023.emnlp-main.223}
}

@inproceedings{fan-etal-2019-eli5,
  title={{ELI5}: Long Form Question Answering},
  author={Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3558--3567},
  year={2019},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/P19-1346/},
  doi={10.18653/v1/P19-1346}
}

@article{maynez2020faithfulnessfactualityabstractivesummarization,
  title={On Faithfulness and Factuality in Abstractive Summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  journal={arXiv preprint arXiv:2005.00661},
  year={2020},
  url={https://arxiv.org/abs/2005.00661}
}

@article{zhu2021tatqaquestionansweringbenchmark,
  title={TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance},
  author={Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2105.07624},
  year={2021},
  url={https://arxiv.org/abs/2105.07624}
}
@article{googlet5,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}

@misc{wu2023bloomberggpt,
      title={BloombergGPT: A Large Language Model for Finance}, 
      author={Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
      year={2023},
      eprint={2303.17564},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{shahul2023ragas,
  author = {Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
  title = {RAGAS: Automated Evaluation of Retrieval Augmented Generation},
  year = {2023},
  archivePrefix = {arXiv},
  eprint = {2309.15217}
}
@misc{huggingface_investopedia_dataset,
  author = {Hugging Face},
  title = {Investopedia Embedding Dataset},
  year = {2025},
  url = {https://huggingface.co/datasets/FinLang/investopedia-embedding-dataset},
  note = {Accessed: 2025-01-16}
}
@article{katayama1997sr,
  title={The SR-tree: An index structure for high-dimensional nearest neighbor queries},
  author={Katayama, Norio and Satoh, Shin'ichi},
  journal={ACM Sigmod Record},
  volume={26},
  number={2},
  pages={369--380},
  year={1997},
  publisher={ACM New York, NY, USA}
}
@misc{umap2018,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03426}, 
}
@article{setty2024improving,
  author = {Spurthi Setty and Harsh Thakkar and Alyssa Lee and Eden Chung and Natan Vidra},
  title = {Improving Retrieval for RAG based Question Answering Models on Financial Documents},
  year = {2024},
  archivePrefix = {arXiv},
  eprint = {2404.07221v2},
  primaryClass = {cs.IR},
  month = jun,
  day = {18}
}
@inproceedings{NEURIPS2023_PIXIU,
 author = {Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and Lopez-Lira, Alejandro and Huang, Jimin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {33469--33484},
 publisher = {Curran Associates, Inc.},
 title = {PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6a386d703b50f1cf1f61ab02a15967bb-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}
@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V. Do and Yan Xu and Pascale Fung},
  journal={ArXiv preprint},
  volume={abs/2302.04023},
  year={2023}
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}
@misc{singh2017replaceretrievekeywordsdocuments,
      title={Replace or Retrieve Keywords In Documents at Scale}, 
      author={Vikash Singh},
      year={2017},
      eprint={1711.00046},
      archivePrefix={arXiv},
      primaryClass={cs.DS},
      url={https://arxiv.org/abs/1711.00046}, 
}
@misc{edge2024localglobalgraphrag,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Jonathan Larson},
      year={2024},
      eprint={2404.16130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16130}, 
}
@inproceedings{cai2022retrieval,
  author = {Cai, Deng and Wang, Yan and Liu, Lemao and Shi, Shuming},
  title = {Recent Advances in Retrieval-Augmented Text Generation},
  year = {2022},
  isbn = {9781450387323},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3477495.3532682},
  doi = {10.1145/3477495.3532682},
  booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {3417--3419},
  numpages = {3},
  keywords = {information retrieval, text generation},
  location = {Madrid, Spain},
  series = {SIGIR '22}
}
@inproceedings{10.1145/3477495.3532682,
author = {Cai, Deng and Wang, Yan and Liu, Lemao and Shi, Shuming},
title = {Recent Advances in Retrieval-Augmented Text Generation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532682},
doi = {10.1145/3477495.3532682},
abstract = {Recently retrieval-augmented text generation has achieved state-of-the-art performance in many NLP tasks and has attracted increasing attention of the NLP and IR community, this tutorial thereby aims to present recent advances in retrieval-augmented text generation comprehensively and comparatively. It firstly highlights the generic paradigm of retrieval-augmented text generation, then reviews notable works for different text generation tasks including dialogue generation, machine translation, and other generation tasks, and finally points out some limitations and shortcomings to facilitate future research.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3417–3419},
numpages = {3},
keywords = {information retrieval, text generation},
location = {Madrid, Spain},
series = {SIGIR '22}
}
@misc{huang2023surveyhallucinationlargelanguage,
      title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}, 
      author={Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
      year={2023},
      eprint={2311.05232},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.05232}, 
}
@inproceedings{guerreiro2023needle,
  title={Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation},
  author={Nuno M. Guerreiro and Elena Voita and André Martins},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={1059--1075},
  year={2023},
  location={Dubrovnik, Croatia},
  publisher={Association for Computational Linguistics}
}
@inproceedings{salinas-alvarado-etal-2015-domain,
    title = "Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment",
    author = "Salinas Alvarado, Julio Cesar  and
      Verspoor, Karin  and
      Baldwin, Timothy",
    editor = "Hachey, Ben  and
      Webster, Kellie",
    booktitle = "Proceedings of the Australasian Language Technology Association Workshop 2015",
    month = dec,
    year = "2015",
    address = "Parramatta, Australia",
    url = "https://aclanthology.org/U15-1010",
    pages = "84--90",
}

@misc{lu2023bbtfin,
      title={BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark}, 
      author={Dakuan Lu and Hengkui Wu and Jiaqing Liang and Yipei Xu and Qianyu He and Yipeng Geng and Mengkun Han and Yingsi Xin and Yanghua Xiao},
      year={2023},
      eprint={2302.09432},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{DuEE-Fin,
author="Han, Cuiyun
and Zhang, Jinchuan
and Li, Xinyu
and Xu, Guojin
and Peng, Weihua
and Zeng, Zengfeng",
editor="Lu, Wei
and Huang, Shujian
and Hong, Yu
and Zhou, Xiabing",
title="DuEE-Fin: A Large-Scale Dataset for Document-Level Event Extraction",
booktitle="Natural Language Processing and Chinese Computing",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="172--183",
abstract="To tackle the data scarcity problem of document-level event extraction, we come up with a large-scale benchmark, DuEE-Fin, which consists of 15,000+ events categorized into 13 event types, and 81,000+ event arguments mapped in 92 argument roles. We constructed DuEE-Fin from real-world Chinese financial news, which allows one document to contain several events, multiple arguments to share the same argument role and one argument to play different roles in different events. Therefore, it presents some considerable challenges in document-level event extraction task such as multi-event recognition and multi-value argument identification, that are referred to as key issues for document-level event extraction task. Along with DuEE-Fin, we also hosted an open competition, which has attracted 1,690 teams and achieved exciting results. We performed experiments on DuEE-Fin with most popular document-level event extraction systems. However, results showed that even some SOTA models performed poorly with our data. Facing these challenges, we found it necessary to propose more effective methods.",
isbn="978-3-031-17120-8"
}



@inproceedings{chen-etal-2022-convfinqa,
    title = "{C}onv{F}in{QA}: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering",
    author = "Chen, Zhiyu  and
      Li, Shiyang  and
      Smiley, Charese  and
      Ma, Zhiqiang  and
      Shah, Sameena  and
      Wang, William Yang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.421",
    doi = "10.18653/v1/2022.emnlp-main.421",
    pages = "6279--6292",
    abstract = "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at \url{https://github.com/czyssrs/ConvFinQA}.",
}

@article{xing2018natural,
  title={Natural language based financial forecasting: a survey},
  author={Xing, Frank Z and Cambria, Erik and Welsch, Roy E},
  journal={Artificial Intelligence Review},
  volume={50},
  number={1},
  pages={49--73},
  year={2018},
  publisher={Springer}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@misc{islam2023financebenchnewbenchmarkfinancial,
      title={FinanceBench: A New Benchmark for Financial Question Answering}, 
      author={Pranab Islam and Anand Kannappan and Douwe Kiela and Rebecca Qian and Nino Scherrer and Bertie Vidgen},
      year={2023},
      eprint={2311.11944},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.11944}, 
}
@misc{zheng2023judgingllmasajudgemtbenchchatbot,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.05685}, 
}

@misc{janik2024aspectshumanmemorylarge,
      title={Aspects of human memory and Large Language Models}, 
      author={Romuald A. Janik},
      year={2024},
      eprint={2311.03839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.03839}, 
}
@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{shah2022flue,
  title={When flue meets flang: Benchmarks and large pre-trained language model for financial domain},
  author={Shah, Raj Sanjay and Chawla, Kunal and Eidnani, Dheeraj and Shah, Agam and Du, Wendi and Chava, Sudheer and Raman, Natraj and Smiley, Charese and Chen, Jiaao and Yang, Diyi},
  journal={arXiv preprint arXiv:2211.00083},
  year={2022}
}

@article{guo2023chatgpt,
  title={Is chatgpt a financial expert? evaluating language models on financial natural language processing},
  author={Guo, Yue and Xu, Zian and Yang, Yi},
  journal={arXiv preprint arXiv:2310.12664},
  year={2023}
}

@misc{aksitov2023restmeetsreactselfimprovement,
      title={ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent}, 
      author={Renat Aksitov and Sobhan Miryoosefi and Zonglin Li and Daliang Li and Sheila Babayan and Kavya Kopparapu and Zachary Fisher and Ruiqi Guo and Sushant Prakash and Pranesh Srinivasan and Manzil Zaheer and Felix Yu and Sanjiv Kumar},
      year={2023},
      eprint={2312.10003},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10003}, 
}

@misc{es2023ragasautomatedevaluationretrieval,
      title={RAGAS: Automated Evaluation of Retrieval Augmented Generation}, 
      author={Shahul Es and Jithin James and Luis Espinosa-Anke and Steven Schockaert},
      year={2023},
      eprint={2309.15217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15217}, 
}

@inproceedings{raptor,
    title={RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval},
    author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D.},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2024}
}

@article{li2023chatgpt,
  title={Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? an examination on several typical tasks},
  author={Li, Xianzhi and Zhu, Xiaodan and Ma, Zhiqiang and Liu, Xiaomo and Shah, Sameena},
  journal={arXiv preprint arXiv:2305.05862},
  year={2023}
}

@article{chen2021finqa,
  title={Finqa: A dataset of numerical reasoning over financial data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others},
  journal={arXiv preprint arXiv:2109.00122},
  year={2021}
}

@article{xie2024finben,
  title={The FinBen: An Holistic Financial Benchmark for Large Language Models},
  author={Xie, Qianqian and Han, Weiguang and Chen, Zhengyu and Xiang, Ruoyu and Zhang, Xiao and He, Yueru and Xiao, Mengxi and Li, Dong and Dai, Yongfu and Feng, Duanyu and others},
  journal={arXiv preprint arXiv:2402.12659},
  year={2024}
}
@misc{DocFinQA,
      title={DocFinQA: A Long-Context Financial Reasoning Dataset}, 
      author={Varshini Reddy and Rik Koncel-Kedziorski and Viet Dac Lai and Michael Krumdick and Charles Lovering and Chris Tanner},
      year={2024},
      eprint={2401.06915},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06915}, 
}
@article{kim2024financial,
  title={Financial statement analysis with large language models},
  author={Kim, Alex and Muhn, Maximilian and Nikolaev, Valeri},
  journal={arXiv preprint arXiv:2407.17866},
  year={2024}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@inproceedings{metropolitansky2024claimify,
  title={Towards Effective Extraction and Evaluation of Factual Claims},
  author={Metropolitansky, Dasha and Larson, Jonathan},
  booktitle={Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2024},
  organization={ACL}
}

@inproceedings{karpukhin2020dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781"
}
@article{bm25,
  author       = {Stephen Robertson and Hugo Zaragoza},
  title        = {The Probabilistic Relevance Framework: BM25 and Beyond},
  journal      = {Foundations and Trends® in Information Retrieval},
  volume       = {3},
  number       = {4},
  pages        = {333--389},
  year         = {2009},
  doi          = {10.1561/1500000019},
  url          = {http://dx.doi.org/10.1561/1500000019}
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@article{zheng2023stepback,
  title={Take a step back: Evoking reasoning via abstraction in large language models},
  author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.06117},
  year={2023}
}

@inproceedings{cormack2009reciprocal,
  title={Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  author={Cormack, Gordon V and Clarke, Charles LA and Buettcher, Stefan},
  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages={758--759},
  year={2009}
}

@misc{finlang2025finance,
  author       = {FinLang},
  title        = {Finance Embeddings Investopedia},
  year         = 2025,
  howpublished = {\url{https://huggingface.co/FinLang/finance-embeddings-investopedia}},
  note         = {Accessed: 2025-02-14}
}

@misc{bm25s,
      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring}, 
      author={Xing Han Lù},
      year={2024},
      eprint={2407.03618},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.03618}, 
}

@inproceedings{ghosh2021finrad,
    title = "{F}in{R}ead: A Transfer Learning Based Tool to Assess Readability of Definitions of Financial Terms",
    author = "Ghosh, Sohom  and
      Sengupta, Shovon  and
      Naskar, Sudip  and
      Singh, Sunny Kumar",
    booktitle = "Proceedings of the 18th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2021",
    address = "National Institute of Technology Silchar, Silchar, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2021.icon-main.81",
    pages = "658--659"
    }

@inproceedings{sai2019annualreport,
  title={Analysing performance of company through annual reports using text analytics},
  author={Sai, Pradeep K and Gupta, Pooja and Fernandes, Semila Fenelly},
  booktitle={2019 International Conference on Digitization (ICD)},
  pages={21--31},
  year={2019},
  organization={IEEE}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@article{li2024investorbench,
  title={INVESTORBENCH: A Benchmark for Financial Decision-Making Tasks with LLM-based Agent},
  author={Li, Haohang and Cao, Yupeng and Yu, Yangyang and Javaji, Shashidhar Reddy and Deng, Zhiyang and He, Yueru and Jiang, Yuechen and Zhu, Zining and Subbalakshmi, Koduvayur and Xiong, Guojun and others},
  journal={arXiv preprint arXiv:2412.18174},
  year={2024}
}

@article{lyu2025crudrag,
author = {Lyu, Yuanjie and Li, Zhiyu and Niu, Simin and Xiong, Feiyu and Tang, Bo and Wang, Wenjin and Wu, Hao and Liu, Huanyong and Xu, Tong and Chen, Enhong},
title = {CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3701228},
doi = {10.1145/3701228},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {41},
numpages = {32},
keywords = {Retrieval-Augmented Generation, Large Language Models, Evaluation}
}

@article{chen2024hiqa,
  title={HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA},
  author={Chen, Xinyue and Gao, Pengyu and Song, Jiangjiang and Tan, Xiaoyang},
  journal={arXiv preprint arXiv:2402.01767},
  year={2024}
}

@misc{langchain,
  author       = {LangChain},
  title        = {LangChain: Building applications with LLMs through composability},
  year         = {n.d.},
  note         = {Retrieved February 15, 2025},
  url          = {https://www.langchain.com/}
}