\section{Methodology}
\label{sec:methodology}

FinGEAR is a modular retrieval framework designed to align with the structure and terminology of long financial filings. It is motivated by the observation that 10-K reports embed rich domain-specific signals, such as SEC Item headings, disclosure hierarchy, and financial terms. These signals can be extracted and used for improved retrieval. Rather than relying on flat chunking or dense-only similarity, FinGEAR builds hierarchical representations and uses a financial lexicon to guide search with hybrid matching, enabling retrieval with structural fidelity and domain specificity.

\subsection{Pre-Retrieval Pipeline}

Before retrieval, FinGEAR builds indexing structures for context-aware navigation: (1) \textit{structure extraction} to model disclosure hierarchy, and (2) \textit{Financial Lexicon-Aware Mapping (FLAM)} to steer search toward financially salient content.
Figure~\ref{fig:pre_retrieval} summarizes this pre-retrieval stage, showing dual-tree construction (Summary/Question Trees) alongside FLAM’s lexicon-to-Item weighting that will drive per-Item budgets used later (Section~\ref{sec:in_retrieval}).

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{flam_weights.pdf}
\vspace{-0.6em}
\caption{Global navigation with FLAM. Lexicon clusters map query terms to disclosure Items and assign weights $w\!\in\![0,1]$ based on relative frequency across filings; these weights determine per-Item budgets $k_i^{\ast}$ used for traversal.}
\label{fig:flam_weights}
\vspace{-0.8em}
\end{figure}

\subsubsection{Structure Extraction}
\label{sec:structure_extraction}

FinGEAR extracts fine-grained structure within each 10-K Item. While SEC-mandated itemization provides a high-level layout~\cite{alberg1970sec}, each Item can span dozens of pages and include heterogeneous content. To support retrieval at multiple granularities, FinGEAR builds semantic trees within each Item using topic-based clustering. This hierarchical organization is depicted in Figure~\ref{fig:pre_retrieval} as the Summary/Question Tree index built during pre-retrieval.

\paragraph{Chunking and Encoding}
Each Item is segmented into approximately 2{,}000-token chunks with a 100-token overlap (see Appendix~\ref{appendix:token_statistics}, Table~\ref{tab:token_statistics}). We encode each chunk with sentence embeddings fine-tuned on financial data, yielding a domain-aligned representation space.

\paragraph{Hierarchical Expansion via Topic Clustering}
We then build the hierarchy bottom–up: UMAP is first applied for dimensionality reduction, followed by Gaussian Mixture Models (GMM) for soft clustering. Leaves correspond to the original chunks; each internal node represents a soft cluster and is summarized to guide top-down traversal. The result is a content hierarchy that captures themes from coarse to fine granularity (hyperparameters in Section~\ref{sec:experiments_setup}; schematic in Figure~\ref{fig:pre_retrieval}). An example of the resulting hierarchy is shown in Figure~\ref{fig:tree_printout}.

\paragraph{Summary and Question Trees (Outputs)}
Structure extraction yields two structurally identical hierarchical indices. The \textit{Summary Tree} stores node summaries at internal nodes and the original text chunks at leaves. The \textit{Question Tree} mirrors the same topology but stores LLM-generated sub-questions (embedded in the same query space) at internal nodes; its leaves point to the same chunk IDs as the Summary Tree. We derive the Question Tree by generating sub-questions for each Summary-Tree node and embedding them with a FinQA-aligned encoder. Sharing topology while differing in node content enables hybrid sparse–dense traversal during retrieval. Fine-tuning and further construction details appear in Appendix~\ref{appendix:embedding_finetuning}.

\subsubsection{Financial Lexicon-Aware Mapping (FLAM)}

FLAM provides domain-aware guidance by weighting Items before traversal. Candidate terms are extracted using a rule-based method~\cite{singh2017replaceretrievekeywordsdocuments} from a curated subset of the FinRAD lexicon~\cite{ghosh2021finrad}, and then clustered with sentence embeddings fine-tuned on financial language (Appendix~\ref{appendix:embedding_finetuning}). Each relevant term is assigned a weight using \textit{Relative Frequency}:
\[
\text{weight}(k_i) = \frac{\text{count}(k_i)}{\sum_j \text{count}(k_j)},
\]
chosen for its interpretability and robustness across heterogeneous filings. (Ablations appear in Section~\ref{sec:results} and Section~\ref{sec:ablation_study}.)

\vspace{0.35\baselineskip}
\noindent\textbf{FLAM vs.\ structure extraction.}
FLAM operates at the corpus level: it clusters \emph{lexical terms} (FinRAD-tuned embeddings) and converts them into per-Item \emph{weights} that allocate the global retrieval budget.
Structure extraction operates within each 10-K Item: it clusters \emph{text chunks} (FinQA-tuned embeddings) to build \emph{local trees} used for traversal.
In short, FLAM decides \emph{where} to look across Items, while structure extraction decides \emph{how} to search within an Item.

\subsection{In-Retrieval Pipeline}
\label{sec:in_retrieval}
% \textcolor{red}{Tiejun: can this name be a bit more clear}

At query time, FLAM first identifies Items that are likely to contain relevant content. Within these Items, dual-tree traversal retrieves candidate passages: the \textit{Summary Tree} provides sparse, high-level routing; the \textit{Question Tree} provides dense, query-specific refinement. Figure~\ref{fig:in_retrieval} illustrates this process for an example FinQA query: \emph{“What was JPMorgan Chase \& Co.’s CET1 ratio in 2008?”}.

\begin{figure}[t]
\centering
\setlength{\fboxsep}{6pt}
\fbox{%
\begin{minipage}{0.92\linewidth}
\ttfamily\small
Key Performance (\#docs=0, \#qn=5)\\
├── Revenue Trends (\#docs=0, \#qn=5)\\
│   ├── YoY growth (\#docs=3, \#qn=5)\\
│   └── Segment revenue (\#docs=4, \#qn=5)\\
└── Capital Ratios (\#docs=0, \#qn=5)\\
    ├── CET1 ratio (\#docs=2, \#qn=5)\\
    └── Tier 1 capital (\#docs=3, \#qn=5)\\
\end{minipage}}
\caption{Example tree printout. Internal nodes are summaries; leaves are retrievable chunks. “\#qn” is the number of sub-questions stored at the corresponding Question Tree node.}
\label{fig:tree_printout}
\vspace{-0.6em}
\end{figure}

\subsubsection{Global Navigation}

Global Navigation selects which SEC Items to search before any tree traversal. Using FLAM, we (i) expand the query into clusters of related financial terms, (ii) measure how strongly each Item is associated with those clusters to obtain normalized weights \(w_i\), and (iii) convert the weights into per-Item retrieval budgets \(k_i^{\ast}\) such that \(\sum_i k_i^{\ast}=k\). Items with \(k_i^{\ast}=0\) are skipped; the rest proceed to within-Item traversal.

\noindent\textbf{Procedure.}
\begin{enumerate}[leftmargin=*, itemsep=2pt, topsep=2pt]
    \item \textit{Term detection and expansion.} Extract salient financial terms from the query (e.g., \textit{CET1}, \textit{capital ratio}) and expand them using FLAM’s lexicon clusters (FinRAD-tuned embeddings) to capture close variants.
    \item \textit{Map terms to Items.} For each expanded term cluster, count its occurrences in each SEC Item across the recovered filings and aggregate these counts per Item. Convert counts to normalized Item weights
    \[
      w_i \;=\; \frac{\mathrm{freq}_i}{\sum_j \mathrm{freq}_j}\,,\qquad \sum_i w_i = 1,\; w_i \in [0,1].
    \]
    \item \textit{Allocate the budget.} Given a total retrieval budget \(k\), assign an Item-level budget
    \[
      k_i^{\ast} \;=\; \mathrm{round}\!\big(k \cdot w_i\big)
    \]
    (with a final adjustment so that \(\sum_i k_i^{\ast}=k\); ties are broken by larger \(w_i\)).
    \item \textit{Handoff to traversal.} For each Item with \(k_i^{\ast}>0\), pass the Item and its budget to the within-Item search over the Summary and Question Trees.
\end{enumerate}

\noindent Item weights \(w_i\) (e.g., \(w_{1}\), \(w_{1\mathrm{A}}\), \(w_{7}\)) are illustrated in Figure~\ref{fig:flam_weights} to match the notation used in Section~\ref{sec:in_retrieval}. For example, with \(k{=}10\): Item~7 receives \(k^{\ast}{=}6\), Item~1 \(k^{\ast}{=}2\), and Item~1A \(k^{\ast}{=}2\).

\subsubsection{Within-Item Search}

Within each selected Item, we traverse two trees with identical topology, the \emph{Summary Tree} and the \emph{Question Tree}. The structure (parent–child layout and leaf set) is the same; what differs is the node content used for scoring during traversal: Summary-Tree nodes carry summaries, while Question-Tree nodes carry LLM-generated sub-questions embedded in the query space. Leaf nodes in both trees reference the same chunk IDs, so leaf hits are merged and deduplicated into a single candidate pool.

\begin{itemize}
    \item \textit{Summary-based retrieval (sparse):} nodes are matched to the query using a bag-of-words scorer over node \emph{summaries}. This favors sections dense in relevant financial terms and headings.
    \item \textit{Question-based retrieval (dense):} nodes are matched using cosine similarity between the query embedding and LLM-generated \emph{sub-questions} stored at each node (financial QA–tuned embeddings).
\end{itemize}

\noindent\textit{Scoring and stopping.} The two trees are traversed independently. In the Summary Tree, nodes are scored with BM25 over node summaries (sparse signal). In the Question Tree, nodes are scored by cosine similarity between the query and each node’s sub-questions (dense signal). At each internal node we expand only the top \(b\) children by that node’s score (default \(b{=}3\) in all experiments). This per-node child limit is separate from the per-Item budget \(k_i^{\ast}\). We continue descending until leaves. Because the trees are built with depth at most 2 (Section~\ref{sec:experiments_setup}), traversal reaches leaves in at most two steps. All leaves visited across both trees form the Item’s candidate pool, from which we select up to \(k_i^{\ast}\) evidence chunks.

\paragraph{Reranking.}
\textbf{Stage 1 (cross-tree):} merge and rerank candidates from the Summary and Question traversals with a cross-encoder (\emph{BAAI/bge-reranker-large}, XLM-R-Large, 560M).\\
\textbf{Stage 2 (cross-Item):} rerank the top spans across different Items to prioritize globally informative, coherent answers.
This two-stage reranking sharpens precision while preserving coverage for downstream QA.

Finally, we select up to \(k_i^{\ast}\) highest-scoring unique chunks for that Item and pass them to reranking.