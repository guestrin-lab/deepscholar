
\section{Methods} 

% This section is organized as follows. Section~\ref{sec: hierarchical-frame} first outlines our \textbf{hierarchical control framework} including low-level locomotion policy and high-level world model planner. Section~\ref{sec:wm} then details the architecture and training process of our \textbf{world model}. Finally, Section~\ref{sec:mpc} explains how this learned model is leveraged by our \textbf{value-guided sampling MPC} framework for high-level planning.

This section begins by outlining our \textbf{hierarchical control framework} in Section~\ref{sec: hierarchical-frame}, composed of a low-level policy and our high-level planner. The subsequent sections then detail the two components of this high-level planner: the \textbf{world model's architecture and training} (Section~\ref{sec:wm}), and the \textbf{value-guided sampling MPC} (Section~\ref{sec:mpc}).

\begin{figure*}
    \centering
    \includegraphics[width=0.98\linewidth]{fig/fig_mpc5.png}
    \caption{\textbf{Value-Guided Sampling MPC.} This figure illustrates how the trained world model is used for planning via value-guided sampling MPC. This process performs open-loop prediction to find the best action sequence starting from a single real observation. At inference time, this process begins by encoding the current observation $o_t$ into its latent state $z_t$, after which the planner samples a batch of $M=1024$ candidate action sequences over a planning horizon of $N = 4$ steps. The world model predicts the future latent state ($h_{t+k}, \hat{z}_{t+k}$) by recursively applying its learned dynamics model. At each prediction step, the surrogate value ($\hat{Q}_{t+k}$) evaluates the sampled actions, while the termination signal, $\hat{d}_{t}$, predicts the probability of robot failure, such as falling; if this probability exceeds a threshold of 0.9, all subsequent value estimates, $\hat{Q}$, for that trajectory are set to zero. The planner evaluates $M$ candidate trajectories, where the score for each trajectory is calculated by the objective function $\hat{J}_N$ in Eq.~\eqref{eq: final-obj}. This set of scored trajectories is then optimized using the Cross-Entropy Method (CEM) to find the optimal action sequence.}
    \label{fig:mpc}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Hierarchical Framework and Data Collection}
\label{sec: hierarchical-frame}
Our framework consists of two components: a \textbf{low-level whole-body policy} capable of tracking diverse commands, and a \textbf{high-level planner} whose action specifies which commands to track, as shown in Fig.~\ref{fig:wm}.
% Our method is applied to vision-based, whole-body control via a high-level interface for contact planning. To reduce sampling dimensionality, we use a hierarchical approach. 

The low-level controller tracks commands $c = \left[v^{\top}, p_{ee}^{\top}, h_{body}\right]^{\top}$, where $v$ is the desired linear velocity, $p_{ee}$ the desired end-effector position, and $h_{body}$ the body height, while maintaining balance. 
Its observation space is purely proprioceptive, including angular velocity ($\omega$), the projected gravity vector ($g$), the command ($c$), joint positions ($q$), and joint velocities ($\dot{q}$).
The controller is represented by a deep neural network and is trained in simulation using PPO, following established approaches~\cite{NVIDIA_Isaac_Sim, LeggedLab}. 

Once a reliable low-level controller is obtained, the high-level planner is designed to account for both contact and task planning. It specifies actions $a_t = \left[p_{ee}^{\top}, h_{body}\right]^{\top}$ to the low-level controller based on an observation space $o_t$ combining proprioception (as in the low-level controller) with visual input from a downsampled 64×48 ego-centric depth image. We exclude base velocity $v$ from the planner's action space to force the robot to solve contact-rich problems through postural manipulation, rather than learning locomotion-based evasion strategies. The high-level planner consists of a world model and a sampling-based MPC framework.
We train the world model from the offline dataset and, at inference time, use it to evaluate $M$ candidate action sequences over an $N$-step horizon and optimize the action $a_t$.



% The high-level planner for contact and task planning is formulated as an offline RL algorithm. 
Our offline dataset, $\mathcal{D}$,  is generated by collecting trajectories, $\tau$, from the simulated humanoid. At each timestep, the robot receives an observation $o_t$, executes a randomly sampled action $a_t$, and in return, receives the reward $r_t$ and termination signal $d_t$ from the environment. These collected transition tuples $\{o_t, a_t, r_t, d_t\}$ are then stored in a final trajectory dataset structured as \texttt{[Batch, Time, Data]}.
At each step, we sample the finite differences of planner actions $a_t = a_{t-1} + \eta \cdot \delta $ from $\delta \sim \mathcal{U}(-1, 1)$, where $\eta$ is a scalar step that controls the magnitude of the update. This step is performed after normalizing the task space $\left[p_{ee}^{\top}, h_{body}\right]^{\top}$ of the low-level controller. $\eta$ is set to $0.32$. 
The purpose of using such a method to sample the actions is to (1) avoid using any demonstration, which is expensive to obtain for the whole-body commands of a humanoid, and (2) heuristically avoid ineffective and jitter behavior data far outside the command range.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{fig/exp_all.pdf}
    \caption{Real-World experiments validating the proposed framework. (a) A demonstration of sequential task execution and generalization, where the robot traverses an arch (i) and then blocks a previously unseen box (ii). (b) Support the wall to maintain balance by bracing the wall with the hands when pushed towards the wall. (c) Blocking both an in-distribution ball (with a size consistent with the training data) and an unseen box; (d) Squat and traverse an arch.}
    \label{fig:exp_all}
    \vspace{-15pt}
\end{figure*}

\subsection{Ego-Vision Humanoid World Model}
\label{sec:wm}
% Prior methods have utilized auto-regressive models to learn system dynamics from existing controller, for tasks such as velocity tracking\cite{li2025offline, li2025robotic}. However, in complex systems that rely on high-dimensional, image observations, this approach suffers from compounding errors. Even minor inaccuracies in single-step predictions can accumulate, leading to an exponential degradation in accuracy over long prediction horizons\cite{lecun2022path}. This problem is exacerbated in scenarios involving contact, where it is not only difficult to infer the precise state from observations but also intractable to define a desired goal trajectory directly in the pixel space.
Prior auto-regressive models learn system dynamics by mimicking existing controllers for continuous tasks like velocity tracking~\cite{li2025offline}. However, when applied to high-dimensional image observations, this pixel-prediction approach suffers from compounding errors over long horizons. This issue is exacerbated in \textbf{contact-aware} scenarios where defining a goal trajectory in pixel space is often intractable.

To address this, we draw inspiration from general world models like Dreamer~\cite{hafner2023mastering} and JEPA~\cite{assran2025v}. We focus on predicting abstract latent of future observations, enabling the model to capture more fundamental structures within the data. As illustrated in Fig. \ref{fig:wm}, our world model is composed of several key components detailed below.

First, the world model leverages a recurrent neural network (RNN) to maintain a deterministic dynamics latent state $h_t$, which summarizes dynamics information across time. 
At each step, a stochastic latent state $z_t$, which extracts the abstract latent of the current observation, is inferred from the current observation $o_t$ and the latent state $h_{t}$. 
Similar to an autoencoder, the model is trained to reconstruct the observation $o_t$ as $\hat{o}_t$ after passing it through a latent bottleneck, which compels the latent state $z_t$ to encode the most salient and abstract features. For notational simplicity, we let $\phi$ denote the parameters of all world model components, with $q_{\phi}$ and $p_{\phi}$ representing the encoder and decoder, respectively. The overall process can then be expressed as:
\begin{align}
    h_t &:= f_{\phi}(h_{t-1}, z_{t-1}, a_{t-1}) \\
    z_t &\sim q_{\phi}(z_t \mid h_t, o_t) \\
    \hat{o}_t &\sim p_{\phi}(\hat{o}_t \mid h_t, z_t).
\end{align}
We assume $z_t$ and $\hat{o}_t$ to follow the Gaussian distribution.


In addition, we introduce a model that estimates the stochastic latent without using the current observation: given $h_t$, it yields a latent $\hat{z}_t \sim p_{\phi}(\hat{z_t} \mid h_t)$ that closely approximates $z_t$, thereby enabling rollouts in latent space.

Different from Dreamer~\cite{hafner2023mastering}, we need to consider an architecture that better addresses the challenges unique to robotics in the real world, such as (1) significant partial observability, (2) high sensor noise, and (3) sparse contact. These factors make it difficult to predict contact-aware rewards from the observation. Therefore, we design specialized heads that, conditioned on the latent state $(h_t, z_t)$ and a candidate action $a_t$, directly estimate the expected long-term outcome. Specifically, we predict a termination probability $\hat{d}_t$ and a surrogate value $\hat{Q}_t$, which represents the expected cumulative return. This allows the robot to evaluate the potential response to different actions directly from its learned latent state. 
\begin{align}
    \hat{d}_t &\sim D_{\phi}(\hat{d}_t \mid h_t, z_t), \\
    \hat{Q}_t &:= Q_\phi(h_t, z_t, a_t).
\end{align}

% On-policy algorithms, such as PPO, present significant challenges for multi-task learning. Defining a unified reward function across diverse tasks is often intractable, and implementing a sampling curriculum for varied object interactions requires substantial engineering effort, particularly within parallelized simulation environments.

Our surrogate value could condition on the latent state $z_t$, allowing the robot to infer the current task context from its observations and dynamically adapt its objective. This enables us to train the model directly on a mixed dataset containing data from all tasks. We opt for a computationally efficient design consisting of a CNN for image feature extraction and MLPs for all other components.

% The training of our world model is governed by a unified objective:
% \begin{equation}
%     \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{rec}} + \mathcal{L}_{\text{jep}} + \mathcal{L}_{Q}
% \end{equation}

% \begin{equation}
%     \mathcal{L}_{\text{pred}} = \mathbb{E}_{\hat{\mathbf{z}}_t \sim q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t)} \left[ \mathcal{L}_{\text{obs}} + \mathcal{L}_{\text{ext\_obs}} + \mathcal{L}_{\text{term}} \right]
% \end{equation}

% \begin{equation}
%     \mathcal{L}_{Q} = \mathbb{E}_{\hat{\mathbf{z}}_t \sim q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t)} \left[ \left(\hat{Q}_{\psi}(\mathbf{h}_t, \mu(\hat{\mathbf{z}}_t), \mathbf{a}_t) - Q_{\text{target}}\right)^2 \right]
% \end{equation}

% % \begin{equation}
% %     Q_{\text{target}} = \sum_{k=t}^{T} \gamma^{k-t} r_k
% % \end{equation}

% \begin{align}
% \label{eq:jep_loss}
% \mathcal{L}_{\text{jep}} = D_{\text{KL}}\Big( \text{sg}\big(q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t)\big) \ \big|\big| \ p_{\phi}(\mathbf{z}_t|\mathbf{h}_t) \Big) \nonumber \\
% + D_{\text{KL}}\Big( q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t) \ \big|\big| \ \text{sg}\big(p_{\phi}(\mathbf{z}_t|\mathbf{h}_t)\big) \Big)
% \end{align}

The model is optimized by minimizing the single-step loss as shown below. This total loss is a simple sum of three main components: a reconstruction loss ($\mathcal{L}_{\text{rec}}$), a joint-embedding predictive loss ($\mathcal{L}_{\text{jep}}$), and a Q-loss ($\mathcal{L}_{\hat{Q}}$):

\begin{equation}
\label{eq:total_loss}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{rec}} + \mathcal{L}_{\text{jep}} + \mathcal{L}_{\hat{Q}}.
\end{equation}


The reconstruction loss, $\mathcal{L}_{\text{rec}}$, ensures the world model can extract a tight latent space of the environment. It is defined as a combination of a Negative Log-Likelihood (NLL) for the observation reconstruction ($\mathcal{L}_{\text{obs}}$) and a Binary Cross-Entropy (BCE) loss for the termination signal ($\mathcal{L}_{\text{term}}$):
\begin{equation}
\label{eq:reconstruction_loss}
\mathcal{L}_{\text{rec}} = \mathbb{E}_{\hat{\mathbf{z}}_t \sim q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t)} \left[ \mathcal{L}_{\text{obs}} + \mathcal{L}_{\text{term}} \right].
\end{equation}
The joint-embedding predictive loss, $\mathcal{L}_{\text{jep}}$, consists of two KL divergence terms that enforce a consistent and non-collapsing latent space~\cite{chen2021exploring,hafner2023mastering}. 
\begin{align}
\label{eq:jep_loss}
\mathcal{L}_{\text{jep}} = D_{\text{KL}}\Big( \text{sg}\big(q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t)\big) \ \big|\big| \ p_{\phi}(\mathbf{z}_t|\mathbf{h}_t) \Big) \nonumber \\
+ D_{\text{KL}}\Big( q_{\phi}(\mathbf{z}_t|\mathbf{o}_t, \mathbf{h}_t) \ \big|\big| \ \text{sg}\big(p_{\phi}(\mathbf{z}_t|\mathbf{h}_t)\big) \Big),
\end{align}
%
where sg is the stop-gradient operator.

The surrogate value loss, $\mathcal{L}_{\hat{Q}}$, is a mean-squared error term that trains the value function to estimate the target $Q_{\text{target}}$. We simply apply a Monte Carlo (MC) estimator here to get $Q_{\text{target}}$, and we empirically found that using an MC estimator yields more stable results in our scenario than TD-error:
\begin{equation}
\label{eq:q_loss}
\mathcal{L}_{\hat{Q}} = \mathbb{E}_{\mathbf{\tau} \sim \mathbf{D}} \sum_{t}\mathbb{E}_{\mathbf{z}_t \sim q_{\phi}(\cdot| \mathbf{o}_t, \mathbf{h}_t)}\left[ \left({Q}_{\phi}(\mathbf{h}_t, \mathbf{z}_t, \mathbf{a}_t) - Q_{\text{target}}\right)^2 \right].
\end{equation}


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/fig_data_efficient2.png}
    \caption{Sample efficiency comparison: Our method uses an offline dataset collected from random action, while PPO collects data from environments at every iteration. The x-axis represents the number of step transitions used, while the y-axis shows the reward for each task. A greater value on the x-axis indicates a larger amount of data used, and a higher value on the y-axis signifies better performance. While our method utilizes a dataset of at most 1M steps, we continued to train PPO for a greater number of steps to determine when it could achieve comparable performance. }
    \label{fig:exp-data-effi}
    \vspace{-15pt}
\end{figure*}

\vspace{-10pt}

\subsection{Value-Guided Sampling MPC}
\label{sec: mpc}

In practical applications involving complex robotics and perception, learning a perfect, optimal state-action value function and greedily maximizing it remains a significant challenge, which is further compounded when extending to observation-action value functions. This difficulty stems from two primary sources:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Challenges in Offline Learning:} Finite offline datasets provide incomplete coverage, leading to unreliable value estimates for out-of-distribution actions.
    \item \textbf{Partial Observability and Physical Non-idealities:} Robotic systems suffer from partial observability, as the full state, such as contact forces, is not observable. is not directly measured, and is subject to sensor noise and action delays, both of which degrade value estimation.
\end{enumerate}
To address this, we introduce a Value-Guided Sampling MPC framework. This approach explicitly treats the learned value function not as an optimal oracle, but as a powerful, albeit imperfect, heuristic to guide a robust, receding-horizon planning process.
% Though the Q function-based control framework is assumed to provide the optimal control input, in practice, the approximated Q functions are fitted from samples. The high variance of the estimated Q function may result in performance degradation. In this section, we propose to mitigate this issue by optimizing a surrogate loss instead of the original Q functions. 


% \paragraph{Monte Carlo vs. TD-target for Q-fitting.}
% Unlike prior works that often rely on online data, our approach uses a purely offline dataset, which aligns with modern offline reinforcement learning techniques. Given that our dataset is collected with a uniform action distribution and we do not update this distribution, a TD-target would introduce significant distributional shift bias. Furthermore, our optimization method is a sampling-based MPC. As discussed in \cite{fujimoto2022should}, while the TD error can effectively guide policy gradients, its absolute value estimation is less accurate than that of a Monte Carlo estimator, which provides a more reliable objective for our value-based optimization.




Consider a standard MDP with state $s_t$ and action $a_t$ for the analysis of the variance reduction of our proposed method. We denote the estimation of the true $Q$ functions as $\hat{Q}$. When the $Q$ is available, we can leverage the Bellman principle to obtain the optimal control input:
\begin{equation}
\label{eq:greedy_policy}
\pi(s_t) = \arg\max_{a_t} {Q}(s_t, a_t). 
\end{equation}
However, in practice, only the estimation $\hat{Q}$ is available, which may have a large variance. The imperfection of $\hat{Q}$ may lead to performance degradation. To mitigate this issue, instead of directly optimizing on the estimated Q function, we consider an $N$-step surrogate optimization:
\begin{equation}
\begin{aligned}
    \hat{\pi}(s_t) = \arg\max_{\{a_{0:N-1}\}}& \hat{J}_N := \frac{1}{N}\sum_{t=0}^{N-1} \hat{Q}(s_t, a_t) \\
    \mathrm{s.t. } & \quad s_{t+1} = f(s_t, a_t)
\end{aligned}
\label{eq:obj}
\end{equation}
% \junfeng{
% Should be:
% \begin{equation}
% \begin{aligned}
%     \hat{\pi}(s_t) &= \arg\max_{\{a_{0:N-1}\}} \hat{J}_N := \frac{1}{N}\sum_{t=0}^{N-1} \hat{Q}(s_t, a_t) \\
%     \mathrm{s.t }& \quad s_{t+1} = f(s_t, a_t)
% \end{aligned}
% \end{equation}
% }
By the definition of the Q function, $\hat{\pi}(\cdot)$ and $\pi^*(\cdot)$ are obtained by solving two different optimal control problems, thus they are in general not identical. Despite the biases, we show that the surrogate loss can reduce the variance. 

Consider the residual between $\hat{Q}$ and $Q$ as $\epsilon_Q$, and $$\hat{Q}(s_t, a_t) = Q(s_t, a_t) + \epsilon_Q(s_t, a_t).$$ For shorthand notation, we use the subscript $(t)$ to denote the quantity at time step $t$. We have the surrogate loss function:
\begin{equation}
    \frac{1}{N}\sum_{t=0}^{N-1} \hat{Q}_t = \frac{1}{N}\sum^{N-1}_{t=0}\left(Q_t + \epsilon_{t}\right).
\end{equation}
% We assume that the variance of $\epsilon_Q$ is bounded by $\operatorname{Var}[\epsilon_Q] \le V$.
As we use Monte Carlo estimation to compute $\hat{Q}$ as an unbiased estimation, the mean of $\epsilon_t$ is zero, and thus the variance on the RHS can be obtained by
\begin{equation}
\begin{aligned}
    &\operatorname{Cov}[\hat{J}_N]=\operatorname{Var}[\frac{1}{N}\sum^{N-1}_{t=0}\epsilon_{t}] = \frac{1}{N^2}\sum_{(i, j)} \operatorname{Cov}[\epsilon_{ i}, \epsilon_{j}]. \\
\end{aligned}
\end{equation}
We assume that the variance of $\epsilon_t, \forall t,$ satisfies: \begin{equation}
    0 \le V_{\min} \le \operatorname{Var}[\epsilon_{ t}] \le V_{\max} \tag{Bounded Variance}
\end{equation}
and the correlation is bounded by $\rho < 1$, such that $\forall i, j$ \begin{equation}
    |\operatorname{Cov}[\epsilon_i, \epsilon_j]| \le \rho \operatorname{Var}[\epsilon_i]\operatorname{Var}[\epsilon_j].  \tag{Bounded Correlation}
\end{equation} 
% On the condition that the $\hat{Q}(s_t, a_t)$ are uncorrelated at step $i \neq j$, we have the lower bound covariance
% \begin{equation}
% \operatorname{Var}[\frac{1}{N}\sum^{N-1}_{t=0}\epsilon_{t}] \ge \frac{1}{N^2}\sum_i \operatorname{Var}[\epsilon_{ i}] \ge \frac{V_{\min}}{N}
% \end{equation}

The upper bound variance is achieved if all steps are positively correlated: 
\begin{equation}
\begin{aligned}
        % \operatorname{Var}[\frac{1}{N}\sum^{N-1}_{t=0}\epsilon_{ t}]  & 
        \operatorname{Cov}[\hat{J}_N] \le \frac{N+\rho(N^2-N)}{N^2} V_{\max} =: V_{\operatorname{ub}}.
        % \frac{1}{N^2}N^2 \max_{(i, j)}\operatorname{Cov}[\epsilon_{ i}, \epsilon_{ j}] = V_{\max}.
\end{aligned}
\end{equation}
For the lower bound, similarly, we have:
\begin{equation}
\begin{aligned}
                % \operatorname{Var}[\frac{1}{N}\sum^{N-1}_{t=0}\epsilon_{ t}]  & 
                \operatorname{Cov}[\hat{J}_N] \ge \max \left\{ \frac{N V_{\min} - \rho(N^2-N)V_{\max}}{N^2}, 0 \right\} =: V_{\operatorname{lb}}.
\end{aligned}
\end{equation}
Thus, we have limit when $\rho \rightarrow 0: V_{\operatorname{ub}} \rightarrow \frac{V_{\max}}{N}, V_{\operatorname{lb}} \rightarrow \frac{V_{\min}}{N}.$ As we compute $\hat{Q}$ from an offline dataset generated by random action in each step, the correlation between $\hat{Q}_i$ from different time steps is weaker than by sampling using a state feedback policy. Thus, it is possible that the $\rho$ is small and thus $\hat{J}_N$ has a substantially lower variance than $\hat{Q}$. Although additional bias is introduced because the surrogate loss does not preserve the local optimum, if the variance of $\hat{Q}$ dominates, this strategy can significantly improve the performance. 

% \mgj{How does it affect convergence and stability? How good a strategy is it?}

% On condition that $N\rightarrow \infty$, $V_{\operatorname{ub}}\rightarrow \rho V_{\max} $ and $V_{\operatorname{ub}}\rightarrow \rho V_{\max}$

% Thus, we show that the $N$-step strategy improves the estimation of the cost by
% \begin{equation}
%     \frac{1}{N}\operatorname{Var}{\hat{Q}} \le \operatorname{Var}[\hat{J}_N] \le \operatorname{Var}{\hat{Q}}.
% \end{equation}


% \junfeng{I think here might be a problem, $\max_{(i, j)}\operatorname{Cov}[\epsilon_{ i}, \epsilon_{ j}]$ is not necessarily less than $\operatorname{Var}[\epsilon_{ 0}]$. Thus, better stability is not guaranteed.}

% \begin{table}[h]
% \renewcommand{\arraystretch}{2} 
% \centering
% \caption{Comparison of the variance of different cost functions. Assume the residual of the learned reward is $\epsilon_r := \hat{r} - r^*$. The residual of the learned value function is $\epsilon_{TD} := \hat{Q}_{TD} - Q^*$ that has non-zero bias compared with MC estimation of Q. \tsl{Will check the consistency of notations.}}
% \begin{tabular}{ccc}
% \hline
% Methods & Cost Function & Lower Bound Variance \\
% \hline
% Q + MC & $\frac{\sum_{k=0}^{H-1} \hat{r}_k + \hat{Q}_{MC,H}}{H+N}$ & $\frac{\operatorname{Var}[\epsilon_r]}{H+N}$ \\ \hline 
% Q + TD & $\sum_{k=0}^{H-1} \hat{r}_k + \hat{Q}_{TD,H}$ & $\frac{H \operatorname{Var}[\epsilon_r] + \operatorname{Var}[\epsilon_{TD}]}{H+1}$ \tsl{Scale?} \\ \hline
% Reward & $\frac{1}{H}\sum_{k=0}^{H-1} \hat{r}_k$ &  $\frac{\operatorname{Var}[\epsilon_r]}{H}$ \\ \hline 
% \textbf{Bootstrap Q + MC}& $\frac{1}{H*N}\sum_{k=0}^{H-1} \hat{Q}_{MC,k}$ & $\frac{\operatorname{Var}[\epsilon_r]}{H*N}$  \\
% \hline
% \end{tabular}
% \label{table:cons_2nd_retraction}
% \end{table}


% Unlike previous work~\cite{roth2025learned,hafner2023mastering}, which optimized future accumulated reward or risks, we have found that the long-term value is significantly less sensitive to noise than a direct accumulative reward. 

Based on the above analysis, we apply the surrogate \textbf{objective function} $\hat{J}_N$ from ~\eqref{eq:obj} using the latent variables $h_t$ and $\hat{z}$ as the representation of the robot states. The optimal sequence, denoted $A_t^* = \{a_t^*, a_{t+1}^*, ..., a_{t+N-1}^*\}$, is the one that maximizes our surrogate objective $\hat{J}_N$:
\begin{equation}
\begin{aligned}
    A_t^* = \arg\max_{A_t} &\ \ \frac{1}{N}\sum_{k=0}^{N-1} Q_\phi(h_{t+k}, \hat{z}_{t+k}, a_{t+k}), \\
    \mathrm{s.t.} & \quad h_{t+k+1} = f_{\phi}(h_{t+k}, \hat{z}_{t+k}, a_{t+k}), \\
                 & \quad \hat{z}_{t+k} \sim p_{\phi}(\hat{z}_{t+k} \mid h_{t+k}).
\label{eq: final-obj}
\end{aligned}
\end{equation}
As shown in Fig. \ref{fig:mpc}, the framework’s memory is maintained in two latent states: a dynamics latent state $h_t$ and a current observation latent state $z_t$. The $h_t$ is computed by the RNN from the previous latent ($h_{t-1}, z_{t-1}$) and the last action ($a_{t-1}$). The $z_t$ is then generated in one of two ways: when an observation $o_t$ is available, $z_t$ is inferred using both the latent state $h_t$ and the observation $o_t$; for future predictions, it is generated from the latent state $h_t$ alone.

Our world model also predicts the probability of robot failure, $\hat{d}_{t}$, such as falling; if this probability exceeds a threshold of 0.9, all subsequent value estimates for that trajectory are set to zero.

We use the Cross-Entropy Method (CEM) to find optimal action sequence $A_t^*$. Once identified, only the first action $a_t^*$ is executed. This iterative re-planning allows the robot to continuously incorporate feedback from the environment, enabling it to react to disturbances and correct for model inaccuracies in real-time. From Table~\ref{tab:results}, we select a planning horizon $N=4$ as our default, as it provides the best overall performance across all tasks.


% For a theoretical foundation, let us assume first we can learn both a perfect world model $f$ and a perfect Q-function $Q$ that precisely satisfies the Bellman optimality equation. In this ideal scenario, the optimal greedy policy at time $t$ is to select the action that maximizes the Q-value:
% \begin{equation}
% \label{eq:greedy_policy}
% \pi^*(s_t) = \arg\max_{a_t} Q^*(s_t, a_t)
% \end{equation}
% Furthermore, since the Q-function satisfies the global optimality condition, the trajectory of actions that maximizes the cumulative future Q-values is equivalent to the one-step greedy action at time $t$. This means both solutions converge to the same global optimum.
% \begin{equation}
% \label{eq:optimality_equivalence}
% \arg\max_{a_t} Q^*(s_t, a_t) = \Pi_0 \left( \arg\max_{\mathbf{a}_{t:t+H}} \mathbb{E} \left[ \sum_{k=0}^{H} Q^*(s_{t+k}, a_{t+k}) \right] \right)
% \end{equation}

% However, in practice, we can neither obtain a perfect Q-function nor a perfect dynamics model. An imperfect dynamics model can, however, still be leveraged to reduce the variance of the Q-function.\textbf{PROOF}

% Therefore, we propose Q-bootstrapped MPC, which directly optimizes the accumulated future surrogate Q-values over a receding horizon. The objective is to find the optimal action sequence $\mathbf{a}_{t:t+H-1}$ that maximizes the predicted cumulative Q-values:
% \begin{equation}
% \label{eq:q_mpc_objective}
% \max_{\mathbf{a}_{t:t+H}} \mathbb{E} \left[ \sum_{k=0}^{H} Q(s_{t+k}, a_{t+k}) \right]
% \end{equation}
% This optimization problem is efficiently solved using a sampling-based method, specifically the Cross-Entropy Method (CEM) \cite{ref_cem_paper}.





\label{sec:mpc}
