\section{Introduction}


% \mgj{The central insight of the paper is that by combining a learned ego-centric world model with value bootstrapped sampling-based MPC, humanoid robots can robustly and efficiently learn and plan complex contact-rich behaviors from offline, demonstration-free data.}

% The expectation of humanoids is rapidly advancing from achieving dynamic locomotion~\cite{raibert1986legged,rudin2022learning,hwangbo2019learning} to exhibiting intelligent physical interaction with complex, unstructured environments. A critical step towards this goal is moving beyond mere contact avoidance to a paradigm of purposeful contact exploitation. To be truly effective, a humanoid must learn to use its body to intelligently interact with the world as humans do, \textit{e.g.,} bracing a hand against a wall to recover balance, blocking an incoming object for safety, or ducking under an obstacle to make progress. Mastering such contact-aware skills is fundamental to unlocking the next level of autonomy, robustness, and physical intelligence in robotic systems.

Humanoids are expected to advance from dynamic locomotion~\cite{raibert1986legged,rudin2022learning} to intelligent interaction in complex, unstructured environments. Achieving this requires purposeful contact exploitation rather than simple avoidance. Effective humanoids must use their bodies to interact with the world as humans do, such as bracing against a wall for balance, blocking objects for safety, or ducking under obstacles. These contact-aware skills are essential for greater autonomy, robustness, and physical intelligence in robots.






% However, enabling a humanoid to wisely reason about contact is an outstanding challenge \cite{jenelten2024dtc, roth2025learned, liu2025discrete}. Traditional optimization-based approaches \cite{sleiman2021unified,winkler2018gait}, while grounded in rigorous mathematics, struggle with the combinatorial complexity of \emph{real-time} contact scheduling and are sensitive to inaccuracies in system models, which limits their adaptability to unforeseen situations. 

Reasoning about contact remains challenging for humanoids~\cite{jenelten2024dtc, roth2025learned, liu2025discrete}. Traditional optimization-based methods~\cite{sleiman2021unified,winkler2018gait} struggle with the combinatorial complexity of \emph{real-time} contact scheduling and are sensitive to model inaccuracies, reducing adaptability to unforeseen situations.
% Since the introduction of parallelized simulation \cite{NVIDIA_Isaac_Sim}, on-policy RL has achieved remarkable success in robot control \cite{rudin2022learning,}, including quadruped \cite{lee2020learning,cheng2024extreme}, bipedal \cite{li2021reinforcement, li2025reinforcement} and humanoid robots \cite{truong2025beyondmimic, radosavovic2024real}. But they are sample-inefficient, particularly when learning from visual inputs \cite{cheng2024extreme}. Furthermore, online RL often struggles with multi-task learning and navigating complex scene interactions.
Parallelized simulation~\cite{NVIDIA_Isaac_Sim} has enabled on-policy RL to succeed in robot control for quadrupeds~\cite{cheng2024extreme}, bipeds~\cite{li2025reinforcement}, and humanoids~\cite{radosavovic2024real}. However, these methods are sample-inefficient, especially with visual inputs~\cite{cheng2024extreme}, and struggle with multi-task learning and complex scene interactions.

\begin{figure}[t]
    \centering
    % \vspace{-5pt}
    \includegraphics[width=0.99\linewidth]{fig/teaser3.png}
    \vspace{-10pt}
    \caption{An illustration of our framework in the ``Support the Wall'' task. When subjected to a sudden perturbation (left), the robot uses its learned world model to predict and plan a stabilizing action within its planning horizon (center). This allows it to successfully execute the plan and brace its hands against the wall to make contact and maintain balance (right).}
    \label{fig:placeholder}
    \vspace{-10pt}
\end{figure}

% This paper introduces a novel approach to bridge the gap by integrating a learned world model with a sampling-based Model Predictive Control (MPC) framework. We propose learning a scalable world model from a random, demonstration-free offline dataset. This model learns to predict future outcomes in a compact latent space rather than in raw pixel space and is also capable of understanding the consequences of different actions. To leverage this, we introduce a learned surrogate value function to guide the planning process. The MPC planner then utilizes this learned model of dynamics to predict the results of candidate action sequences. This synergy between a predictive world model and a value-guided planner yields a data-efficient and high-performance system that enables agile, vision-based contact planning for humanoid robots across various tasks.

We address this by integrating a learned world model with sampling-based Model Predictive Control (MPC). Our approach trains a scalable world model from a random, demonstration-free offline dataset, predicting future outcomes in compressed latent space rather than raw pixels, and understanding action consequences. We introduce a surrogate value function to guide planning, allowing MPC to efficiently evaluate candidate action sequences. This synergy enables agile, vision-based contact planning for humanoids across tasks with improved data efficiency and performance.

The main contributions of this work are as follows:

\begin{enumerate}[leftmargin=*]
    \item \textbf{A Scalable Visual World Model for Dynamic Robots}: We learn a visual world model that scalably captures the dynamics of diverse contact tasks, trained entirely on a demonstration-free offline dataset.
    \item \textbf{Planning from Pixels with Value-Guidance}: We introduce a sampling-based MPC framework using a learned surrogate value function to guide the planning process.
    \item \textbf{Agile and Robust Real-world Visual Contact Planning}: We validate the proposed framework on a physical humanoid robot, demonstrating multiple novel agile and robust contact planning tasks solely from ego-centric depth images and proprioceptive feedback.
\end{enumerate}



