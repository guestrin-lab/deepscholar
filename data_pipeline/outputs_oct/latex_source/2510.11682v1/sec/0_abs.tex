\begin{abstract}

% Enabling humanoid robots to purposefully exploit physical contact, rather than merely avoiding it as a collision, is a critical step towards advanced autonomy in unstructured environments. However, traditional optimization-based planners struggle with the combinatorial complexity of contact, while on-policy reinforcement learning (RL) is sample-inefficient and has limited multi-task abilities. We present a novel framework for the humanoid contact planning algorithm that integrates a learned world model with a sampling-based Model Predictive Control (MPC). Our world model is trained on a demonstration-free offline dataset to predict future outcomes in a compact latent space. To overcome sparse contact rewards and sensor noise, the MPC is guided by a learned surrogate value function that provides a dense and robust planning objective. We demonstrate that a single, scalable model can realize multiple challenging contact-aware tasks, such as exploiting the wall for support after a perturbation, blocking incoming objects, and ducking under obstacles, with better data efficiency and multi-task capabilities than on-policy RL. The entire framework is deployed on a physical humanoid, achieving robust, real-time contact planning solely from proprioception and ego-centric depth images.

Enabling humanoid robots to exploit physical contact, rather than simply avoid collisions, is crucial for autonomy in unstructured environments. Traditional optimization-based planners struggle with contact complexity, while on-policy reinforcement learning (RL) is sample-inefficient and has limited multi-task ability. We propose a framework combining a learned world model with sampling-based Model Predictive Control (MPC), trained on a demonstration-free offline dataset to predict future outcomes in a compressed latent space. To address sparse contact rewards and sensor noise, the MPC uses a learned surrogate value function for dense, robust planning. Our single, scalable model supports contact-aware tasks, including wall support after perturbation, blocking incoming objects, and traversing height-limited arches, with improved data efficiency and multi-task capability over on-policy RL. Deployed on a physical humanoid, our system achieves robust, real-time contact planning from proprioception and ego-centric depth images. Website: 
    \href{https://ego-vcp.github.io/}{\textcolor{magenta}{https://ego-vcp.github.io/}}

\end{abstract}