

High-performance computing (HPC) systems and cloud data centers have been developed to address different goals---HPC aimed at peak performance, while cloud platforms focused on usability.
%
Recent years have brought a shift towards the convergence of HPC and cloud systems: the architecture of supercomputers is becoming more commoditized~\cite{doi:10.1177/10943420231166608,9810039},
and the popularity of HPC workloads is growing, fueled by the massive demand for machine learning (ML) training.
%
Cloud introduces new user types and software development philosophies such as containers.
%
Containers are built on the fundamental assumption that an application is shipped with all its dependencies configured and compiled, regardless of the type of system on which it will be deployed.
%
Thus, containers offer workload portability, enabling execution at different cloud providers with minimal deployment complexity.
%
For example, they can efficiently support rapid deployment to spot virtual machines, which are offered at a discount but might not provide the exact hardware specification expected by the user.
%
Thus, users can decrease computation costs by scaling up their applications on a mix of virtual machines with different hardware configurations that cannot always be predicted.
%
In HPC, containers can help with seamless deployment across systems with heterogeneous hardware configurations~\cite{7103433}.
%
However, the adoption of containers in HPC is limited by the lack of \textbf{performance portability}, i.e., the ability to
\emph{"achieve excellent performance on a variety of architectures"}~\cite{hoefler2024xaas}.
%
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/xaas_spectrum}
	\caption{Continuum of software deployment in HPC: performance-portable \toolname{} containers provide better productivity than optimized builds, while avoiding limitations of traditional containers.}
	\vspace{-2em}
	\label{fig:xaas_new_posterchild}
\end{figure}


While containers simplify deployment, they do not necessarily benefit HPC system providers.
%
% from \cite{e4s}
% "A crisis point has been reached where supercomputing centers are employing teams of system professionals just to maintain and upgrade software packages, largely because the software dependencies for such applications typically run deep â€“ down to the specific versions of the Kernel, glibc library and runtime communication library. Significant system resources are now being spent to maintain the dependency tree associated with each application."
Traditional HPC systems use modules containing carefully tailored applications and libraries that allow operators to \emph{nudge} users toward performant solutions.
%
However, this adds a major human cost of managing HPC software with complex dependencies~\cite{e4s}.
%
However, when users can simply run any container image, neither they nor administrators will optimize the build, leading to poor performance as users bring ready-to-use but unoptimized software.
%
To encourage performant solutions, we need a different approach to containers in HPC.



The adoption of containers is also limited by the many parameter configurations that need to be supported, with a combinatorial explosion of containers specialized toward different systems, runtimes, compilers, and parallelism approaches~\cite{10.1145/2807591.2807623}.
%
HPC containers can be (re)specialized at runtime with hooks defined by the Open Container Initiative (OCI) standard.
%
These hooks replace libraries inside the container with system-specific versions.
%
A common example is replacing MPI libraries~\cite{10.1007/978-3-030-34356-9_5},
which requires implementations with compatible Application Binary Interface (ABI)~\cite{10.1145/3615318.3615319}.
%
The Libfabric installation can be replaced to access proprietary and custom network providers,
which accelerates communication without changes to MPI~\cite{10029965}.
%
However, solutions using Libfabric are not fully portable due to differences in the capabilities of network providers (Section~\ref{sec:hpc-software-portability}).
%
Furthermore, Fortran applications are known to lack ABI compatibility, which prevents straightforward runtime replacement of libraries such as BLAS or LAPACK.
%
Finally, it is often too late to overcome the consequences of prior decisions that affect the generated code, like GPU acceleration and vectorization.

%Instead, 
We propose that containers should be agnostic of selected configurations and target platforms, distributing software packages almost ready for installation while deferring performance decisions until the target system is known.
%
However, this problem is difficult because HPC build systems are sophisticated, often Turing-complete tools that hardcode performance-critical decisions early in the build process, making complete analysis of these systems not only a massive engineering undertaking but potentially intractable.

Acceleration as a Service (XaaS)~\cite{hoefler2024xaas} introduced a new vision of HPC, where \emph{performance-portable} containers can offer the convenience of containers with the performance of specialized builds.
%
We realize this vision with the concept of \textbf{Source} and \textbf{Intermediate Representation (IR)} containers.
%
We aim to strike %the middle ground 
a balance between traditional HPC practices, where builds are conducted entirely on the destination system, and limited container optimizations at runtime (Figure~\ref{fig:xaas_new_posterchild}).
%
We propose to deploy applications in a portable manner and provide the benefits of traditional containers: smaller size, faster deployment, and the ability to hide the application's source code.
%
Both representations enable configuration for a specific system, allowing us to improve performance by tuning parameters such as vectorization, which enables hardware features unavailable on all platforms (Figure~\ref{fig:gromacs-microbenchmark}).
%
Instead of distributing \emph{multi-arch} Docker containers targeting different Instruction Set Architectures (ISAs), we distribute \emph{multi-arch-IR} containers to support different compilers and toolchains, as long as they offer an intermediate representation to the end user.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/vectorization_comparison.pdf}
	\caption{The impact of vectorization in GROMACS (16 threads, 100 timesteps, I/O time excluded): enabling newer features can improve performance, at the cost of creating a non-portable deployment.}
	\vspace{-1.5em}
	\label{fig:gromacs-microbenchmark}
\end{figure}

We first analyze the broad world of HPC software to determine their specialization mechanisms (Section~\ref{sec:hpc-software}).
%
Based on those results, we examine the compilation pipeline and analyze at which levels performance-critical decisions are made and how they can be delayed until the final hardware specification is known (Section~\ref{sec:xaas-containers}).
%
We configure many instances of the same project with different parameters.
%
We isolate a shared core of IR files where compilation is identical across configurations or is unaffected by the change in performance-critical parameters (Section~\ref{sec:ir-container}).
%
We build a container image that is fully optimized and lowered to the target architecture only during \emph{deployment} on the selected HPC system (Section~\ref{sec:ir-integration}).
%
We demonstrate a prototype of IR containers with LLVM IR~\cite{1281665}.

\input{secs/focus-points-table-new}

\begin{table*}[t!]
	\caption{Levels of code portability and their implementations. Libraries like MPI can be shipped with a container and replaced dynamically at runtime or fully mounted inside the container during execution, as long as ABI compatibility is provided.}
	\label{tab:portability-layers}
	\adjustbox{max width=\textwidth}{
	%\begin{tabular}{p{2cm}p{2.5cm}p{3cm}p{3cm}p{3cm}}
	\begin{tabular}{lcccc}
		\textbf{Level}                                                                          & \textbf{Technology} & \textbf{Description} & \textbf{Portability Approach} & \textbf{Dependency Integration} \\
		\midrule

		Building                                                                                &
		Spack~\cite{10.1145/2807591.2807623}, EasyBuild~\cite{6495863}                          &
		From-source package manager                                                             &
		Parameterized package compilation                                                       &
		Automatic, dependency resolver                                                                                                                                                                         \\

		\midrule

		Linking                                                                                 &
		Sarus~\cite{10.1007/978-3-030-34356-9_5}, Apptainer~\cite{10.1371/journal.pone.0177459} &
		HPC container runtime                                                                   &
		Runtime binding, OCI hooks                                                              &
		Manual, CLI option, and host bind                                                                                                                                                                      \\

		\midrule

		Lowering                                                                                &
		Linux Popcorn~\cite{10.1145/3093337.3037738}                                            &
		Multi-ISA binary system                                                                 &
		Heterogeneous-OS containers                                                             &
		No direct integration                                                                                                                                                                                  \\

		                                                                                        &
		H-containers~\cite{10.1145/3381052.3381321,10.1145/3524452}                             &
		ISA-agnostic container with IRs                                                         &
		Container + recompilation                                                               &
		No direct integration                                                                                                                                                                                  \\

		                                                                                        &
		NVIDIA PTX                                                                              &
		Runtime JIT compilation                                                                 &
		Virtual GPU architecture                                                                &
		No direct integration                                                                                                                                                                                  \\

		\midrule

		Emulation                                                                               &
		Wi4MPI~\cite{9556086}, mpixlate~\cite{mpixlatecray}                                     &
		MPI compatibility layer                                                                 &
		Runtime emulation of MPI ABIs                                                           &
		No direct integration                                                                                                                                                                                  \\
	\end{tabular}}
\end{table*}


In this paper, we make the following contributions:
\begin{itemize}
	\item We study specialization points in HPC applications and propose reorienting container deployment around them.
	\item We analyze the multi-layer HPC compilation stack, and design two solutions for performance-portable containers.
	\item We introduce IR containers that combine the convenience of generic containers with performance of specialized builds.
\end{itemize}
