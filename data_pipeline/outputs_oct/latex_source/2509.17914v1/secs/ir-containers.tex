

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/xaas_containers_spectrum.pdf}
	\caption{Performance-portable \toolname{} containers provide better productivity while avoiding limitations of traditional containers.}
	\vspace{-1em}
	\label{fig:xaas_spectrum}
\end{figure}

In \toolname{}, we aim to resolve the two major limitations of existing containers---lack of performance portability and a combinatorial explosion of the number of final representations.
%
First, we deploy \textbf{source containers} that bring the application and its environment to the final system (Section~\ref{sec:xaas-source-containers}).
%
The source container images contain the HPC application with development tools (Figure~\ref{fig:xaas_spectrum}), and are only built for the target system once hardware configuration and all dependencies are known.
%
Then, we propose that \emph{intermediate representation} becomes a new mode for distributing software (Section~\ref{sec:xaas-ir-containers}).
%
Intuitively, we distribute a container image where build steps are conducted until we cannot progress further without making performance-critical decisions (Section~\ref{sec:xaas-ir-containers-pipeline}).


The new types of containers require a new \textbf{deployment step}, when specialization points are matched against the system specification and user preferences.
%
The remaining source files are compiled, architecture-specific optimizations are applied, and the entire application is lowered to the selected ISA.
%
As a result, we obtain a new image different from the one provided in the registry, which allows for specialization of the application to the selected HPC system.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/xaas_pipeline_source.pdf}
	\caption{Deployment of source containers on HPC system.}
	\vspace{-1.5em}
	\label{fig:xaas-source-pipeline}
\end{figure}


\subsection{Source Containers}
\label{sec:xaas-source-containers}
%
Source containers deliver the application source code, an open-source MPI implementation, and the build toolchain to the HPC system.
%
This solution can support HPC applications and systems that benefit from specialized and vendor-provided compilers, which often do not expose their intermediate representation explicitly.
%
Since no build steps are conducted before the deployment, this approach does not suffer from the large number of combinations: only one image is needed per toolchain and architecture.

The \textbf{deployment} begins by automatically detecting CPU features, accelerators, and the development environment (Figure~\ref{fig:xaas-source-pipeline}).
%
This step must be conducted on a compute node, and in an environment with all standard modules loaded.
%
We augment the results with knowledge of standard HPC environments.
%
For example, when a ROCm or CUDA installation is discovered, we assume the availability of rocFFT and cuFFT, respectively, even if they are not explicitly detected.
%
The discovery can be enhanced with solutions for labeling microarchitectural features, e.g., archspec~\cite{9297044}, and strengthened with explicit system specification provided by system operators.

Then, we perform the automatic intersection of specializations (Section~\ref{sec:containers-specialization-discovery}), and the user selects the best fit from the available options.
%
After that, we generate a Dockerfile to create a new image that inherits from the source container and builds the application with selected options.
%
We implement support for a subset of popular dependencies, inheriting dependency versions from the system environment when possible, and provide them as Docker layers or build steps.
%
Other dependencies could be supported by employing package managers like Spack.
%
Furthermore, we allow switching base images at deployment times to use optimized and recommended images for a specific platform, e.g., oneAPI images in Aurora (Section~\ref{eval:sec-source-containers}).

The new container is no longer portable and can often only be executed on that specific system.
%
However, images derived from source containers should achieve near-native performance since we enable specializations available for bare metal applications, and the performance losses can only come from the container runtime itself.
%
By providing the infrastructure for building and storing a single deployed container, we avoid the situation where users manually build multiple copies of the same application.
%
From the user's point of view, the entire process is still convenient and relatively automatic---only a \emph{cold pull} takes longer than a traditional container build since the very first user of a container on a system will have to wait for the build to finish.
%
Users are only expected to select the values for discovered specialization points.
%
However, this step could also be accelerated by allowing system operators to supply preferred configurations, e.g., preferring MKL on Intel systems over other BLAS/FFT libraries, relying on third-party configuration like in Spack~\cite{10.1145/2807591.2807623}, or providing the AI system with the application documentation to suggest the best option for the target platform.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/xaas_pipeline_ir_build.pdf}
	\caption{\toolname{} IR container. The modular pipeline reduces the build cost by detecting IR files shared by different configurations.}
	\vspace{-1em}
	\label{fig:xaas-source-pipeline-build}
\end{figure*}

\vspace{-1em}

\subsection{IR Containers}
\label{sec:xaas-ir-containers}
%
IR containers are close to the original idea of containers, with the main goal of \emph{build once and run anywhere}.
%
However, the original build is augmented with the deployment step, responsible for the final optimizations and lowering to the target architecture.
%
The application is distributed in the compiler's intermediate representation, and the image should not contain any object code that depends on the final architecture, as this would be neither portable nor performance-portable.
%
In addition to selecting the architecture of the container image, we specify the \emph{IR}, e.g., LLVM IR.
%

IR containers can suffer the same problem of combinatorial explosion that affects performance-oblivious containers.
%
With multiple possible choices for parallelization, acceleration, hardware specialization, and communication, the number of \emph{build configurations} grows combinatorially.
%
The cost of building containers that include all combinations would be too high for many applications, and their size would be a major deployment problem.
%
To make IR deployments practical, we need to deduplicate build configurations and build only the \emph{unique} intermediate representation files:

\begin{hyp}
	%
	Let $P_{1}, P_{2}, \dots, P_{N}$ be $N$ different configurations of the same HPC application.
	%
	Each configuration $P_{i}$ compiles $T_{i}$ different IR files.
	%
	Let $T'$ be the total number of \textbf{distinct} IR files produced in all $N$ configurations.
	%
	Then, $T' < \sum_{i} T_{i}$.
\end{hyp}

The set of unique results of compilation is not immediately detectable, as different compilation settings will obscure the analysis while not affecting the result.
%
Many build configurations apply compilation flags globally to targets, e.g., C/C++ include flags or enabling OpenMP.
%
To resolve the problem of too many build configurations, we apply a \emph{behavioral} approach.
%
Due to the complexity and intractability of the problem, we do not attempt to understand what build systems do but examine the compilation instructions of each target created in build configurations.
%
We identify the differences between configurations and build only the delta when selecting a specific option.
%
When two different configurations produce different targets from the same input, we build two different IR files for that target.
%
Instead of storing all results of many different builds, we use one common set of IR files shared across all configurations, and a set of deltas applied only to selected configuration.

\subsubsection{System Dependency}
%
Before assembling the IR container, we define the necessary conditions for deploying the application's source files in that form.

\begin{definition}
	A system-independent source file ($SI$) can be passed through the configuration and compilation stages without specifying the final software and hardware configuration.
\end{definition}

A typical HPC example of such a source file would be numerical computations.
%
Computations can be parallelized with OpenMP since the file can be compiled twice to IR, once with and once without OpenMP.
%
However, MPI dependencies are not permitted for this category due to the lack of ABI compatibility in current runtimes.
%
In practice, such files can be compiled with MPICH to be deployed with a widely accepted binary interface~\cite{mpichabi}.
%
CUDA's PTX can be included in this category, while the binary representation of a compiled kernel (cubin) cannot.

\begin{definition}
	A system-dependent source file ($SD$) cannot be compiled to a shared IR without sacrificing portability.
	%due to inherent dependency on changing interfaces. 
\end{definition}

This category includes files with functionality conditionally enabled only for some configurations,
and files requiring a dedicated compiler that does not expose its intermediate representation.

\begin{hyp}
	HPC applications can be decomposed into two sets of source files: system-independent ($SI$) and system-dependent ($SD$).
	%
	Most importantly, for most practical applications, $|SI| >> |SD|$.
\end{hyp}

The corollary of the last part of the hypothesis is critical: the effort of building a specialized pipeline makes sense only if the majority of the source code can be processed without knowing the final system; otherwise, source containers are a better solution.

\vspace{-0.75em}

\subsection{IR Containers Pipeline}
%
\label{sec:xaas-ir-containers-pipeline}

We create a modular container build pipeline (Figure~\ref{fig:xaas-source-pipeline-build}) that solves multiple problems to determine the unique set of IR files:
\begin{enumerate}
	\item Combinatorial explosion of build configurations on projects with many specialization points.
	\item Code modules that can be excluded during the project's configuration, depending on specialization points.
	\item C/C++ preprocessor that can encode the effects of specialization points.
	\item Compilation flags that do not affect the result.
\end{enumerate}
%
In particular, we implement optimizations that analyze the effects of OpenMP and vectorization flags.

\textbf{Configuration}
%
While we try to constrain the cost of building a container (Problem 1), we need to ensure that we do not prematurely exclude code modules that might become necessary during the deployment step (Problem 2).
%
First, we generate a specialized build configuration for each combination of provided specialization points, e.g., LULESH~\cite{LULESH:spec} with two specialization points---MPI and OpenMP---will produce four different configurations.
%
Each build configuration is created in a containerized environment, where the build directory is always mounted under the same path.
%
This helps remove the effects of different locations on the generated compilation flags.
%
The container is assembled from layers that provide the toolchain and dependencies of the selected application.

For each build configuration, we obtain the list of all compilation steps and associated compilation flags, e.g., by examining the compile commands database generated by CMake, which can be obtained without analyzing the internal structure of each build system.
%
Other build systems can be supported by extracting compilation flags with third-party tools and compiler wrappers.
%
We identify \emph{compilation targets} and not source files, since a single source file can be mapped to multiple targets but with different compilation flags.
%
Then, we compare the results of each build profile to identify the common denominator, a shared core of files that are always built in the same manner.
%
In the case of LULESH, where each build consists of five source files, we obtain 20 IR files.

\textbf{Preprocessing}
%
The configuration step is followed by a preprocessor evaluation to determine if different compile-time definitions produce a semantically different source file.
%
Thus, we create preprocessed C/C++ files, hash them, and look for identical files.
%
In LULESH, this step does not change the result since enabling MPI changes the source files, and the OpenMP compilation flag is attached to all files.
%
Since not all files will use OpenMP, we apply a Clang AST analysis pass to detect if the processed file contains any OpenMP constructs.
%
If two compilation targets from different build configurations have the same hash but differ only in the OpenMP flag, then we can treat them as the same.
%
After that step, LULESH has been reduced from 20 to 14 different IR files.

% If we had space, we could have a small SAXPY example:
% clang -O3 -mavx512f produces vector instructions of AVX-512
% clang -O3 -emit-llvm + opt -O3 -mattr=+avx512f + lowering produces vector instructions of AVX (likely wrong vectorized loop structure)
% clang -O3 -mllvm -disable-llvm-optzns + opt -O3 -mattr=+avx512f + lowering produces AVX-512 (identical to first case)
Vectorization is another example of divergence across many builds of the same application.
%
For example, GROMACS~\cite{https://doi.org/10.1002/jcc.20291} supports nine different configurations for vectorization on x86 CPUs.
%
Since LLVM vectorizers work at the IR level, we can ignore the architecture-specific flags when comparing different configurations of the same file.
%
Instead, the vectorization will be applied during deployment once the final ISA is known.
%
Our experiments show that LLVM optimizations need to be delayed as well, as otherwise the code will not be re-vectorized efficiently once the target is known.

\textbf{Compilation}
%
During compilation, applications become aware of types that might introduce incompatibilities between different implementations of the same library.
%
Thus, we need to isolate them from the rest of the codebase: the "default" partition ($SI$) can continue compilation as previously, while the new partition dependent on the ABI-problematic library ($SD$) will not be compiled at all until the final deployment for the target system.

MPI applications are the most important source of ABI compatibility, and we compile against MPICH to provide wide portability.
%
Since we expect future MPI runtimes to be ABI compatible~\cite{10.1145/3615318.3615319}, we do not focus on this problem.
%
To support Open MPI, XaaS containers could detect all files dependent on MPI and compile them multiple times against different ABIs, or employ portability layers~\cite{mukautuva}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/xaas_pipeline_ir_deploy.pdf}
	\caption{Deploying \toolname{} IR container: user selects one configuration that will be optimized and lowered to the architecture.}
	\vspace{-1.5em}
	\label{fig:xaas-ir-pipeline-deploy}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{figures/xaas_gpu_compatibility.pdf}
	\caption{CUDA compatibility is determined by six parameters: two on host (driver and device capability), and four in container (runtime, PTX version, compute capability of PTX and device binary cubin).}
	\vspace{-1em}
	\label{fig:xaas-ir-gpu}
\end{figure}

\textbf{Container Build:}
%
We generate a container with the LLVM, all build directories, IR files, and the source repository.
%
The latter is necessary to support system-dependent files and perform the final installation.
%
For each build configuration, we generate a specific installation file with instructions for compiling IR files and placing them in their respective locations.
%
We do not include all image layers, e.g., GPU runtimes, as these will be reassembled at deployment.

\textbf{GPU Compatibility:}
%
GPU runtimes target multiple architectures by generating either direct device code or portable PTX ISA for later JIT compilation (Figure~\ref{fig:xaas-ir-gpu}).
%
Portable containers can use the oldest supported CUDA runtime to ensure backward compatibility, while newer runtimes offer updated libraries and support for new hardware features at the cost of additional compatibility steps.
%
We provide compatibility across CUDA minor versions, e.g., CUDA 12.x.
%
First, we search for any use of compile-time definition indicating CUDA runtime version, which is a pessimistic check if the application might depend conditionally on API features unavailable in older drivers.
%
Once we decide if the newest CUDA runtime can be used, we emit device binaries for all architectures and a PTX for the latest compute capability to support newer devices.


\subsubsection{Deployment}
%
The user selects specialization points from the list of parameters and their values chosen at configuration time.
%
Then, we create a new container by assembling dependencies explicitly defined for that specialization.
%
We select a subset of IRs for that configuration, optimize and compile them, and let the build system finish linking (Figure~\ref{fig:xaas-ir-pipeline-deploy}).
%
Image tag includes specialization points to support the coexistence of many builds.

\textbf{Code Generation:}
%
We lower all IR files of a selected build configuration to the target architecture.
%
This step can be much faster than a complete compilation of a C/C++ application.
%
We also apply vectorization at this stage if it is detected during container build.

\textbf{Linking:}
%
Once the binary code is generated, we can use the existing project configuration to link them together into final libraries and executables.
%
Alternatively, linking flags for each target can be inferred from the build system, e.g., CMake exposes them explicitly.
%
For runtime replacement system-optimized libraries, we can rely on the capabilities of existing HPC containers.

