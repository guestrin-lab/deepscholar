

To understand the challenge of performance portability in HPC, we begin with identifying configuration parameters that affect the performance by \emph{specializing} to the target machine (Section~\ref{sec:hpc-software-specialization}).
%
Then, we analyze the existing approaches for code portability, focusing on \emph{when} the optimizations are applied (Section~\ref{sec:hpc-software-portability}).
%
This process helps us decide which build steps should be conducted before distributing software to the end user (Section~\ref{sec:xaas-containers}).

\subsection{Specialization Points}
\label{sec:hpc-software-specialization}

HPC applications are highly configurable since they aim to run on heterogeneous systems, with many built on custom and specialized hardware (Table~\ref{tab:focus-points}).
%
We define \textbf{specialization points} as \emph{application parameters that must be known at the configuration and build time, stay constant throughout the entire application's lifetime, and whose values affect the final performance and portability}.
%
In particular, we focus on parameters that dictate which specific hardware and software solutions should be employed by the final application.
%
These options are not always mutually exclusive, as the application can support multiple GPU backends that are only selected at runtime.
%
We consider the following categories of specialization points.
%
\begin{itemize}[topsep=0pt]
	\item Network fabric and communication library like MPI.
	\item Acceleration, such as NVIDIA, AMD, or Intel GPUs.
	\item CPU-specific optimizations such as vectorization.
	\item Libraries like BLAS, LAPACK, and FFT. % have different implementations with varying performance on various platform.
\end{itemize}

\vspace{-1em}

\subsection{Portability Layers}
\label{sec:hpc-software-portability}

%We categorize existing approaches to HPC portability solutions into four broad categories based on the portion of the application build that is conducted on the target system (Table~\ref{tab:portability-layers}).
We classify portability solutions into four categories based on the fraction of the build that is conducted on the target system (Table~\ref{tab:portability-layers}).

\textbf{Building} performs a full compilation of the application on the destination system.
%
This approach provides the highest performance portability, at the cost of increased complexity - each user builds their copy manually or with the help of a package manager.

In \textbf{linking}, the dynamic dependencies of an existing application are replaced at runtime with an optimized and systems-specific implementation, e.g., through OCI hooks for containers.
%
The main constraint here is the requirement of ABI compatibility, which prevents such replacements for BLAS/LAPACK libraries.
%
For example, Libfabric allows the implementation of network communication with a standardized API and dynamic selection of network providers at runtime~\cite{githublibfabric}.
%
In practice, it still requires manual and specialized implementations because network providers differ in the support of libfabric features (Table~\ref{tab:libfabric-features}).
%
Furthermore, while libfabric replacement can accelerate a containerized MPI runtime~\cite{10029965}, it might require additional plugins to support intra-node messaging~\cite{https://doi.org/10.1002/cpe.8203}.
%
Thus, relinking the libfabric installation is not a general method for performance specialization of an already compiled application.

\begin{figure*}[t!]
	\centering
	\begin{minipage}[t]{0.24\textwidth}
		\centering
		\begin{minted}[fontsize=\footnotesize]{cpp}
#if not defined(HAVE_ANY_BLAS)
void transpose(double* A,
 double* B, int rows, int cols) {
 for (int i; i < rows; i++) {
  for (int j; j < cols; j++) {
   B[j * rows + i] = 
    A[i * cols + j];
  }}
}
#endif
        \end{minted}
		\textbf{(a)} Manual Implementation
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.24\textwidth}
		\centering
		\begin{minted}[fontsize=\footnotesize]{cpp}
#if defined(HAVE_OPENBLAS)
void transpose(double* A,
 double* B, int rows, int cols) {
 cblas_domatcopy(
  CblasRowMajor, CblasTrans,
  rows, cols, 1.0, A,
  cols, B, rows
 );
}
#endif
        \end{minted}
		\textbf{(b)} OpenBLAS Implementation
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.22\textwidth}
		\centering
		\begin{minted}[fontsize=\footnotesize]{cpp}
#if defined(HAVE_MKL)
void transpose(double* A,
 double* B, int rows,
 int cols) {
 mkl_domatcopy(
  'R', 'T', rows, cols,
  1.0, A, cols, B, rows
 );
}
#endif
        \end{minted}
		\textbf{(c)} Intel MKL Implementation
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.28\textwidth}
		\centering
		\begin{minted}[fontsize=\footnotesize]{cpp}
#if defined(HAVE_CUBLAS)
void transpose(double* A, double* B,
    int rows, int cols) {
 double alpha = 1.0, beta = 0.0;
 cublasDgeam(handle, CUBLAS_OP_T,
  CUBLAS_OP_N, rows, cols, &alpha, A,
  cols, &beta, nullptr, rows, B, rows
 );
}
#endif
        \end{minted}
		\textbf{(d)} cuBLAS Implementation
	\end{minipage}
	\caption{Matrix transposition: a simple linear algebra kernel that has not been standardized, requiring a custom solution chosen at build time. While manual implementation is a safe choice that will work everywhere, it will prevent achieving the highest performance.}
	\vspace{-1em}
	\label{fig:transpose-implementations}
\end{figure*}

\textbf{Lowering} replaces the intermediate representation with the final binary product at the target system.
%
These solutions support multiple ISAs, even when the hardware popularity changes over time.
%
HPC applications cannot be limited to x86 deployments, with primary examples of contenders being PowerPC in the past and ARM today, e.g., Fugaku's A64FX~\cite{9355239}, Graviton CPUs~\cite{9835543} in AWS cloud, and NVIDIA's Grace Hopper superchip~\cite{10.1145/3636480.3637097}.
%
Similarly, this approach provides compatibility with different NVIDIA GPU architectures by deploying Parallel Thread Execution (PTX), an ISA for virtual GPU architectures in CUDA.
%
PTX is JIT-compiled to a binary code, providing portability across many GPU generations~\cite{cudadocsvirtualarch}.

Finally, \textbf{emulation} attempts to patch incompatibilities at runtime without code modifications.
%
An example is replacing MPI runtimes when the application has been built against MPI that is not ABI compatible with the host implementation~\cite{9556086,mpixlatecray,schnetter_2022_6174409}.

\input{secs/table-libfabric-short}
