@INPROCEEDINGS{9668286,
  author={Ustiugov, Dmitrii and Amariucai, Theodor and Grot, Boris},
  booktitle={2021 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Analyzing Tail Latency in Serverless Clouds with STeLLAR}, 
  year={2021},
  volume={},
  number={},
  pages={51-62},
  doi={10.1109/IISWC53511.2021.00016}
}

@INPROCEEDINGS {9946318,
author = {S. Werner and T. Schirmer},
booktitle = {2022 IEEE International Conference on Cloud Engineering (IC2E)},
title = {HARDLESS: A Generalized Serverless Compute Architecture for Hardware Processing Accelerators},
year = {2022},
volume = {},
issn = {},
pages = {79-84},
abstract = {The increasing use of hardware processing accelerators tailored for specific applications, such as the Vision Processing Unit (VPU) for image recognition, further increases developers&#x27; configuration, development, and management over-head. Developers have successfully used fully automated elastic cloud services such as serverless computing to counter these additional efforts and shorten development cycles for applications running on CPUs. Unfortunately, current cloud solutions do not yet provide these simplifications for applications that require hardware acceleration. However, as the development of special-ized hardware acceleration continues to provide performance and cost improvements, it will become increasingly important to enable ease of use in the cloud. In this paper, we present an initial design and implemen-tation of Hardless, an extensible and generalized serverless computing architecture that can support workloads for arbitrary hardware accelerators. We show how Hardless can scale across different commodity hardware accelerators and support a variety of workloads using the same execution and programming model common in serverless computing today.},
keywords = {image recognition;costs;processor scheduling;computational modeling;serverless computing;graphics processing units;computer architecture},
doi = {10.1109/IC2E55432.2022.00016},
url = {https://doi.ieeecomputersociety.org/10.1109/IC2E55432.2022.00016},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep}
}

@misc{pemberton2022kernelasaservice,
      title={Kernel-as-a-Service: A Serverless Interface to GPUs}, 
      author={Nathan Pemberton and Anton Zabreyko and Zhoujie Ding and Randy Katz and Joseph Gonzalez},
      year={2022},
      eprint={2212.08146},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{10.1145/3503222.3507732,
author = {Du, Dong and Liu, Qingyuan and Jiang, Xueqiang and Xia, Yubin and Zang, Binyu and Chen, Haibo},
title = {Serverless Computing on Heterogeneous Computers},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507732},
doi = {10.1145/3503222.3507732},
abstract = {Existing serverless computing platforms are built upon homogeneous computers, limiting the function density and restricting serverless computing to limited scenarios. We introduce Molecule, the first serverless computing system utilizing heterogeneous computers. Molecule enables both general-purpose devices (e.g., Nvidia DPU) and domain-specific accelerators (e.g., FPGA and GPU) for serverless applications that significantly improve function density (50\% higher) and application performance (up to 34.6x). To achieve these results, we first propose XPU-Shim, a distributed shim to bridge the gap between underlying multi-OS systems (when using general-purpose devices) and our serverless runtime (i.e., Molecule). We further introduce vectorized sandbox, a sandbox abstraction to abstract hardware heterogeneity (when using domain-specific accelerators). Moreover, we also review state-of-the-art serverless optimizations on startup and communication latency and overcome the challenges to implement them on heterogeneous computers. We have implemented Molecule on real platforms with Nvidia DPUs and Xilinx FPGAs and evaluate it using benchmarks and real-world applications.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {797–813},
numpages = {17},
keywords = {serverless computing, Cloud computing, heterogeneous computers, operating system, function-as-a-service},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@INPROCEEDINGS{9820659,

  author={Fingler, Henrique and Zhu, Zhiting and Yoon, Esther and Jia, Zhipeng and Witchel, Emmett and Rossbach, Christopher J.},

  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 

  title={DGSF: Disaggregated GPUs for Serverless Functions}, 

  year={2022},

  volume={},

  number={},

  pages={739-750},

  doi={10.1109/IPDPS53621.2022.00077}}

@inproceedings{agache2020firecracker,
  title={Firecracker: Lightweight virtualization for serverless applications},
  author={Agache, Alexandru and Brooker, Marc and Iordache, Alexandra and Liguori, Anthony and Neugebauer, Rolf and Piwonka, Phil and Popa, Diana-Maria},
  booktitle={17th USENIX symposium on networked systems design and implementation (NSDI 20)},
  pages={419--434},
  year={2020}
}

@inproceedings{chard2020funcx,
  title={Funcx: A federated function serving fabric for science},
  author={Chard, Ryan and Babuji, Yadu and Li, Zhuozhao and Skluzacek, Tyler and Woodard, Anna and Blaiszik, Ben and Foster, Ian and Chard, Kyle},
  booktitle={Proceedings of the 29th International symposium on high-performance parallel and distributed computing},
  pages={65--76},
  year={2020}
}

@misc{reddi2019mlperf,
    title={MLPerf Inference Benchmark},
    author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},
    year={2019},
    eprint={1911.02549},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{10.1145/3464298.3493398,
author = {Eismann, Simon and Bui, Long and Grohmann, Johannes and Abad, Cristina and Herbst, Nikolas and Kounev, Samuel},
title = {Sizeless: Predicting the Optimal Size of Serverless Functions},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3493398},
doi = {10.1145/3464298.3493398},
abstract = {Serverless functions are an emerging cloud computing paradigm that is being rapidly adopted by both industry and academia. In this cloud computing model, the provider opaquely handles resource management tasks such as resource provisioning, deployment, and auto-scaling. The only resource management task that developers are still in charge of is selecting how much resources are allocated to each worker instance. However, selecting the optimal size of serverless functions is quite challenging, so developers often neglect it despite its significant cost and performance benefits. Existing approaches aiming to automate serverless functions resource sizing require dedicated performance tests, which are time-consuming to implement and maintain.In this paper, we introduce an approach to predict the optimal resource size of a serverless function using monitoring data from a single resource size. As our approach does not require dedicated performance tests, it enables cloud providers to implement resource sizing on a platform level and automate the last resource management task associated with serverless functions. We evaluate our approach on four different serverless applications on AWS, where it predicts the execution time of the other memory sizes based on monitoring data for a single memory size with an average prediction error of 15.3\%. Based on these predictions, it selects the optimal memory size for 79.0\% of the serverless functions and the second-best memory size for 12.3\% of the serverless functions, which results in an average speedup of 39.7\% while also decreasing average costs by 2.6\%.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {248–259},
numpages = {12},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@misc{lambdapowertuning,
      title={AWS Lambda Power Tuning},
      author={Alex Casalboni},
      year={2023},
      url={https://github.com/alexcasalboni/aws-lambda-power-tuning},
      note={Accessed: 2024-01-02}
}
@misc{lambci,
      title={LambCI: A continuous integration system built on AWS Lambda},
      author={LambCI},
      year={2020},
      url={https://github.com/lambci/lambci},
      note={Accessed: 2024-01-02}
}
@misc{iceoryx,
      title={iceoryx - true zero-copy inter-process-communication},
      author={iceoryx},
      year={2023},
      url={https://github.com/eclipse-iceoryx/iceoryx},
      note={Accessed: 2024-01-02}
}

@inproceedings {234886,
  author = {Sadjad Fouladi and Francisco Romero and Dan Iter and Qian Li and Shuvo Chatterjee and Christos Kozyrakis and Matei Zaharia and Keith Winstein},
  title = {From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  year = {2019},
  isbn = {978-1-939133-03-8},
  address = {Renton, WA},
  pages = {475--488},
  url = {http://www.usenix.org/conference/atc19/presentation/fouladi},
  publisher = {USENIX Association},
  month = jul
}

@inproceedings{wawrzoniak2021boxer,
  title={Boxer: Data analytics on network-enabled serverless platforms},
  author={Wawrzoniak, Mike and M{\"u}ller, Ingo and Fraga Barcelos Paulus Bruno, Rodrigo and Alonso, Gustavo},
  booktitle={11th Annual Conference on Innovative Data Systems Research (CIDR 2021)},
  year={2021}
}

@misc{wawrzoniak2022shortlived,
      title={Short-lived Datacenter}, 
      author={Michael Wawrzoniak and Ingo Müller and Rodrigo Bruno and Ana Klimovic and Gustavo Alonso},
      year={2022},
      eprint={2202.06646},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@INPROCEEDINGS{9355790,

  author={Choi, Sean and Shahbaz, Muhammad and Prabhakar, Balaji and Rosenblum, Mendel},

  booktitle={2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS)}, 

  title={$\lambda$-NIC: Interactive Serverless Compute on Programmable SmartNICs}, 

  year={2020},

  volume={},

  number={},

  pages={67-77},

  keywords={Reduced instruction set computing;Computational modeling;Pricing;Programming;Throughput;Web servers;Resource management;Serverless Compute;SmartNIC;Programmable Network;Low Latency Compute;Cloud Computing},

  doi={10.1109/ICDCS47774.2020.00029}}
@inproceedings {288627,
author = {Xingda Wei and Fangming Lu and Tianxia Wang and Jinyu Gu and Yuhan Yang and Rong Chen and Haibo Chen},
title = {No Provisioned Concurrency: Fast {RDMA-codesigned} Remote Fork for Serverless Computing},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {497--517},
url = {https://www.usenix.org/conference/osdi23/presentation/wei-rdma},
publisher = {USENIX Association},
month = jul
}

@inproceedings{10.1145/3472883.3486982,
author = {Daw, Nilanjan and Bellur, Umesh and Kulkarni, Purushottam},
title = {Speedo: Fast dispatch and orchestration of serverless workflows},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486982},
doi = {10.1145/3472883.3486982},
abstract = {Structuring cloud applications as collections of interacting fine-grained microservices makes them scalable and affords the flexibility of hot upgrading parts of the application. The current avatar of serverless computing (FaaS) with its dynamic resource allocation and auto-scaling capabilities make it the deployment model of choice for such applications. FaaS platforms operate with user space dispatchers that receive requests over the network and make a dispatch decision to one of multiple workers (usually a container) distributed in the data center. With the granularity of microservices approaching execution times of a few milliseconds combined with loads approaching tens of thousands of requests a second, having a low dispatch latency of less than one millisecond becomes essential to keep up with line rates. When these microservices are part of a workflow making up an application, the orchestrator that coordinates the sequence in which microservices execute also needs to operate with microsecond latency. Our observations reveal that the most significant component of the dispatch/orchestration latency is the time it takes for the request to traverse into and out of the user space from the network. Motivated by the presence of a multitude of low power cores on today's SmartNICs, one approach to keeping up with these high line rates and the stringent latency expectations is to run both the dispatcher and the orchestrator close to the network on a SmartNIC. Doing so will save valuable cycles spent in transferring requests to and back from the user space. The operating characteristics of short-lived ephemeral state and low CPU burst requirements of FaaS dispatcher/orchestrator make them ideal candidates for offloading from the server to the NIC cores. This also brings other benefit of freeing up the server CPU. In this paper, we present Speedo--- a design for offloading of FaaS dispatch and orchestration services to the SmartNIC from the user space. We implemented Speedo on ASIC based Netronome Agilio SmartNICs and our comprehensive evaluation shows that Speedo brings down the dispatch latency from ~150ms to ~140μs at a load of 10K requests per second.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {585–599},
numpages = {15},
keywords = {orchestration, programmable SmartNIC, serverless workflows},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

  

@inproceedings {254432,
author = {Simon Shillaker and Peter Pietzuch},
title = {Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing},
booktitle = {2020 USENIX Annual Technical Conference (USENIX ATC 20)},
year = {2020},
isbn = {978-1-939133-14-4},
pages = {419--433},
url = {https://www.usenix.org/conference/atc20/presentation/shillaker},
publisher = {USENIX Association},
month = jul
}
@misc{shillaker2023faabric,
      title={Faabric: Fine-Grained Distribution of Scientific Workloads in the Cloud}, 
      author={Simon Shillaker and Carlos Segarra and Eleftheria Mappoura and Mayeul Fournial and Lluis Vilanova and Peter Pietzuch},
      year={2023},
      eprint={2302.11358},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@inproceedings{dakkakTrIMSTransparentIsolated2019,
  title = {{{TrIMS}}: {{Transparent}} and {{Isolated Model Sharing}} for {{Low Latency Deep Learning Inference}} in {{Function-as-a-Service}}},
  shorttitle = {{{TrIMS}}},
  booktitle = {2019 {{IEEE}} 12th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author = {Dakkak, Abdul and Li, Cheng and Garcia De Gonzalo, Simon and Xiong, Jinjun and Hwu, Wen-mei},
  date = {2019-07},
  pages = {372--382},
  publisher = {IEEE},
  location = {Milan, Italy},
  doi = {10.1109/CLOUD.2019.00067},
  url = {https://ieeexplore.ieee.org/document/8814494/},
  urldate = {2024-03-17},
  abstract = {Deep neural networks (DNNs) have become core computation components within low latency Function as a Service (FaaS) prediction pipelines. Cloud computing, as the de-facto backbone of modern computing infrastructure, has to be able to handle user-defined FaaS pipelines containing diverse DNN inference workloads while maintaining isolation and latency guarantees with minimal resource waste. The current solution for guaranteeing isolation and latency within FaaS is inefficient. A major cause of the inefficiency is the need to move large amount of data within and across servers. We propose TrIMS as a novel solution to address this issue. TrIMS is a generic memory sharing technique that enables constant data to be shared across processes or containers while still maintaining isolation between users. TrIMS consists of a persistent model store across the GPU, CPU, local storage, and cloud storage hierarchy, an efficient resource management layer that provides isolation, and a succinct set of abstracts, application APIs, and container technologies for easy and transparent integration with FaaS, Deep Learning (DL) frameworks, and user code. We demonstrate our solution by interfacing TrIMS with the Apache MXNet framework and demonstrate up to 24× speedup in latency for image classification models, up to 210× speedup for large models, and up to 8× system throughput improvement.},
  eventtitle = {2019 {{IEEE}} 12th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  isbn = {978-1-72812-705-7},
  langid = {english},
  file = {/home/mcopik/polybox/Zotero/storage/KYEHLY9W/Dakkak et al. - 2019 - TrIMS Transparent and Isolated Model Sharing for .pdf}
}

@online{zhaoGPUenabledFunctionasaServiceMachine2023,
  title = {{{GPU-enabled Function-as-a-Service}} for {{Machine Learning Inference}}},
  author = {Zhao, Ming and Jha, Kritshekhar and Hong, Sungho},
  date = {2023-03-09},
  eprint = {2303.05601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.05601},
  urldate = {2024-03-30},
  abstract = {Function-as-a-Service (FaaS) is emerging as an important cloud computing service model as it can improve the scalability and usability of a wide range of applications, especially Machine-Learning (ML) inference tasks that require scalable resources and complex software configurations. These inference tasks heavily rely on GPUs to achieve high performance; however, support for GPUs is currently lacking in the existing FaaS solutions. The unique event-triggered and short-lived nature of functions poses new challenges to enabling GPUs on FaaS, which must consider the overhead of transferring data (e.g., ML model parameters and inputs/outputs) between GPU and host memory. This paper proposes a novel GPU-enabled FaaS solution that enables ML inference functions to efficiently utilize GPUs to accelerate their computations. First, it extends existing FaaS frameworks such as OpenFaaS to support the scheduling and execution of functions across GPUs in a FaaS cluster. Second, it provides caching of ML models in GPU memory to improve the performance of model inference functions and global management of GPU memories to improve cache utilization. Third, it offers co-designed GPU function scheduling and cache management to optimize the performance of ML inference functions. Specifically, the paper proposes locality-aware scheduling, which maximizes the utilization of both GPU memory for cache hits and GPU cores for parallel processing. A thorough evaluation based on real-world traces and ML models shows that the proposed GPU-enabled FaaS works well for ML inference tasks, and the proposed locality-aware scheduler achieves a speedup of 48x compared to the default, load balancing only schedulers.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/home/mcopik/polybox/Zotero/storage/2VIWSQQ5/Zhao et al. - 2023 - GPU-enabled Function-as-a-Service for Machine Lear.pdf}
}

@inproceedings{guFaSTGShareEnablingEfficient2023,
  title = {{{FaST-GShare}}: {{Enabling Efficient Spatio-Temporal GPU Sharing}} in {{Serverless Computing}} for {{Deep Learning Inference}}},
  shorttitle = {{{FaST-GShare}}},
  booktitle = {Proceedings of the 52nd {{International Conference}} on {{Parallel Processing}}},
  author = {Gu, Jianfeng and Zhu, Yichao and Wang, Puxuan and Chadha, Mohak and Gerndt, Michael},
  date = {2023-08-07},
  eprint = {2309.00558},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {635--644},
  doi = {10.1145/3605573.3605638},
  url = {http://arxiv.org/abs/2309.00558},
  urldate = {2024-03-30},
  abstract = {Serverless computing (FaaS) has been extensively utilized for deep learning (DL) inference due to the ease of deployment and payper-use benefits. However, existing FaaS platforms utilize GPUs in a coarse manner for DL inferences, without taking into account spatio-temporal resource multiplexing and isolation, which results in severe GPU under-utilization, high usage expenses, and SLO (Service Level Objectives) violation. There is an imperative need to enable an efficient and SLO-aware GPU-sharing mechanism in serverless computing to facilitate cost-effective DL inferences. In this paper, we propose FaST-GShare, an efficient FaaS-oriented Spatio-Temporal GPU Sharing architecture for deep learning inferences. In the architecture, we introduce the FaST-Manager to limit and isolate spatio-temporal resources for GPU multiplexing. In order to realize function performance, the automatic and flexible FaST-Profiler is proposed to profile function throughput under various resource allocations. Based on the profiling data and the isolation mechanism, we introduce the FaST-Scheduler with heuristic auto-scaling and efficient resource allocation to guarantee function SLOs. Meanwhile, FaST-Scheduler schedules function with efficient GPU node selection to maximize GPU usage. Furthermore, model sharing is exploited to mitigate memory contention. Our prototype implementation on the OpenFaaS platform and experiments on MLPerf-based benchmark prove that FaST-GShare can ensure resource isolation and function SLOs. Compared to the time sharing mechanism, FaST-GShare can improve throughput by 3.15x, GPU utilization by 1.34x, and SM (Streaming Multiprocessor) occupancy by 3.13x on average.},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/home/mcopik/polybox/Zotero/storage/UNYSLEZG/Gu et al. - 2023 - FaST-GShare Enabling Efficient Spatio-Temporal GP.pdf}
}

@inproceedings{kimGPUEnabledServerless2018,
  title = {{{GPU Enabled Serverless Computing Framework}}},
  booktitle = {2018 26th {{Euromicro International Conference}} on {{Parallel}}, {{Distributed}} and {{Network-based Processing}} ({{PDP}})},
  author = {Kim, Jaewook and Jun, Tae Joon and Kang, Daeyoun and Kim, Dohyeun and Kim, Daeyoung},
  date = {2018-03},
  pages = {533--540},
  publisher = {IEEE},
  location = {Cambridge},
  doi = {10.1109/PDP2018.2018.00090},
  url = {https://ieeexplore.ieee.org/document/8374513/},
  urldate = {2024-03-30},
  abstract = {A new form of cloud computing, serverless computing, is drawing attention as a new way to design micro-services architectures. In a serverless computing environment, services are developed as service functional units. The function development environment of all serverless computing framework at present is CPU based. In this paper, we propose a GPU-supported serverless computing framework that can deploy services faster than existing serverless computing framework using CPU. Our core approach is to integrate the open source serverless computing framework with NVIDIA-Docker and deploy services based on the GPU support container. We have developed an API that connects the open source framework to the NVIDIA-Docker and commands that enable GPU programming. In our experiments, we measured the performance of the framework in various environments. As a result, developers who want to develop services through the framework can deploy high-performance micro services and developers who want to run deep learning programs without a GPU environment can run code on remote GPUs with little performance degradation.},
  eventtitle = {2018 26th {{Euromicro International Conference}} on {{Parallel}}, {{Distributed}} and {{Network-based Processing}} ({{PDP}})},
  isbn = {978-1-5386-4975-6},
  langid = {english},
  file = {/home/mcopik/polybox/Zotero/storage/BPLI5IR5/Kim et al. - 2018 - GPU Enabled Serverless Computing Framework.pdf}
}


@inproceedings{10.1145/3627703.3629578,
author = {Strati, Foteini and Ma, Xianzhe and Klimovic, Ana},
title = {Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications},
year = {2024},
isbn = {9798400704376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627703.3629578},
doi = {10.1145/3627703.3629578},
abstract = {GPUs are critical for maximizing the throughput-per-Watt of deep neural network (DNN) applications. However, DNN applications often underutilize GPUs, even when using large batch sizes and eliminating input data processing or communication stalls. DNN workloads consist of data-dependent operators, with different compute and memory requirements. While an operator may saturate GPU compute units or memory bandwidth, it often leaves other GPU resources idle. Despite the prevalence of GPU sharing techniques, current approaches are not sufficiently fine-grained or interference-aware to maximize GPU utilization while minimizing interference at the granularity of 10s of μs. We propose Orion, a system that transparently intercepts GPU kernel launches from multiple clients sharing a GPU. Orion schedules work on the GPU at the granularity of individual operators and minimizes interference by taking into account each operator's compute and memory requirements. We integrate Orion in PyTorch and demonstrate its benefits in various DNN workload collocation use cases. Orion significantly improves tail latency compared to state-of-the-art baselines for a high-priority inference job while collocating best-effort inference jobs to increase per-GPU request throughput by up to 7.3\texttimes{}, or while collocating DNN training, saving up to 1.49\texttimes{} in training costs compared to dedicated GPU allocation.},
booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
pages = {1075–1092},
numpages = {18},
keywords = {GPUs, Machine Learning},
location = {Athens, Greece},
series = {EuroSys '24}
}

@misc{anand2025onlinecodespecializationsystems,
      title={Towards Online Code Specialization of Systems}, 
      author={Vaastav Anand and Deepak Garg and Antoine Kaufmann},
      year={2025},
      eprint={2501.11366},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2501.11366}, 
}


@inproceedings{alpayOnePassBind2023,
  title = {One {{Pass}} to {{Bind Them}}: {{The First Single-Pass SYCL Compiler}} with {{Unified Code Representation Across Backends}}},
  shorttitle = {One {{Pass}} to {{Bind Them}}},
  booktitle = {International {{Workshop}} on {{OpenCL}}},
  author = {Alpay, Aksel and Heuveline, Vincent},
  year = {2023},
  month = apr,
  pages = {1--12},
  publisher = {ACM},
  address = {Cambridge United Kingdom},
  doi = {10.1145/3585341.3585351},
  urldate = {2025-03-23},
  abstract = {Current SYCL implementations rely on multiple compiler invocations to generate code for host and device, and typically even employ one compiler invocation per required backend code format such as SPIR-V, PTX or amdgcn. This makes generating ``universal'' binaries that can run on all devices supported by a SYCL implementation very time-consuming, or outright impractical. The ability to generate such universal binaries is however important e.g. when a software vendor wishes to distribute binaries to users that rely on unknown hardware configurations.},
  isbn = {9798400707452},
  langid = {english},
  keywords = {/unread},
  file = {/work/projects/Zotero/storage/43EC9ZCY/Alpay and Heuveline - 2023 - One Pass to Bind Them The First Single-Pass SYCL .pdf}
}
