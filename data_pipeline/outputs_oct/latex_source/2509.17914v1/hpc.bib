
@inproceedings{10.1145/3615318.3615319,
  author = {Hammond, Jeff and Dalcin, Lisandro and Schnetter, Erik and P\'{e}
            Rache, Marc and Besnard, Jean-Baptiste and Brown, Jed and Gadeschi,
            Gonzalo Brito and Byrne, Simon and Schuchart, Joseph and Zhou, Hui},
  title = {MPI Application Binary Interface Standardization},
  year = {2023},
  isbn = {9798400709135},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3615318.3615319},
  doi = {10.1145/3615318.3615319},
  abstract = {MPI is the most widely used interface for high-performance
              computing (HPC) workloads. Its success lies in its embrace of
              libraries and ability to evolve while maintaining backward
              compatibility for older codes, enabling them to run on new
              architectures for many years. In this paper, we propose a new level
              of MPI compatibility: a standard Application Binary Interface
              (ABI). We review the history of MPI implementation ABIs, identify
              the constraints from the MPI standard and ISO C, and summarize
              recent efforts to develop a standard ABI for MPI. We provide the
              current proposal from the MPI Forum’s ABI working group, which has
              been prototyped both within MPICH and as an independent abstraction
              layer called Mukautuva. We also list several use cases that would
              benefit from the definition of an ABI while outlining the remaining
              constraints.},
  booktitle = {Proceedings of the 30th European MPI Users' Group Meeting},
  articleno = {1},
  numpages = {12},
  keywords = {MPI},
  location = {Bristol, United Kingdom},
  series = {EuroMPI '23},
}
@article{CARTEREDWARDS20143202,
  title = {Kokkos: Enabling manycore performance portability through polymorphic
           memory access patterns},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {74},
  number = {12},
  pages = {3202-3216},
  year = {2014},
  note = {Domain-Specific Languages and High-Level Frameworks for
          High-Performance Computing},
  issn = {0743-7315},
  doi = {https://doi.org/10.1016/j.jpdc.2014.07.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731514001257},
  author = {H. {Carter Edwards} and Christian R. Trott and Daniel Sunderland},
  keywords = {Parallel computing, Thread parallelism, Manycore, GPU, Performance
              portability, Multidimensional array, Mini-application},
  abstract = {The manycore revolution can be characterized by increasing thread
              counts, decreasing memory per thread, and diversity of continually
              evolving manycore architectures. High performance computing (HPC)
              applications and libraries must exploit increasingly finer levels
              of parallelism within their codes to sustain scalability on these
              devices. A major obstacle to performance portability is the diverse
              and conflicting set of constraints on memory access patterns across
              devices. Contemporary portable programming models address manycore
              parallelism (e.g., OpenMP, OpenACC, OpenCL) but fail to address
              memory access patterns. The Kokkos C++ library enables applications
              and domain libraries to achieve performance portability on diverse
              manycore architectures by unifying abstractions for both fine-grain
              data parallelism and memory access patterns. In this paper we
              describe Kokkos’ abstractions, summarize its application programmer
              interface (API), present performance results for unit-test kernels
              and mini-applications, and outline an incremental strategy for
              migrating legacy C++ codes to Kokkos. The Kokkos library is under
              active research and development to incorporate capabilities from
              new generations of manycore architectures, and to address a growing
              list of applications and domain libraries.},
}


@article{9484790,
  author = {Pennycook, S. John and Sewall, Jason D. and Jacobsen, Douglas W. and
            Deakin, Tom and McIntosh-Smith, Simon},
  journal = {Computing in Science \& Engineering},
  title = {Navigating Performance, Portability, and Productivity},
  year = {2021},
  volume = {23},
  number = {5},
  pages = {28-38},
  keywords = {Measurement;Productivity;Performance
              evaluation;Navigation;Harmonic analysis},
  doi = {10.1109/MCSE.2021.3097276},
}

@inproceedings{10.1145/3458817.3476176,
  author = {Ziogas, Alexandros Nikolaos and Schneider, Timo and Ben-Nun, Tal and
            Calotoiu, Alexandru and De Matteis, Tiziano and de Fine Licht,
            Johannes and Lavarini, Luca and Hoefler, Torsten},
  title = {Productivity, portability, performance: data-centric Python},
  year = {2021},
  isbn = {9781450384421},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3458817.3476176},
  doi = {10.1145/3458817.3476176},
  abstract = {Python has become the de facto language for scientific computing.
              Programming in Python is highly productive, mainly due to its rich
              science-oriented software ecosystem built around the NumPy module.
              As a result, the demand for Python support in High Performance
              Computing (HPC) has skyrocketed. However, the Python language
              itself does not necessarily offer high performance. In this work,
              we present a workflow that retains Python's high productivity while
              achieving portable performance across different architectures. The
              workflow's key features are HPC-oriented language extensions and a
              set of automatic optimizations powered by a data-centric
              intermediate representation. We show performance results and
              scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up
              to 23,328 cores), with 2.47x and 3.75x speedups over previous-best
              solutions, first-ever Xilinx and Intel FPGA results of annotated
              Python, and up to 93.16\% scaling efficiency on 512 nodes.},
  booktitle = {Proceedings of the International Conference for High Performance
               Computing, Networking, Storage and Analysis},
  articleno = {95},
  numpages = {13},
  keywords = {NumPy, data-centric, high performance computing, python},
  location = {St. Louis, Missouri},
  series = {SC '21},
}

@inproceedings{8945721,
  author = {Beckingsale, David A. and Burmark, Jason and Hornung, Rich and Jones
            , Holger and Killian, William and Kunen, Adam J. and Pearce, Olga and
            Robinson, Peter and Ryujin, Brian S. and Scogland, Thomas RW},
  booktitle = {2019 IEEE/ACM International Workshop on Performance, Portability
               and Productivity in HPC (P3HPC)},
  title = {RAJA: Portable Performance for Large-Scale Scientific Applications},
  year = {2019},
  volume = {},
  number = {},
  pages = {71-81},
  keywords = {C++ languages;Graphics processing
              units;Programming;Production;Computer architecture;Kernel;Libraries
              },
  doi = {10.1109/P3HPC49587.2019.00012},
}

@inproceedings{10.1145/3529538.3529980,
  author = {Breyer, Marcel and Van Craen, Alexander and Pfl\"{u}ger, Dirk},
  title = {A Comparison of SYCL, OpenCL, CUDA, and OpenMP for Massively Parallel
           Support Vector Machine Classification on Multi-Vendor Hardware},
  year = {2022},
  isbn = {9781450396585},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3529538.3529980},
  doi = {10.1145/3529538.3529980},
  abstract = {In scientific computing and Artificial Intelligence (AI), which
              both rely on massively parallel tasks, frameworks like the Compute
              Unified Device Architecture (CUDA) and the Open Computing Language
              (OpenCL) are widely used to harvest the computational power of
              accelerator cards, in particular of Graphics Processing Units
              (GPUs). A few years ago, GPUs from NVIDIA were used almost
              exclusively for these tasks but meanwhile, AMD and Intel are
              increasing their shares of the GPUs market. This introduces many
              new challenges for code development, as the prevailing CUDA code
              can only run on NVIDIA hardware and must be adapted or even
              completely rewritten to run on GPUs from AMD or Intel. In this
              paper, we compare the different competing programming frameworks
              OpenMP, CUDA, OpenCL, and SYCL, paying special attention to the two
              SYCL implementations hipSYCL and DPC++. Thereby, we investigate the
              different frameworks with respect to their usability, performance,
              and performance portability on a variety of hardware platforms from
              different vendors, i.e., GPUs from NVIDIA, AMD, and Intel and
              Central Processing Units (CPUs) from AMD and Intel. Besides
              discussing the runtimes of these frameworks on the different
              hardware platforms, we also focus our comparison on the differences
              between the nd_range kernel formulation and the SYCL specific
              hierarchical kernels. Our Parallel Least Squares Support Vector
              Machine (PLSSVM) library implements backends for the four
              previously mentioned programming frameworks for a Least Squares
              Support Vector Machines (LS-SVMs). At its example, we show which of
              the frameworks is best suited for a standard workload that is
              frequently employed in scientific computing and AI, depending on
              the target hardware: The most computationally intensive part of our
              PLSSVM library is solving a system of linear equations using the
              Conjugate Gradient (CG) method. Specifically, we parallelize the
              implicit matrix-vector multiplication inside the CG method, a
              workload common in many scientific codes. The PLSSVM code, utility
              scripts, and documentation are all available on GitHub:
              https://github.com/SC-SGS/PLSSVM.},
  booktitle = {Proceedings of the 10th International Workshop on OpenCL},
  articleno = {2},
  numpages = {12},
  keywords = {CPU, CUDA, GPU, Machine Learning, OpenCL, OpenMP, Performance
              Evaluation, Performance Portability, SVM, SYCL},
  location = {Bristol, United Kingdom, United Kingdom},
  series = {IWOCL '22},
}

@misc{dockerqemu,
  title = {Multi-platform builds},
  author = {Docker},
  year = {2025},
  url = {https://docs.docker.com/build/building/multi-platform/},
  note = {Accessed: 2025-01-04},
}


@article{9242268,
  author = {Zhao, Nannan and Tarasov, Vasily and Albahar, Hadeel and Anwar, Ali
            and Rupprecht, Lukas and Skourtis, Dimitrios and Paul, Arnab K. and
            Chen, Keren and Butt, Ali R.},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  title = {Large-Scale Analysis of Docker Images and Performance Implications
           for Container Storage Systems},
  year = {2021},
  volume = {32},
  number = {4},
  pages = {918-930},
  keywords = {Containers;Image
              coding;Cows;Crawlers;Measurement;Libraries;Ecosystems;Containers;Docker;container
              images;container registry;deduplication;Docker hub;container
              storage drivers},
  doi = {10.1109/TPDS.2020.3034517},
}

@inproceedings{9556086,
  author = {León, Edgar A. and Joos, Marc and Hanford, Nathan and Cotte, Adrien
            and Delforge, Tony and Diakhaté, François and Ducrot, Vincent and
            Karlin, Ian and Pérache, Marc},
  booktitle = {2021 IEEE International Conference on Cluster Computing (CLUSTER)
               },
  title = {On-the-Fly, Robust Translation of MPI Libraries},
  year = {2021},
  volume = {},
  number = {},
  pages = {504-515},
  keywords = {Productivity;Heart;Architecture;Conferences;Graphics processing
              units;Computer architecture;Switches;Wi4MPI;ABI compatibility;ABI
              dynamic translation;portable MPI;MPI libraries},
  doi = {10.1109/Cluster48925.2021.00026},
} 


@inproceedings{10.1145/3688351.3689152,
  author = {Heerekar, Balvansh and Philippidis, Cesar and Chuang, Ho-Ren and
            Olivier, Pierre and Barbalace, Antonio and Ravindran, Binoy},
  title = {Offloading Datacenter Jobs to RISC-V Hardware for Improved
           Performance and Power Efficiency},
  year = {2024},
  isbn = {9798400711817},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3688351.3689152},
  doi = {10.1145/3688351.3689152},
  abstract = {The end of Moore's Law has brought significant changes in the
              architecture of servers used in data centers, increasingly
              incorporating new ISAs beyond x86-64 as well as diverse
              accelerators. Further, single-board computers have become
              increasingly efficient and can run certain Linux applications at
              significantly lower equipment and energy costs compared to
              traditional servers. Past research has demonstrated that offloading
              applications at runtime from x86-based servers to ARM-based
              single-board computers can result in increases in throughput and
              energy efficiency. The RISC-V architecture has recently gained
              significant commercial interest, and OS-capable single-board
              computers with RISC-V cores are increasingly available at the
              commodity scale.In this paper we propose a system that offloads
              jobs from an x86 server to a RISC-V single-board computer at
              runtime, with the goals of improving job throughput and energy
              saved. Towards this, we port the Popcorn Linux multi-ISA toolchain
              and runtime framework to RISC-V, enabling the live migration of
              applications between an x86 Xeon server and a SiFive HiFive RISC-V
              board. We further propose a scheduling policy, Lowest Slowdown
              First (LSF) that drives the offloading of long-running and stateful
              datacenter background jobs from the server to the board, to
              alleviate workload congestion on the server. LSF's policy relies on
              monitoring jobs' performance on the server, predicting the slowdown
              they would suffer if running on the board, and migrating the jobs
              with the lowest estimated slowdown. Our evaluation shows that LSF
              yields up to 20\% increase in throughput while also gaining 16\%
              more energy efficiency for compute-intensive workloads.},
  booktitle = {Proceedings of the 17th ACM International Systems and Storage
               Conference},
  pages = {39–52},
  numpages = {14},
  keywords = {RISC-V, execution migration, heterogeneous ISA, x86},
  location = {Virtual, Israel},
  series = {SYSTOR '24},
}

@misc{nvidiangc,
  title = {NVIDIA NGC Containers},
  author = {NVIDIA},
  year = {2025},
  url = {https://www.nvidia.com/en-us/gpu-cloud/},
  note = {Accessed: 2025-01-04},
}

@misc{amdinfinity,
  title = {AMD Infinity Hub},
  author = {AMD},
  year = {2025},
  url = {https://www.amd.com/en/developer/resources/infinity-hub.html},
  note = {Accessed: 2025-01-04},
}

@inproceedings{10.1007/978-3-030-34356-9_5,
  author = "Benedicic, Lucas and Cruz, Felipe A. and Madonna, Alberto and
            Mariotti, Kean",
  editor = "Weiland, Mich{\`e}le and Juckeland, Guido and Alam, Sadaf and Jagode
            , Heike",
  title = "Sarus: Highly Scalable Docker Containers for HPC Systems",
  booktitle = "High Performance Computing",
  year = "2019",
  publisher = "Springer International Publishing",
  address = "Cham",
  pages = "46--60",
  abstract = "The convergence of HPC and cloud computing is pushing HPC service
              providers to enrich their service portfolio with workflows based on
              complex software stacks. Such transformation presents an
              opportunity for the science community to improve its computing
              practices with solutions developed in enterprise environments.
              Software containers increase productivity by packaging applications
              into portable units that are easy to deploy, but generally come at
              the expense of performance and scalability. This work presents
              Sarus, a container engine for HPC environments that offers security
              oriented to multi-tenant systems, container filesystems tailored
              for parallel storage, compatibility with Docker images, user-scoped
              image management, and integration with workload managers. Docker
              containers of HPC applications deployed with Sarus on up to 2888
              GPU nodes show two significant results: OCI hooks allow users and
              system administrators to transparently benefit from plugins that
              enable system-specific hardware; and the same level of performance
              and scalability than native execution is achieved.",
  isbn = "978-3-030-34356-9",
}

@inproceedings{10029965,
  author = {Madonna, Alberto and Aliaga, Tomas},
  booktitle = {2022 IEEE/ACM 4th International Workshop on Containers and New
               Orchestration Paradigms for Isolated Environments in HPC
               (CANOPIE-HPC)},
  title = {Libfabric-based Injection Solutions for Portable Containerized MPI
           Applications},
  year = {2022},
  volume = {},
  number = {},
  pages = {45-56},
  keywords = {Runtime;High-speed networks;Linux;Message passing;High performance
              computing;Containers;Benchmark testing;High Performance
              Computing;Container;MPI;Libfabric},
  doi = {10.1109/CANOPIE-HPC56864.2022.00010},
}

@inproceedings{10030014,
  author = {Stephey, Laurie and Canon, Shane and Gaur, Aditi and Fulton, Daniel
            and Younge, Andrew J.},
  booktitle = {2022 IEEE/ACM 4th International Workshop on Containers and New
               Orchestration Paradigms for Isolated Environments in HPC
               (CANOPIE-HPC)},
  title = {Scaling Podman on Perlmutter: Embracing a community-supported
           container ecosystem},
  year = {2022},
  volume = {},
  number = {},
  pages = {25-35},
  keywords = {Technological
              innovation;Runtime;Conferences;Ecosystems;Production;Containers;Benchmark
              testing;High-Performance Computing;Containers},
  doi = {10.1109/CANOPIE-HPC56864.2022.00008},
}

@article{10.1371/journal.pone.0177459,
  doi = {10.1371/journal.pone.0177459},
  author = {Kurtzer, Gregory M. AND Sochat, Vanessa AND Bauer, Michael W.},
  journal = {PLOS ONE},
  publisher = {Public Library of Science},
  title = {Singularity: Scientific containers for mobility of compute},
  year = {2017},
  month = {05},
  volume = {12},
  url = {https://doi.org/10.1371/journal.pone.0177459},
  pages = {1-20},
  abstract = {Here we present Singularity, software developed to bring
              containers and reproducibility to scientific computing. Using
              Singularity containers, developers can work in reproducible
              environments of their choosing and design, and these complete
              environments can easily be copied and executed on other platforms.
              Singularity is an open source initiative that harnesses the
              expertise of system and software engineers and researchers alike,
              and integrates seamlessly into common workflows for both of these
              groups. As its primary use case, Singularity brings mobility of
              computing to both users and HPC centers, providing a secure means
              to capture and distribute software and compute environments. This
              ability to create and deploy reproducible environments across these
              centers, a previously unmet need, makes Singularity a game changing
              development for computational science.},
  number = {5},
}

@misc{apptainerdocs,
  title = {Apptainer and MPI applications},
  author = {Apptainer},
  year = {2025},
  url = {https://apptainer.org/docs/user/latest/mpi.html},
  note = {Accessed: 2025-01-04},
}

@inproceedings{10.1145/3037697.3037738,
  author = {Barbalace, Antonio and Lyerly, Robert and Jelesnianski, Christopher
            and Carno, Anthony and Chuang, Ho-Ren and Legout, Vincent and
            Ravindran, Binoy},
  title = {Breaking the Boundaries in Heterogeneous-ISA Datacenters},
  year = {2017},
  isbn = {9781450344654},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3037697.3037738},
  doi = {10.1145/3037697.3037738},
  abstract = {Energy efficiency is one of the most important design
              considerations in running modern datacenters. Datacenter operating
              systems rely on software techniques such as execution migration to
              achieve energy efficiency across pools of machines. Execution
              migration is possible in datacenters today because they consist
              mainly of homogeneous-ISA machines. However, recent market trends
              indicate that alternate ISAs such as ARM and PowerPC are pushing
              into the datacenter, meaning current execution migration techniques
              are no longer applicable. How can execution migration be applied in
              future heterogeneous-ISA datacenters?In this work we present a
              compiler, runtime, and an operating system extension for enabling
              execution migration between heterogeneous-ISA servers. We present a
              new multi-ISA binary architecture and heterogeneous-OS containers
              for facilitating efficient migration of natively-compiled
              applications. We build and evaluate a prototype of our design and
              demonstrate energy savings of up to 66\% for a workload running on
              an ARM and an x86 server interconnected by a high-speed network.},
  booktitle = {Proceedings of the Twenty-Second International Conference on
               Architectural Support for Programming Languages and Operating
               Systems},
  pages = {645–659},
  numpages = {15},
  keywords = {state transformation, replicated-kernel os, process migration,
              heterogeneous isas, compilers},
  location = {Xi'an, China},
  series = {ASPLOS '17},
}

@article{10.1145/3093337.3037738,
  author = {Barbalace, Antonio and Lyerly, Robert and Jelesnianski, Christopher
            and Carno, Anthony and Chuang, Ho-Ren and Legout, Vincent and
            Ravindran, Binoy},
  title = {Breaking the Boundaries in Heterogeneous-ISA Datacenters},
  year = {2017},
  issue_date = {March 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {45},
  number = {1},
  issn = {0163-5964},
  url = {https://doi.org/10.1145/3093337.3037738},
  doi = {10.1145/3093337.3037738},
  abstract = {Energy efficiency is one of the most important design
              considerations in running modern datacenters. Datacenter operating
              systems rely on software techniques such as execution migration to
              achieve energy efficiency across pools of machines. Execution
              migration is possible in datacenters today because they consist
              mainly of homogeneous-ISA machines. However, recent market trends
              indicate that alternate ISAs such as ARM and PowerPC are pushing
              into the datacenter, meaning current execution migration techniques
              are no longer applicable. How can execution migration be applied in
              future heterogeneous-ISA datacenters?In this work we present a
              compiler, runtime, and an operating system extension for enabling
              execution migration between heterogeneous-ISA servers. We present a
              new multi-ISA binary architecture and heterogeneous-OS containers
              for facilitating efficient migration of natively-compiled
              applications. We build and evaluate a prototype of our design and
              demonstrate energy savings of up to 66\% for a workload running on
              an ARM and an x86 server interconnected by a high-speed network.},
  journal = {SIGARCH Comput. Archit. News},
  month = apr,
  pages = {645–659},
  numpages = {15},
  keywords = {state transformation, replicated-kernel os, process migration,
              heterogeneous isas, compilers},
}

@inproceedings{10.1145/3381052.3381321,
  author = {Barbalace, Antonio and Karaoui, Mohamed L. and Wang, Wei and Xing,
            Tong and Olivier, Pierre and Ravindran, Binoy},
  title = {Edge computing: the case for heterogeneous-ISA container migration},
  year = {2020},
  isbn = {9781450375542},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3381052.3381321},
  doi = {10.1145/3381052.3381321},
  abstract = {Edge computing is a recent computing paradigm that brings cloud
              services closer to the client. Among other features, edge computing
              offers extremely low client/server latencies. To consistently
              provide such low latencies, services need to run on edge nodes that
              are physically as close as possible to their clients. Thus, when a
              client changes its physical location, a service should migrate
              between edge nodes to maintain proximity. Differently from cloud
              nodes, edge nodes are built with CPUs of different Instruction Set
              Architectures (ISAs), hence a server program natively compiled for
              one ISA cannot migrate to another. This hinders migration to the
              closest node.We introduce H-Container, which migrates
              natively-compiled containerized applications across compute nodes
              featuring CPUs of different ISAs. H-Container advances over
              existing heterogeneous-ISA migration systems by being a) highly
              compatible - no source code nor compiler toolchain modifications
              are needed; b) easily deployable - fully implemented in user space,
              thus without any OS or hypervisor dependency, and c) largely Linux
              compliant - can migrate most Linux software, including server
              applications and dynamically linked binaries. H-Container targets
              Linux, adopts LLVM, extends CRIU, and integrates with Docker.
              Experiments demonstrate that H-Container adds no overhead on
              average during program execution, while between 10ms and 100ms are
              added during migration. Furthermore, we show the benefits of
              H-Container in real scenarios, proving for example up to 94\%
              increase in Redis throughput when unlocking heterogeneity.},
  booktitle = {Proceedings of the 16th ACM SIGPLAN/SIGOPS International
               Conference on Virtual Execution Environments},
  pages = {73–87},
  numpages = {15},
  keywords = {containers, edge, heterogeneous ISA, migration},
  location = {Lausanne, Switzerland},
  series = {VEE '20},
}

@article{10.1145/3524452,
  author = {Xing, Tong and Barbalace, Antonio and Olivier, Pierre and Karaoui,
            Mohamed L. and Wang, Wei and Ravindran, Binoy},
  title = {H-Container: Enabling Heterogeneous-ISA Container Migration in Edge
           Computing},
  year = {2022},
  issue_date = {November 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {39},
  number = {1–4},
  issn = {0734-2071},
  url = {https://doi.org/10.1145/3524452},
  doi = {10.1145/3524452},
  abstract = {Edge computing is a recent computing paradigm that brings cloud
              services closer to the client. Among other features, edge computing
              offers extremely low client/server latencies. To consistently
              provide such low latencies, services should run on edge nodes that
              are physically as close as possible to their clients. Thus, when
              the physical location of a client changes, a service should migrate
              between edge nodes to maintain proximity. Differently from cloud
              nodes, edge nodes integrate CPUs of different Instruction Set
              Architectures (ISAs), hence a program natively compiled for a given
              ISA cannot migrate to a server equipped with a CPU of a different
              ISA. This hinders migration to the closest node.We introduce
              H-Container, a system that migrates natively compiled containerized
              applications across compute nodes featuring CPUs of different ISAs.
              H-Container advances over existing heterogeneous-ISA migration
              systems by being (a) highly compatible – no user’s source-code nor
              compiler toolchain modifications are needed; (b) easily deployable
              – fully implemented in user space, thus without any OS or
              hypervisor dependency, and (c) largely Linux-compliant – it can
              migrate most Linux software, including server applications and
              dynamically linked binaries. H-Container targets Linux and its
              already-compiled executables, adopts LLVM, extends CRIU, and
              integrates with Docker. Experiments demonstrate that H-Container
              adds no overheads during program execution, while 10–100 ms are
              added during migration. Furthermore, we show the benefits of
              H-Container in real-world scenarios, demonstrating, for example, up
              to 94\% increase in Redis throughput when client/server proximity
              is maintained through heterogeneous container migration.},
  journal = {ACM Trans. Comput. Syst.},
  month = jul,
  articleno = {5},
  numpages = {36},
  keywords = {Edge, heterogeneous ISA, containers, migration},
}

@misc{cudacompilationtrajectory,
  title = {The CUDA Compilation Trajectory},
  author = {NVIDIA},
  year = {2025},
  url = {https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#the-cuda-compilation-trajectory},
  note = {Accessed: 2025-08-25},
}

@misc{cudadocsvirtualarch,
  title = {CUDA: Virtual Architectures},
  author = {NVIDIA},
  year = {2025},
  url = {
         https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#virtual-architectures
         },
  note = {Accessed: 2025-01-04},
}

@inproceedings{10.1145/2807591.2807623,
  author = {Gamblin, Todd and LeGendre, Matthew and Collette, Michael R. and Lee
            , Gregory L. and Moody, Adam and de Supinski, Bronis R. and Futral,
            Scott},
  title = {The Spack package manager: bringing order to HPC software chaos},
  year = {2015},
  isbn = {9781450337236},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2807591.2807623},
  doi = {10.1145/2807591.2807623},
  abstract = {Large HPC centers spend considerable time supporting software for
              thousands of users, but the complexity of HPC software is quickly
              outpacing the capabilities of existing software management tools.
              Scientific applications require specific versions of compilers, MPI
              , and other dependency libraries, so using a single, standard
              software stack is infeasible. However, managing many configurations
              is difficult because the configuration space is combinatorial in
              size.We introduce Spack, a tool used at Lawrence Livermore National
              Laboratory to manage this complexity. Spack provides a novel,
              recursive specification syntax to invoke parametric builds of
              packages and dependencies. It allows any number of builds to
              coexist on the same system, and it ensures that installed packages
              can find their dependencies, regardless of the environment. We show
              through real-world use cases that Spack supports diverse and
              demanding applications, bringing order to HPC software chaos.},
  booktitle = {Proceedings of the International Conference for High Performance
               Computing, Networking, Storage and Analysis},
  articleno = {40},
  numpages = {12},
  location = {Austin, Texas},
  series = {SC '15},
}

@inproceedings{6495863,
  author = {Hoste, Kenneth and Timmerman, Jens and Georges, Andy and De Weirdt,
            Stijn},
  booktitle = {2012 SC Companion: High Performance Computing, Networking Storage
               and Analysis},
  title = {EasyBuild: Building Software with Ease},
  year = {2012},
  volume = {},
  number = {},
  pages = {572-582},
  keywords = {Software;Software packages;Tools;Python;Libraries;Task
              analysis;Maintenance engineering;scientific
              software;compilation;installation;Python;automation;build procedure
              },
  doi = {10.1109/SC.Companion.2012.81},
}

@inproceedings{7081225,
  author = {Geimer, Markus and Hoste, Kenneth and McLay, Robert},
  booktitle = {2014 First International Workshop on HPC User Support Tools},
  title = {Modern Scientific Software Management Using EasyBuild and Lmod},
  year = {2014},
  volume = {},
  number = {},
  pages = {41-51},
  keywords = {Software
              packages;Libraries;Loading;Communities;Collaboration;Buildings},
  doi = {10.1109/HUST.2014.8},
}

@inproceedings{10046107,
  author = {Gamblin, Todd and Culpo, Massimiliano and Becker, Gregory and
            Shudler, Sergei},
  booktitle = {SC22: International Conference for High Performance Computing,
               Networking, Storage and Analysis},
  title = {Using Answer Set Programming for HPC Dependency Solving},
  year = {2022},
  volume = {},
  number = {},
  pages = {1-15},
  keywords = {Runtime;Semantics;Full stack;Programming;Syntactics;Search
              problems;Software;High performance computing;Software
              packages;Package management;Logic programming;Answer set
              programming;Software reusability;Dependency management},
  doi = {10.1109/SC41404.2022.00040},
}


@inproceedings{10793143,
  author = {Nichols, Daniel and Menon, Harshitha and Gamblin, Todd and Bhatele,
            Abhinav},
  booktitle = {SC24: International Conference for High Performance Computing,
               Networking, Storage and Analysis},
  title = {A Probabilistic Approach To Selecting Build Configurations in Package
           Managers},
  year = {2024},
  volume = {},
  number = {},
  pages = {1-13},
  keywords = {Software packages;High performance computing;Computational
              modeling;Buildings;Full stack;Machine learning;Predictive
              models;Probabilistic logic;Libraries;package managers;build
              configuration;version selection;machine learning},
  doi = {10.1109/SC41406.2024.00090},
}


@misc{spackbinary,
  title = {Announcing public binaries for Spack},
  author = {Spack},
  year = {2022},
  url = {https://spack.io/spack-binary-packages/},
  note = {Accessed: 2025-01-04},
}

@article{https://doi.org/10.1002/spe.3075,
  author = {Dröge, Bob and Holanda Rusu, Victor and Hoste, Kenneth and van
            Leeuwen, Caspar and O'Cais, Alan and Röblitz, Thomas},
  title = {EESSI: A cross-platform ready-to-use optimised scientific software
           stack},
  journal = {Software: Practice and Experience},
  volume = {53},
  number = {1},
  pages = {176-210},
  keywords = {high-performance computing, scientific software, supercomputing},
  doi = {https://doi.org/10.1002/spe.3075},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3075},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.3075},
  abstract = {Abstract Getting scientific software installed correctly and
              ensuring it performs well has been a ubiquitous problem for several
              decades now, which is compounded currently by the changing
              landscape of computational science with the (re-)emergence of
              different microprocessor families, and the expansion to additional
              scientific domains like artificial intelligence and next-generation
              sequencing. The European Environment for Scientific Software
              Installations (EESSI) project aims to provide a ready-to-use stack
              of scientific software installations that can be leveraged easily
              on a variety of platforms, ranging from personal workstations to
              cloud environments and supercomputer infrastructure, without making
              compromises with respect to performance. In this article, we
              provide a detailed overview of the project, highlight potential use
              cases, and demonstrate that the performance of the provided
              scientific software installations can be competitive with
              system-specific installations.},
  year = {2023},
}

@article{https://doi.org/10.1002/cpe.8203,
  author = {Shehata, Amir and Naughton, Thomas and Bernholdt, David E. and
            Pritchard, Howard},
  title = {Bringing HPE Slingshot 11 support to Open MPI},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {36},
  number = {22},
  pages = {e8203},
  keywords = {high performance computing, libfabric, message passing interface,
              Open MPI, Slingshot},
  doi = {https://doi.org/10.1002/cpe.8203},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.8203},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.8203},
  abstract = {Summary The Cray HPE Slingshot 11 network is used on the new
              exascale systems arriving at the U.S. Department of Energy (DoE)
              laboratories (e.g., Frontier, Aurora, Perlmutter). As such, the
              support of this network is an important capability to meet the
              needs of exascale applications. This article highlights recent work
              to develop supporting infrastructure to enable Open MPI to
              efficiently support these new platforms. A key component of this
              effort involves development of a new Open Fabrics Interface (OFI)
              provider, LinkX. We discuss the design and development of
              enhancements that take advantage of the new Slingshot 11 network
              and AMD GPUs. We include performance data from tests on the
              Frontier supercomputer using synthetic communication benchmarks,
              and the vendor provided MPI as a baseline for comparison. The tests
              demonstrate full functionality of Open MPI on the system and
              initial results show favorable performance when compared to the
              highly tuned vendor implementation.},
  year = {2024},
}
@inproceedings{7103433,
  author = {Ahn, Dong H. and Garlick, Jim and Grondona, Mark and Lipari, Don and
            Springmeyer, Becky and Schulz, Martin},
  booktitle = {2014 43rd International Conference on Parallel Processing
               Workshops},
  title = {Flux: A Next-Generation Resource Management Framework for Large HPC
           Centers},
  year = {2014},
  volume = {},
  number = {},
  pages = {9-17},
  keywords = {Resource management;Scheduling;Computational
              modeling;Software;Synchronization;Processor
              scheduling;Prototypes;resource management;communication
              framework;run-time;key value store;scalable process management
              services},
  doi = {10.1109/ICPPW.2014.15},
}

@misc{githublibfabric,
  title = {Open Fabric Interfaces},
  author = {OFIWG},
  year = {2024},
  url = {https://github.com/ofiwg/libfabric/tree/v2.0.0},
  note = {Accessed: 2025-01-04},
}
@inproceedings{9355239,
  author = {Sato, Mitsuhisa and Ishikawa, Yutaka and Tomita, Hirofumi and Kodama
            , Yuetsu and Odajima, Tetsuya and Tsuji, Miwako and Yashiro, Hisashi
            and Aoki, Masaki and Shida, Naoyuki and Miyoshi, Ikuo and Hirai,
            Kouichi and Furuya, Atsushi and Asato, Akira and Morita, Kuniki and
            Shimizu, Toshiyuki},
  booktitle = {SC20: International Conference for High Performance Computing,
               Networking, Storage and Analysis},
  title = {Co-Design for A64FX Manycore Processor and ”Fugaku”},
  year = {2020},
  volume = {},
  number = {},
  pages = {1-15},
  keywords = {Industries;Instruction sets;High performance
              computing;Supercomputers;Manycore processors;Next generation
              networking;Pragmatics;exascale computing;co-design;many core
              processor;high-performance computing},
  doi = {10.1109/SC41405.2020.00051},
}
@inproceedings{10.1145/3636480.3637097,
  author = {Simakov, Nikolay A. and Jones, Matthew D. and Furlani, Thomas R. and
            Siegmann, Eva and Harrison, Robert J.},
  title = {First Impressions of the NVIDIA Grace CPU Superchip and NVIDIA Grace
           Hopper Superchip for Scientific Workloads},
  year = {2024},
  isbn = {9798400716522},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3636480.3637097},
  doi = {10.1145/3636480.3637097},
  abstract = {The engineering samples of the NVIDIA Grace CPU Superchip and
              NVIDIA Grace Hopper Superchips were tested using different
              benchmarks and scientific applications. The benchmarks include HPCC
              and HPCG. The real application-based benchmark includes
              AI-Benchmark-Alpha (a TensorFlow benchmark), Gromacs, OpenFOAM, and
              ROMS. The performance was compared to multiple Intel, AMD, ARM CPUs
              and several x86 with NVIDIA GPU systems. A brief energy efficiency
              estimate was performed based on TDP values. We found that in HPCC
              benchmark tests, the per-core performance of Grace is similar to or
              faster than AMD Milan cores, and the high core count often allows
              NVIDIA Grace CPU Superchip to have per-node performance similar to
              Intel Sapphire Rapids with High Bandwidth Memory: slower in matrix
              multiplication (by 17\%) and FFT (by 6\%), faster in Linpack (by 9
              \%)). In scientific applications, the NVIDIA Grace CPU Superchip
              performance is slower by 6\% to 18\% in Gromacs, faster by 7\% in
              OpenFOAM, and right between HBM and DDR modes of Intel Sapphire
              Rapids in ROMS. The combined CPU-GPU performance in Gromacs is
              significantly faster (by 20\% to 117\% faster) than any tested
              x86-NVIDIA GPU system. Overall, the new NVIDIA Grace Hopper
              Superchip and NVIDIA Grace CPU Superchip Superchip are
              high-performance and most likely energy-efficient solutions for HPC
              centers.},
  booktitle = {Proceedings of the International Conference on High Performance
               Computing in Asia-Pacific Region Workshops},
  pages = {36–44},
  numpages = {9},
  keywords = {ARM, GPU, HPC, benchmarks, energy efficiency, x86},
  location = {Nagoya, Japan},
  series = {HPCAsia '24 Workshops},
}

  

@inproceedings{9835543,
  author = {Xu, Shulei and Shafi, Aamir and Subramoni, Hari and Panda,
            Dhabaleswar K.},
  booktitle = {2022 IEEE International Parallel and Distributed Processing
               Symposium Workshops (IPDPSW)},
  title = {Arm meets Cloud: A Case Study of MPI Library Performance on AWS
           Arm-based HPC Cloud with Elastic Fabric Adapter},
  year = {2022},
  volume = {},
  number = {},
  pages = {449-456},
  keywords = {Performance evaluation;Protocols;Web
              services;Clouds;Libraries;Fabrics;Reliability;HPC
              Cloud;MPI;Arm;Elastic Fabric Adapter;Scalable Reliable Datagram},
  doi = {10.1109/IPDPSW55747.2022.00083},
}

@article{https://doi.org/10.1002/jcc.20291,
  author = {Van Der Spoel, David and Lindahl, Erik and Hess, Berk and Groenhof,
            Gerrit and Mark, Alan E. and Berendsen, Herman J. C.},
  title = {GROMACS: Fast, flexible, and free},
  journal = {Journal of Computational Chemistry},
  volume = {26},
  number = {16},
  pages = {1701-1718},
  keywords = {GROMACS, molecular simulation software, molecular dynamics, free
              energy computation, parallel computation},
  doi = {https://doi.org/10.1002/jcc.20291},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.20291},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcc.20291},
  abstract = {Abstract This article describes the software suite GROMACS
              (Groningen MAchine for Chemical Simulation) that was developed at
              the University of Groningen, The Netherlands, in the early 1990s.
              The software, written in ANSI C, originates from a parallel
              hardware project, and is well suited for parallelization on
              processor clusters. By careful optimization of neighbor searching
              and of inner loop performance, GROMACS is a very fast program for
              molecular dynamics simulation. It does not have a force field of
              its own, but is compatible with GROMOS, OPLS, AMBER, and ENCAD
              force fields. In addition, it can handle polarizable shell models
              and flexible constraints. The program is versatile, as force
              routines can be added by the user, tabulated functions can be
              specified, and analyses can be easily customized. Nonequilibrium
              dynamics and free energy determinations are incorporated.
              Interfaces with popular quantum-chemical packages (MOPAC, GAMES-UK,
              GAUSSIAN) are provided to perform mixed MM/QM simulations. The
              package includes about 100 utility and analysis programs. GROMACS
              is in the public domain and distributed (with source code and
              documentation) under the GNU General Public License. It is
              maintained by a group of developers from the Universities of
              Groningen, Uppsala, and Stockholm, and the Max Planck Institute for
              Polymer Research in Mainz. Its Web site is http://www.gromacs.org.
              © 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1701–1718, 2005},
  year = {2005},
}

@article{Giannozzi_2009,
  doi = {10.1088/0953-8984/21/39/395502},
  url = {https://dx.doi.org/10.1088/0953-8984/21/39/395502},
  year = {2009},
  month = {sep},
  publisher = {},
  volume = {21},
  number = {39},
  pages = {395502},
  author = {Giannozzi, Paolo and Baroni, Stefano and Bonini, Nicola and Calandra
            , Matteo and Car, Roberto and Cavazzoni, Carlo and Ceresoli, Davide
            and Chiarotti, Guido L and Cococcioni, Matteo and Dabo, Ismaila and
            Dal Corso, Andrea and de Gironcoli, Stefano and Fabris, Stefano and
            Fratesi, Guido and Gebauer, Ralph and Gerstmann, Uwe and Gougoussis,
            Christos and Kokalj, Anton and Lazzeri, Michele and Martin-Samos,
            Layla and Marzari, Nicola and Mauri, Francesco and Mazzarello,
            Riccardo and Paolini, Stefano and Pasquarello, Alfredo and Paulatto,
            Lorenzo and Sbraccia, Carlo and Scandolo, Sandro and Sclauzero,
            Gabriele and Seitsonen, Ari P and Smogunov, Alexander and Umari,
            Paolo and Wentzcovitch, Renata M},
  title = {QUANTUM ESPRESSO: a modular and open-source software project for
           quantum simulations of materials},
  journal = {Journal of Physics: Condensed Matter},
  abstract = {QUANTUM ESPRESSO is an integrated suite of computer codes for
              electronic-structure calculations and materials modeling, based on
              density-functional theory, plane waves, and pseudopotentials
              (norm-conserving, ultrasoft, and projector-augmented wave). The
              acronym ESPRESSO stands for opEn Source Package for Research in
              Electronic Structure, Simulation, and Optimization. It is freely
              available to researchers around the world under the terms of the
              GNU General Public License. QUANTUM ESPRESSO builds upon
              newly-restructured electronic-structure codes that have been
              developed and tested by some of the original authors of novel
              electronic-structure algorithms and applied in the last twenty
              years by some of the leading materials modeling groups worldwide.
              Innovation and efficiency are still its main focus, with special
              attention paid to massively parallel architectures, and a great
              effort being devoted to user friendliness. QUANTUM ESPRESSO is
              evolving towards a distribution of independent and interoperable
              codes in the spirit of an open-source project, where researchers
              active in the field of electronic-structure calculations are
              encouraged to participate in the project by contributing their own
              codes or by implementing their own ideas into existing codes.},
}

@misc{mpixlatecray,
  title = {mpixlate},
  author = {Cray},
  year = {2025},
  url = {https://cpe.ext.hpe.com/docs/24.03/mpt/mpixlate/mpixlate.html},
  note = {Accessed: 2025-01-04},
}
@software{schnetter_2022_6174409,
  author = {Schnetter, Erik},
  title = {MPItrampoline},
  month = feb,
  year = 2022,
  publisher = {Zenodo},
  version = {v3.2.0},
  doi = {10.5281/zenodo.6174409},
  url = {https://doi.org/10.5281/zenodo.6174409},
}
@article{9810039,
  author = {Hoefler, Torsten and Hendel, Ariel and Roweth, Duncan},
  journal = {Computer},
  title = {The Convergence of Hyperscale Data Center and High-Performance
           Computing Networks},
  year = {2022},
  volume = {55},
  number = {7},
  pages = {29-37},
  keywords = {Data centers;High performance computing;Supercomputers;Convergence
              },
  doi = {10.1109/MC.2022.3158437},
}
@article{doi:10.1177/10943420231166608,
  author = {Satoshi Matsuoka and Jens Domke and Mohamed Wahib and Aleksandr
            Drozd and Torsten Hoefler},
  title = {Myths and legends in high-performance computing},
  journal = {The International Journal of High Performance Computing
             Applications},
  volume = {37},
  number = {3-4},
  pages = {245-259},
  year = {2023},
  doi = {10.1177/10943420231166608},
  URL = { https://doi.org/10.1177/10943420231166608 },
  eprint = { https://doi.org/10.1177/10943420231166608 },
}
@inproceedings{10.1145/3173162.3173182,
  author = {Ginsbach, Philip and Remmelg, Toomas and Steuwer, Michel and Bodin,
            Bruno and Dubach, Christophe and O'Boyle, Michael F. P.},
  title = {Automatic Matching of Legacy Code to Heterogeneous APIs: An Idiomatic
           Approach},
  year = {2018},
  isbn = {9781450349116},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3173162.3173182},
  doi = {10.1145/3173162.3173182},
  abstract = {Heterogeneous accelerators often disappoint. They provide the
              prospect of great performance, but only deliver it when using
              vendor specific optimized libraries or domain specific languages.
              This requires considerable legacy code modifications, hindering the
              adoption of heterogeneous computing. This paper develops a novel
              approach to automatically detect opportunities for accelerator
              exploitation. We focus on calculations that are well supported by
              established APIs: sparse and dense linear algebra, stencil codes
              and generalized reductions and histograms. We call them idioms and
              use a custom constraint-based Idiom Description Language (IDL) to
              discover them within user code. Detected idioms are then mapped to
              BLAS libraries, cuSPARSE and clSPARSE and two DSLs: Halide and
              Lift. We implemented the approach in LLVM and evaluated it on the
              NAS and Parboil sequential C/C++ benchmarks, where we detect 60
              idiom instances. In those cases where idioms are a significant part
              of the sequential execution time, we generate code that achieves
              1.26x to over 20x speedup on integrated and external GPUs.},
  booktitle = {Proceedings of the Twenty-Third International Conference on
               Architectural Support for Programming Languages and Operating
               Systems},
  pages = {139–153},
  numpages = {15},
  keywords = {computer systems organization},
  location = {Williamsburg, VA, USA},
  series = {ASPLOS '18},
}

@inproceedings{10.1145/3578360.3580262,
  author = {Mart\'{\i}nez, Pablo Antonio and Woodruff, Jackson and
            Armengol-Estap\'{e}, Jordi and Bernab\'{e}, Gregorio and Garc\'{\i}a,
            Jos\'{e} Manuel and O’Boyle, Michael F. P.},
  title = {Matching Linear Algebra and Tensor Code to Specialized Hardware
           Accelerators},
  year = {2023},
  isbn = {9798400700880},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3578360.3580262},
  doi = {10.1145/3578360.3580262},
  booktitle = {Proceedings of the 32nd ACM SIGPLAN International Conference on
               Compiler Construction},
  pages = {85–97},
  numpages = {13},
  keywords = {GEMM, LLVM, Offloading, Program synthesis},
  location = {Montr\'{e}al, QC, Canada},
  series = {CC 2023},
}

  

@inproceedings{8891611,
  author = { Collie, Bruce and Ginsbach, Philip and O'Boyle, Michael F.P. },
  booktitle = { 2019 28th International Conference on Parallel Architectures and
               Compilation Techniques (PACT) },
  title = {{ Type-Directed Program Synthesis and Constraint Generation for
           Library Portability }},
  year = {2019},
  volume = {},
  ISSN = {},
  pages = {55-67},
  abstract = { Fast numerical libraries have been a cornerstone of scientific
              computing for decades, but this comes at a price. Programs may be
              tied to vendor specific software ecosystems resulting in polluted,
              non-portable code. As we enter an era of heterogeneous computing,
              there is an explosion in the number of accelerator libraries
              required to harness specialized hardware. We need a system that
              allows developers to exploit ever-changing accelerator libraries,
              without over-specializing their code. As we cannot know the
              behavior of future libraries ahead of time, this paper develops a
              scheme that assists developers in matching their code to new
              libraries, without requiring the source code for these libraries.
              Furthermore, it can recover equivalent code from programs that use
              existing libraries and automatically port them to new interfaces.
              It first uses program synthesis to determine the meaning of a
              library, then maps the synthesized description into generalized
              constraints which are used to search the program for replacement
              opportunities to present to the developer. We applied this approach
              to existing large applications from the scientific computing and
              deep learning domains. Using our approach, we show speedups ranging
              from 1.1× to over 10× on end to end performance when using
              accelerator libraries. },
  keywords = {
              Libraries;Informatics;Synthesizers;Semantics;Software;Ecosystems;Deep
              learning},
  doi = {10.1109/PACT.2019.00013},
  url = {https://doi.ieeecomputersociety.org/10.1109/PACT.2019.00013},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  month = sep,
}

@article{doi:10.1177/109434209100500406,
  author = {Claude Bernard and Michael C. Ogilvie and Thomas A. DeGrand and
            Carleton E. DeTar and Steven A. Gottlieb and A. Krasnitz and R.L.
            Sugar and D. Toussaint},
  title = {Studying Quarks and Gluons On Mimd Parallel Computers},
  journal = {The International Journal of Supercomputing Applications},
  volume = {5},
  number = {4},
  pages = {61-70},
  year = {1991},
  doi = {10.1177/109434209100500406},
  URL = {https://doi.org/10.1177/109434209100500406 },
  eprint = {https://doi.org/10.1177/109434209100500406},
}
@article{1386650,
  author = {Frigo, M. and Johnson, S.G.},
  journal = {Proceedings of the IEEE},
  title = {The Design and Implementation of FFTW3},
  year = {2005},
  volume = {93},
  number = {2},
  pages = {216-231},
  keywords = {Discrete Fourier transforms;Multidimensional
              systems;Hardware;Software libraries;Discrete transforms;Fast
              Fourier transforms;Optimizing compilers;Discrete cosine
              transforms;Data structures;Fourier transforms;Adaptive
              software;cosine transform;fast Fourier transform (FFT);Fourier
              transform;Hartley transform;I/O tensor},
  doi = {10.1109/JPROC.2004.840301},
}

@article{10.1145/1731022.1731031,
  author = {Stathopoulos, Andreas and McCombs, James R.},
  title = {PRIMME: preconditioned iterative multimethod eigensolver—methods and
           software description},
  year = {2010},
  issue_date = {April 2010},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {37},
  number = {2},
  issn = {0098-3500},
  url = {https://doi.org/10.1145/1731022.1731031},
  doi = {10.1145/1731022.1731031},
  abstract = {This article describes the PRIMME software package for solving
              large, sparse Hermitian standard eigenvalue problems. The
              difficulty and importance of these problems have increased over the
              years, necessitating the use of preconditioning and near optimally
              converging iterative methods. However, the complexity of tuning or
              even using such methods has kept them outside the reach of many
              users. Responding to this problem, we have developed PRIMME, a
              comprehensive package that brings state-of-the-art methods from
              “bleeding edge” to production, with the best possible robustness,
              efficiency, and a flexible, yet highly usable interface that
              requires minimal or no tuning. We describe (1) the PRIMME
              multimethod framework that implements a variety of algorithms,
              including the near optimal methods GD+k and JDQMR; (2) a host of
              algorithmic innovations and implementation techniques that endow
              the software with its robustness and efficiency; (3) a multilayer
              interface that captures our experience and addresses the needs of
              both expert and end users.},
  journal = {ACM Trans. Math. Softw.},
  month = apr,
  articleno = {21},
  numpages = {30},
  keywords = {software package, preconditioning, locking, iterative,
              eigenvectors, eigenvalues, conjugate gradient, block, Lanczos,
              Jacobi-Davidson, Hermitian, Davidson},
}

@inproceedings{234898,
  author = { Choi, J. and Dongarra, J.J. and Pozo, R. and Walker, D.W. },
  booktitle = { The Fourth Symposium on the Frontiers of Massively Parallel
               Computation },
  title = {{ ScaLAPACK: a scalable linear algebra library for distributed memory
           concurrent computers }},
  year = {1992},
  volume = {},
  ISSN = {},
  pages = {120,121,122,123,124,125,126,127},
  abstract = { The authors describe ScaLAPACK, a distributed memory version of
              the LAPACK software package for dense and banded matrix
              computations. Key design features are the use of distributed
              versions of the Level 3 BLAS as building blocks, and an
              object-oriented interface to the library routines. The square block
              scattered decomposition is described. The implementation of a
              distributed memory version of the right-looking LU factorization
              algorithm on the Intel Delta multicomputer is discussed, and
              performance results are presented that demonstrate the scalability
              of the algorithm. },
  keywords = {Linear algebra;Concurrent computing;Distributed computing;Software
              libraries;Scalability;Software packages;Brillouin
              scattering;Algorithm design and analysis;Sparse matrices;Contracts},
  doi = {10.1109/FMPC.1992.234898},
  url = {https://doi.ieeecomputersociety.org/10.1109/FMPC.1992.234898},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  month = Oct,
}

@article{AUCKENTHALER2011783,
  title = {Parallel solution of partial symmetric eigenvalue problems from
           electronic structure calculations},
  journal = {Parallel Computing},
  volume = {37},
  number = {12},
  pages = {783-794},
  year = {2011},
  note = {6th International Workshop on Parallel Matrix Algorithms and
          Applications (PMAA'10)},
  issn = {0167-8191},
  doi = {https://doi.org/10.1016/j.parco.2011.05.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0167819111000494},
  author = {T. Auckenthaler and V. Blum and H.-J. Bungartz and T. Huckle and R.
            Johanni and L. Krämer and B. Lang and H. Lederer and P.R. Willems},
  keywords = {Electronic structure calculations, Eigenvalue and eigenvector
              computation, Blocked Householder transformations,
              Divide-and-conquer tridiagonal eigensolver, Parallelization},
  abstract = {The computation of selected eigenvalues and eigenvectors of a
              symmetric (Hermitian) matrix is an important subtask in many
              contexts, for example in electronic structure calculations. If a
              significant portion of the eigensystem is required then typically
              direct eigensolvers are used. The central three steps are: reduce
              the matrix to tridiagonal form, compute the eigenpairs of the
              tridiagonal matrix, and transform the eigenvectors back. To better
              utilize memory hierarchies, the reduction may be effected in two
              stages: full to banded, and banded to tridiagonal. Then the back
              transformation of the eigenvectors also involves two stages. For
              large problems, the eigensystem calculations can be the
              computational bottleneck, in particular with large numbers of
              processors. In this paper we discuss variants of the
              tridiagonal-to-banded back transformation, improving the parallel
              efficiency for large numbers of processors as well as the
              per-processor utilization. We also modify the divide-and-conquer
              algorithm for symmetric tridiagonal matrices such that it can
              compute a subset of the eigenpairs at reduced cost. The
              effectiveness of our modifications is demonstrated with numerical
              experiments.},
}
@article{10.1063/1.2840133,
  author = {Bowers, K. J. and Albright, B. J. and Yin, L. and Bergen, B. and
            Kwan, T. J. T.},
  title = {Ultrahigh performance three-dimensional electromagnetic relativistic
           kinetic plasma simulationa)},
  journal = {Physics of Plasmas},
  volume = {15},
  number = {5},
  pages = {055703},
  year = {2008},
  month = {03},
  abstract = {The algorithms, implementation details, and applications of VPIC,
              a state-of-the-art first principles 3D electromagnetic relativistic
              kinetic particle-in-cell code, are discussed. Unlike most codes,
              VPIC is designed to minimize data motion, as, due to physical
              limitations (including the speed of light!), moving data between
              and even within modern microprocessors is more time consuming than
              performing computations. As a result, VPIC has achieved
              unprecedented levels of performance. For example, VPIC can perform
              ∼0.17 billion cold particles pushed and charge conserving
              accumulated per second per processor on IBM’s Cell
              microprocessor—equivalent to sustaining Los Alamos’s planned
              Roadrunner supercomputer at ∼0.56 petaflop (quadrillion floating
              point operations per second). VPIC has enabled previously
              intractable simulations in numerous areas of plasma physics,
              including magnetic reconnection and laser plasma interactions; next
              generation supercomputers like Roadrunner will enable further
              advances.},
  issn = {1070-664X},
  doi = {10.1063/1.2840133},
  url = {https://doi.org/10.1063/1.2840133},
  eprint = {
            https://pubs.aip.org/aip/pop/article-pdf/doi/10.1063/1.2840133/14083352/055703
            \_1\_online.pdf},
}

@misc{openqcd,
  title = {openQCD},
  author = {Martin Lüscher, Stefan Schaefer},
  year = {2024},
  url = {https://luscher.web.cern.ch/luscher/openQCD/},
  note = {Accessed: 2025-01-04},
}

@misc{llamacpp,
  title = {llama.cpp},
  author = {Georgi Gerganov},
  year = {2025},
  url = {https://github.com/ggml-org/llama.cpp},
  note = {Accessed: 2025-01-04},
}

@article{BLIS1,
  author = {Field G. {V}an~{Z}ee and Robert A. {v}an~{d}e~{G}eijn},
  title = {{BLIS}: A Framework for Rapidly Instantiating {BLAS} Functionality},
  journal = {ACM Transactions on Mathematical Software},
  volume = {41},
  number = {3},
  pages = {14:1--14:33},
  month = {June},
  year = {2015},
  issue_date = {June 2015},
  url = {https://doi.acm.org/10.1145/2764454},
}

@misc{cloudsc,
  title = {CLOUDSC},
  author = {Michael Lange, Willem Deconinck, Balthasar Reuter},
  year = {2025},
  url = {https://github.com/ecmwf-ifs/dwarf-p-cloudsc},
  note = {Accessed: 2025-01-04},
}
@inproceedings{1437325,
  author = {Whaley, R.C. and Dongarra, J.J.},
  booktitle = {SC '98: Proceedings of the 1998 ACM/IEEE Conference on
               Supercomputing},
  title = {Automatically Tuned Linear Algebra Software},
  year = {1998},
  volume = {},
  number = {},
  pages = {38-38},
  keywords = {Linear algebra;Timing;Kernel;Lifting equipment;Production;Embedded
              software;Workstations;Algorithms;Buildings;Parallel
              processing;BLAS;high performance;tuning;optimization;linear
              algebra;code generation},
  doi = {10.1109/SC.1998.10004},
}

@article{icon1,
  title = {The {{ICON}} ({{ICOsahedral Non-hydrostatic}}) Modelling Framework of
           {{DWD}} and {{MPI-M}}: {{Description}} of the Non-Hydrostatic
           Dynamical Core},
  author = {Z{\"a}ngl, G{\"u}nther and Reinert, Daniel and R{\'i}podas, Pilar
            and Baldauf, Michael},
  year = {2015},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {141},
  number = {687},
  eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.2378},
  pages = {563--579},
  doi = {10.1002/qj.2378},
  url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2378},
  abstract = {Abstract This article describes the non-hydrostatic dynamical core
              developed for the ICOsahedral Non-hydrostatic (ICON) modelling
              framework. ICON is a joint project of the German Weather Service
              (DWD) and the Max Planck Institute for Meteorology (MPI-M),
              targeting a unified modelling system for global numerical weather
              prediction (NWP) and climate modelling. Compared with the existing
              models at both institutions, the main achievements of ICON are
              exact local mass conservation, mass-consistent tracer transport, a
              flexible grid nesting capability and the use of non-hydrostatic
              equations on global domains. The dynamical core is formulated on an
              icosahedral-triangular Arakawa C grid. Achieving mass conservation
              is facilitated by a flux-form continuity equation with density as
              the prognostic variable. Time integration is performed with a
              two-time-level predictor--corrector scheme that is fully explicit,
              except for the terms describing vertical sound-wave propagation. To
              achieve competitive computational efficiency, time splitting is
              applied between the dynamical core on the one hand and tracer
              advection, physics parametrizations and horizontal diffusion on the
              other hand. A sequence of tests with varying complexity indicates
              that the ICON dynamical core combines high numerical stability over
              steep mountain slopes with good accuracy and reasonably low
              diffusivity. Preliminary NWP test suites initialized with
              interpolated analysis data reveal that the ICON modelling system
              already achieves better skill scores than its predecessor at DWD,
              the operational hydrostatic Global Model Europe (GME), and at the
              same time requires significantly fewer computational resources.},
  keywords = {dynamical cores,model development,numerical weather prediction},
}

@techreport{LULESH:spec,
  title = {{H}ydrodynamics {C}hallenge {P}roblem, {L}awrence {L}ivermore {N}
           ational {L}aboratory},
  year = {2011},
  number = {LLNL-TR-490254},
  location = {Livermore, CA},
  pages = {1-17},
}
@misc{mercurium,
  title = {Mercurium},
  author = {Barcelona Supercomputing Cente},
  year = {2022},
  url = {https://pm.bsc.es/mcxx},
  note = {Accessed: 2025-01-04},
}
@misc{mukautuva,
  title = {Mukautuva},
  author = {Jeff Hammond},
  year = {2025},
  url = {https://github.com/jeffhammond/mukautuva},
  note = {Accessed: 2025-01-04},
}

@inproceedings{10.1145/3126908.3126925,
  author = {Priedhorsky, Reid and Randles, Tim},
  title = {Charliecloud: unprivileged containers for user-defined software
           stacks in HPC},
  year = {2017},
  isbn = {9781450351140},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3126908.3126925},
  doi = {10.1145/3126908.3126925},
  abstract = {Supercomputing centers are seeing increasing demand for
              user-defined software stacks (UDSS), instead of or in addition to
              the stack provided by the center. These UDSS support user needs
              such as complex dependencies or build requirements, externally
              required configurations, portability, and consistency. The
              challenge for centers is to provide these services in a usable
              manner while minimizing the risks: security, support burden,
              missing functionality, and performance. We present Charliecloud,
              which uses the Linux user and mount namespaces to run
              industry-standard Docker containers with no privileged operations
              or daemons on center resources. Our simple approach avoids most
              security risks while maintaining access to the performance and
              functionality already on offer, doing so in just 800 lines of code.
              Charliecloud promises to bring an industry-standard UDSS user
              workflow to existing, minimally altered HPC resources.},
  booktitle = {Proceedings of the International Conference for High Performance
               Computing, Networking, Storage and Analysis},
  articleno = {36},
  numpages = {10},
  keywords = {containers, least privilege, user environments},
  location = {Denver, Colorado},
  series = {SC '17},
}

@misc{yan2025mofadiscoveringmaterialscarbon,
  title = {MOFA: Discovering Materials for Carbon Capture with a GenAI- and
           Simulation-Based Workflow},
  author = {Xiaoli Yan and Nathaniel Hudson and Hyun Park and Daniel Grzenda and
            J. Gregory Pauloski and Marcus Schwarting and Haochen Pan and Hassan
            Harb and Samuel Foreman and Chris Knight and Tom Gibbs and Kyle Chard
            and Santanu Chaudhuri and Emad Tajkhorshid and Ian Foster and Mohamad
            Moosavi and Logan Ward and E. A. Huerta},
  year = {2025},
  eprint = {2501.10651},
  archivePrefix = {arXiv},
  primaryClass = {cs.DC},
  url = {https://arxiv.org/abs/2501.10651},
}

@inproceedings{9297044,
  author = {Culpo, Massimiliano and Becker, Gregory and Gutierrez, Carlos
            Eduardo Arango and Hoste, Kenneth and Gamblin, Todd},
  booktitle = {2020 2nd International Workshop on Containers and New
               Orchestration Paradigms for Isolated Environments in HPC
               (CANOPIE-HPC)},
  title = {archspec: A library for detecting, labeling, and reasoning about
           microarchitectures},
  year = {2020},
  volume = {},
  number = {},
  pages = {45-52},
  keywords = {
              Microarchitecture;Databases;Containers;Tools;Libraries;Optimization;Computer
              architecture;microarchitecture;isa;binary;packaging;containers;archspec;distribution
              },
  doi = {10.1109/CANOPIEHPC51917.2020.00011},
}

@misc{mpichabi,
  title = {MPICH ABI Compatibility Initiative},
  author = {MPICH},
  year = {2013},
  url = {https://www.mpich.org/abi/},
  note = {Accessed: 2025-01-04},
}
@misc{llvmdocs,
  title = {"Can I compile C or C++ code to platform-independent LLVM bitcode?"},
  author = {LLVM},
  year = {2025},
  url = {
         https://llvm.org/docs/FAQ.html#can-i-compile-c-or-c-code-to-platform-independent-llvm-bitcode
         },
  note = {Accessed: 2025-01-04},
}

@article{Gerhardt_2017,
  doi = {10.1088/1742-6596/898/8/082021},
  url = {https://dx.doi.org/10.1088/1742-6596/898/8/082021},
  year = {2017},
  month = {oct},
  publisher = {IOP Publishing},
  volume = {898},
  number = {8},
  pages = {082021},
  author = {Gerhardt, Lisa and Bhimji, Wahid and Canon, Shane and Fasel, Markus
            and Jacobsen, Doug and Mustafa, Mustafa and Porter, Jeff and Tsulaia,
            Vakho},
  title = {Shifter: Containers for HPC},
  journal = {Journal of Physics: Conference Series},
  abstract = {Bringing HEP computing to HPC can be difficult. Software stacks
              are often very complicated with numerous dependencies that are
              difficult to get installed on an HPC system. To address this issue,
              NERSC has created Shifter, a framework that delivers Docker-like
              functionality to HPC. It works by extracting images from native
              formats and converting them to a common format that is optimally
              tuned for the HPC environment. We have used Shifter to deliver the
              CVMFS software stack for ALICE, ATLAS, and STAR on the
              supercomputers at NERSC. As well as enabling the distribution
              multi-TB sized CVMFS stacks to HPC, this approach also offers
              performance advantages. Software startup times are significantly
              reduced and load times scale with minimal variation to 1000s of
              nodes. We profile several successful examples of scientists using
              Shifter to make scientific analysis easily customizable and
              scalable. We will describe the Shifter framework and several
              efforts in HEP and NP to use Shifter to deliver their software on
              the Cori HPC system.},
}

@inproceedings{1281665,
  author = {Lattner, C. and Adve, V.},
  booktitle = {International Symposium on Code Generation and Optimization,
               2004. CGO 2004.},
  title = {LLVM: a compilation framework for lifelong program analysis \&
           transformation},
  year = {2004},
  volume = {},
  number = {},
  pages = {75-86},
  doi = {10.1109/CGO.2004.1281665},
}
@inproceedings{10.1145/3295500.3356173,
  author = {Ben-Nun, Tal and de Fine Licht, Johannes and Ziogas, Alexandros N.
            and Schneider, Timo and Hoefler, Torsten},
  title = {Stateful dataflow multigraphs: a data-centric model for performance
           portability on heterogeneous architectures},
  year = {2019},
  isbn = {9781450362290},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3295500.3356173},
  doi = {10.1145/3295500.3356173},
  abstract = {The ubiquity of accelerators in high-performance computing has
              driven programming complexity beyond the skill-set of the average
              domain scientist. To maintain performance portability in the future
              , it is imperative to decouple architecture-specific programming
              paradigms from the underlying scientific computations. We present
              the Stateful DataFlow multiGraph (SDFG), a data-centric
              intermediate representation that enables separating program
              definition from its optimization. By combining fine-grained data
              dependencies with high-level control-flow, SDFGs are both
              expressive and amenable to program transformations, such as tiling
              and double-buffering. These transformations are applied to the SDFG
              in an interactive process, using extensible pattern matching, graph
              rewriting, and a graphical user interface. We demonstrate SDFGs on
              CPUs, GPUs, and FPGAs over various motifs --- from fundamental
              computational kernels to graph analytics. We show that SDFGs
              deliver competitive performance, allowing domain scientists to
              develop applications naturally and port them to approach peak
              hardware performance without modifying the original scientific
              code.},
  booktitle = {Proceedings of the International Conference for High Performance
               Computing, Networking, Storage and Analysis},
  articleno = {81},
  numpages = {14},
  location = {Denver, Colorado},
  series = {SC '19},
}

  
@inproceedings{9309042,
  author = {Ben-Nun, Tal and Gamblin, Todd and Hollman, D. S. and Krishnan, Hari
            and Newburn, Chris J.},
  booktitle = {2020 IEEE/ACM International Workshop on Performance, Portability
               and Productivity in HPC (P3HPC)},
  title = {Workflows are the New Applications: Challenges in Performance,
           Portability, and Productivity},
  year = {2020},
  volume = {},
  number = {},
  pages = {57-69},
  keywords = {Data models;Training;Productivity;Computational
              modeling;Instruments;Real-time systems;Next generation
              networking;HPC;performance;portability;productivity;workflows},
  doi = {10.1109/P3HPC51967.2020.00011},
}


@misc{llvmbitcodeindependent,
  title = {More Target Independent LLVM Bitcode},
  author = {Jin-Gu Kang},
  year = {2011},
  howpublished = {LLVM 2011 European User Group Meeting},
  url = {https://llvm.org/devmtg/2011-09-16/},
  note = {Accessed: 2025-01-04},
}

@inproceedings{10.1145/3636480.3637219,
  author = {Sumimoto, Shinji and Hanawa, Toshihiro and Nakajima, Kengo},
  title = {MPI-Adapter2: An Automatic ABI Translation Library Builder for MPI
           Application Binary Portability},
  year = {2024},
  isbn = {9798400716522},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3636480.3637219},
  doi = {10.1145/3636480.3637219},
  booktitle = {Proceedings of the International Conference on High Performance
               Computing in Asia-Pacific Region Workshops},
  pages = {63–68},
  numpages = {6},
  keywords = {Automatic library builder, MPI ABI Translation, MPI Bbinary
              Execution Portability},
  location = {Nagoya, Japan},
  series = {HPCAsia '24 Workshops},
}

  

@article{10513439,
  author = {Willenbring, James M. and Shende, Sameer S. and Gamblin, Todd},
  journal = {Computing in Science \& Engineering},
  title = {Providing a Flexible and Comprehensive Software Stack Via Spack, an
           Extreme-Scale Scientific Software Stack, and Software Development Kits
           },
  year = {2024},
  volume = {26},
  number = {1},
  pages = {20-30},
  keywords = {Software development management;Ecosystems;Testing;Syntactics;Full
              stack;Programming;Exascale computing;Interoperability;Best
              practices;Full stack;Collaboration;High performance computing},
  doi = {10.1109/MCSE.2024.3395016},
}


@misc{e4s,
  title = {E4S: Extreme-scale scientific software stack},
  author = {Heroux, M and Willenbring, J and Shende, S and Coti, C and Spear, W
            and Peyralans, J and Skutnik, J and Keever, E},
  year = {2020},
  howpublished = {LLVM 2011 European User Group Meeting},
  url = {
         https://collegeville.github.io/CW20/WorkshopResources/WhitePapers/heroux-willenbring-shende-coti-spear-et-al-E4S.pdf
         },
  note = {Accessed: 2025-01-04},
}


@misc{ocimagespec,
  title = {The OpenContainers Image Spec},
  author = {Open Containers Initiative (OCI)},
  year = {2024},
  url = {https://specs.opencontainers.org/image-spec/},
  note = {Accessed: 2025-01-04},
}

@misc{ueabs,
  title = {Unified European Applications Benchmark Suite},
  author = {PRACE},
  year = {2024},
  url = {https://repository.prace-ri.eu/git/UEABS/ueabs},
  note = {Accessed: 2025-01-04},
}
@article{liu-etal-2024-lost,
  title = "Lost in the Middle: How Language Models Use Long Contexts",
  author = "Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin
            and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "12",
  year = "2024",
  address = "Cambridge, MA",
  publisher = "MIT Press",
  url = "https://aclanthology.org/2024.tacl-1.9/",
  doi = "10.1162/tacl_a_00638",
  pages = "157--173",
  abstract = "While recent language models have the ability to take long
              contexts as input, relatively little is known about how well they
              use longer context. We analyze the performance of language models
              on two tasks that require identifying relevant information in their
              input contexts: multi-document question answering and key-value
              retrieval. We find that performance can degrade significantly when
              changing the position of relevant information, indicating that
              current language models do not robustly make use of information in
              long input contexts. In particular, we observe that performance is
              often highest when relevant information occurs at the beginning or
              end of the input context, and significantly degrades when models
              must access relevant information in the middle of long contexts,
              even for explicitly long-context models. Our analysis provides a
              better understanding of how language models use their input context
              and provides new evaluation protocols for future long-context
              language models.",
}

@inproceedings{pritchard2023open,
  title={Open MPI for HPE Cray EX Systems},
  author={Pritchard, Howard P and Naughton III, Thomas and Shehata, Amir and Bernholdt, David},
  year={2023},
  booktitle={Proceedings of the Cray User Group (CUG) Conference},
  location = {Helsinki, Finland},
  url={https://cug.org/proceedings/cug2023_proceedings/includes/files/pap140s2-file1.pdf}
}

@misc{gromacsexotic,
  title = {Installation guide for exotic configurations},
  author = {GROMACS},
  year = {2025},
  url = {https://manual.gromacs.org/2025.2/install-guide/exotic.html},
  note = {Accessed: 2025-08-25},
}

@misc{onemath,
  title = {oneAPI Math Library (oneMath)},
  author = {Unified Acceleration (UXL) Foundation},
  year = {2025},
  url = {https://github.com/uxlfoundation/oneMath/tree/v0.8},
  note = {Accessed: 2025-08-25},
}

@misc{codeplaycudaplugin,
  title = {Install oneAPI for NVIDIA GPUs},
  author = {Codeplay},
  year = {2024},
  url = {https://developer.codeplay.com/products/oneapi/nvidia/2024.2.0/guides/get-started-guide-nvidia},
  note = {Accessed: 2025-08-25},
}

@ARTICLE{10036080,

  author={Tolmachev, Dmitrii},

  journal={IEEE Access}, 

  title={VkFFT-A Performant, Cross-Platform and Open-Source GPU FFT Library}, 

  year={2023},

  volume={11},

  number={},

  pages={12039-12058},

  keywords={Graphics processing units;Discrete Fourier transforms;Task analysis;Parallel processing;Optimization;Metals;FFT;GPU;parallel computing;Vulkan;CUDA;HIP;OpenCL;Level Zero;Metal},

  doi={10.1109/ACCESS.2023.3242240}}

@article{DECONINCK2017188,
title = {Atlas : A library for numerical weather prediction and climate modelling},
journal = {Computer Physics Communications},
volume = {220},
pages = {188-204},
year = {2017},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2017.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010465517302138},
author = {Willem Deconinck and Peter Bauer and Michail Diamantakis and Mats Hamrud and Christian Kühnlein and Pedro Maciel and Gianmarco Mengaldo and Tiago Quintino and Baudouin Raoult and Piotr K. Smolarkiewicz and Nils P. Wedi},
keywords = {Numerical weather prediction, Climate, Earth system, High performance computing, Meteorology, Flexible mesh data structure},
abstract = {The algorithms underlying numerical weather prediction (NWP) and climate models that have been developed in the past few decades face an increasing challenge caused by the paradigm shift imposed by hardware vendors towards more energy-efficient devices. In order to provide a sustainable path to exascale High Performance Computing (HPC), applications become increasingly restricted by energy consumption. As a result, the emerging diverse and complex hardware solutions have a large impact on the programming models traditionally used in NWP software, triggering a rethink of design choices for future massively parallel software frameworks. In this paper, we present Atlas, a new software library that is currently being developed at the European Centre for Medium-Range Weather Forecasts (ECMWF), with the scope of handling data structures required for NWP applications in a flexible and massively parallel way. Atlas provides a versatile framework for the future development of efficient NWP and climate applications on emerging HPC architectures. The applications range from full Earth system models, to specific tools required for post-processing weather forecast products. The Atlas library thus constitutes a step towards affordable exascale high-performance simulations by providing the necessary abstractions that facilitate the application in heterogeneous HPC environments by promoting the co-design of NWP algorithms with the underlying hardware.}
}

@article{CLARK20101517,
title = {Solving lattice QCD systems of equations using mixed precision solvers on GPUs},
journal = {Computer Physics Communications},
volume = {181},
number = {9},
pages = {1517-1528},
year = {2010},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2010.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010465510001426},
author = {M.A. Clark and R. Babich and K. Barros and R.C. Brower and C. Rebbi},
keywords = {CUDA, GPGPU, GPU, Lattice QCD, Mixed precision},
abstract = {Modern graphics hardware is designed for highly parallel numerical tasks and promises significant cost and performance benefits for many scientific applications. One such application is lattice quantum chromodynamics (lattice QCD), where the main computational challenge is to efficiently solve the discretized Dirac equation in the presence of an SU(3) gauge field. Using NVIDIA's CUDA platform we have implemented a Wilson–Dirac sparse matrix–vector product that performs at up to 40, 135 and 212 Gflops for double, single and half precision respectively on NVIDIA's GeForce GTX 280 GPU. We have developed a new mixed precision approach for Krylov solvers using reliable updates which allows for full double precision accuracy while using only single or half precision arithmetic for the bulk of the computation. The resulting BiCGstab and CG solvers run in excess of 100 Gflops and, in terms of iterations until convergence, perform better than the usual defect-correction approach for mixed precision.}
}

@misc{quda,
  title = {QUDA},
  year = {2021},
  url = {https://github.com/lattice/quda},
  note = {Accessed: 2025-08-25},
}

@ARTICLE{9444146,
author={Bird, Robert and Tan, Nigel and Luedtke, Scott V. and Harrell, Stephen Lien and Taufer, Michela and Albright, Brian},
journal={ IEEE Transactions on Parallel \& Distributed Systems },
title={{ VPIC 2.0: Next Generation Particle-in-Cell Simulations }},
year={2022},
volume={33},
number={04},
ISSN={1558-2183},
pages={952-963},
abstract={ VPIC is a general purpose particle-in-cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC's capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this article, we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC's modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors. },
keywords={Plasmas;Hardware;Physics;Libraries;Mathematical model;Shape;Layout},
doi={10.1109/TPDS.2021.3084795},
url = {https://doi.ieeecomputersociety.org/10.1109/TPDS.2021.3084795},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=apr}

  
