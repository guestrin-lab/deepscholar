\begin{thebibliography}{100}

\bibitem{Invoice-to-Json}
Invoice-to-json: A document understanding and information extraction dataset, 2024.

\bibitem{abdin2024phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock {\em arXiv preprint arXiv:2404.14219}, 2024.

\bibitem{abouelenin2025phi}
Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et~al.
\newblock Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras.
\newblock {\em arXiv preprint arXiv:2503.01743}, 2025.

\bibitem{acharya2019tallyqa}
Manoj Acharya, Kushal Kafle, and Christopher Kanan.
\newblock Tallyqa: Answering complex counting questions.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 8076--8084, 2019.

\bibitem{ahmad2025opencodereasoning}
Wasi~Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg.
\newblock Opencodereasoning: Advancing data distillation for competitive coding.
\newblock 2025.

\bibitem{allal2025smollm2smolgoesbig}
Loubna~Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel~Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíček, Agustín~Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf.
\newblock Smollm2: When smol goes big -- data-centric training of a small language model, 2025.

\bibitem{Vision_OCR_Financial_Reports_10k}
Anas989898.
\newblock Vision ocr financial reports 10k.
\newblock \url{https://huggingface.co/datasets/Anas989898/Vision-OCR-Financial-Reports-10k}, 2024.

\bibitem{claude3series2024}
{Anthropic}.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock \url{https://www.anthropic.com}, 2024.

\bibitem{apoidea_fintabnet}
apoidea.
\newblock fintabnet.
\newblock \url{https://huggingface.co/datasets/apoidea/fintabnet-html}, 2024.

\bibitem{apple-core-ML}
Apple.
\newblock On device llama 3.1 with core ml.
\newblock \url{https://machinelearning.apple.com/research/core-ml-on-device-llama?utm_source=chatgpt.com}, 2024.

\bibitem{awadalla2024mint}
Anas Awadalla, Le~Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Guha, Sheng Shen, Mohamed Awadalla, Silvio Savarese, Caiming Xiong, et~al.
\newblock Mint-1t: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens.
\newblock {\em Advances in Neural Information Processing Systems}, 37:36805--36828, 2024.

\bibitem{bai2021uibert}
Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et~al.
\newblock Uibert: Learning generic multimodal representations for ui understanding.
\newblock {\em arXiv preprint arXiv:2107.13731}, 2021.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{bai2023qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{bai2025qwen2}
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et~al.
\newblock Qwen2. 5-vl technical report.
\newblock {\em arXiv preprint arXiv:2502.13923}, 2025.

\bibitem{bai2025qwen2_5}
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et~al.
\newblock Qwen2.5-vl technical report.
\newblock {\em arXiv preprint arXiv:2502.13923}, 2025.

\bibitem{bi2024deepseekllm}
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock {\em arXiv preprint arXiv:2401.02954}, 2024.

\bibitem{CC1984_mall_receipt_extraction_dataset}
CC1984.
\newblock mall receipt extraction dataset.
\newblock \url{https://huggingface.co/datasets/CC1984/mall_receipt_extraction_dataset}, 2023.

\bibitem{chai2024amex}
Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li.
\newblock Amex: Android multi-annotation expo dataset for mobile gui agents.
\newblock {\em arXiv preprint arXiv:2407.17490}, 2024.

\bibitem{chang2022mapqa}
Shuaichen Chang, David Palzer, Jialin Li, Eric Fosler-Lussier, and Ningchuan Xiao.
\newblock Mapqa: A dataset for question answering on choropleth maps.
\newblock {\em arXiv preprint arXiv:2211.08545}, 2022.

\bibitem{chen2022unigeo}
Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang.
\newblock Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression.
\newblock {\em arXiv preprint arXiv:2212.02746}, 2022.

\bibitem{chen2021geoqa}
Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric~P Xing, and Liang Lin.
\newblock Geoqa: A geometric question answering benchmark towards multimodal numerical reasoning.
\newblock {\em arXiv preprint arXiv:2105.14517}, 2021.

\bibitem{chen2024sharegpt4v}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock In {\em European Conference on Computer Vision}, pages 370--387. Springer, 2024.

\bibitem{chen2024mmstar}
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu~Qiao, Dahua Lin, et~al.
\newblock Are we on the right way for evaluating large vision-language models?
\newblock {\em arXiv preprint arXiv:2403.20330}, 2024.

\bibitem{chen2023sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock {\em arXiv preprint arXiv:2311.12793}, 2023.

\bibitem{chen2024internevo}
Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang, Qinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, et~al.
\newblock Internevo: Efficient long-sequence large language model training via hybrid parallelism and redundant sharding.
\newblock {\em arXiv preprint arXiv:2401.09149}, 2024.

\bibitem{chen2025advancing}
Shuang Chen, Yue Guo, Zhaochen Su, Yafu Li, Yulun Wu, Jiacheng Chen, Jiayu Chen, Weijie Wang, Xiaoye Qu, and Yu~Cheng.
\newblock Advancing multimodal reasoning: From optimized cold start to staged reinforcement learning.
\newblock {\em arXiv preprint arXiv:2506.04207}, 2025.

\bibitem{chen2024expanding}
Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et~al.
\newblock Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling.
\newblock {\em arXiv preprint arXiv:2412.05271}, 2024.

\bibitem{chen2024internvl_1_5}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et~al.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock {\em arXiv preprint arXiv:2404.16821}, 2024.

\bibitem{chen2023internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24185--24198, 2024.

\bibitem{chen2021finqa}
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et~al.
\newblock Finqa: A dataset of numerical reasoning over financial data.
\newblock {\em arXiv preprint arXiv:2109.00122}, 2021.

\bibitem{cheng2024seeclick}
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu.
\newblock Seeclick: Harnessing gui grounding for advanced visual gui agents.
\newblock {\em arXiv preprint arXiv:2401.10935}, 2024.

\bibitem{cheng2022iam}
Liying Cheng, Lidong Bing, Ruidan He, Qian Yu, Yan Zhang, and Luo Si.
\newblock Iam: a comprehensive and large-scale dataset for integrated argument mining tasks.
\newblock {\em arXiv preprint arXiv:2203.12257}, 2022.

\bibitem{chi2019complicated}
Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin, and Xian-Ling Mao.
\newblock Complicated table structure recognition.
\newblock {\em arXiv preprint arXiv:1908.04729}, 2019.

\bibitem{chia2024puzzlevqa}
Yew~Ken Chia, Vernon Toh~Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria.
\newblock Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns.
\newblock {\em arXiv preprint arXiv:2403.13315}, 2024.

\bibitem{chu2023mobilevlm}
Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo~Zhang, Xiaolin Wei, et~al.
\newblock Mobilevlm: A fast, strong and open vision language assistant for mobile devices.
\newblock {\em arXiv preprint arXiv:2312.16886}, 2023.

\bibitem{chu2024mobilevlm}
Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo~Zhang, et~al.
\newblock Mobilevlm v2: Faster and stronger baseline for vision language model.
\newblock {\em arXiv preprint arXiv:2402.03766}, 2024.

\bibitem{cognitivecomputations_SystemChat}
cognitive computations.
\newblock Systemchat-2.0.
\newblock \url{https://huggingface.co/datasets/cognitivecomputations/SystemChat-2.0}, 2024.

\bibitem{opencompass2023}
OpenCompass Contributors.
\newblock Opencompass: A universal evaluation platform for foundation models.
\newblock \url{https://github.com/open-compass/opencompass}, 2023.

\bibitem{realworldqa}
X.AI Corp.
\newblock Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model.
\newblock \url{https://x.ai/blog/grok-1.5v}, 2024.

\bibitem{davis2019deep}
Brian Davis, Bryan Morse, Scott Cohen, Brian Price, and Chris Tensmeyer.
\newblock Deep visual template-free form parsing.
\newblock In {\em 2019 International Conference on Document Analysis and Recognition (ICDAR)}, pages 134--141. IEEE, 2019.

\bibitem{genimi-nano}
Deepmind.
\newblock Gemini-nano.
\newblock \url{https://deepmind.google/models/gemini/nano/}, 2024.

\bibitem{gemini2_0pro}
Google Deepmind.
\newblock Gemini 2.0 is now available to everyone.
\newblock \url{https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/}, 202.

\bibitem{gemini2_0}
Google Deepmind.
\newblock Introducing gemini 2.0: our new ai model for the agentic era.
\newblock \url{https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/}, 2024.

\bibitem{dehghani2023patch}
Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim~M Alabdulmohsin, et~al.
\newblock Patch n’pack: Navit, a vision transformer for any aspect ratio and resolution.
\newblock {\em Advances in Neural Information Processing Systems}, 36:2252--2274, 2023.

\bibitem{deitke2024molmo}
Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae~Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et~al.
\newblock Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models.
\newblock {\em arXiv preprint arXiv:2409.17146}, 2024.

\bibitem{deka2017rico}
Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar.
\newblock Rico: A mobile app dataset for building data-driven design applications.
\newblock In {\em Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, pages 845--854, 2017.

\bibitem{deng2024mind2web}
Xiang Deng, Yu~Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu~Su.
\newblock Mind2web: Towards a generalist agent for the web.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{dettmers2023qloraefficientfinetuningquantized}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms, 2023.

\bibitem{doan2024vintern1befficientmultimodallarge}
Khang~T. Doan, Bao~G. Huynh, Dung~T. Hoang, Thuc~D. Pham, Nhat~H. Pham, Quan T.~M. Nguyen, Bang~Q. Vo, and Suong~N. Hoang.
\newblock Vintern-1b: An efficient multimodal large language model for vietnamese, 2024.

\bibitem{dong2025qianfan}
Daxiang Dong, Mingming Zheng, Dong Xu, Bairong Zhuang, Wenyu Zhang, Chunhua Luo, Haoran Wang, Zijian Zhao, Jie Li, Yuxuan Li, et~al.
\newblock Qianfan-vl: Domain-enhanced universal vision-language models.
\newblock {\em arXiv preprint arXiv:2509.18189}, 2025.

\bibitem{dong2025scalable}
Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, and Jiao Ran.
\newblock Scalable vision language model training via high quality data curation.
\newblock {\em arXiv preprint arXiv:2501.05952}, 2025.

\bibitem{du2019cocoqa}
Tianjiao Du, Junming Cao, Qinyue Wu, Wei Li, Beijun Shen, and Yuting Chen.
\newblock Cocoqa: Question answering for coding conventions over knowledge graphs.
\newblock In {\em 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, pages 1086--1089. IEEE, 2019.

\bibitem{duan2024vlmevalkit}
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et~al.
\newblock Vlmevalkit: An open-source toolkit for evaluating large multi-modality models.
\newblock In {\em Proceedings of the 32nd ACM International Conference on Multimedia}, pages 11198--11201, 2024.

\bibitem{vgg_via}
Abhishek Dutta, Ankush Gupta, and Andrew Zisserman.
\newblock Vgg image annotator (via).
\newblock \url{https://www.robots.ox.ac.uk/~vgg/software/via/}.

\bibitem{MMC_Instructed_Dataset}
Felprot75.
\newblock Mmc instructed dataset.
\newblock \url{https://huggingface.co/datasets/Felprot75/MMC_Instructed_Dataset}, 2024.

\bibitem{fini2025multimodal}
Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor G~Turrisi da~Costa, Louis B{\'e}thune, Zhe Gan, et~al.
\newblock Multimodal autoregressive pre-training of large vision encoders.
\newblock In {\em Proceedings of the Computer Vision and Pattern Recognition Conference}, pages 9641--9654, 2025.

\bibitem{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{fu2024blink}
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu~Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah~A Smith, Wei-Chiu Ma, and Ranjay Krishna.
\newblock Blink: Multimodal large language models can see but not perceive.
\newblock {\em arXiv preprint arXiv:2404.12390}, 2024.

\bibitem{Labeled_ORAND_CAR_A}
Fzkuji.
\newblock orand-car-a.
\newblock \url{https://github.com/Fzkuji/Labeled_ORAND-CAR-A}, 2024.

\bibitem{llamacpp}
Ggerganov.
\newblock llama.cpp - llm inference with minimal setup and state-of-the-art performance on a wide range of hardware.
\newblock \url{https:github.com/ggerganov/ llama.cpp/}, 2023.

\bibitem{glaiveai_glaive_function_calling}
glaiveai.
\newblock glaive-function-calling.
\newblock \url{https://huggingface.co/datasets/glaiveai/glaive-function-calling}, 2023.

\bibitem{gonzalez2024metrics}
Oth{\'o}n Gonz{\'a}lez-Ch{\'a}vez, Guillermo Ruiz, Daniela Moctezuma, and Tania Ramirez-delReal.
\newblock Are metrics measuring what they should? an evaluation of image captioning task metrics.
\newblock {\em Signal Processing: Image Communication}, 120:117071, 2024.

\bibitem{gu2022wukong}
Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu~Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et~al.
\newblock Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26418--26431, 2022.

\bibitem{gu2024aquilavl}
Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et~al.
\newblock Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data.
\newblock {\em arXiv preprint arXiv:2410.18558}, 2024.

\bibitem{guan2023hallusionbench}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et~al.
\newblock Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models.
\newblock {\em arXiv preprint arXiv:2310.14566}, 2023.

\bibitem{guha2025openthoughtsdatarecipesreasoning}
Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike~A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros~G. Dimakis, and Ludwig Schmidt.
\newblock Openthoughts: Data recipes for reasoning models, 2025.

\bibitem{guo2025deepseek}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock {\em arXiv preprint arXiv:2501.12948}, 2025.

\bibitem{guo2019eaten}
He~Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding.
\newblock Eaten: Entity-aware attention for single shot visual text extraction.
\newblock In {\em International Conference on Document Analysis and Recognition}, pages 254--259, 2019.

\bibitem{guo2024mammoth}
Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo~Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue.
\newblock Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale.
\newblock {\em arXiv preprint arXiv:2412.05237}, 2024.

\bibitem{gurari2018vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey~P Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3608--3617, 2018.

\bibitem{he2023wanjuan}
Conghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Dahua Lin.
\newblock Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models.
\newblock {\em arXiv preprint arXiv:2308.10755}, 2023.

\bibitem{hosu2020koniq}
Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
\newblock Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment.
\newblock {\em IEEE Transactions on Image Processing}, 29:4041--4056, 2020.

\bibitem{howard_hou_COCO_Text}
howard hou.
\newblock Coco-text.
\newblock \url{https://huggingface.co/datasets/howard-hou/COCO-Text}, 2024.

\bibitem{hsiao2022screenqa}
Yu-Chung Hsiao, Fedir Zubach, Gilles Baechler, Victor Carbune, Jason Lin, Maria Wang, Srinivas Sunkara, Yun Zhu, and Jindong Chen.
\newblock Screenqa: Large-scale question-answer pairs over mobile app screenshots.
\newblock {\em arXiv preprint arXiv:2209.08199}, 2022.

\bibitem{hu2024mplug}
Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo~Zhang, Chen Li, Ji~Zhang, Qin Jin, Fei Huang, et~al.
\newblock mplug-docowl 1.5: Unified structure learning for ocr-free document understanding.
\newblock {\em arXiv preprint arXiv:2403.12895}, 2024.

\bibitem{hu2024mplug2}
Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji~Zhang, Qin Jin, Fei Huang, and Jingren Zhou.
\newblock mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding.
\newblock {\em arXiv preprint arXiv:2409.03420}, 2024.

\bibitem{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em ICLR}, 1(2):3, 2022.

\bibitem{Huang2024OpenCoderTO}
Siming Huang, Tianhao Cheng, Jason~Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J.~Yang, J.~H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge~Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu.
\newblock Opencoder: The open cookbook for top-tier code large language models.
\newblock 2024.

\bibitem{ilhamxx_dataset_receipt}
ilhamxx.
\newblock dataset receipt.
\newblock \url{https://huggingface.co/datasets/ilhamxx/dataset_receipt}, 2024.

\bibitem{Hermes-Function-Calling-Dataset-V1}
"Teknium" "interstellarninja".
\newblock Hermes-function-calling-dataset-v1.
\newblock \url{https://huggingface.co/NousResearch/hermes-function-calling-v1}.

\bibitem{iyer2023automated}
Venkatraman Iyer, Sungho Lee, Semun Lee, Juitem~Joonwoo Kim, Hyunjun Kim, and Youngjae Shin.
\newblock Automated backend allocation for multi-model, on-device ai inference.
\newblock {\em Proceedings of the ACM on Measurement and Analysis of Computing Systems}, 7(3):1--33, 2023.

\bibitem{jia2024leopard}
Mengzhao Jia, Wenhao Yu, Kaixin Ma, Tianqing Fang, Zhihan Zhang, Siru Ouyang, Hongming Zhang, Meng Jiang, and Dong Yu.
\newblock Leopard: A vision language model for text-rich multi-image tasks.
\newblock {\em arXiv preprint arXiv:2410.01744}, 2024.

\bibitem{visualwebinstruct}
Yiming Jia, Jiachen Li, Xiang Yue, Bo~Li, Ping Nie, Kai Zou, and Wenhu Chen.
\newblock Visualwebinstruct: Scaling up multimodal instruction data through web search.
\newblock {\em arXiv preprint arXiv:2503.10582}, 2025.

\bibitem{jiang2025r}
Jie Jiang, Qi~Yang, Bolin Ni, Shiming Xiang, Han Hu, and Houwen Peng.
\newblock R-4b: Incentivizing general-purpose auto-thinking capability in mllms via bi-mode annealing and reinforce learning.
\newblock {\em arXiv preprint arXiv:2508.21113}, 2025.

\bibitem{jiang2020mnn}
Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu~Cai, Tianhang Yu, et~al.
\newblock Mnn: A universal and efficient inference engine.
\newblock {\em Proceedings of Machine Learning and Systems}, 2:1--13, 2020.

\bibitem{johnson2017clevr}
Justin Johnson, Bharath Hariharan, Laurens Van Der~Maaten, Li~Fei-Fei, C~Lawrence~Zitnick, and Ross Girshick.
\newblock Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2901--2910, 2017.

\bibitem{JourneyBench_Hallucination}
JourneyBench.
\newblock Journeybench hallucination.
\newblock \url{https://huggingface.co/datasets/JourneyBench/JourneyBench_Hallucination}, 2024.

\bibitem{jung2024bco}
Seungjae Jung, Gunsoo Han, Daniel~Wontae Nam, and Kyoung-Woon On.
\newblock Binary classifier optimization for large language model alignment.
\newblock {\em arXiv preprint arXiv:2404.04656}, 2024.

\bibitem{kafle2018dvqa}
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.
\newblock Dvqa: Understanding data visualizations via question answering.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5648--5656, 2018.

\bibitem{kahou2017figureqa}
Samira~Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, {\'A}kos K{\'a}d{\'a}r, Adam Trischler, and Yoshua Bengio.
\newblock Figureqa: An annotated figure dataset for visual reasoning.
\newblock {\em arXiv preprint arXiv:1710.07300}, 2017.

\bibitem{kapoor2025omniact}
Raghav Kapoor, Yash~Parag Butala, Melisa Russak, Jing~Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov.
\newblock Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web.
\newblock In {\em European Conference on Computer Vision}, pages 161--178. Springer, 2025.

\bibitem{kashindra_mahato_nutritional_data_poie}
kashindra mahato.
\newblock nutritional-data-poie.
\newblock \url{https://huggingface.co/datasets/kashindra-mahato/nutritional-data-poie-1}, 2024.

\bibitem{katanaml_invoices_donut_data_v1}
katanaml org.
\newblock invoices-donut-data-v1.
\newblock \url{https://huggingface.co/datasets/katanaml-org/invoices-donut-data-v1}, 2023.

\bibitem{kazemi2023geomverse}
Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi~Chen, and Radu Soricut.
\newblock Geomverse: A systematic evaluation of large models for geometric reasoning.
\newblock {\em arXiv preprint arXiv:2312.12241}, 2023.

\bibitem{kembhavi2016diagram}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 235--251. Springer, 2016.

\bibitem{kembhavi2016ai2d}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In {\em European Conference on Computer Vision}, pages 235--251, 2016.

\bibitem{kim2022synthdog}
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
\newblock Ocr-free document understanding transformer.
\newblock In {\em European Conference on Computer Vision}, pages 498--517. Springer, 2022.

\bibitem{kim2019korean}
Jin-Hwa Kim, Soohyun Lim, Jaesun Park, and Hansu Cho.
\newblock Korean localization of visual question answering for blind people.
\newblock In {\em SK T-Brain-AI for Social Good Workshop at NeurIPS}, volume~2, 2019.

\bibitem{krishna2017vg}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced dense image annotations.
\newblock {\em International Journal of Computer Vision}, 123:32--73, 2017.

\bibitem{laion_laion_gpt4v}
laion.
\newblock laion-gpt4v.
\newblock \url{https://huggingface.co/datasets/laion/gpt4v-dataset}, 2024.

\bibitem{2024docmatrix}
Hugo Lauren{\c{c}}on, Andr{\'e}s Marafioti, Victor Sanh, and L{\'e}o Tronchon.
\newblock Building and better understanding vision-language models: insights and future directions.
\newblock {\em arXiv preprint arXiv:2408.12637}, 2024.

\bibitem{laurenccon2024building}
Hugo Lauren{\c{c}}on, Andr{\'e}s Marafioti, Victor Sanh, and L{\'e}o Tronchon.
\newblock Building and better understanding vision-language models: insights and future directions.
\newblock In {\em Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models}, 2024.

\bibitem{laurenccon2024matters}
Hugo Lauren{\c{c}}on, L{\'e}o Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?
\newblock {\em Advances in Neural Information Processing Systems}, 37:87874--87907, 2024.

\bibitem{Cauldron}
Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh.
\newblock What matters when building vision-language models?, 2024.

\bibitem{laurencon2024unlocking}
Hugo Laurençon, Léo Tronchon, and Victor Sanh.
\newblock Unlocking the conversion of web screenshots into html code with the websight dataset, 2024.

\bibitem{lerner2022viquae}
Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv{\'e} Le~Borgne, Romaric Besan{\c{c}}on, Jos{\'e}~G Moreno, and Jes{\'u}s Lov{\'o}n~Melgarejo.
\newblock Viquae, a dataset for knowledge-based visual question answering about named entities.
\newblock In {\em Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 3108--3120, 2022.

\bibitem{li2024llavaov}
Bo~Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.
\newblock Llava-onevision: Easy visual task transfer.
\newblock {\em arXiv preprint arXiv:2408.03326}, 2024.

\bibitem{li2024seedbench2plus}
Bohao Li, Yuying Ge, Yi~Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan.
\newblock Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension.
\newblock {\em arXiv preprint arXiv:2404.16790}, 2024.

\bibitem{li2022paddleocr}
Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, et~al.
\newblock Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system.
\newblock {\em arXiv preprint arXiv:2206.03001}, 2022.

\bibitem{li2024r}
Chunyi Li, Jianbo Zhang, Zicheng Zhang, Haoning Wu, Yuan Tian, Wei Sun, Guo Lu, Xiaohong Liu, Xiongkuo Min, Weisi Lin, et~al.
\newblock R-bench: Are your large multimodal model robust to real-world corruptions?
\newblock {\em arXiv preprint arXiv:2410.05474}, 2024.

\bibitem{li2025screenspot}
Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua.
\newblock Screenspot-pro: Gui grounding for professional high-resolution computer use.
\newblock {\em arXiv preprint arXiv:2504.07981}, 2025.

\bibitem{li2023silkie}
Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong.
\newblock Silkie: Preference distillation for large visual language models.
\newblock {\em arXiv preprint arXiv:2312.10665}, 2023.

\bibitem{li2024transformer}
Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie.
\newblock Transformer-lite: High-efficiency deployment of large language models on mobile phone gpus.
\newblock {\em arXiv preprint arXiv:2403.20041}, 2024.

\bibitem{li2025bild}
Minchong Li, Feng Zhou, and Xiaohui Song.
\newblock Bild: Bi-directional logits difference loss for large language model distillation.
\newblock In {\em Proceedings of the 31st International Conference on Computational Linguistics}, pages 1168--1182, 2025.

\bibitem{li2024omnicorpus}
Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et~al.
\newblock Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text.
\newblock {\em arXiv preprint arXiv:2406.08418}, 2024.

\bibitem{lieffects}
Wei Li, William~E Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva.
\newblock On the effects of data scale on ui control agents.
\newblock In {\em The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2024.

\bibitem{li2020widget}
Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan.
\newblock Widget captioning: Generating natural language description for mobile user interface elements.
\newblock {\em arXiv preprint arXiv:2010.04295}, 2020.

\bibitem{li2023pope}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock In {\em The Conference on Empirical Methods in Natural Language Processing}, pages 292--305, 2023.

\bibitem{li2024snapkv}
Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen.
\newblock Snapkv: Llm knows what you are looking for before generation.
\newblock {\em Advances in Neural Information Processing Systems}, 37:22947--22970, 2024.

\bibitem{li2024eagle}
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang.
\newblock Eagle-2: Faster inference of language models with dynamic draft trees.
\newblock {\em arXiv preprint arXiv:2406.16858}, 2024.

\bibitem{li2023monkey}
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
\newblock Monkey: Image resolution and text label are important things for large multi-modal models.
\newblock {\em arXiv preprint arXiv:2311.06607}, 2023.

\bibitem{li2024ferret}
Zhangheng Li, Keen You, Haotian Zhang, Di~Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad~Sathya Moorthy, Jeff Nichols, Yinfei Yang, and Zhe Gan.
\newblock Ferret-ui 2: Mastering universal user interface understanding across platforms.
\newblock {\em arXiv preprint arXiv:2410.18967}, 2024.

\bibitem{LIME_DATA_ai2d_train}
LIME-DATA.
\newblock Lime-data-ai2d-train.
\newblock \url{https://huggingface.co/datasets/LIME-DATA/ai2d}, 2024.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European Conference on Computer Vision}, pages 740--755, 2014.

\bibitem{liu2024deepseek}
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et~al.
\newblock Deepseek-v3 technical report.
\newblock {\em arXiv preprint arXiv:2412.19437}, 2024.

\bibitem{liu2023aligning}
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.
\newblock Aligning large multi-modal model with robust instruction tuning.
\newblock {\em CoRR}, 2023.

\bibitem{liu2024llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge.
\newblock \url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}, January 2024.

\bibitem{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{liu2024harnessingwebpageuistextrich}
Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue.
\newblock Harnessing webpage uis for text-rich visual understanding, 2024.

\bibitem{liu2024cmmmath}
Wentao Liu, Qianjun Pan, Yi~Zhang, Zhuo Liu, Ji~Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo~Jiang, and Liang He.
\newblock Cmm-math: A chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models.
\newblock {\em arXiv preprint arXiv:2409.02834}, 2024.

\bibitem{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{liu2019curved}
Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang.
\newblock Curved scene text detection via transverse and longitudinal sequence connection.
\newblock {\em Pattern Recognition}, 90:337--345, 2019.

\bibitem{liu2023ocrbench}
Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et~al.
\newblock On the hidden mystery of ocr in large multimodal models.
\newblock {\em arXiv preprint arXiv:2305.07895}, 2023.

\bibitem{liu2022taisu}
Yulong Liu, Guibo Zhu, Bin Zhu, Qi~Song, Guojing Ge, Haoran Chen, GuanHui Qiao, Ru~Peng, Lingxiang Wu, and Jinqiao Wang.
\newblock Taisu: A 166m large-scale high-quality dataset for chinese vision-language pre-training.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16705--16717, 2022.

\bibitem{liu2024chatqa}
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Chatqa: Surpassing gpt-4 on conversational qa and rag.
\newblock {\em arXiv preprint arXiv:2401.10225}, 2024.

\bibitem{LooksJuicy_ruozhiba}
LooksJuicy.
\newblock ruozhiba.
\newblock \url{https://huggingface.co/datasets/LooksJuicy/ruozhiba}, 2024.

\bibitem{lozhkov2024fineweb-edu}
Anton Lozhkov, Loubna Ben~Allal, Leandro von Werra, and Thomas Wolf.
\newblock Fineweb-edu: the finest collection of educational content, 2024.

\bibitem{lu2023mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
\newblock {\em arXiv preprint arXiv:2310.02255}, 2023.

\bibitem{lu2021inter}
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.
\newblock Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.
\newblock {\em arXiv preprint arXiv:2105.04165}, 2021.

\bibitem{lu2022learn}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock In {\em The 36th Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem{lu2022dynamic}
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying~Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
\newblock Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
\newblock {\em arXiv preprint arXiv:2209.14610}, 2022.

\bibitem{lu2021iconqa}
Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.
\newblock Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning.
\newblock {\em arXiv preprint arXiv:2110.13214}, 2021.

\bibitem{lu2024gui}
Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu~Qiao, and Ping Luo.
\newblock Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices.
\newblock {\em arXiv preprint arXiv:2406.08451}, 2024.

\bibitem{lu2024ovis}
Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye.
\newblock Ovis: Structural embedding alignment for multimodal large language model.
\newblock {\em arXiv preprint arXiv:2405.20797}, 2024.

\bibitem{lu2024bluelm}
Xudong Lu, Yinghao Chen, Cheng Chen, Hui Tan, Boheng Chen, Yina Xie, Rui Hu, Guanxin Tan, Renshou Wu, Yan Hu, et~al.
\newblock Bluelm-v-3b: Algorithm and system co-design for multimodal large language models on mobile devices.
\newblock {\em arXiv preprint arXiv:2411.10640}, 2024.

\bibitem{Luckyjhg_Geo170K}
Luckyjhg.
\newblock Geo170k.
\newblock \url{https://huggingface.co/datasets/Luckyjhg/Geo170K}, 2024.

\bibitem{deepscaler2025}
Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca~Ada Popa, and Ion Stoica.
\newblock Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl.
\newblock \url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}, 2025.
\newblock Notion Blog.

\bibitem{ma2024groma}
Chuofan Ma, Yi~Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi.
\newblock Groma: Localized visual tokenization for grounding multimodal large language models.
\newblock {\em arXiv preprint arXiv:2404.13013}, 2024.

\bibitem{marafioti2025smolvlm}
Andr{\'e}s Marafioti, Orr Zohar, Miquel Farr{\'e}, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna~Ben Allal, Anton Lozhkov, Nouamane Tazi, et~al.
\newblock Smolvlm: Redefining small and efficient multimodal models.
\newblock {\em arXiv preprint arXiv:2504.05299}, 2025.

\bibitem{masry2022chartqa}
Ahmed Masry, Xuan~Long Do, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock Chartqa: A benchmark for question answering about charts with visual and logical reasoning.
\newblock In {\em Proceedings of the Annual Meeting of the Association for Computational Linguistics}, pages 2263--2279, 2022.

\bibitem{mathew2022infographicvqa}
Minesh Mathew, Viraj Bagal, Rub{\`e}n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV~Jawahar.
\newblock Infographicvqa.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 1697--1706, 2022.

\bibitem{mathew2021docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV~Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 2200--2209, 2021.

\bibitem{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et~al.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock {\em arXiv preprint arXiv:2403.09611}, 2024.

\bibitem{Methani_2020_WACV}
Nitesh Methani, Pritha Ganguly, Mitesh~M. Khapra, and Pratyush Kumar.
\newblock Plotqa: Reasoning over scientific plots.
\newblock In {\em The IEEE Winter Conference on Applications of Computer Vision (WACV)}, March 2020.

\bibitem{ONNX-Runtime}
Microsoft.
\newblock Accelerated edge machine learning.
\newblock \url{https://onnxruntime.ai/}, 2023.

\bibitem{mishra2019ocrvqa}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em International Conference on Document Analysis and Recognition}, pages 947--952, 2019.

\bibitem{MiXaiLL76_TextOCR_OCR}
MiXaiLL76.
\newblock Textocr ocr.
\newblock \url{https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR}, 2025.

\bibitem{MMR1_Math_RL_Data_v0}
MMR1.
\newblock Mmr1-math-rl-data-v0.
\newblock \url{https://huggingface.co/datasets/MMR1/MMR1-Math-RL-Data-v0}, 2025.

\bibitem{moshkov2025aimo2}
Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman.
\newblock Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset.
\newblock {\em arXiv preprint arXiv:2504.16891}, 2025.

\bibitem{ds_receipts_v2_train}
mychen76.
\newblock ds receipts v2 train.
\newblock \url{https://huggingface.co/datasets/mychen76/ds_receipts_v2_train}, 2023.

\bibitem{mychen76_invoices_receipts_ocr_v1}
mychen76.
\newblock invoices and receipts ocr v1.
\newblock \url{https://huggingface.co/datasets/mychen76/invoices-and-receipts_ocr_v1}, 2023.

\bibitem{mychen76_invoices_receipts_ocr_v2}
mychen76.
\newblock invoices and receipts ocr v2.
\newblock \url{https://huggingface.co/datasets/mychen76/invoices-and-receipts_ocr_v2}, 2023.

\bibitem{NemotronPostTrainingDatasetV1}
Dhruv Nathawani, Igor Gitman, Somshubra Majumdar, Evelina Bakhturina, Ameya Sunil~Mahabaleshwarkar, , Jian Zhang, and Jane Polak~Scowcroft.
\newblock {Nemotron-Post-Training-Dataset-v1}, 2025.

\bibitem{nimapourjafar_LACR_I2I}
nimapourjafar.
\newblock Lacr i2i.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_LACR_I2I}, 2024.

\bibitem{nimapourjafar_mm_LADD}
nimapourjafar.
\newblock Ladd.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_LADD}, 2024.

\bibitem{nimapourjafar_mm_datikz}
nimapourjafar.
\newblock mm datikz.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_datikz}, 2024.

\bibitem{nimapourjafar_diagram_image_to_text}
nimapourjafar.
\newblock mm diagram image to text.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_diagram_image_to_text}, 2024.

\bibitem{nimapourjafar_mm_intergps}
nimapourjafar.
\newblock mm intergps.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_intergps}, 2024.

\bibitem{nimapourjafar_mm_tqa}
nimapourjafar.
\newblock mm tqa.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_tqa}, 2024.

\bibitem{nimapourjafar_mm_vqarad}
nimapourjafar.
\newblock mm vqarad.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_vqarad}, 2024.

\bibitem{nimapourjafar_robut_wikisql}
nimapourjafar.
\newblock robut-wikisql.
\newblock \url{https://huggingface.co/datasets/nimapourjafar/mm_robut_wikisql}, 2024.

\bibitem{nz_arxiv_ocr}
nz.
\newblock arxiv-ocr.
\newblock \url{https://huggingface.co/datasets/nz/arxiv-ocr-v0.1-sft}, 2024.

\bibitem{obeid2020chart}
Jason Obeid and Enamul Hoque.
\newblock Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model.
\newblock {\em arXiv preprint arXiv:2010.09142}, 2020.

\bibitem{InfinityInstruct2024}
Beijing~Academy of~Artificial Intelligence~(BAAI).
\newblock Infinity instruct.
\newblock {\em arXiv preprint arXiv:2406.XXXX}, 2024.

\bibitem{mOpenR1_Math_220k}
open r1.
\newblock Openr1-math-220k.
\newblock \url{https://huggingface.co/datasets/open-r1/OpenR1-Math-220k}, 2025.

\bibitem{chatgpt4o}
OpenAI.
\newblock Gpt-4o system card.
\newblock \url{https://openai.com/index/gpt-4o-system-card/}, 2025.

\bibitem{chatgpt}
OpenAI.
\newblock Introducing chatgpt.
\newblock \url{https://openai.com/index/chatgpt/}, 2025.

\bibitem{OpenGVLab_ShareGPT_4o}
OpenGVLab.
\newblock Sharegpt-4o.
\newblock \url{https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o}, 2005.

\bibitem{park2019cord}
Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee.
\newblock Cord: A consolidated receipt dataset for post-ocr parsing.
\newblock 2019.

\bibitem{multimath_300k}
pengshuai rin.
\newblock multimath-300k.
\newblock \url{https://huggingface.co/datasets/pengshuai-rin/multimath-300k}, 2024.

\bibitem{plummer2015flickr30k}
Bryan~A Plummer, Liwei Wang, Chris~M Cervantes, Juan~C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2641--2649, 2015.

\bibitem{PontTuset_eccv2020}
Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari.
\newblock Connecting vision and language with localized narratives.
\newblock In {\em ECCV}, 2020.

\bibitem{qgyd2021_chinese_ner_sft}
qgyd2021.
\newblock chinese ner sft.
\newblock \url{https://huggingface.co/datasets/qgyd2021/chinese_ner_sft}, 2023.

\bibitem{qgyd2021_few_shot_ner_sft}
qgyd2021.
\newblock few shot ner sft.
\newblock \url{https://huggingface.co/datasets/qgyd2021/few_shot_ner_sft}, 2024.

\bibitem{qiao2025wemath}
Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, Jie Wang, Chong Sun, Chen Li, and Honggang Zhang.
\newblock We-math 2.0: A versatile mathbook system for incentivizing visual mathematical reasoning, 2025.

\bibitem{rafailov2024dpo}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{rawles2024androidinthewild}
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap.
\newblock Androidinthewild: A large-scale dataset for android device control.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{reid2024gemini1_5}
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock {\em arXiv preprint arXiv:2403.05530}, 2024.

\bibitem{schuhmann2022laion5b}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:25278--25294, 2022.

\bibitem{schwenk2022okvqa}
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
\newblock A-okvqa: A benchmark for visual question answering using world knowledge.
\newblock In {\em European conference on computer vision}, pages 146--162. Springer, 2022.

\bibitem{shah2019kvqa}
Sanket Shah, Anand Mishra, Naganand Yadati, and Partha~Pratim Talukdar.
\newblock Kvqa: Knowledge-aware visual question answering.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 8876--8884, 2019.

\bibitem{shao2019objects365}
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.
\newblock Objects365: A large-scale, high-quality dataset for object detection.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8430--8439, 2019.

\bibitem{shao2024deepseekmath}
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK~Li, Yang Wu, et~al.
\newblock Deepseekmath: Pushing the limits of mathematical reasoning in open language models.
\newblock {\em arXiv preprint arXiv:2402.03300}, 2024.

\bibitem{shen2024measuring}
Jianhao Shen, Ye~Yuan, Srbuhi Mirzoyan, Ming Zhang, and Chenguang Wang.
\newblock Measuring vision-language stem skills of neural models.
\newblock {\em arXiv preprint arXiv:2402.17205}, 2024.

\bibitem{shi2024math}
Wenhao Shi, Zhiqiang Hu, Yi~Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee.
\newblock Math-llava: Bootstrapping mathematical reasoning for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2406.17294}, 2024.

\bibitem{shi2024mathv}
Wenhao Shi, Zhiqiang Hu, Yi~Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee.
\newblock Math-llava: Bootstrapping mathematical reasoning for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2406.17294}, 2024.

\bibitem{shibing624_sharegpt_gpt4}
shibing624.
\newblock sharegpt gpt4.
\newblock \url{https://huggingface.co/datasets/shibing624/sharegpt_gpt4}, 2023.

\bibitem{singh2019towards}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{singh2019textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8317--8326, 2019.

\bibitem{sr5434_CodegebraGPT_data}
sr5434.
\newblock Codegebragpt data.
\newblock \url{https://huggingface.co/datasets/sr5434/CodegebraGPT_data}, 2024.

\bibitem{kexuefm-8397}
Jianlin Su.
\newblock Transformer upgrade path: 4. rotary position encoding for two-dimensional positions, May 2021.

\bibitem{suhr2019nlvr2}
Alane Suhr and Yoav Artzi.
\newblock Nlvr2 visual bias analysis.
\newblock {\em arXiv preprint arXiv:1909.10411}, 2019.

\bibitem{Sujet-Finance-QA-Vision-100k}
Hamed~Rahimi Sujet~AI, Allaa~Boutaleb.
\newblock Sujet-finance-qa-vision-100k: A large-scale dataset for financial document vqa, 2024.

\bibitem{sun2024parrot}
Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, et~al.
\newblock Parrot: Multilingual visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2406.02539}, 2024.

\bibitem{sun2024generative}
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
\newblock Generative multimodal models are in-context learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14398--14409, 2024.

\bibitem{tan2013spot}
Lynette~Yihui Tan.
\newblock Spot the diff.
\newblock \url{https://huggingface.co/datasets/Lancelot53/spot-the-diff}, 2013.

\bibitem{tang2023vistext}
Benny~J Tang, Angie Boggust, and Arvind Satyanarayan.
\newblock Vistext: A benchmark for semantically rich chart captioning.
\newblock {\em arXiv preprint arXiv:2307.05356}, 2023.

\bibitem{tang2024mtvqa}
Jingqun Tang, Qi~Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz~Bin Mahmood, Hao Feng, Zhen Zhao, et~al.
\newblock Mtvqa: Benchmarking multilingual text-centric visual question answering.
\newblock {\em arXiv preprint arXiv:2405.11985}, 2024.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{gemmateam2025gemma3}
Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi~Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi~Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André~Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish
  Shenoy, Ayan Chakrabarti, Bilal Piot, Bo~Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline~Le Lan, Christopher~A. Choquette-Choo, CJ~Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree~Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal~Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju~yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar
  Bunyan, Pankil Botarda, Paul Caron, Paul~Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier~Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu~Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi~Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz~Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D.~Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis,
  Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot.
\newblock Gemma 3 technical report, 2025.

\bibitem{MLC}
MLC team.
\newblock Mlc-llm - universal llm deployment engine with ml compilation.
\newblock \url{https://github.com/ mlc-ai/mlc-llm/}, 2023-2024.

\bibitem{Theonewhomadethings_fsc147_controlnet}
Theonewhomadethings.
\newblock fsc147-controlnet.
\newblock \url{https://huggingface.co/datasets/Theonewhomadethings/fsc147-controlnet}, 2024.

\bibitem{tito2022hierarchical}
Rub{\`e}n Tito, Dimosthenis Karatzas, and Ernest Valveny.
\newblock Hierarchical multimodal transformers for multi-page docvqa.
\newblock {\em arXiv preprint arXiv:2212.05935}, 2022.

\bibitem{toghrultahirov_handwritten_text_ocr}
toghrultahirov.
\newblock handwritten text ocr.
\newblock \url{https://huggingface.co/datasets/toghrultahirov/handwritten_text_ocr}, 2024.

\bibitem{tong2024cambrian}
Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai~Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et~al.
\newblock Cambrian-1: A fully open, vision-centric exploration of multimodal llms.
\newblock {\em arXiv preprint arXiv:2406.16860}, 2024.

\bibitem{tschannen2025siglip}
Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad~Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye~Xia, Basil Mustafa, et~al.
\newblock Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features.
\newblock {\em arXiv preprint arXiv:2502.14786}, 2025.

\bibitem{tuo2023anytext}
Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie.
\newblock Anytext: Multilingual visual text generation and editing.
\newblock 2023.

\bibitem{geminipro2.5}
v~DeepMind.
\newblock Gemini 2.5 pro.
\newblock \url{https://deepmind.google/technologies/gemini/pro/}, 2025.

\bibitem{vikhyatk_st_vqa}
vikhyatk.
\newblock st-vqa.
\newblock \url{https://huggingface.co/datasets/vikhyatk/st-vqa}, 2024.

\bibitem{VLM_Perception_HME100k_400}
VLM-Perception.
\newblock Hme100k-400.
\newblock \url{https://huggingface.co/datasets/VLM-Perception/HME100k-400}, 2025.

\bibitem{wang2024unimernetuniversalnetworkrealworld}
Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo~Zhang, Botian Shi, and Conghui He.
\newblock Unimernet: A universal network for real-world mathematical expression recognition, 2024.

\bibitem{wang2021screen2words}
Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li.
\newblock Screen2words: Automatic mobile ui summarization with multimodal learning.
\newblock In {\em The 34th Annual ACM Symposium on User Interface Software and Technology}, pages 498--510, 2021.

\bibitem{wang2024muirbench}
Fei Wang, Xingyu Fu, James~Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu~Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et~al.
\newblock Muirbench: A comprehensive benchmark for robust multi-image understanding.
\newblock {\em arXiv preprint arXiv:2406.09411}, 2024.

\bibitem{wang2024pin}
Junjie Wang, Yin Zhang, Yatai Ji, Yuxiang Zhang, Chunyang Jiang, Yubo Wang, Kang Zhu, Zekun Wang, Tiezhen Wang, Wenhao Huang, et~al.
\newblock Pin: A knowledge-intensive dataset for paired and interleaved multimodal documents.
\newblock {\em arXiv preprint arXiv:2406.13923}, 2024.

\bibitem{wang2024mathvision}
Ke~Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li.
\newblock Measuring multimodal mathematical reasoning with math-vision dataset.
\newblock {\em arXiv preprint arXiv:2402.14804}, 2024.

\bibitem{wang2024qwen2vl}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et~al.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock {\em arXiv preprint arXiv:2409.12191}, 2024.

\bibitem{wang2024mpo}
Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu~Qiao, and Jifeng Dai.
\newblock Enhancing the reasoning ability of multimodal large language models via mixed preference optimization.
\newblock {\em arXiv preprint arXiv:2411.10442}, 2024.

\bibitem{wang2025internvl3}
Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et~al.
\newblock Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency.
\newblock {\em arXiv preprint arXiv:2508.18265}, 2025.

\bibitem{wang2024allCRPE}
Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et~al.
\newblock The all-seeing project v2: Towards general relation comprehension of the open world.
\newblock In {\em European Conference on Computer Vision}, pages 471--490. Springer, 2024.

\bibitem{wang2025sota}
Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang.
\newblock Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement.
\newblock {\em arXiv preprint arXiv:2504.07934}, 2025.

\bibitem{wu2024qbench}
Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, and Weisi Lin.
\newblock Q-bench: A benchmark for general-purpose foundation models on low-level vision.
\newblock In {\em ICLR}, 2024.

\bibitem{wu2024mobilevlm}
Qinzhuo Wu, Weikai Xu, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, and Shuo Shang.
\newblock Mobilevlm: A vision-language model for better intra-and inter-ui understanding.
\newblock {\em arXiv preprint arXiv:2409.14818}, 2024.

\bibitem{wu2024atlas}
Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul~Pu Liang, et~al.
\newblock Os-atlas: A foundation action model for generalist gui agents.
\newblock {\em arXiv preprint arXiv:2410.23218}, 2024.

\bibitem{xia2023structchart}
Renqiu Xia, Bo~Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Junchi Yan, and Yu~Qiao.
\newblock Structchart: Perception, structuring, reasoning for visual chart understanding.
\newblock {\em arXiv preprint arXiv:2309.11268}, 2023.

\bibitem{xiao2023efficient}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock {\em arXiv preprint arXiv:2309.17453}, 2023.

\bibitem{xiao2024logicvista}
Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang.
\newblock Logicvista: Multimodal llm logical reasoning benchmark in visual contexts.
\newblock {\em arXiv preprint arXiv:2407.04973}, 2024.

\bibitem{xiong2025bluelm}
Baojiao Xiong, Boheng Chen, Chengzhi Wang, Daxiong Luo, Dongsheng Xu, Dongyang Liu, Fan Yang, Fangyuan Li, Fei Teng, Feng Wang, et~al.
\newblock Bluelm-2.5-3b technical report.
\newblock {\em arXiv preprint arXiv:2507.05934}, 2025.

\bibitem{xu2024llava_uhd}
Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong Sun, and Gao Huang.
\newblock Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.
\newblock {\em arXiv preprint arXiv:2403.11703}, 2024.

\bibitem{yang2025qwen3}
An~Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et~al.
\newblock Qwen3 technical report.
\newblock {\em arXiv preprint arXiv:2505.09388}, 2025.

\bibitem{yang2024qwen2}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et~al.
\newblock Qwen2 technical report.
\newblock {\em arXiv preprint arXiv:2407.10671}, 2024.

\bibitem{yang2024qwen2.5}
An~Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, and Fei Huang.
\newblock Qwen2.5 technical report.
\newblock {\em arXiv preprint arXiv:2412.15115}, 2024.

\bibitem{yang2023longqlora}
Jianxin Yang.
\newblock Longqlora: Efficient and effective method to extend context length of large language models.
\newblock {\em arXiv preprint arXiv:2311.04879}, 2023.

\bibitem{yang2025ariauivisualgroundinggui}
Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li.
\newblock Aria-ui: Visual grounding for gui instructions, 2025.

\bibitem{yao2024minicpm}
Yuan Yao, Tianyu Yu, Ao~Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et~al.
\newblock Minicpm-v: A gpt-4v level mllm on your phone.
\newblock {\em arXiv preprint arXiv:2408.01800}, 2024.

\bibitem{Yeenyi_ner_sentiment_analysis_sharegpt}
Yeenyi.
\newblock ner sentiment analysis sharegpt.
\newblock \url{https://huggingface.co/datasets/Yeenyi/ner_sentiment_analysis_sharegpt}, 2024.

\bibitem{yin2025sail}
Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, et~al.
\newblock Sail-vl2 technical report.
\newblock {\em arXiv preprint arXiv:2509.14033}, 2025.

\bibitem{mmtbench}
Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Renrui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yu~Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao.
\newblock Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi.
\newblock {\em arXiv preprint arXiv:2404.16006}, 2024.

\bibitem{you2024ferret}
Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan.
\newblock Ferret-ui: Grounded mobile ui understanding with multimodal llms.
\newblock In {\em European Conference on Computer Vision}, pages 240--255. Springer, 2024.

\bibitem{yu2016refcoco}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em European Conference on Computer Vision}, pages 69--85, 2016.

\bibitem{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock {\em arXiv preprint arXiv:2309.12284}, 2023.

\bibitem{yu2023mmvet}
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
\newblock Mm-vet: Evaluating large multimodal models for integrated capabilities.
\newblock {\em arXiv preprint arXiv:2308.02490}, 2023.

\bibitem{yu2025opencsgchinesecorpusseries}
Yijiong Yu, Ziyun Dai, Zekun Wang, Wei Wang, Ran Chen, and Ji~Pei.
\newblock Opencsg chinese corpus: A series of high-quality chinese datasets for llm training, 2025.

\bibitem{yue2023mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction tuning.
\newblock {\em arXiv preprint arXiv:2309.05653}, 2023.

\bibitem{yue2024pangeafullyopenmultilingual}
Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de~Dieu~Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig.
\newblock Pangea: A fully open multilingual multimodal llm for 39 languages.
\newblock {\em arXiv preprint arXiv:2410.16153}, 2024.

\bibitem{yue2024mmmu}
Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge~Zhang, et~al.
\newblock Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark.
\newblock {\em arXiv preprint arXiv:2409.02813}, 2024.

\bibitem{zhang2019raven}
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.
\newblock Raven: A dataset for relational and analogical visual reasoning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem{zhang2024mm1_5}
Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et~al.
\newblock Mm1.5: Methods, analysis \& insights from multimodal llm fine-tuning.
\newblock {\em arXiv preprint arXiv:2409.20566}, 2024.

\bibitem{ferretv2}
Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William~Yang Wang, Shih-Fu Chang, Zhe Gan, et~al.
\newblock Ferret-v2: An improved baseline for referring and grounding with large language models.
\newblock {\em arXiv preprint arXiv:2404.07973}, 2024.

\bibitem{zhang2024android}
Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang.
\newblock Android in the zoo: Chain-of-action-thought for gui agents.
\newblock {\em arXiv preprint arXiv:2403.02713}, 2024.

\bibitem{zhang2024learning}
Lefan Zhang, Xiaodan Wang, Yanhua Huang, and Ruiwen Xu.
\newblock Learning harmonized representations for speculative sampling.
\newblock {\em arXiv preprint arXiv:2408.15766}, 2024.

\bibitem{zhang2021vsr}
Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi~Niu, and Fei Wu.
\newblock Vsr: a unified framework for document layout analysis combining vision, semantics and relations.
\newblock In {\em Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part I 16}, pages 115--130. Springer, 2021.

\bibitem{zhang2025mathverse}
Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu~Qiao, et~al.
\newblock Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?
\newblock In {\em European Conference on Computer Vision}, pages 169--186. Springer, 2025.

\bibitem{zhang20252}
Wenqi Zhang, Hang Zhang, Xin Li, Jiashuo Sun, Yongliang Shen, Weiming Lu, Deli Zhao, Yueting Zhuang, and Lidong Bing.
\newblock 2.5 years in class: A multimodal textbook for vision-language pretraining.
\newblock {\em arXiv preprint arXiv:2501.00958}, 2025.

\bibitem{zhang2023pmcvqa}
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya~Zhang, Yanfeng Wang, and Weidi Xie.
\newblock Pmc-vqa: Visual instruction tuning for medical visual question answering.
\newblock {\em arXiv preprint arXiv:2305.10415}, 2023.

\bibitem{zhang2023h2o}
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R{\'e}, Clark Barrett, et~al.
\newblock H2o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36:34661--34710, 2023.

\bibitem{zhao2023svit}
Bo~Zhao, Boya Wu, and Tiejun Huang.
\newblock Svit: Scaling up visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2307.04087}, 2023.

\bibitem{zhao2025omnialign}
Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosong Cao, Weiyun Wang, Jiaqi Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, et~al.
\newblock Omnialign-v: Towards enhanced alignment of mllms with human preference.
\newblock {\em arXiv preprint arXiv:2502.18411}, 2025.

\bibitem{zhao2022multihiertt}
Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang.
\newblock Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data.
\newblock {\em arXiv preprint arXiv:2206.01347}, 2022.

\bibitem{zhao2023robut}
Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and Dragomir Radev.
\newblock Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations.
\newblock {\em arXiv preprint arXiv:2306.14321}, 2023.

\bibitem{zheng2023vicuna}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{zhou2024aligning}
Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.
\newblock Aligning modalities in vision large language models via preference fine-tuning.
\newblock {\em arXiv preprint arXiv:2402.11411}, 2024.

\bibitem{zhu2021tat}
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua.
\newblock {TAT}-{QA}: A question answering benchmark on a hybrid of tabular and textual content in finance.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3277--3287, Online, August 2021. Association for Computational Linguistics.

\bibitem{zhu2025internvl3}
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi~Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu~Qiao, Jifeng Dai, and Wenhai Wang.
\newblock Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025.

\bibitem{zhu2023multimodal}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved with text.
\newblock {\em Advances in Neural Information Processing Systems}, 36:8958--8974, 2023.

\bibitem{zou2024dynamath}
Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang.
\newblock Dynamath: A dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models.
\newblock {\em arXiv preprint arXiv:2411.00836}, 2024.

\end{thebibliography}
