\begin{table*}[t!]
\scriptsize
\centering
\setlength\tabcolsep{0.9pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{l|c|c|ccccc|ccccccccc|c}
                                        & open-   &       & \multicolumn{5}{c|}{OCR-related  Benchmarks} & \multicolumn{9}{c|}{General Multimodal Benchmarks}  & \multicolumn{1}{c}{Math}\\

\multirow{-2}{*}{model} & source & \multirow{-2}{*}{\#param} & DocVQA        & ChartQA       & InfoVQA       & TextVQA       & OCRBench     & MME           & RWQA        & AI2D          & MMMU          & MMB-EN/CN                     & CCB           & MMVet         & SEED           & HallB         & MathVista \\
\hline
GPT-4V-1106~\cite{openai2023gpt4}    &   \no                & $-$ & 88.4          & 78.5	         & $-$           & 78.0          & 645          & 1926.6        &61.4          & 78.2          & 56.8          & 77.0 / 74.4                   & 46.5          & \textbf{67.6} & 71.6          & 46.5          &  49.9     \\ 
Gemini Ultra 1.0~\cite{team2023gemini} & \no           & $-$ & 90.9          & 80.8          & \textbf{80.3} & \textbf{82.3} & $-$          & $-$           &$-$           & 79.5          & \textbf{59.4} & $-$ / $-$                     & $-$           & $-$           & $-$           & $-$           &  53.0     \\
Gemini Pro 1.0~\cite{team2023gemini} & \no             & $-$ & 88.1          & 74.1          & 75.2          & 74.6          & 659          & 1933.4        &$-$           & 73.9          & 47.9          & 73.6 / 74.3                   & 52.5          & 64.3          & 70.7          & 45.2          &  45.2     \\
Gemini Pro 1.5~\cite{reid2024gemini1_5} & \no          & $-$ & 86.5          & 81.3          & 72.7          & 73.5          & $-$          & $-$           &67.5          & 80.3          & 58.5          & $-$ / $-$                     & $-$           & $-$           & $-$           & $-$           &  52.1     \\
Qwen-VL-Max~\cite{bai2023qwenvl} &   \no               & $-$ & \textbf{93.1} & 79.8	         & 73.4          & $-$           & 723          &\textbf{2433.6}&$-$           & 79.3          & 51.3          & 77.6 / 75.7                   & 63.5          & 66.6          & $-$           & 41.2          &  51.0     \\
Qwen-VL-Plus~\cite{bai2023qwenvl} &  \no               & $-$ & 91.4          & 78.1          & $-$           & $-$           & 694          & 2183.4        &$-$           & 75.9          & 45.2          & 67.0 / 70.7                   & 55.1          & 61.1          & 72.7          & 40.6          &  43.3     \\
Claude-3 Opus~\cite{claude3series2024} & \no           & $-$ & 89.3          & 80.8          & $-$           & $-$           & 694          & 1586.8        &49.8          & 88.1          & \textbf{59.4} & 63.3 / 59.2                   & 26.3          & 58.1          & $-$           & 37.8          &  50.5     \\
Claude-3 Sonnet~\cite{claude3series2024} & \no         & $-$ & 89.5          & 81.1          & $-$           & $-$	         & 646          & 1625.9        &51.9          & \textbf{88.7} & 53.1          & 67.8 / 64.2                   & 27.8          & $-$           & $-$           & 41.3          &  47.9     \\
Claude-3 Haiku~\cite{claude3series2024} & \no          & $-$ & 88.8          & 81.7          & $-$           & $-$           & 658          & 1453.2        &$-$           & 86.7          & 50.2          & 60.7 / 57.2                   & 24.5          & $-$           & $-$           & 39.2          &  46.4     \\
HPT Pro~\cite{hptpro2024}  &    \no                    & $-$ & $-$           & $-$           & $-$           & $-$           & $-$          & $-$           &$-$           & $-$           & 52.0          & 77.5 / 76.7                   & $-$           & $-$           & 73.1          & $-$           &  $-$      \\
MM1~\cite{mckinzie2024mm1}     &    \no                & 30B & $-$           & $-$           & $-$           & 73.5          & $-$          & 2069.0        &$-$           & $-$           & 44.7          & 75.1 / $-$~~~~                & $-$           & 48.7          & 72.1          & $-$           &  39.4	   \\
Step-1V~\cite{step1v2023}  &      \no                  & 100B& $-$           & $-$           & $-$           & $-$           & 625          & 2206.4        &$-$           & 79.2          & 49.9          & 80.7 / 79.9                   & \textbf{71.2} & 63.3          & 70.3	       & 48.4          &  44.8     \\
Grok-1.5V~\cite{xai2024grokv}   &   \no                & $-$ & 85.6          & 76.1          & $-$           & 78.1          & $-$          & $-$           &\textbf{68.7} & 88.3          & $-$           & $-$ / $-$                     & $-$           & $-$           & $-$           & $-$           &  52.8     \\
\hline
Text-Monkey~\cite{liu2024textmonkey} & \yes            & 10B & 66.7          & 59.9          & 28.6          & 64.3          & 561          & $-$           &$-$           & $-$           & $-$           & $-$ / $-$                     & $-$           & $-$           & $-$           & $-$           &  $-$      \\
DocOwl-1.5~\cite{hu2024mplug_docowl_1_5}  & \yes       & 8B  & 82.2          & 70.2          & 50.7          & 68.6          & 599          & $-$           &$-$           & $-$           & $-$           & $-$ / $-$                     & $-$           & $-$           & $-$           & $-$           &  $-$      \\
Mini-Gemini~\cite{li2024miniGemini} & \yes             & 35B & $-$           & $-$           & $-$           & ~~74.1*       & $-$          & 2141.0        &$-$           & $-$           & 48.0          & 80.6 / $-$~~~~                & $-$           & 59.3          & $-$           & $-$           &  43.3     \\
LLaVA-NeXT~\cite{liu2024llavanext}  & \yes             & 35B & 84.0          & 68.7          & 51.5          & ~~69.5*       & 574          & 2028.0        &$-$           & 74.9          & 51.1          & 81.1 / 79.0                   & 49.2          & 57.4          & 75.9	       & 34.8          &  46.5     \\
\rowcolor{gray!15}
InternVL 1.2 (ours)         &   \yes                   & 40B & 57.7          & 68.0          & 39.5          & ~~72.5*       & 569          & 2175.4        &67.5          & 79.0          & 51.6          & 82.2 / 81.2                   & 59.2          & 48.9          & 75.6          & 47.6          &  47.7     \\
\rowcolor{gray!15}
InternVL 1.5 (ours)       &     \yes                   & 26B & 90.9          & \textbf{83.8} & 72.5          & 80.6          & \textbf{724} & 2187.8        &66.0          & 80.7          & 45.2          & \textbf{82.2} / \textbf{82.0} & 69.8          & 62.8          & \textbf{76.0} & \textbf{49.3} &  \textbf{53.5} \\
% \hline

\end{tabular}
\caption{\textbf{Comparison with SoTA models on 16 multimodal benchmarks.}
% InternVL 1.5 achieves performance comparable to state-of-the-art MLLMs.
OCR-related benchmarks include: DocVQA test \cite{mathew2021docvqa}, ChartQA test \cite{masry2022chartqa}, InfographicVQA test \cite{mathew2022infographicvqa}, TextVQA val~\cite{singh2019textvqa}, and OCRBench~\cite{liu2023ocrbench}.
General multimodal benchmarks encompass: MME~\cite{fu2023mme}, RealWorldQA~\cite{xai2024grokv}, AI2D test~\cite{kembhavi2016ai2d}, MMMU val~\cite{yue2023mmmu}, MMBench-EN/CN test~\cite{liu2023mmbench}, CCBench dev~\cite{liu2023mmbench}, MMVet~\cite{yu2023mmvet}, SEED Image~\cite{li2023seed}, and HallusionBench~\cite{guan2023hallusionbench}. 
Additionally, the math dataset includes MathVista testmini~\cite{lu2023mathvista}.
* denotes that Rosetta OCR tokens are used in the testing of TextVQA.
The MME results we report are the sum of the perception and cognition scores.
The results of OCRBench, MMBench, CCBench, and HallusionBench are collected from the OpenCompass leaderboard~\cite{opencompass2023}.
}
\label{tab:sota_results}
\end{table*}