
\subsubsection{Datasets}

% \paragraph{Long-context Understanding and Reasoning. }
We mainly evaluate the long-context modeling capability of InternLM2 on the following two benchmarks: L-Eval~\cite{DBLP:journals/corr/abs-2307-11088} and LongBench~\cite{DBLP:journals/corr/abs-2308-14508}. 

\noindent \textbf{L-Eval~\cite{DBLP:journals/corr/abs-2307-11088}: }
L-Eval is a long-context benchmark consisting of 18 subtasks\footnote{
We adopt the first version of L-Eval, corresponding to \href{https://arxiv.org/abs/2307.11088v1}{https://arxiv.org/abs/2307.11088v1}}, 
including texts from various fields such as law, economy, and technology.
L-Eval consists of 411 documents and over 2000 test cases, with an average document length of 7217 words. 
Subtasks in this dataset can be categorized into two major classes:
5 close-ended tasks and 13 open-ended categories. 
Closed-ended tasks are evaluated using exact matching based accuracies,
while open-ended tasks adopt the Rouge score as the metric. 

\noindent \textbf{LongBench~\cite{DBLP:journals/corr/abs-2308-14508}: }
LongBench is a long-context benchmark consisting of 21 subtasks with a total of 4750 test cases. 
It is the first bilingual long-context benchmark, with an average English text length of 6711 words and an average Chinese text length of 13386 characters. 
The 21 subtasks are divided into 6 types, providing a more comprehensive evaluation of the model's capabilities in various aspects.

\textbf{Needle-in-the-Haystack: }
``Needle-in-the-Haystack" is a single-needle retrieval task, which is designed to test the Large Language Models' (LLMs) ability to recall a single piece of key information. This is done by inserting a crucial piece of information into a Haystack text of a target length\footnote{Text token length calculations use the GPT-4 tokenizer} at various positions and then querying the model about this key information at the end of the prompt. This method precisely visualizes LLMs' recall capabilities at different positions within long texts of varying lengths. We design a Chinese Haystack following the original idea, and utilize the Skywork/ChineseDomainModelingEval dataset released by \cite{wei2023skywork}, ensuring diversity and quality in the sources of Chinese texts. This dataset covers a wide range of fields from finance to technology, offering high-quality, up-to-date Chinese articles and providing a stable benchmark for assessing different models' abilities to handle domain-specific long texts. For this experiment, we leverage the LMDeploy\footnote{\href{https://github.com/InternLM/lmdeploy}{https://github.com/InternLM/lmdeploy}} \cite{2023lmdeploy} inference engine to accelerate the inference process. 


\subsubsection{Evaluation Results }
We report the evaluation results of InternLM2 on long-context benchmarks in Table~\ref{tab:longtext_chat_results}.
All variants of InternLM2 have demonstrated strong long-context modeling performance across two benchmarks.
InternLM2-Chat-20B-SFT achieves the best performance on L-Eval and outperforms its counterparts by a considerable margin. 
On LongBench, InternLM2-Chat-7B-SFT outperforms other $\leq 7$B Models models across 4 out of 6 subtask categories. 
It obtains a 48.1 overall score, which is only slightly inferior to the 48.4 overall score of ChatGLM3-6B. 
Meanwhile, we noticed that for InternLM2, different parameter sizes do not lead to significant difference in long-context performance, which will be further investigated. 


\input{table/benchmark_long_context_lm}

\begin{figure*}[!t]
    \centering
    \includegraphics[scale=0.4]{figure/internLM2/InternLM2-7B-200K_Needle_In_A_Haystack_Test.pdf}
    \caption{Results on Needle in the Haystack(Chinese).}
    \label{fig:needle_bench}
\end{figure*}



The results of ``Needle-in-the-Haystack" is presented in Figure \ref{fig:needle_bench}, which effectively demonstrate InternLM2's capability for long-context modeling.