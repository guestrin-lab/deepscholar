\begin{table*}[t!]
\scriptsize
\centering
{\fontsize{8}{10}\selectfont 
\renewcommand{\arraystretch}{0.95}
\setlength\tabcolsep{1.3pt}
\newcommand{\MMIU}{\makecell{MMIU}}
\newcommand{\Muir}{\makecell{Muir\\Bench}} % (test)
\newcommand{\BLINK}{\makecell{BLINK\\(val)}}
\newcommand{\Mantis}{\makecell{Mantis\\Eval}} % (test)
\newcommand{\RWQA}{\makecell{RealWorld\\QA}} % (test)
\newcommand{\MMERW}{\makecell{MME-RW\\(EN)}} % TODO
\newcommand{\RBench}{\makecell{R-Bench\\(test)}}
\newcommand{\TaskMe}{\makecell{TaskMe-\\Anything}}
\newcommand{\MMT}{\makecell{MMT\\(val)}}
\newcommand{\WILDV}{\makecell{WildVision\\(win rate)}}
\newcommand{\MIRB}{\makecell{MIRB\\(avg)}}
\newcommand{\RB}{\makecell{R-Bench\\(dis)}}
\newcommand{\TODO}{\textcolor{red}{TODO}}
\newcommand{\lsp}{--\ \ \ \ \ \ }
\newcommand{\rsp}{\ \ \ \ \ --~}
\begin{tabular}{l|ccccccc|ccccc}
Model Name                                 & \BLINK & \Mantis & \MMIU & \Muir & \MMT & \MIRB & Overall &\RWQA  & \MMERW & \WILDV  & \RB  & Overall \\
\hline
LLaVA-OneVision-0.5B~\cite{li2024llavaov}  & 52.1   & 39.6    & --    & 25.5  & --   & --    & --      & 55.6  & --     & --      & --   & --      \\
InternVL2-1B~\cite{chen2024far}            & 38.6   & 46.1    & 37.3  & 29.3  & 49.5 & 31.5  & 38.7    & 50.3  & 40.2   & 17.8    & 55.6 & 41.0    \\
InternVL2.5-1B~\cite{chen2024expanding}    & 42.0   & 51.2    & 38.5  & 29.9  & 50.3 & 35.6  & 41.3    & 57.5  & 44.2   & 43.4    & 59.0 & 51.0    \\
\rowcolor{gray!15}
InternVL3-1B                               & 42.9   & 50.2    & 39.3  & 31.2  & 52.9 & 36.1  & 42.1    & 58.2  & 46.0   & 43.8    & 60.4 & 52.1    \\
Qwen2-VL-2B~\cite{wang2024qwen2vl}         & 44.4   & --      & --    & --    & 55.1 & --    & --      & 62.6  & --     & --      & --   & --      \\
Qwen2.5-VL-3B~\cite{bai2025qwen2}          & 47.6   & --      & --    & 47.7    & -- & --    & --      & 65.4  & 53.1   & --      & --   & --      \\
% Aquila-VL-2B~\cite{gu2024aquilavl}       & --     & --      & --    & --    & 58.3 & --    & --      & 63.9  & --     & --      & --   & --      \\
InternVL2-2B~\cite{chen2024far}            & 43.8   & 48.4    & 39.8  & 32.5  & 50.4 & 32.1  & 41.2    & 57.3  & 47.3   & 31.8    & 56.8 & 48.3    \\
InternVL2.5-2B~\cite{chen2024expanding}    & 44.0   & 54.8    & 43.5  & 40.6  & 54.5 & 36.4  & 45.6    & 60.1  & 48.8   & 44.2    & 62.2 & 53.8    \\
\rowcolor{gray!15}
InternVL3-2B                               & 50.3   & 65.9    & 43.0  & 38.8  & 59.5 & 42.9  & 50.1    & 64.3  & 53.8   & 48.8    & 67.5 & 58.6    \\
\hline
Qwen2-VL-7B~\cite{wang2024qwen2vl}         & 53.2   & --      & --    & --    & 64.0 & --    & --      & 70.1  & 56.5   & --      & 64.0 & --      \\
Qwen2.5-VL-7B~\cite{bai2025qwen2}          & 56.4   & --      & --    & 59.6  & --   & --    & --      & 68.5  & 57.4   & --      & --   & --      \\
MiniCPM-V2.6~\cite{yao2024minicpm}         & 53.0   & 69.0    & --    & --    & 60.8 & --    & --      & 65.0  & --     & --      & --   & --      \\
InternVL2-8B~\cite{chen2024far}            & 50.9   & 65.4    & 42.0  & 48.7  & 60.0 & 50.0  & 52.8    & 64.4  & 53.5   & 54.4    & 67.9 & 60.1    \\
InternVL2.5-8B~\cite{chen2024expanding}    & 54.8   & 67.7    & 46.7  & 51.1  & 62.3 & 52.5  & 55.9    & 70.1  & 59.1   & 62.0    & 70.1 & 65.3    \\
\rowcolor{gray!15}
InternVL3-8B                               & 55.5   & 70.1    & 46.8  & 55.0  & 65.0 & 56.8  & 58.2    & 70.8  & 62.0   & 69.8    & 74.1 & 69.2    \\
\rowcolor{gray!15}
InternVL3-9B                               & 58.6   & 70.1    & 50.4  & 51.4  & 65.4 & 58.6  & 59.1    & 70.5  & 61.3   & 63.8    & 70.3 & 66.5    \\
\rowcolor{gray!15}
InternVL3-14B                              & 60.3   & 76.0    & 50.9  & 56.2  & 70.3 & 59.3  & 62.2    & 70.7  & 64.0   & 69.8    & 69.3 & 68.5    \\
\hline
InternVL-Chat-V1.5~\cite{chen2024far}      & 46.6   & 66.8    & 37.4  & 38.5  & 58.0 & 50.3  & 49.6    & 66.0  & 49.4   & 56.6    & 67.9 & 60.0    \\
InternVL2-26B~\cite{chen2024far}           & 56.2   & 69.6    & 42.6  & 50.6  & 60.6 & 53.7  & 55.6    & 68.3  & 58.7   & 62.2    & 70.1 & 64.8    \\
InternVL2.5-26B~\cite{chen2024expanding}   & 61.8   & 75.6    & 49.4  & 61.1  & 66.9 & 55.7  & 61.8    & 74.5  & 61.8   & 65.2    & 72.9 & 68.6    \\
Cambrian-34B~\cite{tong2024cambrian}       & --     & --      & --    & --    & --   & --    & --      & 67.8  & 44.1   & --      & --   & --      \\
InternVL2-40B~\cite{chen2024far}           & 57.2   & 71.4    & 47.9  & 54.4  & 66.2 & 55.2  & 58.7    & 71.8  & 61.8   & 63.2    & 73.3 & 67.5    \\
InternVL2.5-38B~\cite{chen2024expanding}   & 63.2   & 78.3    & 55.3  & 62.7  & 70.0 & 61.2  & 65.1    & 73.5  & 64.0   & 66.4    & 72.1 & 69.0    \\
\rowcolor{gray!15}
InternVL3-38B                              & 64.0   & 77.9    & 57.4  & 63.8  & 71.8 & 62.3  & 66.2    & 75.6  & 67.3   & 71.6    & 73.3 & 72.0    \\
\hline
GPT-4V~\cite{gpt4v}                        & 54.6   & 62.7    & --    & 62.3  & 64.3 & 53.1  & --      & 61.4  & --     & 71.8    & 65.6 & --      \\
GPT-4o-20240513~\cite{gpt4v}               & 68.0   & --      & 55.7  & 68.0  & 65.4 & --    & --      & 75.4  & 45.2   & 80.6    & 77.7 & 69.7    \\
% Claude-3-Opus~\cite{claude3series2024}   & 44.1   & --      & --    & --    & --   & --    & --      & 49.8  & --     & 53.0    & --   & --      \\
Claude-3.5-Sonnet~\cite{claude3series2024} & --     & --      & 53.4  & --    & --   & --    & --      & 60.1  & 51.6   & --      & --   & --      \\
Gemini-1.5-Pro~\cite{reid2024gemini1_5}    & --     & --      & 53.4  & --    & 64.5 & --    & --      & 67.5  & 38.2   & --      & --   & --      \\
LLaVA-OneVision-72B~\cite{li2024llavaov}   & 55.4   & 77.6    & --    & 54.8  & --   & --    & --      & 71.9  & --     & --      & --   & --      \\
Qwen2-VL-72B~\cite{wang2024qwen2vl}        & --     & --      & --    & --    & 71.8 & --    & --      & 77.8  & --     & --      & --   & --      \\
Qwen2.5-VL-72B~\cite{bai2025qwen2}         & 64.4   & --      & --    & 70.7  & --   & --    & --      & 75.7  & 63.2   & --      & --   & --      \\
InternVL2-Llama3-76B~\cite{chen2024far}    & 56.8   & 73.7    & 44.2  & 51.2  & 67.4 & 58.2  & 58.6    & 72.2  & 63.0   & 65.8    & 74.1 & 68.8    \\
InternVL2.5-78B~\cite{chen2024expanding}   & 63.8   & 77.0    & 55.8  & 63.5  & 70.8 & 61.1  & 65.3    & 78.7  & 62.9   & 71.4    & 77.2 & 72.6    \\
\rowcolor{gray!15}
InternVL3-78B                              & 66.3   & 79.3    & 60.4  & 64.5  & 73.2 & 64.3  & 68.0    & 78.0  & 65.4   & 73.6    & 77.4 & 73.6    \\
% \rowcolor{gray!15}
% InternVL2.5-Pro                            &        &         &       &       &      &       &       &         &        &         &      &         \\
\end{tabular}
}
\caption{\textbf{Comparison of multi-image and real-world understanding performance. }
Multi-image benchmarks include BLINK~\cite{fu2024blink}, Mantis-Eval~\cite{jiang2024mantis}, MMIU~\cite{meng2024mmiu}, MuirBench~\cite{wang2024muirbench}, MMT-Bench~\cite{mmtbench}, and MIRB~\cite{zhao2024mirb}.
Real-world benchmarks encompass RealWorldQA~\cite{realworldqa}, MME-RealWorld~\cite{zhang2024mme}, WildVision~\cite{lu2024wildvision}, and R-Bench~\cite{li2024r}.
Part of the results are sourced from the benchmark papers and the OpenCompass leaderboard~\cite{opencompass2023}.
}
\label{tab:benchmark_multi_image_real_world}
\end{table*}