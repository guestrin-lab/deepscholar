\begin{table*}[t!]
\centering
{\fontsize{8}{10}\selectfont 
\newcommand{\Pretrain}{\makecell{Pre-train\\Mixture}}
\newcommand{\Finetune}{\makecell{Fine-tune\\Mixture}}
\renewcommand{\arraystretch}{1.0}
{\setlength\tabcolsep{2.8pt}
\begin{tabular}{l|cc|cc|cc|ccc}
\multirow{2}{*}{Settings}         & \multicolumn{2}{c|}{InternVL2.5-1B} & \multicolumn{2}{c|}{InternVL2.5-2B} & \multicolumn{2}{c|}{InternVL2.5-4B} & \multicolumn{3}{c}{InternVL2.5-8B}       \\
                                  & Stage 1    & Stage 2                & Stage 1          & Stage 2          & Stage 1          & Stage 2          & Stage 1     & Stage 1.5    & Stage 2     \\
\hline
Dataset                           & \Pretrain  & \Finetune              & \Pretrain        & \Finetune        & \Pretrain        & \Finetune        & \Pretrain   & \Pretrain    & \Finetune   \\
Trainable                         & MLP        & Full Model             & MLP              & Full Model       & MLP              & Full Model       & MLP         & ViT+MLP      & Full Model  \\
% Packed Dataset                  & \yes       & \yes                   & \yes             & \yes             & \yes             & \yes             & \yes        & \yes         & \yes        \\
Packed Batch Size                 & 512        & 512                    & 512              & 512              & 512              & 512              & 512         & 1024         & 512         \\
Learning Rate                     & 2e-4       & 4e-5                   & 2e-4             & 4e-5             & 2e-4             & 4e-5             & 2e-4        & 1e-5         & 4e-5        \\
Context Length                    & 16384      & 16384                  & 16384            & 16384            & 16384            & 16384            & 16384       & 16384        & 16384       \\
% Length Threshold                & 16384      & 16384                  & 16384            & 16384            & 16384            & 16384            & 16384       & 16384        & 16384       \\
Image Tile Threshold              & 48         & 48                     & 48               & 48               & 48               & 48               & 48          & 48           & 48          \\
ViT Drop Path                     & 0.0        & 0.1                    & 0.0              & 0.1              & 0.0              & 0.1              & 0.0         & 0.1          & 0.1         \\
Weight Decay                      & 0.01       & 0.01                   & 0.01             & 0.01             & 0.01             & 0.01             & 0.05        & 0.05         & 0.05        \\
Training Epochs                   & --         & 4                      & --               & 4                & --               & 2                & --          & --           & 1           \\
\hline
Training Tokens                   & $\sim$191B & $\sim$176B             & $\sim$277B       & $\sim$176B       & $\sim$164B        & $\sim$88B        & $\sim$22B   & $\sim$76B    & $\sim$44B  \\
\end{tabular}
}

\bigskip

{
\setlength\tabcolsep{8pt}
\begin{tabular}{l|ccc|cc|cc}
\multirow{2}{*}{Settings}         & \multicolumn{3}{c|}{InternVL2.5-26B}   & \multicolumn{2}{c|}{InternVL2.5-38B} & \multicolumn{2}{c}{InternVL2.5-78B} \\
                                  & Stage 1    & Stage 1.5    & Stage 2    & Stage 1          & Stage 2           & Stage 1          & Stage 2          \\
\hline
Dataset                           & \Pretrain  & \Pretrain    & \Finetune  & \Pretrain        & \Finetune         & \Pretrain        & \Finetune        \\
Trainable                         & MLP        & ViT+MLP      & Full Model & MLP              & Full Model        & MLP              & Full Model       \\
% Packed Dataset                  & \yes       & \yes         & \yes       & \yes             & \yes              & \yes             & \yes             \\
Packed Batch Size                 & 512        & 1024         & 512        & 512              & 512               & 512              & 512              \\
Learning Rate                     & 2e-4       & 1e-5         & 2e-5       & 2e-4             & 2e-5              & 2e-4             & 2e-5             \\
Context Length                    & 16384      & 16384        & 16384      & 16384            & 16384             & 16384            & 16384            \\
% Length Threshold                & 16384      & 16384        & 16384      & 16384            & 16384             & 16384            & 16384            \\
Image Tile Threshold              & 48         & 48           & 48         & 48               & 48                & 48               & 48               \\
ViT Drop Path                     & 0.0        & 0.4          & 0.4        & 0.0              & 0.4               & 0.0              & 0.4              \\
Weight Decay                      & 0.05       & 0.05         & 0.05       & 0.05             & 0.05              & 0.05             & 0.05             \\
Training Epochs                   & --         & --           & 1          & --               & 1                 & --               & 1                \\
\hline
Training Tokens                   & $\sim$31B  & $\sim$146B   & $\sim$44B  & $\sim$107B       & $\sim$44B         & $\sim$76B        & $\sim$44B        \\
\end{tabular}
}
}
\caption{\textbf{Training configurations and hyperparameters for InternVL 2.5.} 
This table presents the training setups for various scales of InternVL 2.5 models. 
The configurations are carefully optimized to ensure efficient scaling and performance across different parameter sizes and training stages.
Notably, Qwen2-VL~\cite{wang2024qwen2vl} processed a cumulative total of 1.4T tokens, while our InternVL2.5-78B is trained on just $\sim$120B tokens.
}
\label{tab:train_hyperparameter}
\end{table*}
