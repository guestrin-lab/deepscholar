\begin{table*}[!t]
\centering

\resizebox{\linewidth}{!}{
\tablestyle{10pt}{1.2}
\begin{tabular}{l|c|ccc|ccccccc}

\multirow{2}{*}{model name} & \multirow{2}{*}{\#param} & \multicolumn{3}{c|}{L-Eval} & \multicolumn{7}{c}{LongBench} \\
& & Close & Open  & avg.  & Single & Multi & Summ & FSL & Syn & Code & avg. \\ 
\hline
% \multicolumn{11}{c}{\greencolor{\textit{ $\leq 7$B Models}}} \\ 
Llama2-7B-Chat & 7B & 20.0 & 33.1 & 29.4 & 23.1 & 17.0 & 18.5 & 7.0 & 5.0 & 15.4 & 14.3 \\
Llama2-13B-Chat & 13B & 27.3 & 34.4 & 32.5 & 18.2 & 9.9 & 18.5 & 6.7 & 10.0 & 18.1 & 13.6 \\

Mistral-7B-Instruct-v0.2 & 7.3B & 54.3 & 34.0 & 39.7 & 31.3 & 26.4 & 21.8 & 46.6 & 59.2 & 44.8 & 38.3 \\
Mixtral-8x7B-Instruct-v0.1 & 46.7B & 65.3 & 35.6 & 43.9 & 35.0 & 25.6 & 17.1 & 35.9 & 68.2 & 40.0 & 37.0 \\

Baichuan2-7B-Chat & 7B & 36.1 & 32.4 & 33.4 & 30.3 & 18.4 & 21.8 & 42.5 & 6.6 & 36.0 & 25.9 \\
Baichuan2-13B-Chat & 13B & 40.6 & 32.3 & 34.6 & 34.3 & 29.1 & 21.3 & 45.5 & 4.0 & 51.4 & 30.9 \\

Qwen-7B-Chat & 7B & 13.6 & 32.6 & 27.3 & 27.9 & 14.2 & 21.0 & 21.8 & 8.3 & 28.9 & 20.4 \\
Qwen-14B-Chat & 14B & 35.3 & 33.5 & 34.0 & 32.6 & 19.4 & 22.5 & 36.9 & 23.7 & 42.9 & 29.7 \\

% Qwen1.5-7B-Chat & 54.2 & 38.1 & 42.6 & 42.7 & 36.1 & 16.8 & 36.7 & 30.7 & 46.7 & 35.0 \\
% Qwen1.5-14B-Chat & 64.2 & 36.4 & 44.1 & 41.7 & 38.9 & 17.2 & 36.9 & 62.3 & {60.3} & 42.9 \\

ChatGLM3-6B & 6.2B & 59.1 & 35.0 & 41.7 & 40.9 & 45.0 & 26.0 & 56.5 & 65.0 & {57.1} & {48.4} \\

\hline
\rowcolor{gray!15}
InternLM2-Chat-7B & 7B & 68.6 & {40.8} & 48.5 & 45.7 & 43.1 & {26.5} & 58.3 & 66.3 & 36.4 & 46.1 \\
\rowcolor{gray!15}
InternLM2-Chat-7B-SFT & 7B & {68.7} & {40.8} & {48.6} & {47.3} & {45.2} & 25.3 & {59.9} & {67.2} & 43.5 & 48.1 \\
\rowcolor{gray!15}
InternLM2-Chat-20B & 20B & 68.6 & 40.6 & 48.4 & {46.9} & 46.7 & {26.0} & 49.1 & 67.3 & 32.6 & 44.8 \\
\rowcolor{gray!15}
InternLM2-Chat-20B-SFT & 20B & {68.8} & {42.0} & {49.4} & 46.8 & {48.7} & 25.6 & {51.2} & {67.9} & 40.4 & {46.8} \\ 
\end{tabular}}
\caption{\textbf{Comparison of chat models on long-context Benchmarks}. 
For each benchmark and task group, we report the average accuracies of intra-group subtasks.
\textbf{L-Eval abbrs}: Close $\rightarrow$ close-ended; Open $\rightarrow$ open-ended. 
\textbf{LongBench abbrs}: Single $\rightarrow$ single-document; Multi $\rightarrow$ multi-document; Summ $\rightarrow$ summarization; FSL $\rightarrow$ few shot learning; Syn $\rightarrow$ synthetic. 
All results are obtained with the OpenCompass~\cite{opencompass2023} evaluation toolkit. }
% \vspace{-2mm}
\label{tab:longtext_chat_results}
\end{table*}
