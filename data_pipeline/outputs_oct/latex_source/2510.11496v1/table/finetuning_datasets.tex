\begin{table*}[t!]
\centering
{
\renewcommand{\arraystretch}{0.97}
\fontsize{8}{10}\selectfont 
\setlength\tabcolsep{3.5pt}
\begin{tabular}{l|l}
Task &  Dataset \\
\hline
\multicolumn{2}{l}{\emph{Type: Single-Image Datasets}} \\
                              & TextCaps (en)~\cite{sidorov2020textcaps}, 
                              ShareGPT4o (en \& zh)~\cite{chen2024far}, 
                              InternVL-SA-1B-Caption (en \& zh)~\cite{chen2023internvl}, \\
\multirow{-2}{*}{Captioning}
                              & NewYorkerCaptionContest (en)~\cite{hessel2023androids},
                              MMInstruct (en \& zh)~\cite{liu2024mminstruct}  \\
\rowcolor{gray!15}
                              & VQAv2 (en)~\cite{goyal2017vqav2}, 
                              GQA (en)~\cite{hudson2019gqa}, 
                              OKVQA (en)~\cite{marino2019okvqa}, 
                              Visual7W (en)~\cite{zhu2016visual7w}, 
                              MMInstruct (en \& zh)~\cite{liu2024mminstruct}, \\
\rowcolor{gray!15}
\multirow{-2}{*}{General QA}
                              &  VSR (en)~\cite{liu2023vsr},
                              FSC147 (en)~\cite{ranjan2021learning},
                              Objects365-YorN (en)~\cite{shao2019objects365}, 
                              Hateful-Memes (en)~\cite{kiela2020hateful} \\
                              & GeoQA+ (en)~\cite{cao2022geoqa_plus}, 
                              CLEVR-Math (en)~\cite{lindstrom2022clevrmath}, 
                              Super-CLEVR (en)~\cite{li2023superclevr},
                              MapQA (en)~\cite{chang2022mapqa}, 
                              MAVIS (en)~\cite{zhang2024mavis}, \\ 
                              & Geometry3K (en)~\cite{lu2021geometry3k}, 
                              TallyQA (en)~\cite{acharya2019tallyqa}, 
                              MetaMath (en)~\cite{yu2023metamath}, 
                              GEOS (en)~\cite{seo2015solving}, 
                              UniGeo (en)~\cite{chen2022unigeo}, \\
\multirow{-3}{*}{Mathematics} 
                              & 
                              GeomVerse (en)~\cite{kazemi2023geomverse}, 
                              CMM-Math (zh)~\cite{liu2024cmmmath}  \\
                              
\rowcolor{gray!15}
                              & ChartQA (en)~\cite{masry2022chartqa}, 
                              MMTab (en)~\cite{zheng2024multimodal}, 
                              PlotQA (en)~\cite{methani2020plotqa}, 
                              FigureQA (en)~\cite{kahou2017figureqa}, 
                              VisText (en)~\cite{tang2023vistext},  \\
\rowcolor{gray!15}
                              & LRV-Instruction (en)~\cite{liu2023lrv-instruction}, 
                              ArxivQA (en)~\cite{li2024multimodal}, 
                              TabMWP (en)~\cite{lu2022tablemwp}, 
                              MMC-Inst (en)~\cite{liu2023mmc},
                              DVQA (en)~\cite{kafle2018dvqa},\\
\rowcolor{gray!15}
                              & UniChart (en)~\cite{masry2023unichart}, 
                              SimChart9K (en)~\cite{xia2023structchart}, 
                              Chart2Text (en)~\cite{obeid2020chart}, 
                              FinTabNet (zh)~\cite{zheng2021global},
                              SciTSR (zh)~\cite{chi2019complicated}, \\
\rowcolor{gray!15}
\multirow{-4}{*}{Chart}
                              & \textcolor{gray}{Synthetic Chart2Markdown (en)}  \\
                              & 
                              OCRVQA (en)~\cite{mishra2019ocrvqa}, 
                              InfoVQA (en)~\cite{mathew2022infographicvqa}, 
                              TextVQA (en)~\cite{singh2019textvqa}, 
                              ArT (en \& zh)~\cite{chng2019art}, 
                              HME100K (en)~\cite{yuan2022syntax},  \\ 
                              & COCO-Text (en)~\cite{veit2016coco}, 
                              CTW (zh)~\cite{yuan2019ctw}, 
                              LSVT (zh)~\cite{sun2019lsvt}, 
                              RCTW-17 (zh)~\cite{shi2017rctw17}, 
                              VCR (en \& zh)~\cite{zhang2024vcr}, \\ 
                              & EST-VQA (en \& zh)~\cite{wang2020general}, 
                              ST-VQA (en)~\cite{biten2019stvqa}, 
                              EATEN (zh)~\cite{guo2019eaten}, 
                              LLaVAR (en)~\cite{zhang2023llavar}, 
                              CASIA (zh)~\cite{liu2020casia},  \\
                              & Chinese-OCR (zh)~\cite{chinese-ocr}, 
                              CyrillicHandwriting (ru)~\cite{cyrillic}, 
                              IAM (en)~\cite{marti2002iam}, 
                              NAF (en)~\cite{davis2019deep}, 
                              POIE (en)~\cite{kuang2023visual}, \\
                              & ReCTs (zh)~\cite{zhang2019rects}, 
                              MTWI (zh)~\cite{he2018icpr2018}, 
                              TextOCR (en)~\cite{singh2021textocr}, 
                              SROIE (en)~\cite{huang2019icdar2019}, 
                              \textcolor{gray}{Synthetic Arxiv OCR (en)}, \\
                              & MTVQA (ko \& ja \& it \& ru \& de \& fr \& th \& ar \& vi)~\cite{tang2024mtvqa}, 
                              \textcolor{gray}{Synthetic Image2Latex (en)}, 
                              \\ 
\multirow{-7}{*}{OCR}         & \textcolor{gray}{Synthetic Handwritten OCR (zh)}, 
                              \textcolor{gray}{Synthetic Infographic2Markdown (en \& zh)} \\
\rowcolor{gray!15}
                              & KVQA (en)~\cite{shah2019kvqa}, 
                              A-OKVQA (en)~\cite{schwenk2022aokvqa}, 
                              ViQuAE (en)~\cite{lerner2022viquae}, 
                              iNaturalist2018 (en)~\cite{van2018inaturalist},
                              MovieNet (en)~\cite{huang2020movienet}, \\
\rowcolor{gray!15}
\multirow{-2}{*}{Knowledge} 
                              & ART500K (en)~\cite{mao2017deepart}, 
                              KonIQ-10K (en)~\cite{hosu2020koniq},
                              \textcolor{gray}{Synthetic Multidisciplinary Knowledge / QA (en \& zh)} \\
Document
                              & DocVQA (en)~\cite{clark2017docqa}, 
                              Docmatix (en)~\cite{2024docmatrix}, 
                              DocReason25K (en)~\cite{hu2024mplug_docowl_1_5}, 
                              Sujet-Finance-QA-Vision (en)~\cite{sujet-finance} \\
\rowcolor{gray!15}
                              & RefCOCO/+/g (en)~\cite{yu2016refcoco,mao2016generation}, 
                              GPT4Gen-RD-BoxCoT (en)~\cite{chen2023shikra}, 
                              All-Seeing-V2 (en)~\cite{wang2024allseeingv2}, \\
\rowcolor{gray!15}
\multirow{-2}{*}{Grounding}   
                              & V3Det (en \& zh)~\cite{wang2023v3det}, 
                              DsLMF (en)~\cite{yang2023open}, 
                              COCO-ReM (en \& zh)~\cite{singh2025benchmarking}, 
                              TolokaVQA (en)~\cite{ustalov2023toloka} \\
\multirow{-1}{*}{Science}     & AI2D (en)~\cite{kembhavi2016ai2d}, 
                              ScienceQA (en)~\cite{lu2022scienceqa}, 
                              TQA (en)~\cite{kembhavi2017tqa}, 
                              ChemVLM Data (en \& zh)~\cite{li2024chemvlm} \\
\rowcolor{gray!15}
                              & ALLaVA (en \& zh)~\cite{chen2024allava}, 
                              Viet-ShareGPT4o (vi)~\cite{doan2024vintern}, 
                              Cambrain-GPT4o (en)~\cite{tong2024cambrian} , 
                              RLAIF-V (en)~\cite{yu2024rlaifv}, \\ 
\rowcolor{gray!15}
                              & Laion-GPT4V (en)~\cite{laion_gpt4v_dataset}, 
                              TextOCR-GPT4V (en)~\cite{textocr_gpt4v_dataset}, 
                              WildVision-GPT4o (en)~\cite{lu2024wildvision}, \\
\rowcolor{gray!15}
\multirow{-3}{*}{Conversation}& \textcolor{gray}{Synthetic Real-World Conversations (en \& zh)}  \\
                              & PMC-VQA (en)~\cite{zhang2023pmc}, 
                              VQA-RAD (en)~\cite{lau2018dataset}, 
                              ImageCLEF (en)~\cite{garcia2015overview}, 
                              PMC (en)~\cite{wu2023towards}, 
                              SLAKE (en \& zh)~\cite{liu2021slake}, \\
                              & GMAI-VL (en \& zh)~\cite{li2024gmai},
                              VQA-Med (en)~\cite{ben2019vqa},
                              Medical-Diff-VQA (en)~\cite{hu2023medical},
                              PathVQA (en)~\cite{he2020pathvqa}, \\
\multirow{-3}{*}{Medical}
                              & PMC-CaseReport (en)~\cite{pmccase}\\
\rowcolor{gray!15}
                             & Screen2Words (en)~\cite{wang2021screen2words}, 
                              WebSight (en)~\cite{laurenccon2024unlocking},
                              Widget-Caption (en)~\cite{li2020widget},
                              RICOSCA (en)~\cite{deka2017rico}, \\
\rowcolor{gray!15}
                              & Seeclick (en)~\cite{cheng2024seeclick},
                              ScreenQA (en)~\cite{hsiao2022screenqa},
                              AMEX (en)~\cite{chai2024amex},
                              AITW (en)~\cite{rawles2024androidinthewild},
                              Odyssey (en)~\cite{lu2024gui}, \\
\rowcolor{gray!15}
\multirow{-3}{*}{GUI}
                              & UIBert (en)~\cite{bai2021uibert},
                              AndroidControl (en)~\cite{lieffects},
                              Mind2Web (en)~\cite{deng2024mind2web},
                              OmniACT (en)~\cite{kapoor2025omniact}, 
                              WaveUI (en)~\cite{agentsea_wave_ui} \\
\hline
\multicolumn{2}{l}{\emph{Type: Multi-Image Datasets}}    \\
                              & Img-Diff (en)~\cite{jiao2024img}, 
                              Birds-to-Words (en)~\cite{jiang2024mantis}, 
                              Spot-the-Diff (en)~\cite{jiang2024mantis}, 
                              MultiVQA (en)~\cite{jiang2024mantis}, 
                              NLVR2 (en)~\cite{suhr2018corpus}, \\ 
\multirow{-2}{*}{General QA}   
                              & ContrastiveCaption (en)~\cite{jiang2024mantis}, 
                              DreamSim (en)~\cite{jiang2024mantis}, 
                              InternVL-SA-1B-Caption (en \& zh)~\cite{chen2023internvl} \\ 
\rowcolor{gray!15}
Document   
                              & MP-DocVQA (en)~\cite{tito2023hier}, 
                              MP-Docmatix (en)~\cite{2024docmatrix} \\
\hline
\multicolumn{2}{l}{\emph{Type: Video Datasets}}    \\
                              & Vript (en \& zh)~\cite{yang2024vript},
                              OpenVid (en)~\cite{nan2024openvid},
                              Mementos (en)~\cite{wang2024mementos}, 
                              ShareGPT4o-Video (en \& zh)~\cite{chen2024far}, \\ 
\multirow{-2}{*}{Captioning} 
                              & 
                              ShareGPT4Video (en \& zh)~\cite{chen2024sharegpt4video},
                              VideoGPT+ (en)~\cite{Maaz2024VideoGPT+} \\
\rowcolor{gray!15}
                              & VideoChat2-IT (en \& zh)~\cite{li2023videochat,li2024mvbench}, 
                              EgoTaskQA (en)~\cite{jia2022egotaskqa}, 
                              NTU RGB+D (en)~\cite{liu2020ntu},
                              CLEVRER (en)~\cite{yi2019clevrer}, \\                              
\rowcolor{gray!15}
                              & LLaVA-Video (en)~\cite{zhang2024video}, 
                              FineVideo (en)~\cite{FineVideo}, 
                              PerceptionTest (en)~\cite{patraucean2024perception},
                              HiREST (en)~\cite{zala2023hierarchical},
                              STAR (en)~\cite{wu2024star},
                              \\
\rowcolor{gray!15}
\multirow{-3}{*}{General QA} 
                             & EgoSchema (en)~\cite{mangalam2023egoschema},
                             ScanQA (en)~\cite{azuma2022scanqa},
                             LSMDC (en)~\cite{rohrbach2015dataset}  \\ 
GUI 
                             & GUI-World (en)~\cite{chen2024gui}  \\

\hline
\multicolumn{2}{l}{\emph{Type: Text Datasets}}    \\


                              & UltraFeedback (en)~\cite{cui2023ultrafeedback}, 
                              UltraChat (en)~\cite{ding2023enhancing}, 
                              Unnatural-Instructions (en)~\cite{honovich2022unnatural}, 
                              NoRobots (en)~\cite{no_robots}, \\
                              & MOSS (en)~\cite{sun2023moss}, 
                              LIMA (en)~\cite{zhou2024lima},  
                              SlimOrca (en)~\cite{SlimOrca}, 
                              WizardLM-Evol-Instruct-70K (en)~\cite{xu2024wizardlm}, \\
                              & Llama-3-Magpie-Pro (en)~\cite{xu2024magpie}, 
                              Magpie-Qwen2-Pro (en \& zh)~\cite{xu2024magpie}, 
                              KOpen-HQ-Hermes-2.5-60K (ko)~\cite{MarkrAI_KOpen_HQ_Hermes_2.5_60K},  \\
                              & Firefly (zh)~\cite{Firefly}, 
                              Dolly (en)~\cite{conover2023free}, 
                              OpenAI-Summarize-TLDR (en)~\cite{CarperAI_openai_summarize_tldr},
                              Know-Saraswati-CoT (en)~\cite{knowrohit07_know_saraswati_cot}, \\
\multirow{-5}{*}{General QA} 
                              &  
                              FLAN (en)~\cite{wei2021finetuned}, 
                              FLANv2 (en \& zh)~\cite{chung2024scaling} \\

\rowcolor{gray!15}
                              & Code-Feedback (en)~\cite{opencodeinterpreter}, 
                              Glaive-Code-Assistant (en)~\cite{glaive_code_assistant_v3}, 
                              XCoder-80K (en)~\cite{wang2024xcoder80k},
                              LeetCode (en \& zh), \\
\rowcolor{gray!15}
\multirow{-2}{*}{Code}        
                              & Evol-Instruct-Code (en)~\cite{luo2023wizardcoder}, 
                              InternLM2-Code (en \& zh)~\cite{cai2024internlm2} \\

\multirow{2}{*}{Long Context}
                              & Long-Instruction-with-Paraphrasing (en \& zh)~\cite{yu2023paraphrasing}, 
                              LongCite (en \& zh)~\cite{zhang2024longcite}, 
                              LongQLoRA (en)~\cite{yang2023longqlora}, \\ & 
                              LongAlpaca (en)~\cite{long-alpaca}  \\

\rowcolor{gray!15}
                              & GSM8K-Socratic (en)~\cite{cobbe2021training}, 
                              NuminaMath-CoT/TIR (en)~\cite{li2024numinamath}, 
                              Orca-Math (en)~\cite{mitra2024orcamath}, 
                              MathQA (en)~\cite{amini2019mathqa},  \\
\rowcolor{gray!15}
\multirow{-2}{*}{Mathematics}
                              & InfinityMATH (en)~\cite{zhang2024inifinitymath},
                              InternLM2-Math (en \& zh)~\cite{cai2024internlm2} \\  

Knowledge
                              & \textcolor{gray}{Synthetic Multidisciplinary Knowledge / QA (en)} \\

\end{tabular}
\caption{
\textbf{Summary of the fine-tuning data mixture of InternVL 2.5.}
We expanded our fine-tuning data mixture through extensive collection of open-source datasets and self-synthesized data. This mixture is predominantly in English (en) and Chinese (zh), with smaller portions in other languages, including Korean (ko), Japanese (ja), Italian (it), Russian (ru), German (de), French (fr), Thai (th), Arabic (ar), and Vietnamese (vi).
}
\label{tab:finetuning_datasets}
}
\end{table*}
