\subsection{Hallucination Evaluation}
\subsubsection{Benchmarks}

We evaluate AndesVLâ€™s hallucination alleviation ability through a range of widely used hallucination benchmarks. These evaluations test the model's capabilities under  visual hallucination settings. The following three benchmarks are included in these evaluations.

\textbf{HallusionBench}~\cite{guan2023hallusionbench}: HallusionBench mainly evaluates a model's capabilities under language hallucination and visual illusion settings. The average of its three metrics---aAcc, fAcc, and qAcc---is taken as the reported performance score. 

\textbf{CRPE}~\cite{wang2024allCRPE}: CRPE quantitatively evaluates the object recognition and relation comprehension ability of MLLMs. The accuracy on the CRPE Relation subset is reported.

\textbf{POPE}~\cite{li2023pope}: POPE evaluates object hallucination tendencies in MLLMs. The overall average score is reported.
