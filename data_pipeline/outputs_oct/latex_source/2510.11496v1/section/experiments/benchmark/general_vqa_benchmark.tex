\subsection{General VQA}
\subsubsection{Benchmarks}

We evaluate AndesVL’s general visual question-answer ability through a range of benchmarks, including real-world understanding and comprehensive benchmarks. These evaluations test the model's capabilities under complex realistic tasks and various comprehensive tasks. The following six benchmarks are included in these evaluations.

\textbf{MME} ~\cite{fu2023mme}: MME evaluates models’ perception and cognitive abilities across 14 sub-tasks. The overall summarization score across all tasks is reported. Notice that the MME score is divided by 28 to calcute the overall average score.

\textbf{MMBench v1.1}~\cite{liu2023mmbench}: MMBench v1.1 evaluates the multimodal understanding capability of MLLMs. It consists of multimodal questions over 20 dimensions and supports English and Chinese versions. The average performance scores on both test sets are reported. 

\textbf{MMVet~\cite{yu2023mmvet}}: MMVet evaluates six core competencies for MLLMs: recognition, knowledge, spatial awareness, language generation, OCR, and mathematics, across 16 integrated tasks. 

\textbf{MMStar}~\cite{chen2024mmstar}: MMStar evaluates the multimodal capabilities of MLLMs, focusing on advanced perception, reasoning, math, and science\&technology for visual and language understanding.

\textbf{RealWorldQA}~\cite{realworldqa}: RealWorldQA evaluates the spatial understanding capabilities of MLLMs under various real-world scenarios.

\textbf{R-Bench}~\cite{li2024r}: R-Bench focuses on evaluating the robustness of MLLMs to distortion in the real world, which covers 33 distortion dimensions. The accuracy on the distortion set is reported.