\subsection{Multimodal Multilingual Understanding}
\subsubsection{Benchmarks}

We evaluate AndesVLâ€™s multilingual understanding capabilities through the following three benchmarks:

\textbf{MMMB}~\cite{sun2024parrot}: MMMB assesses multilingual capabilities of MLLMs, comprising 6 languages, 15 categories, and 12,000 questions. The average score is reported.

\textbf{Multilingual MMBench}~\cite{sun2024parrot}: Multilingual MMBench extends the original MMBench~\cite{liu2023mmbench} dataset to six languages, including English, Chinese, Portuguese, Arabic, Turkish, and Russian. The average score is reported.

\textbf{MTVQA}~\cite{tang2024mtvqa}: MTVQA evaluates the multilingual capability of MLLMs with human-annotated, text-rich images across 9 diverse languages. The average accuracy on the test set is reported.