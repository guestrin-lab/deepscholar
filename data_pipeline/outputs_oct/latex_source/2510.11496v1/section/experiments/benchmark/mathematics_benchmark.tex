\subsection{Reasoning and Math}
\subsubsection{Benchmarks}
To evaluate AndesVLâ€™s multimodal reasoning and mathematical capabilities, we extensively evaluate the model on various benchmarks for mathematical reasoning as follows:

\textbf{MMMU}~\cite{yue2024mmmu}: MMMU evaluates MLLMs on college-level tasks across six disciplines, testing expert-level reasoning and advanced perception in specific fields. The accuracy results achieved from the model's direct answer on its validation set are recorded. 

\textbf{MMMU Pro}~\cite{yue2024mmmu}: MMMU Pro evaluates the multimodal understanding and reasoning capabilities of the model from a wide range of academic disciplines. It is the upgraded version of the MMMU benchmark. The overall accuracy score of the direct answer is reported.


\textbf{MathVista}~\cite{lu2023mathvista}: MathVista evaluates the mathematical reasoning ability, such as algebra, geometry, and statistics, of MLLMs with visual contexts. The accuracy scores on the testmini set are recorded.

\textbf{MathVision}~\cite{wang2024mathvision}: MathVision is made up of math problems with visual contexts. The problems are sourced from real math competitions. The results on full set of the benchmark are reported.

\textbf{MathVerse}~\cite{zhang2025mathverse}: MathVerse evaluates a model's capability of solving visual diagram-based math problems. The performance of its vision-only set is reported.

\textbf{DynaMath}~\cite{zou2024dynamath}: DynaMath consists of variant-generated questions for a seed question under various conditions. The worst-case accuracy is reported to reflect the model's reliability of MLLMs' reasoning abilities. 

\textbf{WeMath}~\cite{qiao2025wemath}: WeMath decomposes composite visual math problems into sub-problems to hierarchically assess inherent issues in MLLMs' reasoning, covering 67 knowledge concepts across 5 levels of granularity.

\textbf{LogicVista}~\cite{xiao2024logicvista}: LogicVista evaluates models across five logical reasoning tasks: spatial, deductive, inductive, numeric, and mechanical reasoning, leveraging a diverse dataset of visual multiple-choice questions. 