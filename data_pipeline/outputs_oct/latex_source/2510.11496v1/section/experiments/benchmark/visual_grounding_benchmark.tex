\subsection{Visual Grounding}
\subsubsection{Benchmarks}
To evaluate the visual grounding and reference capability of AndesVL, we assess it on referring expression comprehension (REC) tasks on the RefCOCO, RefCOCO+, and RefCOCOg datasets, where the model identifies target objects in images from given descriptions.

\textbf{RefCOCO}~\cite{kazemzadeh2014referitgame}: RefCOCO consists of 19,994 images from COCO with 142,210 referring expressions for 50,000 objects. Its test A is people-focused, and test B is for other objects on REC tasks. 

\textbf{RefCOCO+}~\cite{kazemzadeh2014referitgame}: RefCOCO+ emphasizes attribute-based descriptions without absolute location cues, in contrast to RefCOCO. It contains 19,992 images and 141,564 expressions as descriptive attributes.

\textbf{RefCOCOg}~\cite{mao2016generation}: RefCOCOg has more complex and longer descriptive attributions for objects and requires models to handle intricate REC tasks. It contains 25,799 images and 95,010 expressions.