\subsection{Multi-Image Understanding}
\subsubsection{Benchmarks}

To evaluate AndesVL's capabilities in perception and understanding of multi-image relation, we conducted assessments on various multi-image benchmarks.  

\textbf{BLINK}~\cite{fu2024blink}: BLINK contains visual questions on multiple images from 14 computer vision tasks. Over half of the questions involve multiple images. The accuracy result on the validation set is reported.

\textbf{MMT-Bench}~\cite{mmtbench}: MMT-Bench consists of multimodal tasks across recognition, reasoning, and planning, with many sub-tasks requiring multi-image understanding. The accuracy metric in the validation set is reported.

\textbf{MuirBench}~\cite{wang2024muirbench}: MuirBench evaluates MLLMs' capabilities in multi-image understanding on 12 tasks and 10 types of multi-image relations. The accuracy score is reported.

\textbf{Q-Bench}~\cite{wu2024qbench}: Q-Bench assesses the abilities of MLLMs in low-level visual perception and understanding. The accuracy metric in the validation set is reported.