\section{Experiments}
\label{sec:experiments}
In this section, we provide experimental results to demonstrate the comprehensive capabilities of AndesVL. The general multimodal capabilities of AndesVL are compared with those of SOTA MLLMs using widely adopted multimodal benchmarks. Following this, the domain-specific performances of AndesVL are detailed, covering text-rich image understanding (including OCR, chart, and document comprehension), reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual capability, and GUI-related tasks. Most of the benchmarks are tested using VLMEvalKit~\cite{duan2024vlmevalkit}. 

\subsection{Overall Performance}
\subsubsection{Benchmarks}
We evaluated the performance of AndesVL in comparison to several advanced multimodal models of comparable model size, including Qwen2-VL~\cite{wang2024qwen2vl}, Qwen2.5-VL~\cite{bai2025qwen2_5}, InternVL3~\cite{zhu2025internvl3}, InternVL3.5~\cite{wang2025internvl3}, Gemma3~\cite{gemmateam2025gemma3}, BlueLM-2.5-3B~\cite{xiong2025bluelm}, Phi-3.5-Vision~\cite{abdin2024phi3}, Phi-4-Multimodal~\cite{abouelenin2025phi}, MiniCPM-V~\cite{yao2024minicpm}, R-4B~\cite{jiang2025r}, Qianfan-VL~\cite{dong2025qianfan}, Ovis2~\cite{lu2024ovis}, SAIL-VL-1.5-2B~\cite{dong2025scalable}, SAIL-VL2-2B~\cite{yin2025sail}, and SmolVLM2~\cite{marafioti2025smolvlm}. For fair comparisons, these models are grouped by their parameter sizes in the following evaluations.

The diverse multimodal capabilities of the proposed AndesVL are assessed using 32 commonly adopted benchmarks, covering various multimodal tasks across six domains: reasoning and math, text-rich, multi-image, general VQA, hallucination and multilingual capability. Detailed benchmark information is presented in subsequent subsections.
 