\subsubsection{Evaluation Results}
\begin{table}
    \centering
    \begin{tabular}{l|ccc|c}
        \toprule
         Model &  ScreenSpot & ScreenSpot\_v2 & ScreenSpot\_Pro & Overall \\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 55.5*& 80.9* & 27.3* & 54.6  \\
         OS-Atlas-4B ~\cite{xiong2025bluelm} & 70.1 & 71.9 & - & - \\
         InternVL3.5-4B~\cite{wang2025internvl3} & 83.6& 85.1 & 18.1* & 62.3  \\
         \rowcolor{gray!15} AndesVL-4B-Instruct & 84.3 & 86.1 & 28.2 & 66.2  \\
         \rowcolor{gray!15} AndesVL-4B-Thinking & \textbf{85.2}& \textbf{87.4}& \textbf{32.5} & \textbf{68.4} \\
         \hline
         UI-TARS-2B ~\cite{wang2024qwen2vl} &  \textbf{82.3} & \textbf{84.7} & \textbf{27.7} &  \textbf{64.9}\\
         InternVL3-2B~\cite{zhu2025internvl3} & 45.1*& 47.0*& 1.0*& 31.0\\
         InternVL3.5-2B~\cite{wang2025internvl3} & 77.2*& 79.6*& 12.2*& 56.3\\
         \rowcolor{gray!15} AndesVL-2B-Instruct & 74.6 & 76.3 & 20.9 & 57.3\\
         \rowcolor{gray!15} AndesVL-2B-Thinking & 67.2 & 70.2 & 19.6 & 52.4\\
         \hline
         InternVL3-1B~\cite{zhu2025internvl3} & 31.3*& 30.9*& 0.6*& 20.9\\
         InternVL3.5-1B~\cite{wang2025internvl3} & 60.9*& 61.9*& 9.2*& 44.0\\
         \rowcolor{gray!15} AndesVL-1B-Instruct & 71.8 & 73.2 & \textbf{23.1} & 56.0\\
         \rowcolor{gray!15} AndesVL-1B-Thinking & \textbf{73.9} & \textbf{74.4} & 20.9 & \textbf{56.4}\\
         \bottomrule
    \end{tabular}
    \caption{Comparison of UI understanding performance with other general and specific models on ScreenSpot testset. The best results are marked in \textbf{bold}. Data marked with * are from our evaluation, while others are from their original papers.}
    \label{tab:benchmark_screenspot}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l|ccc|c}
        \toprule
         Model &  Grounding & Referring & QA & Overall \\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 61.4 & 62.1 & 15.0 & 46.2  \\
         InternVL3.5-4B~\cite{wang2025internvl3} & 91.9 & 68.9 & 82.4 & 81.1  \\
         \rowcolor{gray!15} AndesVL-4B-Instruct & \textbf{95.1} & 72.5 & \textbf{82.6} & 83.4  \\
         \rowcolor{gray!15} AndesVL-4B-Thinking & 94.5& \textbf{73.4}& 82.5 & \textbf{83.5} \\
         \bottomrule
    \end{tabular}
    \caption{Comparison of UI understanding performance on AndesUI-Bench testset. The best results are marked in \textbf{bold}. All results are from our evaluation.}
    \label{tab:benchmark_andesui}
\end{table}

In this study, we present a comprehensive analysis of various models' performance in UI understanding tasks. As illustrated in Tab.~\ref{tab:benchmark_screenspot}, AndesVL-4B surpasses other models of comparable size in accuracy, achieving a leading score of 68.4. While slightly trailing behind UI-TARS-2B, a specialized model in the GUI domain, the AndesVL 2B and 1B variants maintain highly competitive performance, demonstrating robust UI comprehension capabilities.

Tab.~\ref{tab:benchmark_andesui} shows the performance comparison between AndesVL and other leading open-source models on the AndesUI-Bench testset. AndesVL-4B delivers outstanding performance across all evaluation metrics, achieving the top score of 83.5. These results collectively demonstrate our model's substantial expertise and distinct competitive advantage in UI understanding and application.





