\subsubsection{Evaluation Results}
\begin{table}
    \centering
    \begin{tabular}{l|ccc|c}
        \toprule
         Model &  Hallucination & CRPE (relation) & POPE (avg) & Overall \\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 46.6 & 64.9*& 89.3*& 66.9 \\
         BlueLM-2.5-3B~\cite{xiong2025bluelm} & 53.7 & - & - & -\\
         BlueLM-2.5-3B-thinking~\cite{xiong2025bluelm} & 57.3 & - & - & -\\
         Qianfan-VL-3B~\cite{dong2025qianfan} &  - & - & 85.1 & - \\
         Gemma3-4B~\cite{gemmateam2025gemma3} &  40.8 & 61.0*& 84.6 & 62.1 \\
         Phi-3.5-Vision-4B~\cite{abdin2024phi3} & 40.5 & 68.5*& 82.8 & 63.9\\
         Phi-4-Multimodal~\cite{abouelenin2025phi} & 40.5 & 72.0*& 85.6 & 66.0\\
         Ovis2-4B~\cite{lu2024ovis} & 53.8 & \textbf{77.0}*& 88.7 & 73.2 \\
         MiniCPM-V-4-4B~\cite{yao2024minicpm} & 50.8 & 74.6*& 82.4 & 69.3\\
         R-4B-RL~\cite{jiang2025r} & 58.9 & - & - & - \\
         InternVL3.5-4B~\cite{wang2025internvl3} & 44.8 & 75.0 & 88.9 & 69.6 \\
         \rowcolor{gray!15} AndesVL-4B-Instruct & 54.7& 75.8& 88.5& 73.0\\
         \rowcolor{gray!15} AndesVL-4B-Thinking & \textbf{59.2} & 75.5 & \textbf{89.8} & \textbf{74.8}\\
         \hline
         Qwen2-VL-2B~\cite{wang2024qwen2vl} &  42.4 & 68.5*& 87.3 & 66.1 \\
         MiniCPM-V-2B~\cite{yao2024minicpm} &  36.1 & 68.5*& 86.3 & 63.6 \\
         SAIL-VL-1.5-2B~\cite{dong2025scalable} &  49.8 & 73.9*& 87.7* &  70.5 \\
        SAIL-VL2-2B~\cite{yin2025sail} &  51.7* & 75.2*& - & - \\
        Ovis2-2B~\cite{lu2024ovis} &  50.2 & 73.0*& 87.8 & 70.3 \\
         InternVL3-2B~\cite{zhu2025internvl3} & 42.5 & 71.5 & 89.6 & 67.9\\
         InternVL3.5-2B~\cite{wang2025internvl3} & 48.6 & \textbf{75.6}& 87.2 & 70.5\\
         \rowcolor{gray!15} AndesVL-2B-Instruct & \textbf{51.8} & 73.0 & 87.9 & 70.9 \\
         \rowcolor{gray!15} AndesVL-2B-Thinking & 51.4 & 74.1& \textbf{89.8} & \textbf{71.8} \\
         \hline
         Ovis2-1B~\cite{lu2024ovis} &  45.2 & 63.2 & 87.7 & 65.4\\
         InternVL3-1B~\cite{zhu2025internvl3} & 41.4 & 64.0 & \textbf{90.7} & 65.4 \\
         InternVL3.5-1B~\cite{wang2025internvl3} & 41.0 & 68.4 & 86.8 & 65.4\\
         \rowcolor{gray!15} AndesVL-1B-Instruct & 43.2 & 68.7 & 89.2 & 67.0\\
         \rowcolor{gray!15} AndesVL-1B-Thinking & \textbf{45.6} & \textbf{68.8} & 87.7 & \textbf{67.4} \\
         \hline
         SmolVLM2-0.5B~\cite{marafioti2025smolvlm} & 27.7 & 52.9*& 82.7 & 54.4 \\
         \rowcolor{gray!15} AndesVL-0.6B-Instruct & \textbf{45.3} & 67.4 & 84.3 & 65.7\\
         \rowcolor{gray!15} AndesVL-0.6B-Thinking & 42.5 & \textbf{68.3} & \textbf{86.8} & \textbf{65.9} \\
         \bottomrule
    \end{tabular}
    \caption{Comparison of hallucination alleviation performance. The best results are marked in \textbf{bold}. Data marked with * are from our evaluation, while others are from their original papers or the OpenCompass leaderboard.}
    \label{tab:benchmark_hallucination}
\end{table}

As illustrated in Table~\ref{tab:benchmark_hallucination}, the AndesVL series achieve exceptionally high scores overall: the 4B, 2B, 1B and 0.6B models attaining 74.8, 71.8, 67.4, and 65.9 points, respectively. It maintains a substantial lead over other models of comparable size by a margin of 1.5 to 11.5 points - a lead that becomes even more pronounced with smaller model scales. This finding demonstrates that our architecture delivers superior hallucination alleviation capabilities while maintaining high accuracy, a key strength that persists even in the smallest 0.6B variants.

