\subsubsection{Evaluation Results}
\begin{table}[tb]
    \centering
    \begin{tabular}{l|cccc|c}
        \toprule
         Model &  BLINK & Q-Bench1 val &  MMT val & MuirBench & Overall\\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 49.3* & 30.3* & 61.0* & 38.7* & 44.8 \\
         Qianfan-VL-3B~\cite{dong2025qianfan} &  50.0 & 73.5 & 62.2 & - & -\\
         Gemma3-4B~\cite{gemmateam2025gemma3} &  43.9 & 14.2* & 53.9 & 40.5* & 38.1\\
         Phi-3.5-Vision-4B~\cite{abdin2024phi3} & 58.3 & 3.7* & 61.6 & 23.6* & 36.8\\
         Phi-4-Multimodal~\cite{abouelenin2025phi} & \textbf{61.3}& 10.2* & 60.4 & 37.6* & 42.4\\
         Ovis2-4B~\cite{lu2024ovis} & 53.0 & 20.9* & 65.2 & 43.4* & 45.6\\
         MiniCPM-V-4-4B~\cite{yao2024minicpm} & 54.0 & 76.5* & 59.7 & 46.1 & 59.1\\
         R-4B-RL~\cite{jiang2025r} & 56.3 & - & - & - & -\\
         InternVL3.5-4B~\cite{wang2025internvl3} & 58.1 & 73.8* & 64.3 & 53.1 & 62.3\\
         \rowcolor{gray!15} AndesVL-4B-Instruct & 58.2 & \textbf{77.8} & 66.4 & 55.5 & 64.5\\
         \rowcolor{gray!15} AndesVL-4B-Thinking & 58.4& 77.5 & \textbf{66.5} & \textbf{68.8} & \textbf{67.8}\\
         \hline
         Qwen2-VL-2B~\cite{wang2024qwen2vl} &  45.2 & 72.8* & 55.0 & 25.9* & 49.7\\
         MiniCPM-V-2B~\cite{yao2024minicpm} &  41.2 & 67.0* & 53.5 & 40.1* & 50.5 \\
         SAIL-VL-1.5-2B~\cite{dong2025scalable} &  45.4* & 75.9* & \textbf{61.7}* &  39.5* & 55.6\\
        Ovis2-2B~\cite{lu2024ovis} & \textbf{65.7} & \textbf{76.2}* & 55.0 & 41.9* & 59.7\\
         InternVL3-2B~\cite{zhu2025internvl3} & 50.3 & 68.4* & 59.5 & 38.8* & 54.3 \\
         InternVL3.5-2B~\cite{wang2025internvl3} & 51.3 & 72.4* & 58.5 & 44.0 & 56.6\\
         \rowcolor{gray!15} AndesVL-2B-Instruct &  48.1 & 73.8 & 58.8 & 45.5 & 56.5\\
         \rowcolor{gray!15} AndesVL-2B-Thinking &  48.6 & 74.6 & 58.5 & \textbf{57.4} & \textbf{59.8}\\
         \hline
         Ovis2-1B~\cite{lu2024ovis} & 44.0 & 71.3 & 54.7* & 42.0* & 53.0\\
         InternVL3-1B~\cite{zhu2025internvl3} & 42.9 & 63.4* & 53.5* & 31.2 & 47.8\\
         InternVL3.5-1B~\cite{wang2025internvl3} & 44.0 & 68.5* & 54.5 & 41.7 & 52.2\\
         \rowcolor{gray!15} AndesVL-1B-Instruct & \textbf{44.7} & 70.4 & 55.2 & 38.0 & 52.1\\
         \rowcolor{gray!15} AndesVL-1B-Thinking & \textbf{44.7} & \textbf{72.4} & \textbf{57.0} & \textbf{43.2} & \textbf{54.3}\\
         \hline
         SmolVLM2-0.5B~\cite{marafioti2025smolvlm} &  40.7 & 56.5  & 44.7 & 26.2* & 42.0\\
         \rowcolor{gray!15} AndesVL-0.6B-Instruct & \textbf{46.6} & 69.2 & 52.0 & 38.0 & 51.5 \\
         \rowcolor{gray!15} AndesVL-0.6B-Thinking & 46.0 & \textbf{71.7} & \textbf{52.7} & \textbf{42.0} & \textbf{53.1}\\
         \bottomrule
    \end{tabular}
    \caption{Comparison of multi-image understanding performance. The best results are marked in \textbf{bold}. Data marked with * are from our evaluation, while others are from their original papers or OpenCompass leaderboard.}
    \label{tab:benchmark_multi-image}
\end{table}

The detailed results presented in Table~\ref{tab:benchmark_multi-image} indicate that AndesVL-4B-Thinking achieves superior outcomes across various multi-image benchmarks, culminating in a top overall score of 67.8, outperforming the previous best (InternVL3.5-4B, 62.3) by a margin of 5.5 points. It also scores the top on three out of four individual multi-image benchmarks. Moreover, as the model scale decreases, the models persist to demonstrate highly competitive accuracy, with the 0.6B variant attaining a score of 53.1. 

This superiority suggests that the advanced pre-training strategies and enhanced training datasets employed in AndesVL significantly enhance its ability to capture and reason about inter-image relationships by concurrently understanding and analyzing the relationships among multiple images.
