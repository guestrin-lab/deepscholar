\subsubsection{Evaluation Results}
\begin{table}
    \centering
    \begin{tabular}{l|ccc|c}
        \toprule
         Model &  MMMB & Multilingual MMBench & MTVQA & Overall \\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 76.9*& 74.9*& 24.8 & 58.9 \\
         Qianfan-VL-3B~\cite{dong2025qianfan} & - & - & 26.5 & - \\
         Gemma3-4B~\cite{gemmateam2025gemma3} &  69.6*& 65.3*& 22.1 & 52.4 \\
         Phi-3.5-Vision-4B~\cite{abdin2024phi3} & 61.3*& 67.4*& 12.3*& 47.0\\
         Phi-4-Multimodal~\cite{abouelenin2025phi} & 74.5*& 74.2*& 14.3& 54.3\\
         Ovis2-4B~\cite{lu2024ovis} &  79.7*& 81.2*& 29.4 & 63.4\\
         MiniCPM-V-4-4B~\cite{yao2024minicpm} & 72.8*& 70.7*& 22.6*& 55.4\\
         InternVL3.5-4B~\cite{wang2025internvl3}  & 80.2& 76.4& 29.6& 62.1\\
         \rowcolor{gray!15} AndesVL-4B-Instruct & \textbf{81.9} & 80.8 & \textbf{31.2}*& 64.6 \\
         \rowcolor{gray!15} AndesVL-4B-Thinking & 81.7 & \textbf{83.2} & 29.9 & \textbf{64.9} \\
         \hline
         Qwen2-VL-2B~\cite{wang2024qwen2vl} &  71.3*& 66.3*& 20.8 & 52.8 \\
         MiniCPM-V-2B~\cite{yao2024minicpm} &  60.0*& 51.3*& 9.3 & 40.2 \\
         SAIL-VL-1.5-2B~\cite{dong2025scalable} & 76.0*& 72.9*& 22.9*&  57.3 \\
        Ovis2-2B~\cite{lu2024ovis} &\textbf{76.6}*& 72.0*& 25.6 & 58.0 \\
         InternVL3-2B~\cite{zhu2025internvl3} &73.6& 71.9& 26.7 & 57.4 \\
         InternVL3.5-2B~\cite{wang2025internvl3} &74.6& 70.9& 28.5 & 58.0 \\
         \rowcolor{gray!15} AndesVL-2B-Instruct & 76.5 & \textbf{75.3} & \textbf{29.1} & \textbf{60.3} \\
         \rowcolor{gray!15} AndesVL-2B-Thinking & 76.5 & 75.0 & 26.7 & 59.4 \\
         \hline
         Ovis2-1B~\cite{lu2024ovis} & 70.8*& 62.6*& 23.7 & 52.4\\
         InternVL3-1B~\cite{zhu2025internvl3} & 63.2& 58.2& 22.2 & 47.9 \\
         InternVL3.5-1B~\cite{wang2025internvl3} & 66.0& 58.5& 22.9 & 49.1\\
         \rowcolor{gray!15} AndesVL-1B-Instruct & \textbf{72.0} & 63.0 & \textbf{24.9} & 53.3 \\
         \rowcolor{gray!15} AndesVL-1B-Thinking & 71.3 & \textbf{67.5} & 23.6 & \textbf{54.1} \\
         \hline
         SmolVLM2-0.5B~\cite{marafioti2025smolvlm} &  46.8*& 23.7*& 7.7 & 26.1 \\
         \rowcolor{gray!15} AndesVL-0.6B-Instruct & \textbf{70.3} & \textbf{60.8} & \textbf{21.8} & \textbf{51.0} \\
         \rowcolor{gray!15} AndesVL-0.6B-Thinking & 69.4 & 58.4 & 21.3 & 49.7 \\
         \bottomrule
    \end{tabular}
    \caption{Comparison of multilingual performance. The best results are marked in \textbf{bold}. Data marked with * are from our evaluation, while others are from their original papers or the OpenCompass leaderboard.}
    \label{tab:benchmark_multilingual}
\end{table}

As demonstrated in Table~\ref{tab:benchmark_multilingual}, both the Thinking and Instruct variants of AndesVL-4B demonstrate exceptional multilingual capabilities, achieving a leading score of 64.9, which surpasses the previous best model, Ovis2-4B~\cite{lu2024ovis}, by 1.5 points. This advantage persists in the smaller-scale variants of AndesVL, with each one achieving top multilingual scores within their respective sub-groups.

The model's professional-grade multilingual capability provides a foundation for the cross-lingual transfer of its multimodal functions, a feature paramount for extending its global utility in mobile applications.

