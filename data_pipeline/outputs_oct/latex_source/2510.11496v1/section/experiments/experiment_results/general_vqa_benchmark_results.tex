\subsubsection{Evaluation Results}
\begin{table}
    \resizebox{1\textwidth}{!}{
    \centering
    \begin{tabular}{l|cccccc|c}
        \toprule
         Model &  MME\_sum & \makecell{MMBench\\v1.1} & MMVet & MMStar & RealWorldQA & \makecell{R-Bench\\(dis)} & Overall \\
         \hline
         Qwen2.5-VL-3B~\cite{bai2025qwen2_5}   & 2,181*& 51.2 & 60.0 & 56.3 & 66.3*& 61.8*& 62.2 \\
         BlueLM-2.5-3B~\cite{xiong2025bluelm} & - & 82.1 & 66.7 & 64.5 & - & - & - \\
         BlueLM-2.5-3B-thinking~\cite{xiong2025bluelm} & - & 78.3 & 65.1 & 66.3 & - & - & -  \\
         Qianfan-VL-3B~\cite{dong2025qianfan} & - & - & 48.2 & 57.9 & 65.8 & - & -  \\
         Gemma3-4B~\cite{gemmateam2025gemma3} &  1,744 & 66.4 & 57.8 & 47.9 & 55.6 & 56.6*& 57.8 \\
         Phi-3.5-Vision-4B~\cite{abdin2024phi3} &1,838 & 67.4 & 43.2 & 47.5 & 53.6 & 55.4*& 55.4 \\
         Phi-4-Multimodal~\cite{abouelenin2025phi} &1,962& 77.2& 51.9& 58.9& 64.1& 62.8*& 64.2\\
         Ovis2-4B~\cite{lu2024ovis} & 2,162 & 81.4 & 65.5 & 61.9 & 71.1*& 70.5*& 71.3 \\
         MiniCPM-V-4-4B~\cite{yao2024minicpm} & 2,298& 79.7& 68.0& 62.8& 68.5& 64.7*& 71.0\\
         R-4B-RL~\cite{jiang2025r} & - & \textbf{84.8} & 73.1 & \textbf{81.9} & 69.1 & - & - \\
         InternVL3.5-4B~\cite{wang2025internvl3} & 2,272 & 79.3*& \textbf{76.6} & 65.0 & 66.3 & 68.7 & 72.8 \\
         \rowcolor{gray!15} AndesVL-4B-Instruct & 2,345& 81.2& 61.2& 66.1& 72.2 & \textbf{71.7}& 72.7\\
         \rowcolor{gray!15} AndesVL-4B-Thinking & \textbf{2,412} & 81.7 & 61.9 & 69.9 & \textbf{73.2} & 69.9 & \textbf{73.8} \\
         \hline
         Qwen2-VL-2B~\cite{wang2024qwen2vl} &  1,899 & 72.2*& 51.5 & 47.5 &  60.7 &  62.8*& 60.5 \\
         MiniCPM-V-2B~\cite{yao2024minicpm} &  1,808 & 65.8*& 41.0 & 39.1 & 55.8& 64.7*& 53.5 \\
         SAIL-VL-1.5-2B~\cite{dong2025scalable} & 2,063 & \textbf{78.5}*& 61.4 &  62.8 & 67.1 & 66.7*& 68.4 \\
        SAIL-VL2-2B~\cite{yin2025sail} & 2,144* & -& 68.7* & \textbf{64.1}* & - & -& - \\
        Ovis2-2B~\cite{lu2024ovis} &  2,005 & 77.0*& 67.9 & 56.7 & 66.0 & 64.2*&  67.2 \\
         InternVL3-2B~\cite{zhu2025internvl3} & 2,221 & 78.0*& 62.2 & 60.7 & 64.3 & \textbf{71.4}*& \textbf{69.4} \\
         InternVL3.5-2B~\cite{wang2025internvl3} & 2,123 & 75.3*& \textbf{71.7} & 62.7 & 62.0 & 62.4 & 68.3 \\
         \rowcolor{gray!15} AndesVL-2B-Instruct & 2,081 & 77.3 & 52.0 & 60.0 & \textbf{67.8}& 65.3 & 66.1 \\
         \rowcolor{gray!15} AndesVL-2B-Thinking & \textbf{2,326} & 75.4 & 59.5 & 62.7 & 64.8 & 64.2 & 68.3 \\
         \hline
         Ovis2-1B~\cite{lu2024ovis} &  1,720 & 68.4*& 50.0*& 52.1 & 63.9 & 61.0*& 59.5 \\
         InternVL3-1B~\cite{zhu2025internvl3} & 1,935 & 68.2*& \textbf{59.5}*& 51.5 & 58.2 & 60.4 & 61.2\\
         InternVL3.5-1B~\cite{wang2025internvl3} & 1,910 & 67.6*& 56.5*& 51.9 & 57.6 & 57.4 & 59.9 \\
         \rowcolor{gray!15} AndesVL-1B-Instruct & 1,938 & 70.9& 43.5 & 52.5 & 65.0 & 63.4 & 60.7 \\
         \rowcolor{gray!15} AndesVL-1B-Thinking & \textbf{2,006} & \textbf{73.1} & 48.9 & \textbf{57.9} & \textbf{65.1} & \textbf{64.0} & \textbf{63.4} \\
         \hline
         SmolVLM2-0.5B~\cite{marafioti2025smolvlm} & 1,448 & 41.6*& 29.9& 38.2 & 52.7*& 47.4*& 43.6 \\
         \rowcolor{gray!15} AndesVL-0.6B-Instruct  & 1,866 & 65.3 & \textbf{39.7} & 44.3 & 58.6 & 57.2 & 55.3 \\
         \rowcolor{gray!15} AndesVL-0.6B-Thinking  & \textbf{1,925} & \textbf{66.3} & 36.1 & \textbf{49.7} & \textbf{61.6} & \textbf{59.8} & \textbf{57.1} \\
         \bottomrule
    \end{tabular}}
    \caption{Comparison of general VQA performance. Notice that the MME score is divided by 28 to calculate the overall score. The best results are marked in \textbf{bold}. Data marked with * are from our evaluation, while others are from their original papers or the OpenCompass leaderboard.}
    \label{tab:benchmark_general_vqa}
\end{table}

As illustrated in Table~\ref{tab:benchmark_general_vqa}, the AndesVL series (4B, 1B, and 0.6B) achieve the top performance in their respective groups, while the 2B model also delivers a highly competitive result. A detailed breakdown reveals that while there remains room for improvement on the MMVet benchmark, the AndesVL series demonstrates exceptionally strong and robust performance on both MME and RealWorldQA. This suggests that our model extracts robust representations and displays a strong ability to comprehend real-world scenarios, enabling it to effectively tackle complex and dynamic tasks.
