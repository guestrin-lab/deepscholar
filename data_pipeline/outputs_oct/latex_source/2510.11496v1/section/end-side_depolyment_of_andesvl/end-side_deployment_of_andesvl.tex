\section{Mobile-side Application of AndesVL}
\label{sec:mobile-side_depoloy_andesvl}
% 端侧应用部署基础架构

Based on the AndesVL model after both pre-training and post-training, we build a \textbf{1+N LoRA}~\cite{yang2023longqlora} on-device AI framework. This architecture comprises a foundational model and multiple scenario-specific LoRA adapters for each scenario. Based on this framework, we further perform quantization and mobile-side acceleration and release multiple on-device AI applications on OPPO AI phones.

\subsection{Multi-scenario LoRA Training}
During multi-scenario deployment of AndesVL models, it is imperative to balance the generalization capacity of the model with its domain-specific adaptability. To address practical requirements during application, based on AndesVL, we further designed a dedicated multi-scenario LoRA training stage, structured as in Fig.~\ref{fig:Multi-scenario training}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Multi-scenario_training.png}
    \caption{Multi-scenario LoRA training based on AndesVL.}
    \label{fig:Multi-scenario training}
\end{figure}

\begin{table}[tb]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Task Type} & \textbf{Reward Criteria} \\
\midrule
OCR tasks & Text detection accuracy and structural integrity \\
Caption generation & Semantic relevance and linguistic conciseness \\
Text summarization & Content coverage and instruction alignment \\
\bottomrule
\end{tabular}
\caption{Reward signals for different specific real-world tasks}
\label{tab:reward_signal_for_multi_scenarios}
\end{table}

In practical deployment scenarios, task-specific fine-tuning is often required while computational resources remain limited. It is infeasible to train separate large models for each individual scenario. Therefore, based on our AndesVL described above, we trained multiple LoRA models for different scenarios while keeping the base model parameters frozen. This approach only requires fine-tuning a minimal number of parameters to adapt to various application scenarios.
It significantly reduces training resource consumption, while effectively prevents catastrophic forgetting and enhances the model's generalization across multiple scenarios. The LoRA training for each scenario consist of two phases: SFT and RL.

The SFT data construction for LoRA fine-tuning was more scenario-specific and customized, where for each scenario we collected and annotated high-quality, strongly relevant dedicated data; designed data labels and task definitions that closely align with actual requirements; and implemented customized training loss functions tailored to specific scenarios.
For example, in image caption generation tasks, to increase entity density, we designed an entity-weighted cross-entropy loss that assigns higher loss weights to entity words (\textit{e.g.}, colors, quantities, object categories) in captions, thereby encouraging the model to focus more on generating these critical pieces of information.
The entity-weighted cross-entropy loss enhances key information generation through differential weighting:
\begin{equation}
\mathcal{L}_{\text{entity}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T_i} \alpha_{i,t} \log P(w_{i,t} | x_i, w_{i,<t}, \theta),
\end{equation}
where $\alpha_{i,t}$ denotes the weighting factor ($\alpha_{i,t} > 1$ for entity tokens, $=1$ otherwise), $N$ represents the batch size, $T_i$ is the sequence length of the $i$-th sample, and $w_{i,t}$ indicates the $t$-th token in the $i$-th sample.
The total training objective that combines entity-focused and fluency-preserving terms is
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{entity}} + \lambda_2 \mathcal{L}_{\text{BLEU/ROUGE}},
\end{equation}
where $\lambda_1$ is the weight for entity preservation, $\lambda_2$ is the fluency control coefficient maintaining grammatical quality and $\mathcal{L}_{\text{BLEU/ROUGE}}$ denotes standard metric-based loss for text quality.

The RL phase in the multi-scenario LoRA stage is also highly customized. For example, in the captioning task, in addition to ensuring the accuracy and conciseness of the generated captions, it is also necessary to achieve a higher density of entity words (\textit{i.e.}, including useful information such as color, quantity, etc.). This enables the model to output more informative content within the same token length, thereby facilitating improvements in downstream tasks such as album search. We define the \textbf{Entity Density Reward} as
\begin{equation}
R_{\text{entity}} = \frac{\text{Number of entity words in caption}}{\text{Total number of words in caption}},
\end{equation}
the \textbf{Key Information Reward}, 
\begin{equation}
R_{\text{info}} = \beta_1 \cdot \mathbb{I}(\text{caption contains color}) + \beta_2 \cdot \mathbb{I}(\text{caption contains number}),
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function (1 if the condition is satisfied and 0 otherwise), and $\beta_1, \beta_2$ are weighting coefficients. So finally, the \textbf{Total Reward} is
\begin{equation}
R_{\text{total}} = \lambda_1 R_{\text{entity}} + \lambda_2 R_{\text{info}} + \lambda_3 R_{\text{BLEU/ROUGE}},
\end{equation}
where $\lambda_1, \lambda_2, \lambda_3$ are weighting coefficients and $\lambda_3$ ensures the fluency and relevance of the caption. Various detailed examples of reward signals are presented in Table~\ref{tab:reward_signal_for_multi_scenarios}. This mechanism ensures consistent and high-quality outputs in diverse scenarios.

The multi-scenario LoRA training phase focuses on \textbf{deep customization} and \textbf{strong adaptation}, utilizing scenario-specific data along with customized loss and reward functions to significantly enhance model precision and practical utility in targeted application scenarios.

\subsection{Quantization and Deployment}
We have established an end-to-end quantization optimization pipeline, comprising a QAT framework for base models and a scenario-specific Quantization-Aware LoRA Fine-Tuning (QALFT) framework. This pipeline leverages cloud-based computational resources and engineering investments, to maximally preserve AndesVL performance on edge devices, while simultaneously enhance on-device inference efficiency through fine-grained mixed-precision quantization.

\subsubsection{Quantization-Aware Training for AndesVL}
\label{sec:QAT_AndesVL}
Although post-training quantization (PTQ) techniques have advanced rapidly, directly deploying models to mobile devices via PTQ still incurs significant performance degradation. Moreover, the inherent unpredictability of PTQ-induced accuracy loss imposes an additional burden on algorithm validation and testing. 

To address these challenges, we have developed a robust and flexible Quantization-Aware Training (QAT) framework. It supports multiple quantization configurations: weights can be quantized to 2, 3, 4, or 8 bits, and activations to 8 or 16 bits. The framework also enables fine-grained mixed-precision combinations and includes automated precision assignment strategies to maintain model accuracy while maximizing inference efficiency. Furthermore, through close collaboration with silicon vendors, we have established a deterministic mapping mechanism that directly translates static-QAT models into hardware-compatible, edge-deployable quantized representations. This approach aims to fundamentally eliminate the performance uncertainty on edge devices that arises from PTQ.

\subsubsection{QALFT}
QAT effectively satisfies the accuracy requirements for deploying a single base model on edge devices. However, in multi-LoRA scenarios, the activation quantization encodings of the base model must jointly account for the activation ranges introduced by all LoRA adapters. Consequently, any update to a LoRA adapter necessitates re-quantizing both the base model and all associated LoRAs to maintain optimal performance across diverse use cases—an impractical requirement for edge deployment.

To overcome this limitation, we co-developed the Quantization-Aware LoRA Fine-Tuning (QALFT) framework in collaboration with MediaTek. QALFT begins by applying PTQ to a QAT-pretrained base model and permanently freezing its quantization encodings. Subsequent LoRA weights are then trained on top of this fixed, quantized backbone—following a paradigm analogous to QLoRA~\cite{dettmers2023qloraefficientfinetuningquantized}. This design enables independent updates of scenario-specific LoRA modules without re-quantizing the base model, thereby eliminating quantization-induced performance degradation during deployment and significantly streamlining the iteration cycle for task-specific algorithms.

\iffalse Empirical evaluations demonstrate that, in this framework, the on-device performance of AndesVL degrades by only 3\% relative to its full-precision model, the marginal loss validates the efficacy of QALFT in real-world applications.\fi

As illustrated in Fig.~\ref{fig:QALFT_framework}, QALFT employs a layered architectural design. Its core principle is the complete decoupling of three essential components: the floating-point base model, training data, and the QALFT trainer. This decoupling ensures that the training logic remains agnostic to and isolated from vendor-specific hardware infrastructure, thereby facilitating seamless and efficient deployment on MediaTek platforms.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figure/QALFT_framework_0926.png}
    \caption{QALFT framework.}
    \label{fig:QALFT_framework}
\end{figure}


\subsection{Mobile-side Acceleration with Cache Eviction}
The key-value cache (KV cache) plays a crucial role in enhancing the inference performance of LLMs. Nevertheless, as the input sequence length expands, the size of the KV cache also grows proportionally—this not only imposes significant pressure on memory resources but also undermines time efficiency. This issue is particularly pronounced for edge devices such as mobile devices: on these platforms, both performing inference on long text inputs and storing massive volumes of KV cache data are highly inefficient and impractical.

Surprisingly, the long text prompt itself is extremely sparse, which means that only a small number of tokens contribute most of the value. Therefore, we can perform an eviction operation on the KV cache.

Classic cache eviction solutions include streamingLLM~\cite{xiao2023efficient}, H2O~\cite{zhang2023h2o}, and snapKV~\cite{li2024snapkv}. The essence of these solutions lies in retaining the latest token and the previous key token based on observations, attention weights, etc. We designed a new solution called OKV that outperforms these solutions while maintaining the same compression rate and supports context lengths up to 128K.

\subsection{Mobile-Side Acceleration with Speculative Decoding}
Due to the sequential nature of auto-regressive LLMs, the decoding phase is expensive and slow. Speculative decoding has been proven to be an effective solution to this problem: EAGLE-2~\cite{li2024eagle} performs auto-regression at the feature level, reusing top-layer features from the target model in drafting to achieve better results than vanilla speculative decoding; HASS~\cite{zhang2024learning} proposes a training-time-testing method, which further improves accept length by reusing features of the draft model in the training phase to maintain consistency in the inference phase.

Based on the characteristics of mobile-side devices, we made some customizations and adaptations to existing Eagle-like methods reusing top layer features, to fully utilize the storage and computation resources on devices. We experimentally evaluate the AndesVL model with speculative decoding on multiple specific tasks. The results show that our customized method achieves a \textbf{block efficiency (BE)} of up to \textbf{7.9}. Additionally, when combined with hardware compression and LLM sparsification, it obtains a \textbf{6.7x} peak speedup ratio over the baseline.
