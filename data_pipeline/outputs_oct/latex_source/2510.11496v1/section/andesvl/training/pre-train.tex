\subsection{Training Pipeline}

In this paper, inspired by the recently released Qwen3-4B-Instruct and Thinking models~\cite{yang2025qwen3}, we propose to develop AndesVL in two distinct models: an instruction model (supporting only non-thinking mode) and a thinking model (supporting both non-thinking and thinking modes). Consequently, the training datasets and methodologies are distinct from one another, as will be thoroughly explained in the subsequent subsections.

\begin{table}[tbh]
\centering
\resizebox{1\textwidth}{!}{
\begin{tabular}{l|ccc}
\toprule
\textbf{Stages} & \textbf{Visual-Language alignment} & \textbf{Joint V-L pre-training} & \textbf{Multi-task pre-training} \\
\midrule
\multirow{2}{*}{Main data type} & Caption & Interleaved image-text & All multi-task data \\
                        & + OCR + VQA & + Pure text + VQA + Long CoT* & + Long CoT* \\
\hline
Trainable parameters & ViT + MLP & Full model & Full model \\
ViT sequence length & 4,096 / 16,384 & 4,096 & 16,384 \\
LLM sequence length & 2,048 / 8,192  & 8,192 & 32,768 \\
Trained tokens & 100B / 100B & 160B & 160B \\
\bottomrule
\end{tabular}}
\caption{Pre-training stages of AndesVL. * indicates data exclusively used for the Thinking models.}
\label{tab:training_stages}
\end{table}

\subsubsection{Pre-train}
As illustrated in Table~\ref{tab:training_stages}, the pre-training phase of AndesVL consists of three stages: vision-language alignment, joint vision-language pre-training, and multi-task pre-training. To improve training efficiency, we pack not only the ViT input but also the LLM input tokens. 
Our pre-training commences with the Qwen3-Instruct/Thinking versions of the language model. Throughout all pre-training stages, we incorporate a proportion of instruction-following data. This allows us to maintain the model's instruction-following capabilities and monitor its performance progression directly via instruction-based evaluation. 

\paragraph{Vision-Language Alignment}
Our primary visual encoder leverages AIMv2-Large~\cite{fini2025multimodal}, a compact 300M parameter model that offers superior power efficiency compared to larger alternatives such as Qwen2VL-ViT-675M~\cite{wang2024qwen2vl}, making it particularly well-suited for mobile deployment. To enhance the encoder's versatility across varying input resolutions, we integrate 2D Rotary Position Embeddings (2D-RoPE)~\cite{kexuefm-8397}, whose strong extrapolation capabilities enable our vision encoder to effectively process high-resolution inputs even when trained on lower resolutions. To maintain model performance, we preserve the original position embeddings with a length of 1024 and adapt them to different resolutions using bicubic interpolation. 
\par
We employ a two-stage training procedure for the visual encoder within our MLLM framework, keeping the LLM frozen while utilizing diverse training data from captions, OCR, and VQA tasks. The first stage processes 100B tokens with a ViT sequence length of 4,096, applying higher learning rates specifically to the randomly initialized MLP layers, while the second stage continues with an additional 100B tokens with a ViT sequence length of 16,384. For our 1B and 2B model variants, we streamline the training process by directly leveraging the vision encoder from our 4B model and performing alignment by training the MLP layer only. For our most compact 0.6B model variant, we adopt the SigLIP2-Base-Patch16-512~\cite{tschannen2025siglip} model, which follows a similar adaptation strategy that combines bicubic interpolation for position embeddings with 2D-RoPE and two-stage training.

\paragraph{Joint Vision-Language Pre-training}
The second stage involves joint vision-language pre-training. After the visual encoder's output aligns well with the LLM's representations, we unfreeze the LLM parameters and conduct full-parameter pretraining using a relatively low learning rate.
In this stage, we utilize a large volume of unsupervised interleaved image-text data, enabling the model to acquire extensive visual knowledge. During pre-training on this data, we compute loss only on text tokens, excluding image tokens from the calculation.
\par
In unidirectional autoregressive transformers, inappropriate image positioning may prevent the model from learning multimodal knowledge effectively. For instance, images placed at the end of a sequence cannot contribute to learning even when encoded. To mitigate this issue, we employed a strategy where, with 50\% probability, we maintained the original image positions. With the remaining 50\% probability, we relocated all images in the data to precede all text content, replacing the images with their corresponding indices. Fig.~\ref{fig:image_reposition} illustrates this transformation.

\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=gray!5, colframe=gray!50, boxrule=0.5pt, arc=2pt, left=5pt, right=5pt, top=5pt, bottom=5pt]
\textbf{Original interleaved document:}
\begin{lstlisting}[basicstyle=\small\ttfamily, breaklines=true, frame=none]
The sunset over the Pacific Ocean was breathtaking. 
<img>pacific_sunset.jpg</img> The vibrant colors painted 
the sky in shades of orange and pink. Later that evening, 
we hiked to the mountain viewpoint. <img>mountain_vista.jpg</img>
\end{lstlisting}

\vspace{0.3em}
\textbf{Transformed format:}
\begin{lstlisting}[basicstyle=\small\ttfamily, breaklines=true, frame=none]
<|image_0|> <img>pacific_sunset.jpg</img>
<|image_1|> <img>mountain_vista.jpg</img>
The sunset over the Pacific Ocean was breathtaking. 
<|image_0|> The vibrant colors painted the sky in shades 
of orange and pink. Later that evening, we hiked to the 
mountain viewpoint. <|image_1|>
\end{lstlisting}
\end{tcolorbox}
\caption{Image repositioning strategy for joint vision-language pre-training. Images are moved to the beginning of the sequence with 50\% probability to ensure effective multimodal learning.}
\label{fig:image_reposition}
\end{figure}

Since interleaved image-text data can be viewed as a multimodal extension of unlabeled text data, we also incorporate unlabeled text data from text pre-training. To maintain instruction-following capabilities, we include text instruction data in this stage as well. Furthermore, a certain proportion of multi-task pre-training data is added to enhance the model's overall multimodal abilities. For the Thinking version of the model, we additionally incorporate long CoT data, which will be detailed in Sec.~\ref{sec:pre-train_data}.

\paragraph{Multi-task Pre-training}
The final stage is multi-task pre-training. In this stage, our approach transitions from self-supervised learning with unsupervised data to supervised learning using annotated data, focusing solely on calculating the text token loss for the answer portions. Data types mainly consist of general VQA, captions, and OCR, alongside task-specific data like grounding/UI. For the Thinking model variant, we continue to incorporate long CoT data as in the previous stage, while increasing the proportion of multimodal types to enhance its step-by-step reasoning capabilities with visual inputs. Although we use 2D RoPE to allow model inference at high resolutions, we increased the ViT patch input from 4,096 to 16,384 to facilitate learning from data that require high resolution. To enhance the LLM's capabilities in long contexts, particularly its reasoning ability in Thinking mode, we expanded the LLM's sequence length from 8,192 to 32,768.

Consequently, by completing the three pre-training stages mentioned above, we developed the base versions for our Instruct and Thinking models, referred to as \textit{AndesVL-Instruct-Base} and \textit{AndesVL-Thinking-Base}, respectively, which are subsequently utilized for post-training.