\subsection{Training Data}
\subsubsection{Pre-train Data}
\label{sec:pre-train_data}
In this section, we present in detail the data we utilized during the pre-training stage, including several open-source datasets and our in-house data, as illustrated in Table~\ref{tab:pretain_data}.

\begin{table}[tb]
    \belowrulesep=0pt
    \aboverulesep=0pt
        \centering
        \begin{tabular}{|c|c|}
            \toprule
            Task Type & Dataset Name \\
            \midrule
            Caption & \thead{Emu2~\cite{sun2024generative}, ShareGPT-4V~\cite{chen2023sharegpt4v}, Laion-ZH~\cite{schuhmann2022laion5b}, \\ Wukong~\cite{gu2022wukong}, Taisu~\cite{liu2022taisu}} \\
            \hline
            OCR & \thead{DocMatrix~\cite{2024docmatrix}, DocStruct~\cite{hu2024mplug,hu2024mplug2}, Leopard-Instruct~\cite{jia2024leopard}, \\Pixmo-doc~\cite{deitke2024molmo}, Anyword-3M~\cite{tuo2023anytext}, PIN-14M~\cite{wang2024pin}, \\In-house collected and synthesized (by Synthdog~\cite{wang2024pin}) OCR data} \\
            \hline
            Visual Grounding & \thead{Visual Genome~\cite{krishna2017vg}, RefCOCOs~\cite{yu2016refcoco, lin2014microsoft}, LVIS~\cite{plummer2015flickr30k},\\Flickr30k-Entities~\cite{plummer2015flickr30k}, Groma~\cite{ma2024groma}}  \\
            \hline
            GUI and Agent & \thead{AITW~\cite{rawles2024androidinthewild}, AITZ~\cite{zhang2024android}, AMEX~\cite{chai2024amex}, Android Control~\cite{lieffects}, Widget Caption~\cite{li2020widget}, \\Rico~\cite{deka2017rico}, SeeClick~\cite{cheng2024seeclick}, UIbert~\cite{bai2021uibert},  Screen2Words~\cite{wang2021screen2words}, MultiUI~\cite{liu2024harnessingwebpageuistextrich}, \\Aria-UI\cite{yang2025ariauivisualgroundinggui}, OS-Atlas~\cite{wu2024atlas}, Mind2Web~\cite{deng2024mind2web}, GUI-Odyssey~\cite{lu2024gui}, OmniAct~\cite{kapoor2025omniact},\\
            In-house AndesUI training set}  \\
            \hline
            VQA & \thead{Infinity-MM~\cite{gu2024aquilavl}, MAmmoTH~\cite{guo2024mammoth}, LLaVA-OneVision~\cite{li2024llavaov}, \\The~ Cauldron~\cite{Cauldron}, 
            VisualWebInstruct~\cite{visualwebinstruct},
            PangeaInstruct~\cite{yue2024pangeafullyopenmultilingual}}\\
            \hline
            Long CoT & \thead{OpenMathReasoning~\cite{moshkov2025aimo2}, OpenCodeReasoning~\cite{ahmad2025opencodereasoning}, OpenThoughts~\cite{guha2025openthoughtsdatarecipesreasoning}, Nemotron~\cite{NemotronPostTrainingDatasetV1}, \\In-house multiModal long CoT data}\\
            \hline
            Interleaved Image-Text & \thead{MMC4~\cite{zhu2023multimodal}, MINT~\cite{awadalla2024mint}, Multimodal Textbook~\cite{zhang20252}, \\Wanjuan~\cite{he2023wanjuan}, OmniCorpus~\cite{li2024omnicorpus},
            \\In-house crawled data from Chinese websites and Apps} \\
            \hline
            Pure Text & \thead{Fineweb-Edu-Chinese~\cite{yu2025opencsgchinesecorpusseries}, Fineweb-Edu~\cite{lozhkov2024fineweb-edu}, 
            FineMath~\cite{allal2025smollm2smolgoesbig},
            OpenCoder~\cite{Huang2024OpenCoderTO}, \\Infinity-Instruct~\cite{InfinityInstruct2024}}  \\
            \bottomrule
        \end{tabular}
        \caption{The detailed lists of pre-training datasets.}
        \label{tab:pretain_data}
\end{table}

\paragraph{Image Caption}
Our image caption data comprises both Chinese and English languages. The Chinese image caption data primarily originates from Laion~\cite{schuhmann2022laion5b}, Wukong~\cite{gu2022wukong}, and Tasisu~\cite{liu2022taisu} datasets. To refine the quality of these descriptions, we utilized Qwen2.5-VL-72B~\cite{bai2025qwen2_5} to generate re-captioned versions. During training, we randomly employed the original captions with a 50\% probability and the re-captioned versions also with a 50\% probability, culminating in a dataset of around 116 million samples. The English image caption data are primarily derived from the Infinity-MM~\cite{InfinityInstruct2024} stage 1 subset, using Emu2~\cite{sun2024generative} for caption generation and consisting of approximately 10 million samples.

\paragraph{OCR}
OCR data plays a critical role in bridging visual and textual modalities within vision-language models (VLMs). Our OCR dataset is primarily derived from three sources: open-source data, synthetic data, and in-house collected data. For real-world textual images, we curated and refined widely used open-source datasets through our dedicated data engine. To further enhance data diversity, we also extracted text-rich images from the LAION-2B~\cite{schuhmann2022laion5b} dataset using PaddleOCR~\cite{li2022paddleocr}.

Synthetic data serves as another essential component in strengthening the model's OCR capabilities. Specifically, to improve recognition accuracy for Chinese characters, we generated a large-scale Chinese OCR dataset using SynthDog~\cite{kim2022synthdog}. Additionally, we produced substantial amounts of non-semantic English OCR data to help mitigate the modelâ€™s tendency toward hallucination. To further improve robustness, we applied extensive data augmentation techniques, including geometric transformations, noise injection, and style variations, ensuring the model generalizes effectively across diverse and challenging real-world scenarios.

\paragraph{Visual Grounding}
We followed the bounding box structure utilized in Qwen2-VL~\cite{wang2024qwen2vl} and prepared data for both single and multiple grounding scenarios. The grounding datasets were chosen from publicly available sources like Visual Genome~\cite{krishna2017vg}, RefCOCOs~\cite{yu2016refcoco, lin2014microsoft}, Flickr30k-Entities~\cite{plummer2015flickr30k}, and Groma~\cite{ma2024groma}. These datasets were screened and categorized into four classifications: object referring, region captioning, referenced entity recognition, and grounded image captioning. Inspired by Ferret~\cite{li2024ferret} and Ferret-v2~\cite{ferretv2}, we ensured an equitable distribution of our grounding data across the Region-in-Text-out and Text-in-Region-Out formats.

\paragraph{GUI and Agent}
We divided the GUI data into four categories, which are detailed caption, recognition, action, and element grounding. In the pre-training stage, the data were formatted in single-turn style. For the element grounding data, we kept the structure the same as our visual grounding data. During the data synthesis and reconstruction stages, we kept the balance between different task categories and platform domains.

Besides the publicly available GUI data, we built an in-house GUI dataset using ColorOS UI and application widgets, namely AndesUI. We gathered 90 apps in total, including 65 popular download apps from the OPPO Software Store, spanning a variety of categories frequently used by users, along with 25 ColorOS pre-installed apps. Annotators were directed to capture screenshots of different heterogeneous pages within each app, ensuring that each screenshot contained unique layouts and content. Ultimately, we collected a total of 10k screenshots from third-party apps and 2.5k from ColorOS pre-installed apps.
Then, we aimed to annotate all the widgets within each screenshot. This included defining bounding boxes, identifying widget types, recording any available text on the widgets, and determining their clickability, among other details. On average, each interface produced 18 widgets. The training dataset resulted in a cumulative total of 227k widgets.
Finally, we needed to construct both basic and advanced data. Basic data consists of grounding and referring datasets, whereas advanced data comprises overall descriptive data and natural question-answer pairs. 
As a result, the training set produced 227k referring data entries, 186k grounding data entries, 13k comprehensive description data, and 108k natural question-answer pairs. All the details of the AndesUI dataset are presented in Appendix~\ref{app:andesui_dataset}.

\paragraph{VQA}
Our VQA dataset primarily originated from the open-source community, encompassing general VQA datasets, Infinity-MM~\cite{gu2024aquilavl}, Llava One Vision~\cite{li2024llavaov}, and The Cauldron~\cite{Cauldron}. Additionally, it included reasoning datasets such as MAmmoTH~\cite{guo2024mammoth} and VisualWebInstruct~\cite{visualwebinstruct}, along with multilingual and multicultural datasets like PangeaInstruct~\cite{yue2024pangeafullyopenmultilingual}.

\paragraph{Interleaved Image-Text}
Interleaved image-text data serves as a natural extension of pure text pretraining data into scenarios encompassing image inputs. Unlike instruction data differentiating between single-image and multi-image contexts, interleaved image-text data is inherently multi-modal and simplifies to pure text corpora when there are no image inputs. This is similar to pretraining on purely textual data, which enables models to develop in-context learning abilities. We collected interleaved image-text data from the open-source community, which includes MMC4~\cite{zhu2023multimodal}, MINT~\cite{awadalla2024mint}, Multimodal Textbook~\cite{zhang20252}, Wanjuan~\cite{he2023wanjuan}, and OmniCorpus~\cite{li2024omnicorpus}. Furthermore, to improve the model's understanding of Chinese language and culture, we created an in-house interleaved image-text dataset based on Chinese web content.

\paragraph{Pure-text}
Pure text data plays a crucial role in maintaining the text capabilities of MLLMs. Our openly accessible pure text corpus comprises the Chinese FineWeb-Edu~\cite{yu2025opencsgchinesecorpusseries}, the English FineWeb-Edu~\cite{lozhkov2024fineweb-edu}, the mathematical corpus FineMath~\cite{allal2025smollm2smolgoesbig}, the code corpus derived from OpenCoder~\cite{Huang2024OpenCoderTO} annealing data. Besides, we also constructed a large quantity of in-house text pre-training corpora. Furthermore, we incorporated text instruction data from Infinity-Instruct~\cite{InfinityInstruct2024}.


\paragraph{Long COT data}
The long CoT data construction pipeline is illustrated in Fig.~\ref{fig:CoT_data_construction_pipeline}. Our long CoT dataset was constructed from two distinct pipelines: one leveraged human annotations in combination with a DeepSeek-based Chain-of-Thought (CoT) data generation pipeline, while the other relied on distilling knowledge from existing CoT models.
\begin{figure}[tbh]
    \centering
    \includegraphics[width=1\linewidth]{CoT_data_pipeline.png}
    \caption{CoT data construction pipeline}
    \label{fig:CoT_data_construction_pipeline}
\end{figure}
In the first pipeline, we began by collecting a diverse set of STEM (science, technology, engineering, and math) samples and common real-life images to serve as our visual inputs. Subsequently, human annotators developed pertinent questions based on these images and derived corresponding correct answers. Concurrently, MLLMs, such as GPT-4o~\cite{chatgpt4o}, were utilized to produce initial image descriptions. These descriptions were then manually refined, and query-related information was extracted to maintain alignment and relevance. 

In the second pipeline, which focuses on distillation from existing CoT models, we employed a hybrid strategy that merges MLLMs with DeepSeek-R1~\cite{guo2025deepseek}. Specifically, MLLMs are used to generate descriptive captions for the input images. These captions, along with the associated queries, are input into DeepSeek, which generates detailed reasoning chains as output responses, thereby producing high-quality CoT examples.