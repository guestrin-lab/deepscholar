\section{AndesVL}
\label{sec:andesvl}
\subsection{Model Architecture}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/Overall_arch.png}
    \caption{The overall architecture of AndesVL mainly includes a visual encoder, an MLP projector, and an LLM.}
    \label{fig:overall_architecture}
\end{figure}

\begin{table*}[htbp]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Model Name} & \textbf{\#Param (B)} & \textbf{Vision Encoder} & \textbf{Language Model} \\ 
\hline
AndesVL-0.6B & 0.695 & SigLIP2-Base~\cite{tschannen2025siglip} & Qwen3-0.6B~\cite{yang2025qwen3} \\
AndesVL-1B   & 0.927 & AIMv2-Large~\cite{fini2025multimodal} & Qwen3-0.6B~\cite{yang2025qwen3} \\
AndesVL-2B   & 2.055 & AIMv2-Large~\cite{fini2025multimodal} & Qwen3-1.7B~\cite{yang2025qwen3} \\
AndesVL-4B   & 4.360 & AIMv2-Large~\cite{fini2025multimodal} & Qwen3-4B~\cite{yang2025qwen3} \\
\hline
\end{tabular}
\caption{AndesVL model architectures of different sizes.}
\label{tab:arch}
\end{table*}

%AndesVL comprises models ranging from 0.5B to 4B parameters. 
AndesVL comprises models with parameters ranging from 0.6B to 4B parameters, with detailed architectures provided in Table~\ref{tab:arch}. Following the paradigm of typical MLLMs~\cite{liu2023llava, chen2024internevo, wang2024qwen2vl}, it consists of three fundamental components: a visual encoder, an MLP projector, and an LLM, as illustrated in Fig.~\ref{fig:overall_architecture}.

As a general-purpose MLLM, AndesVL is designed to handle image inputs with arbitrary aspect ratios and resolutions. To achieve this, we avoid the image cropping methods employed in other works \cite{liu2024llavanext, chen2024internevo, xu2024llava_uhd} and instead implement a Native Resolution ViT (NaViT) \cite{dehghani2023patch}-based strategy, allowing the visual encoder to process input of any resolution directly. This method is particularly beneficial for efficiently processing low-resolution images and ensures consistency between model inputs and the original data. The MLP projector includes two MLP layers used to align the ViT output with the LLM's embedding layer. To decrease the sequence length of the ViT output going into the LLM, a straightforward yet adaptable pixel shuffle operation is used to reduce the sequence length to a quarter of its original size. This operation combines and concatenates the data from adjacent 4Ã—4 patches before passing them to the MLP. For the language model, AndesVL employs Qwen3~\cite{yang2025qwen3}, utilizing the 0.6B, 1.7B, and 4B models from the Qwen3 series. To save memory for embedding parameters, we preserve the tied word embeddings configuration across all LLM variations.
