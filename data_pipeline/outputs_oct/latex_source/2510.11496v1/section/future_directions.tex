\section{Future Directions}
\label{sec:future_directions}

In the future, several promising directions can be explored to further enhance the capabilities of mobile-side MLLMs. First, designing more optimal visual encoder solutions holds great potential. By leveraging advanced network architectures and novel feature extraction strategies, we aim to improve the efficiency and accuracy of visual information processing, enabling the model to better understand complex visual content on resource-constrained mobile-side devices.

Second, developing superior post-training schemes is crucial. Refining the post-training process can optimize the model performance in handling various multimodal tasks, reduce hallucinations, and enhance the consistency and reliability of generated outputs. This may involve exploring new types of training data, adjusting training objectives, and optimizing training algorithms to make the model more adaptable to real-world scenarios.

Third, implementing effective distillation schemes between large and small models can significantly improve the performance-to-resource ratio of mobile-side models. By transferring knowledge from large, high-performance cloud-based models to smaller mobile-side counterparts, we can boost the capabilities of the latter while maintaining low computational costs and memory requirements.

Finally, the development of a unified mobile-side model integrating text, image, and speech modalities (a three-mode integrated model) represents an exciting frontier. Such a model would enable seamless interaction with users across multiple modalities, providing more natural and intelligent user experiences. This will require in-depth research on multimodal fusion techniques, cross-modal representation learning, and efficient inference algorithms to ensure the model's effectiveness and efficiency on mobile-side devices. These research directions will not only drive the progress of mobile-side MLLMs but also expand their application scope in various fields.
