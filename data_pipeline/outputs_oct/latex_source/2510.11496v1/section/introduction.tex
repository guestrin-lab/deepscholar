\section{Introduction}
% 先说large language model时代的影响力，gpt，qwen，deepseek等
% 然后是multimodal large language model更有用，一般是build upon LLM，通过alignment，继续预训练，后训练的手段完成
% 端侧的MLLM还比较少，基本集中在4B及以下，比如vivo的blue-lm-v
% 我们提出的andesvl，主要基于qwen3的llm，设计了0.5B ~4B的套件，选择了不同的image encoder。关注几类通用能力-knowledge，数学，多图，text-rich等等
% 另外我们也重点考虑了手机端的MLLM需要关注的功能，比如grounding&referring，UI理解，对此，我们也提出了for mobile phone的相关benchmark。我们的模型在不同的benchmark上取得了相近尺寸模型的SOTA，包括opencompass， academic，自建的benchmark等。
% 除了4B以内的浮点数模型，要想把大模型运行在端侧，还需要做QAT来把模型做压缩，以及使用1+N lora的架构来使得模型适应不同的任务。端侧的模型也在各个benchmark上取得了很好的效果，相比云测浮点数模型下降仅XX%。端侧运行的内存XX, 时延XX。功耗XX。
%\begin{figure}[tb]
%    \centering
%    \includegraphics[width=1.0\linewidth]{figure/model_performance_grouped_log.png}
%    \caption{Overall performance comparisons over 25 benchmarks of different MLLMs with 0.6B–4B parameters.}
%    \label{fig:overall_performance}
%\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{figure/andesvl_radar_with_minmax.png}
    \caption{Overall performance comparisons over 6 domains (text-rich, reasoning and math, general VQA, multi-image, multilingual and hallucination) of different SOTA MLLMs with 4B parameters.}
    \label{fig:4b_performance}
\end{figure}

In recent years, the advent of large language models (LLMs) represented by ChatGPT~\cite{chatgpt}, the Qwen series~\cite{bai2023qwen, yang2024qwen2, yang2024qwen2.5, yang2025qwen3}, and the DeepSeek series~\cite{bi2024deepseekllm, liu2024deepseek, guo2025deepseek} has ushered in a new era of artificial intelligence. These LLMs have demonstrated remarkable capabilities in natural language processing tasks, such as text generation, question answering, and language translation. Building upon the success of LLMs, multimodal large language models (MLLMs) have emerged, expanding the functionality of large models from pure text to multiple modalities. MLLMs incorporate modalities such as image, video, and even audio, enabling more diverse and comprehensive interactions.

The typical training paradigm of MLLMs involves leveraging a pre-trained LLM. By aligning the LLMs with visual encoders and engaging in continual pre-training and fine-tuning, an MLLM that can process multimodal inputs and generate text outputs is developed. For effective training, a substantial amount of multimodal data is necessary, in addition to extensive text datasets. This data encompasses image-text pairs, optical character recognition (OCR) data, and visual question-answering (VQA) data. These datasets provide the model with a wide range of multimodal capabilities, such as image captioning, OCR, chart question answering, visual semantic recognition, and visual reasoning.

On the cloud side, there are numerous outstanding MLLMs. Models such as the QwenVL series~\cite{bai2023qwenvl, wang2024qwen2vl, bai2025qwen2_5}, the InternVL series~\cite{chen2023internvl, chen2024internvl_1_5, chen2024expanding, zhu2025internvl3, wang2025internvl3}, GPT-4o~\cite{chatgpt4o}, Gemini~\cite{team2023gemini, reid2024gemini1_5, gemini2_0, gemini2_0pro, geminipro2.5} and Claude Sonnet~\cite{claude3series2024} have demonstrated SOTA competence in multimodal tasks. Despite their groundbreaking performance, these models are generally oriented towards reaching maximum performance, involving parameter sizes running into hundreds of billions. Such large-scale parameter demands significantly exceed the processing capabilities of mobile devices such as smartphones and tablets, particularly in terms of memory capacity, running speed, and computing power of chips. Consequently, MLLMs that typically run on mobile platforms are limited to approximately 4 billion parameters, as illustrated by Qwen2.5-VL-3B~\cite{bai2025qwen2_5} and InternVL3.5-4B~\cite{wang2025internvl3}. To maintain optimal functionality on mobile hardware, additional measures, such as quantization-aware training (QAT) and deployment optimization on the mobile side, are essential.

Currently, only a limited number of mobile-device manufacturers and internet companies have started exploring mobile-side MLLMs. For example, vivo has introduced BlueLM-V-3B~\cite{lu2024bluelm} and BlueLM-2.5-3B~\cite{xiong2025bluelm}, Meituan has launched the MobileVLM series~\cite{chu2023mobilevlm, chu2024mobilevlm}, Xiaomi has concentrated on mobile agents with the development of MobileVLM~\cite{wu2024mobilevlm}, and Apple has released the Ferret-UI series~\cite{you2024ferret, li2024ferret} aimed at UI comprehension. Despite these initiatives, a thorough study explicating the training process, deployment strategies, and performance assessments on both general and mobile-specific benchmarks of mobile-side MLLMs is still absent.

In this paper, we introduce the AndesVL suite. By integrating Qwen3~\cite{yang2025qwen3} LLMs and various visual encoders, we have successfully developed mobile-side MLLMs with parameter sizes ranging from 0.6B to 4B. Our models focus on several key general-purpose capabilities, including knowledge acquisition, mathematical reasoning, handling text-rich content, dealing with hallucination issues, processing multi-image and multilingual inputs, and general VQA. We thoroughly introduce the model architectures, training pipeline, and data preparation strategies. Additionally, we have given special consideration to functions crucial for mobile-side MLLMs, such as user interface (UI) understanding. To evaluate the performance of our models, we have developed mobile-specific benchmarks. Specifically, as inspired by Qwen3-4B-Instruct and Qwen3-4B-Thinking~\cite{yang2025qwen3}, we propose to train the instruct and thinking models of AndesVL separately to achieve the best instruct-following and reasoning abilities, respectively.  Our floating-point models have achieved first-tier results among models of similar sizes across various benchmarks, as shown in Fig. \ref{fig:4b_performance}, including 32 open-source benchmarks related to the domains mentioned above.

For practical application on mobile devices, we have also designed a 1 + N Low-Rank Adaptation (LoRA)~\cite{hu2022lora} architecture to make the model adaptable to different tasks. Based on the AndesVL backbone model, downstream tasks can be clustered, and similar tasks can be fine-tuned using a single LoRA module to achieve optimal performance in specific domains. 
In addition to floating-point models within the 4B parameter range, to enable large models to run on the mobile side, the QAT and Quantization-Aware Lora Fine Tuning (QALFT) frameworks are necessary for model compression. With this pipeline, our mobile-side models have also demonstrated excellent results in various realistic applications. Additionally, we meticulously design a comprehensive mobile-side acceleration suite, with cache eviction, speculative decoding and sparsification, which achieve a block efficiency (BE) of up to 7.9, with about a 6.7x end-to-end decoding speedup over the baseline (with auto-regressive decoding and without compression optimization). Furthermore, we achieve a memory reduction of up to 30.9\% and a weight compression of 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. This work represents a significant step forward in the development and deployment of mobile-side MLLMs. 

The structure of this article is as follows: Sec.~\ref{sec:related_works} introduces the mobile-side MLLM and related work on mobile-side deployment. Sec.~\ref{sec:andesvl} focuses on the model architecture, training data, and training scheme of AndesVL. Sec.~\ref{sec:mobile-side_depoloy_andesvl} introduces the mobile-side 1+N LoRA training architecture of AndesVL and the technical scheme for mobile-side deployment. Sec.~\ref{sec:experiments} details the performance of AndesVL on public benchmarks and self-built mobile-side benchmarks, as well as its comparison with SOTA models. Sec.~\ref{sec:experiments_on-device} presents the benchmark results and mobile-side performance of AndesVL after being deployed on mobile devices. Sec.~\ref{sec:future_directions} looks ahead to future directions. Sec.~\ref{sec:conclusion} summarizes the entire article.

The main contributions of this article can be summarized as follows:
\begin{itemize}
    \item First, addressing the speed and performance trade-off for mobile implementations of MLLM, we introduce the AndesVL suite, which is a collection of MLLMs designed for efficient deployment on edge devices, with parameter scales ranging from 0.6B to 4B, demonstrating competitive performance with SOTA models with comparable parameters.
    %\item Second, leveraging our self-built mobile UI dataset (AndesUI), we train our AndesVL to understand complex mobile user interfaces. Concurrently, we introduce a corresponding benchmark named AndesUI-Bench to evaluate UI-related capabilities of MLLMs.
    \item Second, we offer separate models for Instruct and Thinking versions, making each ideal for tackling the challenges associated with high-efficiency tasks in understanding and generation, as well as applications in complex mathematical reasoning and planning.
    \item Third, we design a 1+N Lora training pipeline for mobile deployment, which enables efficient task clustering and adaptation. We further propose the QALFT framework to ensure flexible application of the 1+N Lora architecture on mobile devices. 
    \item Finally, based on our mobile-side acceleration and compression strategies, \textit{e.g.}, customized cache eviction, sparsification, and speculative decoding, AndesVL-4B can achieve a 6.7x peak decoding speedup ratio, a memory reduction of up to 30.9\%, and 1.8 bits-per-weight on MediaTek Dimensity 9500 chips.
\end{itemize}
