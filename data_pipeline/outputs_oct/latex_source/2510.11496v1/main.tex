\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% ready for submission
% \usepackage{neurips_2024}

% to compile a preprint version, \eg, for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2024}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% \usepackage{CJKutf8}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{makecell} 
\usepackage{tabularx}
\usepackage{diagbox}
\usepackage{enumitem} 

% include other packages here
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow} 
\usepackage{colortbl}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{float}
\usepackage{marvosym}
\usepackage{CJKutf8}
\usepackage{xcolor}
% \usepackage[table]{xcolor}
% \definecolor{linkcolor}{HTML}{0071BC}
\definecolor{citecolor}{HTML}{0071BC}
\usepackage[
    pagebackref,           % 生成引用返回页码的超链接
    breaklinks=true,       % 支持链接断行
    colorlinks=true,       % 启用彩色链接
    linkcolor=red,         % 超链接（如 \ref）颜色
    citecolor=citecolor,   % 文献引用的颜色
    urlcolor=black,        % URL 链接颜色
    bookmarks=false        % 不生成书签（根据需要）
]{hyperref}

% define new commands here
\newcommand{\ie}{{\emph{i.e.}}}
\newcommand{\eg}{{\emph{e.g.}}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\etal}{\emph{et al.}}
\newcommand{\vs}{\emph{vs.}}

\newcommand{\yes}{\ding{51}}
\newcommand{\no}{\ding{55}}
\newcommand{\graybox}[1]{\colorbox{gray!15}{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}

% define modelname here
\newcommand{\wwh}[1]{\textcolor{red}{[wwh: #1]}}
\newcommand{\cz}[1]{\textcolor{red}{[cz: #1]}}
\newcommand{\wwy}[1]{\textcolor{orange}{[wwy: #1]}}
\newcommand{\ysl}[1]{\textcolor{orange}{[ysl: #1]}}

% table
\usepackage{array}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{
AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model
}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{
%\textbf{Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi}\\
%\textbf{Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng}\\
%\textbf{Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu}\\
%\textbf{Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang}\\
%\textbf{Zhiguang Zhu, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen\thanks{Corresponding to chenchen4@oppo.com and luhaonan@oppo.com}, and Haonan Lu\footnotemark[1]}\\
\textbf{AndesVL Team, OPPO AI Center}
\\
\github ~ \url{\ghlink}
\\
\huggingface ~ \url{\hflink}
}





\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

\definecolor{baselinecolor}{gray}{.9}
\definecolor{reduce-color}{RGB}{67,178,68}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}

\begin{document}
\begin{CJK}{UTF8}{gbsn} 

\maketitle

\begin{abstract}
 
In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL.
\iffalse Our experiments reveal that AndesVL maintains performance with only ignorable degradation (3\%) after deployment on mobile devices compared to the original floating-point model.\fi
Moreover, utilizing our cache eviction algorithm---OKV---along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9\% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on \url{https://huggingface.co/OPPOer}.

\end{abstract}

\input{section/introduction}

\input{section/related_works/end-side_mllm}

\input{section/related_works/end-side_deployment}

\input{section/andesvl/model_architecture}

\input{section/andesvl/training/pre-train}

\input{section/andesvl/training/post-train}

\input{section/andesvl/training_data/pre-train_data}

\input{section/andesvl/training_data/post-train_data}

\input{section/end-side_depolyment_of_andesvl/end-side_deployment_of_andesvl}

\input{section/experiments/benchmark/general_benchmark}

\input{section/experiments/experiment_results/general_benchmark_results}

\input{section/experiments/benchmark/mathematics_benchmark}

\input{section/experiments/experiment_results/mathematics_benchmark_results}

\input{section/experiments/benchmark/text-rich_benchmark}

\input{section/experiments/experiment_results/text-rich_benchmark_results}

\input{section/experiments/benchmark/multi-image_understanding_benchmark}

\input{section/experiments/experiment_results/multi-image_understanding_benchmark_results}

\input{section/experiments/benchmark/general_vqa_benchmark}

\input{section/experiments/experiment_results/general_vqa_benchmark_results}

\input{section/experiments/benchmark/hallucination_benchmark}

\input{section/experiments/experiment_results/hallucination_benchmark_results}

\input{section/experiments/benchmark/multilingual_benchmark}

\input{section/experiments/experiment_results/multilingual_benchmark_results}


%\input{section/experiments/benchmark/visual_grounding_benchmark}

%\input{section/experiments/experiment_results/visual_grounding_benchmark_results}

\input{section/experiments/benchmark/GUI_benchmark}

\input{section/experiments/experiment_results/GUI_benchmark_results}

%\input{section/experiments/experiment_results/UI_understanding_experiment}

\input{section/experiments/ablation_study/ablation_study}

\input{section/experiments/end-side_experiment_results/end-side_experiment_results}

\input{section/future_directions}

\input{section/conclusion}

{
    \small
    \bibliographystyle{plain}
    \bibliography{main}
}
\newpage
\appendix
\input{appendix/contributor}
\input{table/UI_APPs}
\input{table/AndesUI_basic_numbers}
\input{appendix/andesUI_dataset}
\input{appendix/qualitative_examples}



\end{CJK}
\end{document}
