@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {International Journal of Computer Vision})
@String(CVPR  = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition})
@String(ICDAR  = {International Conference on Document Analysis and Recognition})
@String(ICCV  = {Proceedings of the IEEE/CVF International Conference on Computer Vision})
@String(ECCV  = {European Conference on Computer Vision})
@String(NEURIPS  = {Advances in Neural Information Processing Systems})
@String(ICPR  = {ICPR})
@String(WACV  =	{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(ACL  = {Proceedings of the Annual Meeting of the Association for Computational Linguistics})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICML  = {International Conference on Machine Learning})
@String(ACCV  = {ACCV})
@String(ICLR  = {The International Conference on Learning Representations})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {Proceedings of the AAAI Conference on Artificial Intelligence})
@String(CVPRW= {Conference on Computer Vision and Pattern Recognition Workshop})
@String(CSVT = {IEEE TCSVT})

@article{deitke2024molmo,
  title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
  journal={arXiv preprint arXiv:2409.17146},
  year={2024}
}
@article{chia2024puzzlevqa,
  title={PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns},
  author={Yew Ken Chia and Vernon Toh Yan Han and Deepanway Ghosal and Lidong Bing and Soujanya Poria},
  journal={arXiv preprint arXiv:2403.13315},
  year={2024}
}

@article{zhang2024mm1_5,
  title={Mm1.5: Methods, analysis \& insights from multimodal llm fine-tuning},
  author={Zhang, Haotian and Gao, Mingfei and Gan, Zhe and Dufter, Philipp and Wenzel, Nina and Huang, Forrest and Shah, Dhruti and Du, Xianzhi and Zhang, Bowen and Li, Yanghao and others},
  journal={arXiv preprint arXiv:2409.20566},
  year={2024}
}
@article{chen2025advancing,
  title={Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning},
  author={Chen, Shuang and Guo, Yue and Su, Zhaochen and Li, Yafu and Wu, Yulun and Chen, Jiacheng and Chen, Jiayu and Wang, Weijie and Qu, Xiaoye and Cheng, Yu},
  journal={arXiv preprint arXiv:2506.04207},
  year={2025}
}
@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
@article{shen2024measuring,
  title={Measuring vision-language stem skills of neural models},
  author={Shen, Jianhao and Yuan, Ye and Mirzoyan, Srbuhi and Zhang, Ming and Wang, Chenguang},
  journal={arXiv preprint arXiv:2402.17205},
  year={2024}
}
@inproceedings{ning2023symbolic,
  title={A symbolic characters aware model for solving geometry problems},
  author={Ning, Maizhen and Wang, Qiu-Feng and Huang, Kaizhu and Huang, Xiaowei},
  booktitle={Proceedings of the 31st ACM international conference on multimedia},
  pages={7767--7775},
  year={2023}
}
@article{wang2025sota,
  title={Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement},
  author={Wang, Xiyao and Yang, Zhengyuan and Feng, Chao and Lu, Hongjin and Li, Linjie and Lin, Chung-Ching and Lin, Kevin and Huang, Furong and Wang, Lijuan},
  journal={arXiv preprint arXiv:2504.07934},
  year={2025}
}
@article{qiao2025we,
  title={We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning},
  author={Qiao, Runqi and Tan, Qiuna and Yang, Peiqing and Wang, Yanzi and Wang, Xiaowan and Wan, Enhui and Zhou, Sitong and Dong, Guanting and Zeng, Yuchen and Xu, Yida and others},
  journal={arXiv preprint arXiv:2508.10433},
  year={2025}
}

@article{marafioti2025smolvlm,
  title={SmolVLM: Redefining small and efficient multimodal models},
  author={Marafioti, Andr{\'e}s and Zohar, Orr and Farr{\'e}, Miquel and Noyan, Merve and Bakouch, Elie and Cuenca, Pedro and Zakka, Cyril and Allal, Loubna Ben and Lozhkov, Anton and Tazi, Nouamane and others},
  journal={arXiv preprint arXiv:2504.05299},
  year={2025}
}

@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle=ICLR,
  year={2020}
}

@misc{glaive_code_assistant_v3,
  author       = {GlaiveAI},
  title        = {Glaive Code Assistant V3 Dataset},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3}},
}

@misc{MarkrAI_KOpen_HQ_Hermes_2.5_60K,
  author       = {MarkrAI},
  title        = {KOpen-HQ-Hermes-2.5-60K Dataset},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/MarkrAI/KOpen-HQ-Hermes-2.5-60K}},
}



@misc{CarperAI_openai_summarize_tldr,
  author       = {CarperAI},
  title        = {openai summarize tldr dataset},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/CarperAI/openai\_summarize\_tldr}},
}

@misc{knowrohit07_know_saraswati_cot,
  author       = {knowrohit07},
  title        = {know saraswati cot dataset},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/datasets/knowrohit07/know-saraswati-cot}},
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{dai2024nvlm,
  title={Nvlm: Open frontier-class multimodal llms},
  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2409.11402},
  year={2024}
}

@article{abdin2024phi3,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@article{yang2024think,
    title={{Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces}},
    author={Yang, Jihan and Yang, Shusheng and Gupta, Anjali and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
    year={2024},
    journal={arXiv preprint arXiv:2412.14171},
}
@article{tong2024cambrian,
  title={Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},
  journal={arXiv preprint arXiv:2406.16860},
  year={2024}
}

@article{yao2024minicpm,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}

@article{gao2024mini_internvl,
  title={Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5\% Parameters and 90\% Performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2410.16261},
  year={2024}
}


@article{li2025eagle2,
  title={Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models},
  author={Li, Zhiqi and Chen, Guo and Liu, Shilong and Wang, Shihao and VS, Vibashan and Ji, Yishen and Lan, Shiyi and Zhang, Hao and Zhao, Yilin and Radhakrishnan, Subhashree and others},
  journal={arXiv preprint arXiv:2501.14818},
  year={2025}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{yang2024qwen2.5,
  title={Qwen2.5 Technical Report},
  author={Yang, An  and  Yang, Baosong  and  Zhang, Beichen  and  Hui, Binyuan  and  Zheng, Bo  and  Yu, Bowen  and  Li, Chengyuan  and  Liu, Dayiheng  and  Huang, Fei },
journal={arXiv preprint arXiv:2412.15115},
  year={2024},
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{liu2024mminstruct,
  title={Mminstruct: A high-quality multi-modal instruction tuning dataset with extensive diversity},
  author={Liu, Yangzhou and Cao, Yue and Gao, Zhangwei and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Tian, Hao and Lu, Lewei and Zhu, Xizhou and Lu, Tong and others},
  journal={arXiv preprint arXiv:2407.15838},
  year={2024}
}

@misc{qwq-32b-preview,
  title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
  howpublished = {\url{https://qwenlm.github.io/blog/qwq-32b-preview/}},
  author = {Qwen Team},
  month = {November},
  year = {2024}
}

@article{liu2024points,
  title={Points: Improving your vision-language model with affordable strategies},
  author={Liu, Yuan and Zhao, Zhongyin and Zhuang, Ziyuan and Tian, Le and Zhou, Xiao and Zhou, Jie},
  journal={arXiv preprint arXiv:2409.04828},
  year={2024}
}

@article{lu2024bluelm,
  title={BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices},
  author={Lu, Xudong and Chen, Yinghao and Chen, Cheng and Tan, Hui and Chen, Boheng and Xie, Yina and Hu, Rui and Tan, Guanxin and Wu, Renshou and Hu, Yan and others},
  journal={arXiv preprint arXiv:2411.10640},
  year={2024}
}

@article{xiong2025bluelm,
  title={BlueLM-2.5-3B Technical Report},
  author={Xiong, Baojiao and Chen, Boheng and Wang, Chengzhi and Luo, Daxiong and Xu, Dongsheng and Liu, Dongyang and Yang, Fan and Li, Fangyuan and Teng, Fei and Wang, Feng and others},
  journal={arXiv preprint arXiv:2507.05934},
  year={2025}
}

@article{shi2024eagle,
  title={Eagle: Exploring the design space for multimodal llms with mixture of encoders},
  author={Shi, Min and Liu, Fuxiao and Wang, Shihao and Liao, Shijia and Radhakrishnan, Subhashree and Huang, De-An and Yin, Hongxu and Sapra, Karan and Yacoob, Yaser and Shi, Humphrey and others},
  journal={arXiv preprint arXiv:2408.15998},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{luo2024mono_internvl,
  title={Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training},
  author={Luo, Gen and Yang, Xue and Dou, Wenhan and Wang, Zhaokai and Dai, Jifeng and Qiao, Yu and Zhu, Xizhou},
  journal={arXiv preprint arXiv:2410.08202},
  year={2024}
}

@article{zhang2019rmsnorm,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal=NeurIPS,
  volume={32},
  year={2019}
}

@article{wang2024mpo,
  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},
  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2411.10442},
  year={2024}
}

@article{he2024olympiadbench,
  title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.14008},
  year={2024}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{gu2024aquilavl,
  title={Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data},
  author={Gu, Shuhao and Zhang, Jialing and Zhou, Siyuan and Yu, Kevin and Xing, Zhaohu and Wang, Liangdong and Cao, Zhou and Jia, Jintao and Zhang, Zhuoyi and Wang, Yixuan and others},
  journal={arXiv preprint arXiv:2410.18558},
  year={2024}
}

@article{zhao2025omnialign,
  title={Omnialign-v: Towards enhanced alignment of mllms with human preference},
  author={Zhao, Xiangyu and Ding, Shengyuan and Zhang, Zicheng and Huang, Haian and Cao, Maosong and Wang, Weiyun and Wang, Jiaqi and Fang, Xinyu and Wang, Wenhai and Zhai, Guangtao and others},
  journal={arXiv preprint arXiv:2502.18411},
  year={2025}
}

@article{zhang2023pmcvqa,
      title={PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering}, 
      author={Xiaoman Zhang and Chaoyi Wu and Ziheng Zhao and Weixiong Lin and Ya Zhang and Yanfeng Wang and Weidi Xie},
      year={2023},
      journal={arXiv preprint arXiv:2305.10415},
}

@inproceedings{guo2019eaten,
  title={Eaten: Entity-aware attention for single shot visual text extraction},
  author={Guo, He and Qin, Xiameng and Liu, Jiaming and Han, Junyu and Liu, Jingtuo and Ding, Errui},
  booktitle=ICDAR,
  pages={254--259},
  year={2019}
}

@inproceedings{singh2021textocr,
  title={Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text},
  author={Singh, Amanpreet and Pang, Guan and Toh, Mandy and Huang, Jing and Galuba, Wojciech and Hassner, Tal},
  booktitle=CVPR,
  pages={8802--8812},
  year={2021}
}

@article{masry2023unichart,
  title={Unichart: A universal vision-language pretrained model for chart comprehension and reasoning},
  author={Masry, Ahmed and Kavehzadeh, Parsa and Do, Xuan Long and Hoque, Enamul and Joty, Shafiq},
  journal={arXiv preprint arXiv:2305.14761},
  year={2023}
}

@inproceedings{methani2020plotqa,
  title={Plotqa: Reasoning over scientific plots},
  author={Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M and Kumar, Pratyush},
  booktitle=WACV,
  pages={1527--1536},
  year={2020}
}

@article{liu2020casia,
  title={Offline handwritten Chinese text recognition with convolutional neural networks},
  author={Liu, Brian and Xu, Xianchao and Zhang, Yu},
  journal={arXiv preprint arXiv:2006.15619},
  year={2020}
}


@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{reid2024gemini1_5,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{tong2024mmvp,
  title={Eyes wide shut? exploring the visual shortcomings of multimodal llms},
  author={Tong, Shengbang and Liu, Zhuang and Zhai, Yuexiang and Ma, Yi and LeCun, Yann and Xie, Saining},
  journal={arXiv preprint arXiv:2401.06209},
  year={2024}
}

@inproceedings{kim2022synthdog,
  title={Ocr-free document understanding transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  booktitle=ECCV,
  pages={498--517},
  year={2022},
  organization={Springer}
}

@article{yuan2019ctw,
  title={A large chinese text dataset in the wild},
  author={Yuan, Tai-Ling and Zhu, Zhe and Xu, Kun and Li, Cheng-Jun and Mu, Tai-Jiang and Hu, Shi-Min},
  journal={Journal of Computer Science and Technology},
  volume={34},
  pages={509--521},
  year={2019},
  publisher={Springer}
}

@inproceedings{gupta2016synthtext,
  title={Synthetic data for text localisation in natural images},
  author={Gupta, Ankush and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle=CVPR,
  pages={2315--2324},
  year={2016}
}

@inproceedings{chen2023internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle=CVPR,
  pages={24185--24198},
  year={2024}
}

@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle=ICCV,
  pages={568--578},
  year={2021}
}

@inproceedings{li2023unmasked,
  title={Unmasked teacher: Towards training-efficient video foundation models},
  author={Li, Kunchang and Wang, Yali and Li, Yizhuo and Wang, Yi and He, Yinan and Wang, Limin and Qiao, Yu},
  booktitle=ICCV,
  pages={19948--19960},
  year={2023}
}



@inproceedings{he2022mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=CVPR,
  pages={16000--16009},
  year={2022}
}

@inproceedings{kirillov2019panoptic,
  title={Panoptic feature pyramid networks},
  author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle=CVPR,
  pages={6399--6408},
  year={2019}
}

@inproceedings{xiao2018upernet,
  title={Unified perceptual parsing for scene understanding},
  author={Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  booktitle=ECCV,
  pages={418--434},
  year={2018}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle=ICML,
  pages={10347--10357},
  year={2021}
}

@inproceedings{wang2023internimage,
  title={Internimage: Exploring large-scale vision foundation models with deformable convolutions},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  booktitle=CVPR,
  pages={14408--14419},
  year={2023}
}

@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy V and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and HAZIZA, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={Transactions on Machine Learning Research Journal},
  year={2023}
}

@article{chen2015cococaption,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@article{chen2023palix,
  title={PaLI-X: On Scaling up a Multilingual Vision and Language Model},
  author={Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
  journal={arXiv preprint arXiv:2305.18565},
  year={2023}
}

@article{wei2022fdswin,
  title={Contrastive learning rivals masked image modeling in fine-tuning via feature distillation},
  author={Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2205.14141},
  year={2022}
}

@inproceedings{cheng2022mask2former,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  booktitle=CVPR,
  pages={1290--1299},
  year={2022}
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Dollar, Piotr and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Girshick, Ross},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@article{wang2021pvtv2,
  title={Pvt v2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={415--424},
  year={2022},
  publisher={Springer}
}

@inproceedings{wu2021cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  booktitle=ICCV,
  pages={22--31},
  year={2021}
}

@article{gu2021hrvit,
  title={Hrvit: Multi-scale high-resolution vision transformer},
  author={Gu, Jiaqi and Kwon, Hyoukjun and Wang, Dilin and Ye, Wei and Li, Meng and Chen, Yu-Hsin and Lai, Liangzhen and Chandra, Vikas and Pan, David Z},
  journal={arXiv preprint arXiv:2111.01236},
  year={2021}
}

@article{lu2024deepseekvl,
  title={Deepseek-vl: Towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

@article{kuznetsova2020openimage,
  title={The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale},
  author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and others},
  journal={IJCV},
  volume={128},
  number={7},
  pages={1956--1981},
  year={2020}
}

@article{wang2023cogvlm,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}

@article{yuan2021hrformer,
  title={HRFormer: High-Resolution Vision Transformer for Dense Prediction},
  author={Yuan, Yuhui and Fu, Rao and Huang, Lang and Lin, Weihong and Zhang, Chao and Chen, Xilin and Wang, Jingdong},
  journal=NeurIPS,
  volume={34},
  year={2021}
}


@inproceedings{grounding_dino,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle=ECCV,
  pages={38--55},
  year={2025},
  organization={Springer}
}


@inproceedings{uninext,
  title={Universal Instance Perception as Object Discovery and Retrieval},
  author={B. Yan and Yi Jiang and Jiannan Wu and D. Wang and Ping Luo and Zehuan Yuan and Huchuan Lu},
  booktitle=CVPR,
  year={2023},
}

@article{one-peace,
  title={ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities},
  author={Wang, Peng and Wang, Shijie and Lin, Junyang and Bai, Shuai and Zhou, Xiaohuan and Zhou, Jingren and Wang, Xinggang and Zhou, Chang},
  journal={arXiv:2305.11172},
  year={2023}
}

@article{dong2021cswin,
  title={Cswin transformer: A general vision transformer backbone with cross-shaped windows},
  author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and Zhang, Weiming and Yu, Nenghai and Yuan, Lu and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2107.00652},
  year={2021}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=CVPR,
  pages={770--778},
  year={2016}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle=CVPR,
  pages={11976--11986},
  year={2022}
}

@inproceedings{zhang2020bridging,
  title={Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection},
  author={Zhang, Shifeng and Chi, Cheng and Yao, Yongqiang and Lei, Zhen and Li, Stan Z},
  booktitle=CVPR,
  pages={9759--9768},
  year={2020}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  booktitle=ICCV,
  pages={32--42},
  year={2021}
}

@article{chu2021conditional,
  title={Conditional positional encodings for vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Zhang, Bo and Wang, Xinlong and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal={arXiv preprint arXiv:2102.10882},
  year={2021}
}

@inproceedings{huang2016droppath,
  title={Deep networks with stochastic depth},
  author={Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
  booktitle=ECCV,
  pages={646--661},
  year={2016}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SIGKDD},
  pages={3505--3506},
  year={2020}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}


@article{zhu2023ghost,
  title={Ghost in the Minecraft: Generally Capable Agents for Open-World Enviroments via Large Language Models with Text-based Knowledge and Memory},
  author={Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and others},
  journal={arXiv preprint arXiv:2305.17144},
  year={2023}
}

@inproceedings{zhu2020deformable,
  title={Deformable detr: Deformable transformers for end-to-end object detection},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  booktitle=ICLR,
  year={2020}
}

@article{thomee2016yfcc100m,
  title={YFCC100M: The new data in multimedia research},
  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={64--73},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle=ECCV,
  pages={740--755},
  year={2014}
}

@inproceedings{agrawal2019nocaps,
  title={Nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle=ICCV,
  pages={8948--8957},
  year={2019}
}

@article{liu2023mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author={Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}

@misc{textocr_gpt4v_dataset,
  title = {TextOCR GPT-4V Dataset},
  author = {Jimmycarter},
  howpublished = {\url{https://huggingface.co/datasets/jimmycarter/textocr-gpt4v}},
  organization = {Hugging Face},
  year={2023}
}

@misc{OpenHermes2_5,
  title = {OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants},
  author = {Teknium},
  year = {2023},
  organization = {HuggingFace},
  howpublished = {\url{https://huggingface.co/datasets/teknium/OpenHermes-2.5}}
}

@article{bai2024coig,
  title={COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning},
  author={Bai, Yuelin and Du, Xinrun and Liang, Yiming and Jin, Yonggang and Liu, Ziqiang and Zhou, Junting and Zheng, Tianyu and Zhang, Xincheng and Ma, Nuo and Wang, Zekun and others},
  journal={arXiv preprint arXiv:2403.18058},
  year={2024}
}

@article{wang2023lvisinstruct4v,
  title={To see is to believe: Prompting gpt-4v for better visual instruction tuning},
  author={Wang, Junke and Meng, Lingchen and Weng, Zejia and He, Bo and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2311.07574},
  year={2023}
}

@article{he2023wanjuan,
  title={Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models},
  author={He, Conghui and Jin, Zhenjiang and Xu, Chao and Qiu, Jiantao and Wang, Bin and Li, Wei and Yan, Hang and Wang, Jiaqi and Lin, Dahua},
  journal={arXiv preprint arXiv:2308.10755},
  year={2023}
}

@misc{laion_gpt4v_dataset,
  title = {GPT-4V Dataset},
  author={LAION},
  howpublished = {\url{https://huggingface.co/datasets/laion/gpt4v-dataset}},
  organization = {LAION},
  year={2023}
}

@article{chen2024allava,
  title={ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model},
  author={Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
  journal={arXiv preprint arXiv:2402.11684},
  year={2024}
}

@article{krishna2017vg,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal=IJCV,
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}


@inproceedings{lerner2022viquae,
  title={ViQuAE, a dataset for knowledge-based visual question answering about named entities},
  author={Lerner, Paul and Ferret, Olivier and Guinaudeau, Camille and Le Borgne, Herv{\'e} and Besan{\c{c}}on, Romaric and Moreno, Jos{\'e} G and Lov{\'o}n Melgarejo, Jes{\'u}s},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={3108--3120},
  year={2022}
}
@article{liu2019curved,
  title={Curved scene text detection via transverse and longitudinal sequence connection},
  author={Liu, Yuliang and Jin, Lianwen and Zhang, Shuaitao and Luo, Canjie and Zhang, Sheng},
  journal={Pattern Recognition},
  volume={90},
  pages={337--345},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{PontTuset_eccv2020,
  author    = {Jordi Pont-Tuset and Jasper Uijlings and Soravit Changpinyo and Radu Soricut and Vittorio Ferrari},
  title     = {Connecting Vision and Language with Localized Narratives},
  booktitle = {ECCV},
  year      = {2020}
}

@article{chen2021geoqa,
title={GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning},
author={Chen, Jiaqi and Tang, Jianheng and Qin, Jinghui and Liang, Xiaodan and Liu, Lingbo and Xing, Eric P and Lin, Liang},
journal={arXiv preprint arXiv:2105.14517},
year={2021}
}

@article{kahou2017figureqa,
  title={Figureqa: An annotated figure dataset for visual reasoning},
  author={Kahou, Samira Ebrahimi and Michalski, Vincent and Atkinson, Adam and K{\'a}d{\'a}r, {\'A}kos and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.07300},
  year={2017}
}
@article{lu2021iconqa,
  title={Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning},
  author={Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2110.13214},
  year={2021}
}

@article{zhao2023robut,
  title={RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations},
  author={Zhao, Yilun and Zhao, Chen and Nan, Linyong and Qi, Zhenting and Zhang, Wenlin and Tang, Xiangru and Mi, Boyu and Radev, Dragomir},
  journal={arXiv preprint arXiv:2306.14321},
  year={2023}
}

@misc{
	nimapourjafar_LACR_I2I,
	title = {LACR I2I},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_LACR_I2I}}
}

@misc{
	nimapourjafar_robut_wikisql,
	title = {robut-wikisql},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_robut_wikisql}}
}

@article{hu2024mplug,
  title={mplug-docowl 1.5: Unified structure learning for ocr-free document understanding},
  author={Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
  journal={arXiv preprint arXiv:2403.12895},
  year={2024}
}

@inproceedings{laurenccon2024building,
  title={Building and better understanding vision-language models: insights and future directions},
  author={Lauren{\c{c}}on, Hugo and Marafioti, Andr{\'e}s and Sanh, Victor and Tronchon, L{\'e}o},
  booktitle={Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models},
  year={2024}
}

@article{hsiao2022screenqa,
  title={Screenqa: Large-scale question-answer pairs over mobile app screenshots},
  author={Hsiao, Yu-Chung and Zubach, Fedir and Baechler, Gilles and Carbune, Victor and Lin, Jason and Wang, Maria and Sunkara, Srinivas and Zhu, Yun and Chen, Jindong},
  journal={arXiv preprint arXiv:2209.08199},
  year={2022}
}

@article{laurenccon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={87874--87907},
  year={2024}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@article{suhr2019nlvr2,
  title={Nlvr2 visual bias analysis},
  author={Suhr, Alane and Artzi, Yoav},
  journal={arXiv preprint arXiv:1909.10411},
  year={2019}
}

@inproceedings{du2019cocoqa,
  title={Cocoqa: Question answering for coding conventions over knowledge graphs},
  author={Du, Tianjiao and Cao, Junming and Wu, Qinyue and Li, Wei and Shen, Beijun and Chen, Yuting},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={1086--1089},
  year={2019},
  organization={IEEE}
}

@misc{
OpenGVLab_ShareGPT_4o,

title = {ShareGPT-4o},

author = {OpenGVLab},

year = {2005},

howpublished = {\url{https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o}}

}
@misc{
	nimapourjafar_mm_LADD,
	title = {LADD},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_LADD}}
}

@misc{
	laion_laion_gpt4v,
	title = {laion-gpt4v},
	author = {laion},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/laion/gpt4v-dataset}}
}

@misc{
	Labeled_ORAND_CAR_A,
	title = {orand-car-a},
	author = {Fzkuji},
	year = {2024},
	howpublished = {\url{https://github.com/Fzkuji/Labeled_ORAND-CAR-A}}
}

@misc{
	nimapourjafar_mm_tqa,
	title = {mm tqa},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_tqa}}
}

@misc{
	nimapourjafar_mm_intergps,
	title = {mm intergps},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_intergps}}
}

@misc{
	JourneyBench_Hallucination,
	title = {JourneyBench Hallucination},
	author = {JourneyBench},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/JourneyBench/JourneyBench_Hallucination}}
}

@misc{
	nimapourjafar_diagram_image_to_text,
	title = {mm diagram image to text},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_diagram_image_to_text}}
}

@misc{
	nimapourjafar_mm_vqarad,
	title = {mm vqarad},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_vqarad}}
}

@misc{
	LIME_DATA_ai2d_train,
	title = {LIME-DATA-ai2d-train},
	author = {LIME-DATA},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/LIME-DATA/ai2d}}
}

@misc{
	MMC_Instructed_Dataset,
	title = {MMC Instructed Dataset},
	author = {Felprot75},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/Felprot75/MMC_Instructed_Dataset}}
}

@misc{
	howard_hou_COCO_Text,
	title = {COCO-Text},
	author = {howard-hou},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/howard-hou/COCO-Text}}
}

@misc{
	VLM_Perception_HME100k_400,
	title = {HME100k-400},
	author = {VLM-Perception},
	year = {2025},
	howpublished = {\url{https://huggingface.co/datasets/VLM-Perception/HME100k-400}}
}

@misc{
	vikhyatk_st_vqa,
	title = {st-vqa},
	author = {vikhyatk},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/vikhyatk/st-vqa}}
}

@misc{
	nz_arxiv_ocr,
	title = {arxiv-ocr},
	author = {nz},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nz/arxiv-ocr-v0.1-sft}}
}

@misc{
	apoidea_fintabnet,
	title = {fintabnet},
	author = {apoidea},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/apoidea/fintabnet-html}}
}

@misc{
	MiXaiLL76_TextOCR_OCR,
	title = {TextOCR OCR},
	author = {MiXaiLL76},
	year = {2025},
	howpublished = {\url{https://huggingface.co/datasets/MiXaiLL76/TextOCR_OCR}}
}
@misc{
	mychen76_invoices_receipts_ocr_v2,
	title = {invoices and receipts ocr v2},
	author = {mychen76},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/mychen76/invoices-and-receipts_ocr_v2}}
}

@misc{
	CC1984_mall_receipt_extraction_dataset,
	title = {mall receipt extraction dataset},
	author = {CC1984},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/CC1984/mall_receipt_extraction_dataset}}
}

@misc{
	mychen76_invoices_receipts_ocr_v1,
	title = {invoices and receipts ocr v1},
	author = {mychen76},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/mychen76/invoices-and-receipts_ocr_v1}}
}

@misc{
	ds_receipts_v2_train,
	title = {ds receipts v2 train},
	author = {mychen76},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/mychen76/ds_receipts_v2_train}}
}

@misc{
	ilhamxx_dataset_receipt,
	title = {dataset receipt},
	author = {ilhamxx},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/ilhamxx/dataset_receipt}}
}

@misc{
	katanaml_invoices_donut_data_v1,
	title = {invoices-donut-data-v1},
	author = {katanaml org},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/katanaml-org/invoices-donut-data-v1}}
}

@misc{
	Vision_OCR_Financial_Reports_10k,
	title = {Vision OCR Financial Reports 10k},
	author = {Anas989898},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/Anas989898/Vision-OCR-Financial-Reports-10k}}
}

@misc{
	toghrultahirov_handwritten_text_ocr,
	title = {handwritten text ocr},
	author = {toghrultahirov},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/toghrultahirov/handwritten_text_ocr}}
}

@misc{
	kashindra_mahato_nutritional_data_poie,
	title = {nutritional-data-poie},
	author = {kashindra-mahato},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/kashindra-mahato/nutritional-data-poie-1}}
}

@misc{
	shibing624_sharegpt_gpt4,
	title = {sharegpt gpt4},
	author = {shibing624},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/shibing624/sharegpt_gpt4}}
}

@misc{
	LooksJuicy_ruozhiba,
	title = {ruozhiba},
	author = {LooksJuicy},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/LooksJuicy/ruozhiba}}
}

@misc{
	Yeenyi_ner_sentiment_analysis_sharegpt,
	title = {ner sentiment analysis sharegpt},
	author = {Yeenyi},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/Yeenyi/ner_sentiment_analysis_sharegpt}}
}

@misc{
	qgyd2021_chinese_ner_sft,
	title = {chinese ner sft},
	author = {qgyd2021},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/qgyd2021/chinese_ner_sft}}
}

@misc{
	qgyd2021_few_shot_ner_sft,
	title = {few shot ner sft},
	author = {qgyd2021},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/qgyd2021/few_shot_ner_sft}}
}

@misc{
	cognitivecomputations_SystemChat,
	title = {SystemChat-2.0},
	author = {cognitive computations},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/cognitivecomputations/SystemChat-2.0}}
}

@misc{
	nimapourjafar_mm_datikz,
	title = {mm datikz},
	author = {nimapourjafar},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/nimapourjafar/mm_datikz}}
}

@misc{
	glaiveai_glaive_function_calling,
	title = {glaive-function-calling},
	author = {glaiveai},
	year = {2023},
	howpublished = {\url{https://huggingface.co/datasets/glaiveai/glaive-function-calling}}
}

@misc{
	MMR1_Math_RL_Data_v0,
	title = {MMR1-Math-RL-Data-v0},
	author = {MMR1},
	year = {2025},
	howpublished = {\url{https://huggingface.co/datasets/MMR1/MMR1-Math-RL-Data-v0}}
}

@misc{
	sr5434_CodegebraGPT_data,
	title = {CodegebraGPT data},
	author = {sr5434},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/sr5434/CodegebraGPT_data}}
}

@misc{
	Luckyjhg_Geo170K,
	title = {Geo170K},
	author = {Luckyjhg},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/Luckyjhg/Geo170K}}
}

@misc{
	multimath_300k,
	title = {multimath-300k},
	author = {pengshuai-rin},
	year = {2024},
	howpublished = {\url{https://huggingface.co/datasets/pengshuai-rin/multimath-300k}}
}

@misc{
	mOpenR1_Math_220k,
	title = {OpenR1-Math-220k},
	author = {open-r1},
	year = {2025},
	howpublished = {\url{https://huggingface.co/datasets/open-r1/OpenR1-Math-220k}}
}

@article{gonzalez2024metrics,
  title={Are metrics measuring what they should? An evaluation of Image Captioning task metrics},
  author={Gonz{\'a}lez-Ch{\'a}vez, Oth{\'o}n and Ruiz, Guillermo and Moctezuma, Daniela and Ramirez-delReal, Tania},
  journal={Signal Processing: Image Communication},
  volume={120},
  pages={117071},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{zhang2021vsr,
  title={VSR: a unified framework for document layout analysis combining vision, semantics and relations},
  author={Zhang, Peng and Li, Can and Qiao, Liang and Cheng, Zhanzhan and Pu, Shiliang and Niu, Yi and Wu, Fei},
  booktitle={Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part I 16},
  pages={115--130},
  year={2021},
  organization={Springer}
}

@misc{
Theonewhomadethings_fsc147_controlnet,

title = {fsc147-controlnet},

author = {Theonewhomadethings},

year = {2024},

howpublished = {\url{https://huggingface.co/datasets/Theonewhomadethings/fsc147-controlnet}}

}


@misc{tan2013spot,
  title={Spot the diff.},
  author={Tan, Lynette Yihui},
  year={2013},
howpublished = {\url{https://huggingface.co/datasets/Lancelot53/spot-the-diff}}

}

@inproceedings{marana1997estimation,
  title={Estimation of crowd density using image processing},
  author={Marana, Aparecido Nilceu and Velast{\'\i}n, Sergio A and Costa, LF and Lotufo, RA},
  booktitle={IEE Colloquium on Image Processing for Security Applications (Digest No: 1997/074)},
  pages={11--1},
  year={1997},
  organization={IET}
}

@article{chen2021finqa,
  title={Finqa: A dataset of numerical reasoning over financial data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others},
  journal={arXiv preprint arXiv:2109.00122},
  year={2021}
}

@article{cheng2022iam,
  title={IAM: a comprehensive and large-scale dataset for integrated argument mining tasks},
  author={Cheng, Liying and Bing, Lidong and He, Ruidan and Yu, Qian and Zhang, Yan and Si, Luo},
  journal={arXiv preprint arXiv:2203.12257},
  year={2022}
}

@article{liu2023aligning,
  title={Aligning large multi-modal model with robust instruction tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={CoRR},
  year={2023}
}

@article{zhao2022multihiertt,
  title={MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data},
  author={Zhao, Yilun and Li, Yunxiang and Li, Chenying and Zhang, Rui},
  journal={arXiv preprint arXiv:2206.01347},
  year={2022}
}

@article{kazemi2023geomverse,
  title={Geomverse: A systematic evaluation of large models for geometric reasoning},
  author={Kazemi, Mehran and Alvari, Hamidreza and Anand, Ankit and Wu, Jialin and Chen, Xi and Soricut, Radu},
  journal={arXiv preprint arXiv:2312.12241},
  year={2023}
}

@article{tang2023vistext,
  title={Vistext: A benchmark for semantically rich chart captioning},
  author={Tang, Benny J and Boggust, Angie and Satyanarayan, Arvind},
  journal={arXiv preprint arXiv:2307.05356},
  year={2023}
}

@article{lu2021inter,
  title={Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning},
  author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2105.04165},
  year={2021}
}

@inproceedings{zhu2016visual7w,
  title={Visual7w: Grounded question answering in images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4995--5004},
  year={2016}
}

@article{lu2022dynamic,
  title={Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
  author={Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2209.14610},
  year={2022}
}

@article{liu2024chatqa,
title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
journal={arXiv preprint arXiv:2401.10225},
year={2024}}

@inproceedings{schwenk2022okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European conference on computer vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@article{wang2024pin,
  title={Pin: A knowledge-intensive dataset for paired and interleaved multimodal documents},
  author={Wang, Junjie and Zhang, Yin and Ji, Yatai and Zhang, Yuxiang and Jiang, Chunyang and Wang, Yubo and Zhu, Kang and Wang, Zekun and Wang, Tiezhen and Huang, Wenhao and others},
  journal={arXiv preprint arXiv:2406.13923},
  year={2024}
}



@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{park2019cord,
  title={CORD: A Consolidated Receipt Dataset for Post-OCR Parsing},
  author={Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  booktitle={Document Intelligence Workshop at Neural Information Processing Systems},
  year={2019}
}

@article{yue2023mammoth,
  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

@misc{wang2024unimernetuniversalnetworkrealworld,
      title={UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition}, 
      author={Bin Wang and Zhuangcheng Gu and Guang Liang and Chao Xu and Bo Zhang and Botian Shi and Conghui He},
      year={2024},
      eprint={2404.15254},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.15254}, 
}

@misc{Hermes-Function-Calling-Dataset-V1, 
      howpublished = {\url{https://huggingface.co/NousResearch/hermes-function-calling-v1}}, 
      title={Hermes-Function-Calling-Dataset-V1}, 
      author={"interstellarninja", "Teknium"}
}

@article{shi2024math,
  title={Math-llava: Bootstrapping mathematical reasoning for multimodal large language models},
  author={Shi, Wenhao and Hu, Zhiqiang and Bin, Yi and Liu, Junhua and Yang, Yang and Ng, See-Kiong and Bing, Lidong and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2406.17294},
  year={2024}

}
@misc{laurencon2024unlocking,
      title={Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset}, 
      author={Hugo Laurenon and Lo Tronchon and Victor Sanh},
      year={2024},
      eprint={2403.09029},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{Invoice-to-Json,
  title={Invoice-to-Json: A Document Understanding and Information Extraction Dataset},
  year={2024}
}
@article{tuo2023anytext,
      title={AnyText: Multilingual Visual Text Generation And Editing}, 
      author={Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie},
      year={2023},
      eprint={2311.03054},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{davis2019deep,
  title={Deep visual template-free form parsing},
  author={Davis, Brian and Morse, Bryan and Cohen, Scott and Price, Brian and Tensmeyer, Chris},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={134--141},
  year={2019},
  organization={IEEE}
}
@inproceedings{zhu2021tat,
    title = "{TAT}-{QA}: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance",
    author = "Zhu, Fengbin  and
      Lei, Wenqiang  and
      Huang, Youcheng  and
      Wang, Chao  and
      Zhang, Shuo  and
      Lv, Jiancheng  and
      Feng, Fuli  and
      Chua, Tat-Seng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.254",
    doi = "10.18653/v1/2021.acl-long.254",
    pages = "3277--3287"
}
@inproceedings{li2023superclevr,
  title={Super-CLEVR: A virtual benchmark to diagnose domain robustness in visual reasoning},
  author={Li, Zhuowan and Wang, Xingrui and Stengel-Eskin, Elias and Kortylewski, Adam and Ma, Wufei and Van Durme, Benjamin and Yuille, Alan L},
  booktitle=CVPR,
  pages={14963--14973},
  year={2023}
}

@inproceedings{shah2019kvqa,
  title={Kvqa: Knowledge-aware visual question answering},
  author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim},
  booktitle=AAAI,
  volume={33},
  pages={8876--8884},
  year={2019}
}

@article{amini2019mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}

@article{lu2021geometry3k,
  title={Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning},
  author={Lu, Pan and Gong, Ran and Jiang, Shibiao and Qiu, Liang and Huang, Siyuan and Liang, Xiaodan and Zhu, Song-Chun},
  journal={arXiv preprint arXiv:2105.04165},
  year={2021}
}


@inproceedings{kafle2018dvqa,
  title={Dvqa: Understanding data visualizations via question answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle=CVPR,
  pages={5648--5656},
  year={2018}
}

@article{lindstrom2022clevrmath,
  title={Clevr-math: A dataset for compositional language, visual and mathematical reasoning},
  author={Lindstr{\"o}m, Adam Dahlgren and Abraham, Savitha Sam},
  journal={arXiv preprint arXiv:2208.05358},
  year={2022}
}

@article{lu2022tablemwp,
  title={Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
  author={Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2209.14610},
  year={2022}
}


@inproceedings{cao2022geoqa_plus,
  title={An augmented benchmark dataset for geometric question answering through dual parallel text encoding},
  author={Cao, Jie and Xiao, Jing},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={1511--1520},
  year={2022}
}



@inproceedings{kembhavi2017tqa,
  title={Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
  author={Kembhavi, Aniruddha and Seo, Minjoon and Schwenk, Dustin and Choi, Jonghyun and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle=CVPR,
  pages={4999--5007},
  year={2017}
}

@article{liu2023vsr,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={635--651},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~}
}


@inproceedings{zhu2023languagebind,
  title={LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment},
  author={Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and HongFa, WANG and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and others},
  booktitle=ICLR,
  year={2023}
}

@article{wang2024charxiv,
  title={Charxiv: Charting gaps in realistic chart understanding in multimodal llms},
  author={Wang, Zirui and Xia, Mengzhou and He, Luxi and Chen, Howard and Liu, Yitao and Zhu, Richard and Liang, Kaiqu and Wu, Xindi and Liu, Haotian and Malladi, Sadhika and others},
  journal={arXiv preprint arXiv:2406.18521},
  year={2024}
}

@article{zhang2024vcr,
  title={VCR: Visual Caption Restoration},
  author={Zhang, Tianyu and Wang, Suyuchen and Li, Lu and Zhang, Ge and Taslakian, Perouz and Rajeswar, Sai and Fu, Jie and Liu, Bang and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2406.06462},
  year={2024}
}

@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{li2024seedbench2plus,
  title={Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension},
  author={Li, Bohao and Ge, Yuying and Chen, Yi and Ge, Yixiao and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2404.16790},
  year={2024}
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{gu2022wukong,
  title={Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark},
  author={Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Minzhe, Niu and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and others},
  journal=NeurIPS,
  volume={35},
  pages={26418--26431},
  year={2022}
}

@article{sun2023evaclip,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}

@article{wang2022omnivl,
  title={Omnivl: One foundation model for image-language and video-language tasks},
  author={Wang, Junke and Chen, Dongdong and Wu, Zuxuan and Luo, Chong and Zhou, Luowei and Zhao, Yucheng and Xie, Yujia and Liu, Ce and Jiang, Yu-Gang and Yuan, Lu},
  journal=NeurIPS,
  volume={35},
  pages={5696--5710},
  year={2022}
}

@misc{openclip,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  year         = {2021},
  howpublished = {Zenodo. Version 0.1. \url{https://doi.org/10.5281/zenodo.5143773}},
  note         = {DOI: 10.5281/zenodo.5143773}
}

@inproceedings{li2023pope,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={The Conference on Empirical Methods in Natural Language Processing},
  pages={292--305},
  year={2023}
}


@misc{idefics2023,
  title = {Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model},
  author = {{IDEFICS}},
  year = {2023},
  howpublished = {\url{https://huggingface.co/blog/idefics}}
}


@inproceedings{li2023flip,
  title={Scaling language-image pre-training via masking},
  author={Li, Yanghao and Fan, Haoqi and Hu, Ronghang and Feichtenhofer, Christoph and He, Kaiming},
  booktitle=CVPR,
  pages={23390--23400},
  year={2023}
}

@article{li2023monkey,
  title={Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
  author={Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}

@inproceedings{singh2019textvqa,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle=CVPR,
  pages={8317--8326},
  year={2019}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle=CVPR,
  pages={3608--3617},
  year={2018}
}

@article{carreira2018k600,
  title={A short note about kinetics-600},
  author={Carreira, Joao and Noland, Eric and Banki-Horvath, Andras and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1808.01340},
  year={2018}
}

@article{carreira2019k700,
  title={A short note on the kinetics-700 human action dataset},
  author={Carreira, Joao and Noland, Eric and Hillier, Chloe and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1907.06987},
  year={2019}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle=ICML,
  pages={4904--4916},
  year={2021}
}

@misc{contributors2020mmsegmentation,
  title={MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark},
  author={Contributors, MMSegmentation},
  howpublished = {\url{https://github.com/open-mmlab/mmsegmentation}},
  year={2020}
}

@inproceedings{yang2020muse,
  title={Multilingual Universal Sentence Encoder for Semantic Retrieval},
  author={Yang, Yinfei and Cer, Daniel and Ahmad, Amin and Guo, Mandy and Law, Jax and Constant, Noah and Abrego, Gustavo Hernandez and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and others},
  booktitle=ACL,
  pages={87--94},
  year={2020}
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle=CVPR,
  pages={18123--18133},
  year={2022}
}

@article{bai2023qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@inproceedings{zhou2017ade20k,
  title={Scene parsing through ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle=CVPR,
  pages={633--641},
  year={2017}
}

@inproceedings{touvron2022deit3,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  booktitle=ECCV,
  pages={516--533},
  year={2022}
}

@inproceedings{dehghani2023vit22b,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle=ICML,
  pages={7480--7512},
  year={2023}
}

@inproceedings{carreira2017k400,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle=CVPR,
  pages={6299--6308},
  year={2017}
}

@inproceedings{wang2023internvid,
  title={Internvid: A large-scale video-text dataset for multimodal understanding and generation},
  author={Wang, Yi and He, Yinan and Li, Yizhuo and Li, Kunchang and Yu, Jiashuo and Ma, Xin and Chen, Xinyuan and Wang, Yaohui and Luo, Ping and Liu, Ziwei and others},
  booktitle=ICLR,
  year={2024}
}

@article{yuan2021florence,
  title={Florence: A new foundation model for computer vision},
  author={Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and others},
  journal={arXiv preprint arXiv:2111.11432},
  year={2021}
}

@inproceedings{fei2004learning,
  title={Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories},
  author={Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  booktitle=CVPRW,
  pages={178--178},
  year={2004}
}


@article{ferretv2,
  title={Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models},
  author={Zhang, Haotian and You, Haoxuan and Dufter, Philipp and Zhang, Bowen and Chen, Chen and Chen, Hong-You and Fu, Tsu-Jui and Wang, William Yang and Chang, Shih-Fu and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2404.07973},
  year={2024}
}


@inproceedings{xiao2010sun,
  title={Sun database: Large-scale scene recognition from abbey to zoo},
  author={Xiao, Jianxiong and Hays, James and Ehinger, Krista A and Oliva, Aude and Torralba, Antonio},
  booktitle=CVPRW,
  pages={3485--3492},
  year={2010}
}

@inproceedings{berg2014birdsnap,
  title={Birdsnap: Large-scale fine-grained visual categorization of birds},
  author={Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L and Jacobs, David W and Belhumeur, Peter N},
  booktitle=CVPR,
  pages={2011--2018},
  year={2014}
}

@inproceedings{goodfellow2013fer2013,
  title={Challenges in representation learning: A report on three machine learning contests},
  author={Goodfellow, Ian J and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and others},
  booktitle={ICONIP},
  pages={117--124},
  year={2013}
}

@inproceedings{nilsback2008flowers,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={ICVGIP},
  pages={722--729},
  year={2008}
}

@inproceedings{bossard2014food101,
  title={Food-101--mining discriminative components with random forests},
  author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle=ECCV,
  pages={446--461},
  year={2014}
}

@article{stallkamp2012gtsrb,
  title={Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition},
  author={Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},
  journal={Neural networks},
  volume={32},
  pages={323--332},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{parkhi2012cats,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle=CVPR,
  pages={3498--3505},
  year={2012},
}


@inproceedings{coates2011stl10,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle={AISTAT},
  pages={215--223},
  year={2011}
}

@article{everingham2015pascal,
  title={The pascal visual object classes challenge: A retrospective},
  author={Everingham, Mark and Eslami, SM Ali and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={IJCV},
  volume={111},
  pages={98--136},
  year={2015},
  publisher={Springer}
}

@article{cheng2017resisc45,
  title={Remote sensing image scene classification: Benchmark and state of the art},
  author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
  journal={Proceedings of the IEEE},
  volume={105},
  number={10},
  pages={1865--1883},
  year={2017},
  publisher={IEEE}
}

@article{helber2019eurosat,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={12},
  number={7},
  pages={2217--2226},
  year={2019},
  publisher={IEEE}
}

@inproceedings{krause2013cars,
  title={3d object representations for fine-grained categorization},
  author={Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
  booktitle={ICCVW},
  pages={554--561},
  year={2013}
}

@inproceedings{cimpoi2014d2d,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle=CVPR,
  pages={3606--3613},
  year={2014}
}


@article{maji2013fgvc,
  title={Fine-grained visual classification of aircraft},
  author={Maji, Subhransu and Rahtu, Esa and Kannala, Juho and Blaschko, Matthew and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:1306.5151},
  year={2013}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{li2019cococn,
  title={COCO-CN for cross-lingual image tagging, captioning, and retrieval},
  author={Li, Xirong and Xu, Chaoxi and Wang, Xiaoxu and Lan, Weiyu and Jia, Zhengxiong and Yang, Gang and Xu, Jieping},
  journal={TMM},
  volume={21},
  number={9},
  pages={2347--2360},
  year={2019},
  publisher={IEEE}
}

@article{krizhevsky2009cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009}
}

@article{lecun1998mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle=ICML,
  pages={448--456},
  year={2015}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle=ICCV,
  pages={2641--2649},
  year={2015}
}

@inproceedings{lan2017flickrcn,
  title={Fluency-guided cross-lingual image captioning},
  author={Lan, Weiyu and Li, Xirong and Dong, Jianfeng},
  booktitle={ACM MM},
  pages={1549--1557},
  year={2017}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}

@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=ICCV,
  pages={2961--2969},
  year={2017}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle=ICCV,
  pages={2980--2988},
  year={2017}
}

@article{li2022uniformer,
  title={UniFormer: Unifying Convolution and Self-attention for Visual Recognition},
  author={Li, Kunchang and Wang, Yali and Zhang, Junhao and Gao, Peng and Song, Guanglu and Liu, Yu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2201.09450},
  year={2022}
}

@article{yang2021focal,
  title={Focal self-attention for local-global interactions in vision transformers},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2107.00641},
  year={2021}
}

@article{huang2021shuffle,
  title={Shuffle transformer: Rethinking spatial shuffle for vision transformer},
  author={Huang, Zilong and Ben, Youcheng and Luo, Guozhong and Cheng, Pei and Yu, Gang and Fu, Bin},
  journal={arXiv preprint arXiv:2106.03650},
  year={2021}
}

@article{steiner2021train,
  title={How to train your vit? data, augmentation, and regularization in vision transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}

@inproceedings{he2019rethinking,
  title={Rethinking imagenet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle=ICCV,
  pages={4918--4927},
  year={2019}
}

@inproceedings{hu2018senet,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle=CVPR,
  pages={7132--7141},
  year={2018}
}

@inproceedings{dai2017deformable,
  title={Deformable convolutional networks},
  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  booktitle=ICCV,
  pages={764--773},
  year={2017}
}

@inproceedings{ding2021repvgg,
  title={Repvgg: Making vgg-style convnets great again},
  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle=CVPR,
  pages={13733--13742},
  year={2021}
}

@inproceedings{ghiasi2021simple,
  title={Simple copy-paste is a strong data augmentation method for instance segmentation},
  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},
  booktitle=CVPR,
  pages={2918--2928},
  year={2021}
}

@article{iandola2014densenet,
  title={Densenet: Implementing efficient convnet descriptor pyramids},
  author={Iandola, Forrest and Moskewicz, Matt and Karayev, Sergey and Girshick, Ross and Darrell, Trevor and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1404.1869},
  year={2014}
}

@article{chu2021twins,
  title={Twins: Revisiting the design of spatial attention in vision transformers},
  author={Chu, Xiangxiang and Tian, Zhi and Wang, Yuqing and Zhang, Bo and Ren, Haibing and Wei, Xiaolin and Xia, Huaxia and Shen, Chunhua},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal=NeurIPS,
  volume={30},
  year={2017}
}

@article{zhu2021uni,
  title={Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks},
  author={Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Wang, Xiaogang and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
  journal={arXiv preprint arXiv:2112.01522},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  pages={248--255},
  year={2009}
}

@inproceedings{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={NAACL},
  pages={4171--4186},
  year={2019}
}


@article{baevski2022data2vec,
  title={Data2vec: A general framework for self-supervised learning in speech, vision and language},
  author={Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  journal={arXiv preprint arXiv:2202.03555},
  year={2022}
}

@article{cui2023chinesellama,
  title={Efficient and effective text encoding for chinese llama and alpaca},
  author={Cui, Yiming and Yang, Ziqing and Yao, Xin},
  journal={arXiv preprint arXiv:2304.08177},
  year={2023}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle=CVPR,
  pages={1492--1500},
  year={2017}
}

@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@inproceedings{jaegle2021perceiver,
  title={Perceiver: General perception with iterative attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle=ICML,
  pages={4651--4664},
  year={2021}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={JMLR},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}


@article{girdhar2022omnivore,
  title={Omnivore: A Single Model for Many Visual Modalities},
  author={Girdhar, Rohit and Singh, Mannat and Ravi, Nikhila and van der Maaten, Laurens and Joulin, Armand and Misra, Ishan},
  journal={arXiv preprint arXiv:2201.08377},
  year={2022}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal=NeurIPS,
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{smith2022using,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@inproceedings{stickland2019bert,
  title={Bert and pals: Projected attention layers for efficient adaptation in multi-task learning},
  author={Stickland, Asa Cooper and Murray, Iain},
  booktitle=ICML,
  pages={5986--5995},
  year={2019}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle=ICML,
  pages={2790--2799},
  year={2019}
}

@article{sung2021vl,
  title={VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
  author={Sung, Yi-Lin and Cho, Jaemin and Bansal, Mohit},
  journal={arXiv preprint arXiv:2112.06825},
  year={2021}
}

@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle=ICCV,
  pages={7262--7272},
  year={2021}
}

@inproceedings{ranftl2021vision,
  title={Vision transformers for dense prediction},
  author={Ranftl, Ren{\'e} and Bochkovskiy, Alexey and Koltun, Vladlen},
  booktitle=ICCV,
  pages={12179--12188},
  year={2021}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle=CVPR,
  pages={6881--6890},
  year={2021}
}

@inproceedings{xu2021co,
  title={Co-scale conv-attentional image transformers},
  author={Xu, Weijian and Xu, Yifan and Chang, Tyler and Tu, Zhuowen},
  booktitle=ICCV,
  pages={9981--9990},
  year={2021}
}

@article{han2021transformer,
  title={Transformer in transformer},
  author={Han, Kai and Xiao, An and Wu, Enhua and Guo, Jianyuan and Xu, Chunjing and Wang, Yunhe},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@article{li2021localvit,
  title={Localvit: Bringing locality to vision transformers},
  author={Li, Yawei and Zhang, Kai and Cao, Jiezhang and Timofte, Radu and Van Gool, Luc},
  journal={arXiv preprint arXiv:2104.05707},
  year={2021}
}

@article{liu2021video,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}

@article{liu2021swinv2,
  title={Swin Transformer V2: Scaling Up Capacity and Resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  journal={arXiv preprint arXiv:2111.09883},
  year={2021}
}


@article{zhang2021tip,
  title={Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling},
  author={Zhang, Renrui and Fang, Rongyao and Gao, Peng and Zhang, Wei and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2111.03930},
  year={2021}
}

@article{gao2021clip,
  title={Clip-adapter: Better vision-language models with feature adapters},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2110.04544},
  year={2021}
}


@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle=CVPR,
  pages={2117--2125},
  year={2017}
}


@inproceedings{li2021panoptic,
  title={Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers},
  author={Li, Zhiqi and Wang, Wenhai and Xie, Enze and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Lu, Tong and Luo, Ping},
  booktitle=CVPR,
  year={2022}
}

@article{ali2021xcit,
  title={Xcit: Cross-covariance image transformers},
  author={Ali, Alaaeldin and Touvron, Hugo and Caron, Mathilde and Bojanowski, Piotr and Douze, Matthijs and Joulin, Armand and Laptev, Ivan and Neverova, Natalia and Synnaeve, Gabriel and Verbeek, Jakob and others},
  journal=NeurIPS,
  volume={34},
  year={2021}
}

@article{rosenfeld2018incremental,
  title={Incremental learning through deep adaptation},
  author={Rosenfeld, Amir and Tsotsos, John K},
  journal={TPAMI},
  volume={42},
  number={3},
  pages={651--663},
  year={2018}
}

@inproceedings{rebuffi2018efficient,
  title={Efficient parametrization of multi-domain deep neural networks},
  author={Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  booktitle=CVPR,
  pages={8119--8127},
  year={2018}
}

@article{rebuffi2017learning,
  title={Learning multiple visual domains with residual adapters},
  author={Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
  journal=NeurIPS,
  volume={30},
  year={2017}
}

@article{chen2021simple,
  title={A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation},
  author={Chen, Wuyang and Du, Xianzhi and Yang, Fan and Beyer, Lucas and Zhai, Xiaohua and Lin, Tsung-Yi and Chen, Huizhong and Li, Jing and Song, Xiaodan and Wang, Zhangyang and others},
  journal={arXiv preprint arXiv:2112.09747},
  year={2021}
}

@inproceedings{bolya2020tide,
  title={Tide: A general toolbox for identifying object detection errors},
  author={Bolya, Daniel and Foley, Sean and Hays, James and Hoffman, Judy},
  booktitle=ECCV,
  pages={558--573},
  year={2020}
}

@inproceedings{kamann2020benchmarking,
  title={Benchmarking the robustness of semantic segmentation models},
  author={Kamann, Christoph and Rother, Carsten},
  booktitle=CVPR,
  pages={8828--8838},
  year={2020}
}

@article{michaelis2019benchmarking,
  title={Benchmarking robustness in object detection: Autonomous driving when winter is coming},
  author={Michaelis, Claudio and Mitzkus, Benjamin and Geirhos, Robert and Rusak, Evgenia and Bringmann, Oliver and Ecker, Alexander S and Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:1907.07484},
  year={2019}
}

@inproceedings{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  booktitle=ICLR,
  year={2022}
}

@article{peng2022beitv2,
  title={BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers},
  author={Peng, Zhiliang and Dong, Li and Bao, Hangbo and Ye, Qixiang and Wei, Furu},
  journal={arXiv preprint arXiv:2208.06366},
  year={2022}
}

@inproceedings{peng2021conformer,
  title={Conformer: Local features coupling global representations for visual recognition},
  author={Peng, Zhiliang and Huang, Wei and Gu, Shanzhi and Xie, Lingxi and Wang, Yaowei and Jiao, Jianbin and Ye, Qixiang},
  booktitle=ICCV,
  pages={367--376},
  year={2021}
}

@article{cheng2021masked,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Misra, Ishan and Schwing, Alexander G and Kirillov, Alexander and Girdhar, Rohit},
  journal={arXiv preprint arXiv:2112.01527},
  year={2021}
}

@inproceedings{huang2021fapn,
  title={FaPN: Feature-aligned pyramid network for dense image prediction},
  author={Huang, Shihua and Lu, Zhichao and Cheng, Ran and He, Cheng},
  booktitle=ICCV,
  pages={864--873},
  year={2021}
}

@article{jain2021semask,
  title={SeMask: Semantically Masked Transformers for Semantic Segmentation},
  author={Jain, Jitesh and Singh, Anukriti and Orlov, Nikita and Huang, Zilong and Li, Jiachen and Walton, Steven and Shi, Humphrey},
  journal={arXiv preprint arXiv:2112.12782},
  year={2021}
}

@article{cui2022region,
  title={Region Rebalance for Long-Tailed Semantic Segmentation},
  author={Cui, Jiequan and Yuan, Yuhui and Zhong, Zhisheng and Tian, Zhuotao and Hu, Han and Lin, Stephen and Jia, Jiaya},
  journal={arXiv preprint arXiv:2204.01969},
  year={2022}
}


@inproceedings{fang2019instaboost,
  title={Instaboost: Boosting instance segmentation via probability map guided copy-pasting},
  author={Fang, Hao-Shu and Sun, Jianhua and Wang, Runzhong and Gou, Minghao and Li, Yong-Lu and Lu, Cewu},
  booktitle=ICCV,
  pages={682--691},
  year={2019}
}

@article{liang2021cbnetv2,
  title={Cbnetv2: A composite backbone network architecture for object detection},
  author={Liang, Tingting and Chu, Xiaojie and Liu, Yudong and Wang, Yongtao and Tang, Zhi and Chu, Wei and Chen, Jingdong and Ling, Haibin},
  journal={arXiv preprint arXiv:2107.00420},
  year={2021}
}

@inproceedings{caesar2018coco,
  title={Coco-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle=CVPR,
  pages={1209--1218},
  year={2018}
}

@inproceedings{Cordts_2016_CVPR,
    author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
    title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
    booktitle = {CVPR},
    year = {2016}
}

@inproceedings{neuhold2017mapillary,
  title={The mapillary vistas dataset for semantic understanding of street scenes},
  author={Neuhold, Gerhard and Ollmann, Tobias and Rota Bulo, Samuel and Kontschieder, Peter},
  booktitle=ICCV,
  pages={4990--4999},
  year={2017}
}

@article{tao2020hierarchical,
  title={Hierarchical multi-scale attention for semantic segmentation},
  author={Tao, Andrew and Sapra, Karan and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2005.10821},
  year={2020}
}

@inproceedings{yuan2019segmentation,
  title={Segmentation transformer: Object-contextual representations for semantic segmentation},
  author={Yuan, Yuhui and Chen, Xiaokang and Chen, Xilin and Wang, Jingdong},
  booktitle=ECCV,
  year={2020}
}

@article{bousselham2021efficient,
  title={Efficient Self-Ensemble Framework for Semantic Segmentation},
  author={Bousselham, Walid and Thibault, Guillaume and Pagano, Lucas and Machireddy, Archana and Gray, Joe and Chang, Young Hwan and Song, Xubo},
  journal={arXiv preprint arXiv:2111.13280},
  year={2021}
}

@article{huang2021channelized,
  title={Channelized Axial Attention for Semantic Segmentation--Considering Channel Relation within Spatial Attention for Semantic Segmentation},
  author={Huang, Ye and Kang, Di and Jia, Wenjing and He, Xiangjian and Liu, Liu},
  journal={arXiv preprint arXiv:2101.07434},
  year={2021}
}

@article{lin2022structtoken,
  title={StructToken: Rethinking Semantic Segmentation with Structural Prior},
  author={Lin, Fangjian and Liang, Zhanhao and He, Junjun and Zheng, Miao and Tian, Shengwei and Chen, Kai},
  journal={arXiv preprint arXiv:2203.12612},
  year={2022}
}

@article{huang2022car,
  title={CAR: Class-aware Regularizations for Semantic Segmentation},
  author={Huang, Ye and Kang, Di and Chen, Liang and Zhe, Xuefei and Jia, Wenjing and He, Xiangjian and Bao, Linchao},
  journal={arXiv preprint arXiv:2203.07160},
  year={2022}
}

@inproceedings{mottaghi2014role,
  title={The role of context for object detection and semantic segmentation in the wild},
  author={Mottaghi, Roozbeh and Chen, Xianjie and Liu, Xiaobai and Cho, Nam-Gyu and Lee, Seong-Whan and Fidler, Sanja and Urtasun, Raquel and Yuille, Alan},
  booktitle=CVPR,
  pages={891--898},
  year={2014}
}

@inproceedings{chen2019hybrid,
  title={Hybrid task cascade for instance segmentation},
  author={Chen, Kai and Pang, Jiangmiao and Wang, Jiaqi and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Shi, Jianping and Ouyang, Wanli and others},
  booktitle=CVPR,
  pages={4974--4983},
  year={2019}
}

@inproceedings{dai2021dynamic,
  title={Dynamic head: Unifying object detection heads with attentions},
  author={Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
  booktitle=CVPR,
  pages={7373--7382},
  year={2021}
}

@inproceedings{li2021improved,
  title={Mvitv2: Improved multiscale vision transformers for classification and detection},
  author={Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and Mangalam, Karttikeya and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle=CVPR,
  pages={4804--4814},
  year={2022}
}

@article{zhu2022uni,
  title={Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs},
  author={Zhu, Jinguo and Zhu, Xizhou and Wang, Wenhai and Wang, Xiaohua and Li, Hongsheng and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2206.04674},
  year={2022}
}

@article{wu2022p2t,
  title={P2T: Pyramid pooling transformer for scene understanding},
  author={Wu, Yu-Huan and Liu, Yun and Zhan, Xin and Cheng, Ming-Ming},
  journal={TPAMI},
  year={2022}
}


@article{fang2022unleashing,
  title={Unleashing vanilla vision transformer with masked image modeling for object detection},
  author={Fang, Yuxin and Yang, Shusheng and Wang, Shijie and Ge, Yixiao and Shan, Ying and Wang, Xinggang},
  journal={arXiv preprint arXiv:2204.02964},
  year={2022}
}

@article{li2022exploring,
  title={Exploring plain vision transformer backbones for object detection},
  author={Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2203.16527},
  year={2022}
}

@article{park2022vision,
  title={How Do Vision Transformers Work?},
  author={Park, Namuk and Kim, Songkuk},
  journal={arXiv preprint arXiv:2202.06709},
  year={2022}
}

@article{si2022inception,
  title={Inception Transformer},
  author={Si, Chenyang and Yu, Weihao and Zhou, Pan and Zhou, Yichen and Wang, Xinchao and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2205.12956},
  year={2022}
}

@inproceedings{wu2022pale,
  title={Pale transformer: A general vision transformer backbone with pale-shaped attention},
  author={Wu, Sitong and Wu, Tianyi and Tan, Haoru and Guo, Guodong},
  booktitle=AAAI,
  volume={36},
  number={3},
  pages={2731--2739},
  year={2022}
}

@inproceedings{bodla2017soft,
  title={Soft-NMS--improving object detection with one line of code},
  author={Bodla, Navaneeth and Singh, Bharat and Chellappa, Rama and Davis, Larry S},
  booktitle=ICCV,
  pages={5561--5569},
  year={2017}
}

@article{rao2022hornet,
  title={HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions},
  author={Rao, Yongming and Zhao, Wenliang and Tang, Yansong and Zhou, Jie and Lim, Ser-Nam and Lu, Jiwen},
  journal={arXiv preprint arXiv:2207.14284},
  year={2022}
}

@article{li2022maskdino,
  title={Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation},
  author={Li, Feng and Zhang, Hao and Liu, Shilong and Zhang, Lei and Ni, Lionel M and Shum, Heung-Yeung and others},
  journal={arXiv preprint arXiv:2206.02777},
  year={2022}
}

@article{wei2022kdswin,
  title={Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation},
  author={Wei, Yixuan and Hu, Han and Xie, Zhenda and Zhang, Zheng and Cao, Yue and Bao, Jianmin and Chen, Dong and Guo, Baining},
  journal={arXiv preprint arXiv:2205.14141},
  year={2022}
}



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{jia2022visual,
  title={Visual prompt tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2203.12119},
  year={2022}
}

@article{bahng2022exploring,
  title={Exploring visual prompts for adapting large-scale models},
  author={Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  journal={arXiv preprint arXiv:2203.17274},
  volume={1},
  number={3},
  pages={4},
  year={2022}
}

@article{chen2022adaptformer,
  title={AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition},
  author={Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal={arXiv preprint arXiv:2205.13535},
  year={2022}
}

@inproceedings{japanese-clip,
  author={Makoto Shiin, Tianyu Zhao, Kei Sawada},
  title={Construction and Public Release of Language Image Pretraining Models in Japanese},
  booktitle={MIRU},
  year={2022}
}

@inproceedings{carlsson2022mclip,
  title={Cross-lingual and multilingual clip},
  author={Carlsson, Fredrik and Eisen, Philipp and Rekathati, Faton and Sahlgren, Magnus},
  booktitle={LREC},
  pages={6848--6854},
  year={2022}
}

@inproceedings{hendrycks2021imagenet_a,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle=CVPR,
  pages={15262--15271},
  year={2021}
}

@article{aggarwal2020xtd,
  title={Towards zero-shot Cross-lingual Image retrieval},
  author={Aggarwal, Pranav and Kale, Ajinkya},
  journal={arXiv preprint arXiv:2012.05107},
  year={2020}
}

@article{jain2021mural,
  title={Mural: multimodal, multitask retrieval across languages},
  author={Jain, Aashi and Guo, Mandy and Srinivasan, Krishna and Chen, Ting and Kudugunta, Sneha and Jia, Chao and Yang, Yinfei and Baldridge, Jason},
  journal={arXiv preprint arXiv:2109.05125},
  year={2021}
}

@article{wang2019imagenet_sketch,
  title={Learning robust global representations by penalizing local predictive power},
  author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
  journal=NeurIPS,
  volume={32},
  year={2019}
}

@inproceedings{recht2019imagenetv2,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle=ICML,
  pages={5389--5400},
  year={2019}
}


@article{barbu2019objectnet,
  title={Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
  author={Barbu, Andrei and Mayo, David and Alverio, Julian and Luo, William and Wang, Christopher and Gutfreund, Dan and Tenenbaum, Josh and Katz, Boris},
  journal=NeurIPS,
  volume={32},
  year={2019}
}

@article{zhao2024mirb,
  title={Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning},
  author={Zhao, Bingchen and Zong, Yongshuo and Zhang, Letian and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2406.12742},
  year={2024}
}

@article{lu2024wildvision,
  title={WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences},
  author={Lu, Yujie and Jiang, Dongfu and Chen, Wenhu and Wang, William Yang and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.11069},
  year={2024}
}



@inproceedings{duan2024vlmevalkit,
  title={Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={11198--11201},
  year={2024}
}

@article{huang2023kosmos1,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}

@article{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}

@inproceedings{sun2023emu,
  title={Generative pretraining in multimodality},
  author={Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  booktitle=ICLR,
  year={2024}
}

@inproceedings{dong2023dreamllm,
  title={DreamLLM: Synergistic Multimodal Comprehension and Creation},
  author={Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others},
  booktitle=ICLR,
  year={2024}
}

@article{instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal=NeurIPS,
  volume={36},
  year={2024}
}


@article{chen2023shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
  journal={arXiv preprint arXiv:2306.15195},
  year={2023}
}

@inproceedings{wang2023allseeing,
  title={The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World},
  author={Wang, Weiyun and Shi, Min and Li, Qingyun and Wang, Wenhai and Huang, Zhenhang and Xing, Linjie and Chen, Zhe and Li, Hao and Zhu, Xizhou and Cao, Zhiguo and others},
  booktitle=ICLR,
  year={2024}
}


@article{peng2023kosmos2,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2306.14824},
  year={2023}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal=NeurIPS,
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{hao2022metavlm,
  title={Language models are general-purpose interfaces},
  author={Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2206.06336},
  year={2022}
}

@inproceedings{hendrycks2021imagenet_r,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle=ICCV,
  pages={8340--8349},
  year={2021}
}

@article{bianchi2021clip_italian,
  title={Contrastive Language-Image Pre-training for the Italian Language},
  author={Bianchi, Federico and Attanasio, Giuseppe and Pisoni, Raphael and Terragni, Silvia and Sarti, Gabriele and Lakshmi, Sri},
  journal={arXiv preprint arXiv:2108.08688},
  year={2021}
}


@article{yang2022cnclip,
  title={Chinese clip: Contrastive vision-language pretraining in chinese},
  author={Yang, An and Pan, Junshu and Lin, Junyang and Men, Rui and Zhang, Yichang and Zhou, Jingren and Zhou, Chang},
  journal={arXiv preprint arXiv:2211.01335},
  year={2022}
}

@article{zhang2022neural,
  title={Neural Prompt Search},
  author={Zhang, Yuanhan and Zhou, Kaiyang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2206.04673},
  year={2022}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle=ICML,
  pages={12888--12900},
  year={2022}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle=ICML,
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}


@inproceedings{wang2023beit3,
  title={Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle=CVPR,
  pages={19175--19186},
  year={2023}
}

@article{li2021albef,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal=NeurIPS,
  volume={34},
  pages={9694--9705},
  year={2021}
}

@article{xu2023lvlm,
  title={Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models},
  author={Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2306.09265},
  year={2023}
}

@article{zeng2023lynx,
  title={What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?},
  author={Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
  journal={arXiv preprint arXiv:2307.02469},
  year={2023}
}

@article{shao2023tiny,
  title={Tiny lvlm-ehub: Early multimodal experiments with bard},
  author={Shao, Wenqi and Hu, Yutao and Gao, Peng and Lei, Meng and Zhang, Kaipeng and Meng, Fanqing and Xu, Peng and Huang, Siyuan and Li, Hongsheng and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2308.03729},
  year={2023}
}

@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    howpublished={\url{https://llava-vl.github.io/blog/2024-01-30-llava-next/}},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}


@article{jie2022convolutional,
  title={Convolutional bypasses are better vision transformer adapters},
  author={Jie, Shibo and Deng, Zhi-Hong},
  journal={arXiv preprint arXiv:2207.07039},
  year={2022}
}

@inproceedings{fang2022eva,
  title={EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle=CVPR,
  pages={19358--19369},
  year={2023}
}

@article{liu2023llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal=NeurIPS,
  volume={36},
  year={2023}
}

@article{liu2023ocrbench,
  title={On the hidden mystery of ocr in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Li, Hongliang and Yu, Wenwen and Huang, Mingxin and Peng, Dezhi and Liu, Mingyu and Chen, Mingrui and Li, Chunyuan and Jin, Lianwen and others},
  journal={arXiv preprint arXiv:2305.07895},
  year={2023}
}


@inproceedings{mathew2021docvqa,
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle=WACV,
  pages={2200--2209},
  year={2021}
}

@article{wang2023visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal=NeurIPS,
  volume={36},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zhang2022rest,
  title={Rest v2: simpler, faster and stronger},
  author={Zhang, Qinglong and Yang, Yu-Bin},
  journal=NeurIPS,
  volume={35},
  pages={36440--36452},
  year={2022}
}

@article{zhang2021rest,
  title={Rest: An efficient transformer for visual recognition},
  author={Zhang, Qinglong and Yang, Yu-Bin},
  journal=NeurIPS,
  volume={34},
  pages={15475--15485},
  year={2021}
}

@article{fang2023eva02,
  title={Eva-02: A visual representation for neon genesis},
  author={Fang, Yuxin and Sun, Quan and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.11331},
  year={2023}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal=NeurIPS,
  volume={35},
  pages={16344--16359},
  year={2022}
}


@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal=NeurIPS,
  volume={25},
  year={2012}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal=NeurIPS,
  volume={35},
  pages={24824--24837},
  year={2022}
}


@article{openai2023gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}



@article{schuhmann2022laion5b,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal=NeurIPS,
  volume={35},
  pages={25278--25294},
  year={2022}
}

@misc{byeon2022coyo,
  title={COYO-700m: Image-text pair dataset},
  author={Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  howpublished  = {\url{https://github.com/kakaobrain/coyo-dataset}},
  year={2022}
}

@inproceedings{changpinyo2021cc12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle=CVPR,
  pages={3558--3568},
  year={2021}
}


@inproceedings{chen2022vitadapter,
  title={Vision Transformer Adapter for Dense Predictions},
  author={Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  booktitle=ICLR,
  year={2022}
}

@article{wei2023skywork,
  title={Skywork: A More Open Bilingual Foundation Model},
  author={Wei, Tianwen and Zhao, Liang and Zhang, Lichang and Zhu, Bo and Wang, Lijie and Yang, Haihua and Li, Biye and Cheng, Cheng and L{\"u}, Weiwei and Hu, Rui and others},
  journal={arXiv preprint arXiv:2310.19341},
  year={2023}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle=CVPR,
  pages={6700--6709},
  year={2019}
}

@article{ustalov2023toloka,
  title={Toloka Visual Question Answering Benchmark},
  author={Ustalov, Dmitry and Pavlichenko, Nikita and Koshelev, Sergey and Likhobaba, Daniil and Smirnova, Alisa},
  journal={arXiv preprint arXiv:2309.16511},
  year={2023}
}

@article{driess2023palme,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{liu2023controlllm,
  title={ControlLLM: Augment Language Models with Tools by Searching on Graphs},
  author={Liu, Zhaoyang and Lai, Zeqiang and Gao, Zhangwei and Cui, Erfei and Zhu, Xizhou and Lu, Lewei and Chen, Qifeng and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  journal={arXiv preprint arXiv:2310.17796},
  year={2023}
}

@article{yang2023gpt4tools,
  title={Gpt4tools: Teaching large language model to use tools via self-instruction},
  author={Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{ye2023mplug2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2311.04257},
  year={2023}
}

@article{liu2024convbench,
  title={ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models},
  author={Liu, Shuo and Ying, Kaining and Zhang, Hao and Yang, Yue and Lin, Yuqi and Zhang, Tianle and Li, Chuanhao and Qiao, Yu and Luo, Ping and Shao, Wenqi and others},
  journal={arXiv preprint arXiv:2403.20194},
  year={2024}
}

@article{chen2023videollm,
  title={Videollm: Modeling video sequence with large language models},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Jiahao and Xu, Jilan and Huang, Yifei and Pan, Junting and Wang, Yi and Wang, Yali and Qiao, Yu and Lu, Tong and others},
  journal={arXiv preprint arXiv:2305.13292},
  year={2023}
}

@article{wang2024internvideo2,
  title={InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={arXiv preprint arXiv:2403.15377},
  year={2024}
}

@misc{opencompass2023,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@article{ormazabal2024reka,
  title={Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models},
  author={Ormazabal, Aitor and Zheng, Che and d'Autume, Cyprien de Masson and Yogatama, Dani and Fu, Deyu and Ong, Donovan and Chen, Eric and Lamprecht, Eugenie and Pham, Hai and Ong, Isaac and others},
  journal={arXiv preprint arXiv:2404.12387},
  year={2024}
}

@article{lu2022scienceqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal=NeurIPS,
  volume={35},
  pages={2507--2521},
  year={2022}
}

@article{ma2024groma,
  title={Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models},
  author={Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan},
  journal={arXiv preprint arXiv:2404.13013},
  year={2024}
}

@article{mmtbench,
  title={MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI},
  author={Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and Lei, Jiayi and Lu, Quanfeng and Chen, Runjian and Xu, Peng and Zhang, Renrui and Zhang, Haozhe and Gao, Peng and Wang, Yali and Qiao, Yu and Luo, Ping and Zhang, Kaipeng and Shao, Wenqi},
  journal={arXiv preprint arXiv:2404.16006},
  year={2024}
}

@inproceedings{das2017visualdialog,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle=CVPR,
  pages={326--335},
  year={2017}
}

@article{li2022paddleocr,
  title={PP-OCRv3: More attempts for the improvement of ultra lightweight OCR system},
  author={Li, Chenxia and Liu, Weiwei and Guo, Ruoyu and Yin, Xiaoting and Jiang, Kaitao and Du, Yongkun and Du, Yuning and Zhu, Lingfeng and Lai, Baohua and Hu, Xiaoguang and others},
  journal={arXiv preprint arXiv:2206.03001},
  year={2022}
}

@article{chen2023sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  journal={arXiv preprint arXiv:2311.12793},
  year={2023}
}

@inproceedings{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle=ICCV,
  pages={11888--11898},
  year={2023}
}

@article{lu2024ovis,
  title={Ovis: Structural Embedding Alignment for Multimodal Large Language Model},
  author={Lu, Shiyin and Li, Yang and Chen, Qing-Guo and Xu, Zhao and Luo, Weihua and Zhang, Kaifu and Ye, Han-Jia},
  journal={arXiv preprint arXiv:2405.20797},
  year={2024}
}

@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}


@article{guan2023hallusionbench,
  title={Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  journal={arXiv preprint arXiv:2310.14566},
  year={2023}
}

@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle=CVPR,
  pages={26689--26699},
  year={2024}
}

@article{lu2023mathvista,
  title={Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2310.02255},
  year={2023}
}

@article{li2024llavaov,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@article{wang2024qwen2vl,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@misc{mimo_vl_7b,
  title = {MiMo-VL Technical Report},
  author = {LLM-Core Xiaomi},
  howpublished = {\url{https://github.com/XiaomiMiMo/MiMo-VL/blob/main/MiMo-VL-Technical-Report.pdf}},
  year = {2025}
}

@article{bai2025qwen2,
  title={Qwen2. 5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{teknium2024hermes3,
  title={Hermes 3 Technical Report},
  author={Teknium, Ryan and Quesnelle, Jeffrey and Guang, Chen},
  journal={arXiv preprint arXiv:2408.11857},
  year={2024}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    howpublished = {\url{https://qwenlm.github.io/blog/qwen2.5/}},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@article{dubey2024llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{li2023seed,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}

@article{yang2023mmreact,
  title={Mm-react: Prompting chatgpt for multimodal reasoning and action},
  author={Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
  journal={arXiv preprint arXiv:2303.11381},
  year={2023}
}

@article{yue2023mmmu,
  title={Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  journal={arXiv preprint arXiv:2311.16502},
  year={2023}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle=ICML,
  pages={8748--8763},
  year={2021}
}

@inproceedings{zhu2023minigpt4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle=ICLR,
  year={2024}
}

@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

@article{sun2023moss,
  title={Moss: Training conversational language models from synthetic data},
  author={Sun, Tianxiang and Zhang, Xiaotian and He, Zhengfu and Li, Peng and Cheng, Qinyuan and Yan, Hang and Liu, Xiangyang and Shao, Yunfan and Tang, Qiong and Zhao, Xingjian and others},
  journal={arXiv preprint arXiv:2307.15020},
  volume={7},
  year={2023}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle=ICLR,
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle=ACL,
  pages={320--335},
  year={2022}
}

@inproceedings{caesar2018cocostuff,
  title={Coco-stuff: Thing and stuff classes in context},
  author={Caesar, Holger and Uijlings, Jasper and Ferrari, Vittorio},
  booktitle=CVPR,
  pages={1209--1218},
  year={2018}
}

@inproceedings{chen2022pali,
  title={PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  booktitle=ICLR,
  year={2022}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{zheng2023vicuna,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{baichuan2023baichuan2,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@misc{claude3series2024,
  author = {{Anthropic}},
  title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
  year = {2024},
  howpublished = {\url{https://www.anthropic.com}},
  url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model\_Card\_Claude\_3.pdf}
}

@misc{step1v2023,
  author = {{StepFun Research Team}},
  title = {Step-1V: A Hundred Billion Parameter Multimodal Large Model},
  howpublished = {\url{https://platform.stepfun.com}},
  year = {2024}
}

@article{bi2024deepseekllm,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}

@article{dong2024xc24khd,
  title={InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={arXiv preprint arXiv:2404.06512},
  year={2024}
}

@misc{hptpro2024,
  author = {{HyperGAI Research Team}},
  title = {Introducing HPT: A Family of Leading Multimodal LLMs},
  year = {2024},
  howpublished = {\url{https://www.hypergai.com/blog/introducing-hpt-a-family-of-leading-multimodal-llms}}
}

@article{ye2023mplugdocowl,
  title={mplug-docowl: Modularized multimodal large language model for document understanding},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Dan, Yuhao and Zhao, Chenlin and Xu, Guohai and Li, Chenliang and Tian, Junfeng and others},
  journal={arXiv preprint arXiv:2307.02499},
  year={2023}
}

@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

@article{lin2024moellava,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@article{beyer2020imagenetreal,
  title={Are we done with imagenet?},
  author={Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  journal={arXiv preprint arXiv:2006.07159},
  year={2020}
}

@article{2023interngpt,
  title={InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language},
  author={Liu, Zhaoyang and He, Yinan and Wang, Wenhai and Wang, Weiyun and Wang, Yi and Chen, Shoufa and Zhang, Qinglong and Lai, Zeqiang and Yang, Yang and Li, Qingyun and Yu, Jiashuo and others},
  journal={arXiv preprint arXiv:2305.05662},
  year={2023}
}

@article{wu2023visual,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{mu2023embodiedgpt,
  title={Embodiedgpt: Vision-language pre-training via embodied chain of thought},
  author={Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{ordonez2011sbu,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
  journal=NeurIPS,
  volume={24},
  year={2011}
}

@inproceedings{sharma2018cc3m,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle=ACL,
  year={2018}
}

@misc{schuhmann2022laioncoco,
  title={Laion coco: 600m synthetic captions from laion2b-en.},
  author={Schuhmann, Christoph and Kpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
  howpublished = {\url{https://laion.ai/blog/laion-coco/}},
  year={2022}
}

@article{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{lin2023sphinx,
  title={Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models},
  author={Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
  journal={arXiv preprint arXiv:2311.07575},
  year={2023}
}

@article{lv2023kosmos2_5,
  title={Kosmos-2.5: A multimodal literate model},
  author={Lv, Tengchao and Huang, Yupan and Chen, Jingye and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and Dong, Li and Luo, Weiyao and others},
  journal={arXiv preprint arXiv:2309.11419},
  year={2023}
}

@article{li2024miniGemini,
  title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models},
  author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
  journal={arXiv preprint arXiv:2403.18814},
  year={2024}
}

@article{liu2024textmonkey,
  title={TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
  author={Liu, Yuliang and Yang, Biao and Liu, Qiang and Li, Zhang and Ma, Zhiyin and Zhang, Shuo and Bai, Xiang},
  journal={arXiv preprint arXiv:2403.04473},
  year={2024}
}

@inproceedings{zhai2023siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle=ICCV,
  pages={11975--11986},
  year={2023}
}

@article{tschannen2025siglip,
  title={Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features},
  author={Tschannen, Michael and Gritsenko, Alexey and Wang, Xiao and Naeem, Muhammad Ferjad and Alabdulmohsin, Ibrahim and Parthasarathy, Nikhil and Evans, Talfan and Beyer, Lucas and Xia, Ye and Mustafa, Basil and others},
  journal={arXiv preprint arXiv:2502.14786},
  year={2025}
}

@inproceedings{shi2017rctw17,
  title={Icdar2017 competition on reading chinese text in the wild (rctw-17)},
  author={Shi, Baoguang and Yao, Cong and Liao, Minghui and Yang, Mingkun and Xu, Pei and Cui, Linyan and Belongie, Serge and Lu, Shijian and Bai, Xiang},
  booktitle=ICDAR,
  volume={1},
  pages={1429--1434},
  year={2017}
}

@inproceedings{chng2019art,
  title={Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art},
  author={Chng, Chee Kheng and Liu, Yuliang and Sun, Yipeng and Ng, Chun Chet and Luo, Canjie and Ni, Zihan and Fang, ChuanMing and Zhang, Shuaitao and Han, Junyu and Ding, Errui and others},
  booktitle=ICDAR,
  pages={1571--1576},
  year={2019}
}

@inproceedings{zhang2019rects,
  title={Icdar 2019 robust reading challenge on reading chinese text on signboard},
  author={Zhang, Rui and Zhou, Yongsheng and Jiang, Qianyi and Song, Qi and Li, Nan and Zhou, Kai and Wang, Lei and Wang, Dong and Liao, Minghui and Yang, Mingkun and others},
  booktitle=ICDAR,
  pages={1577--1581},
  year={2019}
}

@misc{xai2024grokv,
  author = {X.ai},
  title = {Grok-1.5 Vision Preview},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@inproceedings{sun2019lsvt,
  title={ICDAR 2019 competition on large-scale street view text with partial labeling-RRC-LSVT},
  author={Sun, Yipeng and Ni, Zihan and Chng, Chee-Kheng and Liu, Yuliang and Luo, Canjie and Ng, Chun Chet and Han, Junyu and Ding, Errui and Liu, Jingtuo and Karatzas, Dimosthenis and others},
  booktitle=ICDAR,
  pages={1557--1562},
  year={2019}
}

@inproceedings{qi2022dureadervis,
  title={DuReadervis: A Chinese dataset for open-domain document visual question answering},
  author={Qi, Le and Lv, Shangwen and Li, Hongyu and Liu, Jing and Zhang, Yu and She, Qiaoqiao and Wu, Hua and Wang, Haifeng and Liu, Ting},
  booktitle=ACL,
  pages={1338--1351},
  year={2022}
}

@article{ye2023ureader,
  title={Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Xu, Guohai and Li, Chenliang and Tian, Junfeng and Qian, Qi and Zhang, Ji and others},
  journal={arXiv preprint arXiv:2310.05126},
  year={2023}
}

@article{xu2024llava_uhd,
  title={LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images},
  author={Xu, Ruyi and Yao, Yuan and Guo, Zonghao and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Sun, Maosong and Huang, Gao},
  journal={arXiv preprint arXiv:2403.11703},
  year={2024}
}

@article{li2023otterhd,
  title={Otterhd: A high-resolution multi-modality model},
  author={Li, Bo and Zhang, Peiyuan and Yang, Jingkang and Zhang, Yuanhan and Pu, Fanyi and Liu, Ziwei},
  journal={arXiv preprint arXiv:2311.04219},
  year={2023}
}

@article{hu2024mplug_docowl_1_5,
  title={mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding},
  author={Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
  journal={arXiv preprint arXiv:2403.12895},
  year={2024}
}

@article{luo2024llava_hr,
  title={Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models},
  author={Luo, Gen and Zhou, Yiyi and Zhang, Yuxin and Zheng, Xiawu and Sun, Xiaoshuai and Ji, Rongrong},
  journal={arXiv preprint arXiv:2403.03003},
  year={2024}
}

@article{wei2023vary,
  title={Vary: Scaling up the vision vocabulary for large vision-language models},
  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2312.06109},
  year={2023}
}


@article{hong2023cogagent,
  title={Cogagent: A visual language model for gui agents},
  author={Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv preprint arXiv:2312.08914},
  year={2023}
}

@article{wang2024allseeingv2,
  title={The All-Seeing Project V2: Towards General Relation Comprehension of the Open World},
  author={Wang, Weiyun and Ren, Yiming and Luo, Haowen and Li, Tiantong and Yan, Chenxiang and Chen, Zhe and Wang, Wenhai and Li, Qingyun and Lu, Lewei and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2402.19474},
  year={2024}
}

@article{tian2024mminterleaved,
  title={Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer},
  author={Tian, Changyao and Zhu, Xizhou and Xiong, Yuwen and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Chen, Yuntao and Lu, Lewei and Lu, Tong and Zhou, Jie and others},
  journal={arXiv preprint arXiv:2401.10208},
  year={2024}
}

@inproceedings{zhang2023llama-adapter,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
  booktitle=ICLR,
  year={2024}
}

@article{gao2023llama-adapterv2,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{zhang2023gpt4roi,
  title={Gpt4roi: Instruction tuning large language model on region-of-interest},
  author={Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
  journal={arXiv preprint arXiv:2307.03601},
  year={2023}
}

@article{wu2023nextgpt,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2309.05519},
  year={2023}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{lai2023lisa,
  title={Lisa: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  journal={arXiv preprint arXiv:2308.00692},
  year={2023}
}

@article{yang2023gpt-4v,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  volume={9},
  year={2023}
}

@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@article{zhang2023video-llama,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle=ECCV,
  pages={742--758},
  year={2020}
}

@inproceedings{goyal2017vqav2,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle=CVPR,
  pages={6904--6913},
  year={2017}
}

@inproceedings{marino2019okvqa,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle=CVPR,
  pages={3195--3204},
  year={2019}
}

@inproceedings{schwenk2022aokvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle=ECCV,
  pages={146--162},
  year={2022}
}


@inproceedings{kembhavi2016ai2d,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle=ECCV,
  pages={235--251},
  year={2016}
}

@inproceedings{das2017visdial,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle=CVPR,
  pages={326--335},
  year={2017}
}

@inproceedings{masry2022chartqa,
  title={ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},
  author={Masry, Ahmed and Do, Xuan Long and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  booktitle=ACL,
  pages={2263--2279},
  year={2022}
}

@inproceedings{biten2019stvqa,
  title={Scene text visual question answering},
  author={Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Mar{\c{c}}al and Valveny, Ernest and Jawahar, CV and Karatzas, Dimosthenis},
  booktitle=ICCV,
  pages={4291--4301},
  year={2019}
}

@inproceedings{clark2017docqa,
  title={Simple and Effective Multi-Paragraph Reading Comprehension},
  author={Clark, Christopher and Gardner, Matt},
  booktitle=ACL,
  pages={845--855},
  year={2018}
}

@inproceedings{lu2022learn,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}


@inproceedings{zhang2019raven, 
    title={RAVEN: A Dataset for Relational and Analogical Visual rEasoNing}, 
    author={Zhang, Chi and Gao, Feng and Jia, Baoxiong and Zhu, Yixin and Zhu, Song-Chun}, 
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    year={2019}
}

@article{tito2022hierarchical,
  title={Hierarchical multimodal transformers for Multi-Page DocVQA},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  journal={arXiv preprint arXiv:2212.05935},
  year={2022}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@inproceedings{mishra2019ocrvqa,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle=ICDAR,
  pages={947--952},
  year={2019}
}

@InProceedings{Methani_2020_WACV,
author = {Methani, Nitesh and Ganguly, Pritha and Khapra, Mitesh M. and Kumar, Pratyush},
title = {PlotQA: Reasoning over Scientific Plots},
booktitle = {The IEEE Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}


@article{zhang2023llavar,
  title={Llavar: Enhanced visual instruction tuning for text-rich image understanding},
  author={Zhang, Yanzhe and Zhang, Ruiyi and Gu, Jiuxiang and Zhou, Yufan and Lipka, Nedim and Yang, Diyi and Sun, Tong},
  journal={arXiv preprint arXiv:2306.17107},
  year={2023}
}

@inproceedings{yu2016refcoco,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle=ECCV,
  pages={69--85},
  year={2016}
}


@article{zhao2023svit,
  title={Svit: Scaling up visual instruction tuning},
  author={Zhao, Bo and Wu, Boya and Huang, Tiejun},
  journal={arXiv preprint arXiv:2307.04087},
  year={2023}
}

@article{liu2023lrv-instruction,
  title={Aligning Large Multi-Modal Model with Robust Instruction Tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}

@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle=WACV,
  pages={1697--1706},
  year={2022}
}

@inproceedings{xu2016msrvtt,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle=CVPR,
  pages={5288--5296},
  year={2016}
}

@article{xue2023alleviating,
  title={Alleviating data insufficiency for Chinese sign language recognition},
  author={Xue, Wanli and Liu, Jingze and Yan, Siyi and Zhou, Yuxi and Yuan, Tiantian and Guo, Qing},
  journal={Visual Intelligence},
  volume={1},
  number={1},
  pages={26},
  year={2023},
  publisher={Springer}
}

@misc{gpt4v,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  howpublished={\url{https://cdn.openai.com/papers/GPTV\_System\_Card.pdf}},
  year={2023}
}
@misc{chatgpt4o,
  title={GPT-4o System Card},
  author={OpenAI},
  howpublished={\url{https://openai.com/index/gpt-4o-system-card/}},
  year={2025}
}

@misc{chatgpt,
  title={Introducing ChatGPT},
  author={OpenAI},
  howpublished={\url{https://openai.com/index/chatgpt/}},
  year={2025}
}

@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle=ICLR,
  year={2020}
}

@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}

@article{huang2023ceval,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@article{Zhang2023gaokao,
  title={Evaluating the performance of large language models on gaokao benchmark},
  author={Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2305.12474},
  year={2023}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{naturalquestion,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019}
}

@article{sun2019investigating,
  title={Investigating prior knowledge for challenging chinese machine reading comprehension},
  author={Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={141--155},
  year={2020}
}

@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@inproceedings{sakaguchi2020winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle=AAAI,
  volume={34},
  pages={8732--8740},
  year={2020}
}

@inproceedings{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle=ACL,
  pages={4791--4800},
  year={2019}
}

@article{suzgun2023bigbench,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}


@article{cheng2024videollama2,
  title={VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}



@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}


@inproceedings{DBLP:conf/nips/HendrycksBKABTS21,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  editor       = {Joaquin Vanschoren and
                  Sai{-}Kit Yeung},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = {Proceedings of the Neural Information Processing Systems Track on
                  Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
                  2021, virtual},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HendrycksBKABTS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/ChenYKLWMXWX23,
  author       = {Wenhu Chen and
                  Ming Yin and
                  Max Ku and
                  Pan Lu and
                  Yixin Wan and
                  Xueguang Ma and
                  Jianyu Xu and
                  Xinyi Wang and
                  Tony Xia},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {TheoremQA: {A} Theorem-driven Question Answering Dataset},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {7889--7901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.489},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/ChenYKLWMXWX23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Fei2024QueryOC,
  title={Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora},
  author={Zhaoye Fei and Yunfan Shao and Linyang Li and Zhiyuan Zeng and Hang Yan and Xipeng Qiu and Dahua Lin},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.14624},
  url={https://api.semanticscholar.org/CorpusID:267301281}
}


@inproceedings{DBLP:conf/iclr/LoshchilovH19,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
  timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}

@article{chen2024agent,
  title={Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models},
  author={Chen, Zehui and Liu, Kuikun and Wang, Qiuchen and Liu, Jiangning and Lin, Dahua and Chen, Kai and Zhao, Feng},
  journal={work in progress},
  year={2024}
}

@inproceedings{ds-1000,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle=ICLR,
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}


@article{DBLP:journals/corr/abs-2306-01116,
  author       = {Guilherme Penedo and
                  Quentin Malartic and
                  Daniel Hesslow and
                  Ruxandra Cojocaru and
                  Alessandro Cappelli and
                  Hamza Alobeidli and
                  Baptiste Pannier and
                  Ebtesam Almazrouei and
                  Julien Launay},
  title        = {The RefinedWeb Dataset for Falcon {LLM:} Outperforming Curated Corpora
                  with Web Data, and Web Data Only},
  journal      = {CoRR},
  volume       = {abs/2306.01116},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.01116},
  doi          = {10.48550/ARXIV.2306.01116},
  eprinttype    = {arXiv},
  eprint       = {2306.01116},
  timestamp    = {Mon, 12 Jun 2023 16:25:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-01116.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2311-16867,
  author       = {Ebtesam Almazrouei and
                  Hamza Alobeidli and
                  Abdulaziz Alshamsi and
                  Alessandro Cappelli and
                  Ruxandra Cojocaru and
                  M{\'{e}}rouane Debbah and
                  {\'{E}}tienne Goffinet and
                  Daniel Hesslow and
                  Julien Launay and
                  Quentin Malartic and
                  Daniele Mazzotta and
                  Badreddine Noune and
                  Baptiste Pannier and
                  Guilherme Penedo},
  title        = {The Falcon Series of Open Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.16867},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.16867},
  doi          = {10.48550/ARXIV.2311.16867},
  eprinttype    = {arXiv},
  eprint       = {2311.16867},
  timestamp    = {Mon, 04 Dec 2023 14:05:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-16867.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2309-10305,
  author       = {Aiyuan Yang and
                  Bin Xiao and
                  Bingning Wang and
                  Borong Zhang and
                  Ce Bian and
                  Chao Yin and
                  Chenxu Lv and
                  Da Pan and
                  Dian Wang and
                  Dong Yan and
                  Fan Yang and
                  Fei Deng and
                  Feng Wang and
                  Feng Liu and
                  Guangwei Ai and
                  Guosheng Dong and
                  Haizhou Zhao and
                  Hang Xu and
                  Haoze Sun and
                  Hongda Zhang and
                  Hui Liu and
                  Jiaming Ji and
                  Jian Xie and
                  Juntao Dai and
                  Kun Fang and
                  Lei Su and
                  Liang Song and
                  Lifeng Liu and
                  Liyun Ru and
                  Luyao Ma and
                  Mang Wang and
                  Mickel Liu and
                  MingAn Lin and
                  Nuolan Nie and
                  Peidong Guo and
                  Ruiyang Sun and
                  Tao Zhang and
                  Tianpeng Li and
                  Tianyu Li and
                  Wei Cheng and
                  Weipeng Chen and
                  Xiangrong Zeng and
                  Xiaochuan Wang and
                  Xiaoxi Chen and
                  Xin Men and
                  Xin Yu and
                  Xuehai Pan and
                  Yanjun Shen and
                  Yiding Wang and
                  Yiyu Li and
                  Youxin Jiang and
                  Yuchen Gao and
                  Yupeng Zhang and
                  Zenan Zhou and
                  Zhiying Wu},
  title        = {Baichuan 2: Open Large-scale Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.10305},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.10305},
  doi          = {10.48550/ARXIV.2309.10305},
  eprinttype    = {arXiv},
  eprint       = {2309.10305},
  timestamp    = {Thu, 28 Sep 2023 16:22:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-10305.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/AinslieLJZLS23,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{GQA:} Training Generalized Multi-Query Transformer Models from Multi-Head
                  Checkpoints},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.298},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/AinslieLJZLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/BrownMRSKDNSSAA20,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23,
  author       = {Aakanksha Chowdhery and
                  Sharan Narang and
                  Jacob Devlin and
                  Maarten Bosma and
                  Gaurav Mishra and
                  Adam Roberts and
                  Paul Barham and
                  Hyung Won Chung and
                  Charles Sutton and
                  Sebastian Gehrmann and
                  Parker Schuh and
                  Kensen Shi and
                  Sasha Tsvyashchenko and
                  Joshua Maynez and
                  Abhishek Rao and
                  Parker Barnes and
                  Yi Tay and
                  Noam Shazeer and
                  Vinodkumar Prabhakaran and
                  Emily Reif and
                  Nan Du and
                  Ben Hutchinson and
                  Reiner Pope and
                  James Bradbury and
                  Jacob Austin and
                  Michael Isard and
                  Guy Gur{-}Ari and
                  Pengcheng Yin and
                  Toju Duke and
                  Anselm Levskaya and
                  Sanjay Ghemawat and
                  Sunipa Dev and
                  Henryk Michalewski and
                  Xavier Garcia and
                  Vedant Misra and
                  Kevin Robinson and
                  Liam Fedus and
                  Denny Zhou and
                  Daphne Ippolito and
                  David Luan and
                  Hyeontaek Lim and
                  Barret Zoph and
                  Alexander Spiridonov and
                  Ryan Sepassi and
                  David Dohan and
                  Shivani Agrawal and
                  Mark Omernick and
                  Andrew M. Dai and
                  Thanumalayan Sankaranarayana Pillai and
                  Marie Pellat and
                  Aitor Lewkowycz and
                  Erica Moreira and
                  Rewon Child and
                  Oleksandr Polozov and
                  Katherine Lee and
                  Zongwei Zhou and
                  Xuezhi Wang and
                  Brennan Saeta and
                  Mark Diaz and
                  Orhan Firat and
                  Michele Catasta and
                  Jason Wei and
                  Kathy Meier{-}Hellstern and
                  Douglas Eck and
                  Jeff Dean and
                  Slav Petrov and
                  Noah Fiedel},
  title        = {PaLM: Scaling Language Modeling with Pathways},
  journal      = {J. Mach. Learn. Res.},
  volume       = {24},
  pages        = {240:1--240:113},
  year         = {2023},
  url          = {http://jmlr.org/papers/v24/22-1144.html},
  timestamp    = {Thu, 19 Oct 2023 09:44:46 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/VaswaniSPUJGKP17,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {NeurIPS 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/ZhangS19a,
  author       = {Biao Zhang and
                  Rico Sennrich},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Root Mean Square Layer Normalization},
  booktitle    = {NeurIPS 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {12360--12371},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
  timestamp    = {Fri, 21 Oct 2022 14:36:34 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ZhangS19a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-05202,
  author       = {Noam Shazeer},
  title        = {{GLU} Variants Improve Transformer},
  journal      = {CoRR},
  volume       = {abs/2002.05202},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.05202},
  eprinttype    = {arXiv},
  eprint       = {2002.05202},
  timestamp    = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-05202.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/BaKH16,
  author       = {Lei Jimmy Ba and
                  Jamie Ryan Kiros and
                  Geoffrey E. Hinton},
  title        = {Layer Normalization},
  journal      = {CoRR},
  volume       = {abs/1607.06450},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.06450},
  eprinttype    = {arXiv},
  eprint       = {1607.06450},
  timestamp    = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/LabanKAFXJW23,
  author       = {Philippe Laban and
                  Wojciech Kryscinski and
                  Divyansh Agarwal and
                  Alexander R. Fabbri and
                  Caiming Xiong and
                  Shafiq Joty and
                  Chien{-}Sheng Wu},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {SummEdits: Measuring {LLM} Ability at Factual Reasoning Through The
                  Lens of Summarization},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {9662--9676},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.600},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LabanKAFXJW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/aaai/BiskZLGC20,
  author       = {Yonatan Bisk and
                  Rowan Zellers and
                  Ronan Le Bras and
                  Jianfeng Gao and
                  Yejin Choi},
  title        = {{PIQA:} Reasoning about Physical Commonsense in Natural Language},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {7432--7439},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6239},
  doi          = {10.1609/AAAI.V34I05.6239},
  timestamp    = {Mon, 04 Sep 2023 16:50:23 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/BiskZLGC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/MihaylovCKS18,
  author       = {Todor Mihaylov and
                  Peter Clark and
                  Tushar Khot and
                  Ashish Sabharwal},
  editor       = {Ellen Riloff and
                  David Chiang and
                  Julia Hockenmaier and
                  Jun'ichi Tsujii},
  title        = {Can a Suit of Armor Conduct Electricity? {A} New Dataset for Open
                  Book Question Answering},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural
                  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages        = {2381--2391},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/d18-1260},
  doi          = {10.18653/V1/D18-1260},
  timestamp    = {Fri, 06 Aug 2021 00:40:21 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/MihaylovCKS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Anonymouscibench,
  title={CIBench: Evaluating Your LLMs with a Code Interpreter Plugin},
  author={Anonymous},
  booktitle={Openreview},
  year={2024},
  url={https://openreview.net/forum?id=O8jmCw5puG}
}

@inproceedings{Anonymousmathbench,
  title={MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark},
  author={Anonymous},
  booktitle={Openreview},
  year={2024},
  url={https://openreview.net/forum?id=4vRO48RwVG}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{DBLP:journals/corr/abs-2307-11088,
  author       = {Chenxin An and
                  Shansan Gong and
                  Ming Zhong and
                  Mukai Li and
                  Jun Zhang and
                  Lingpeng Kong and
                  Xipeng Qiu},
  title        = {L-Eval: Instituting Standardized Evaluation for Long Context Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2307.11088},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.11088},
  doi          = {10.48550/ARXIV.2307.11088},
  eprinttype    = {arXiv},
  eprint       = {2307.11088},
  timestamp    = {Wed, 30 Aug 2023 16:03:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-11088.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2308-14508,
  author       = {Yushi Bai and
                  Xin Lv and
                  Jiajie Zhang and
                  Hongchang Lyu and
                  Jiankai Tang and
                  Zhidian Huang and
                  Zhengxiao Du and
                  Xiao Liu and
                  Aohan Zeng and
                  Lei Hou and
                  Yuxiao Dong and
                  Jie Tang and
                  Juanzi Li},
  title        = {LongBench: {A} Bilingual, Multitask Benchmark for Long Context Understanding},
  journal      = {CoRR},
  volume       = {abs/2308.14508},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.14508},
  doi          = {10.48550/ARXIV.2308.14508},
  eprinttype    = {arXiv},
  eprint       = {2308.14508},
  timestamp    = {Wed, 15 Nov 2023 15:09:17 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-14508.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{2023lmdeploy,
    title={LMDeploy: A Toolkit for Compressing, Deploying, and Serving LLM},
    author={LMDeploy Contributors},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    year={2023}
}

@article{parmar2024nemotron,
  title={Nemotron-4 15B Technical Report},
  author={Parmar, Jupinder and Prabhumoye, Shrimai and Jennings, Joseph and Patwary, Mostofa and Subramanian, Sandeep and Su, Dan and Zhu, Chen and Narayanan, Deepak and Jhunjhunwala, Aastha and Dattagupta, Ayush and others},
  journal={arXiv preprint arXiv:2402.16819},
  year={2024}
}


@inproceedings{romero2022accelerating,
  title={Accelerating collective communication in data parallel training across deep learning frameworks},
  author={Romero, Joshua and Yin, Junqi and Laanait, Nouamane and Xie, Bing and Young, M Todd and Treichler, Sean and Starchenko, Vitalii and Borisevich, Albina and Sergeev, Alex and Matheson, Michael},
  booktitle={19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages={1027--1040},
  year={2022}
}

@article{DBLP:journals/corr/abs-2304-08354,
  author       = {Yujia Qin and
                  Shengding Hu and
                  Yankai Lin and
                  Weize Chen and
                  Ning Ding and
                  Ganqu Cui and
                  Zheni Zeng and
                  Yufei Huang and
                  Chaojun Xiao and
                  Chi Han and
                  Yi Ren Fung and
                  Yusheng Su and
                  Huadong Wang and
                  Cheng Qian and
                  Runchu Tian and
                  Kunlun Zhu and
                  Shihao Liang and
                  Xingyu Shen and
                  Bokai Xu and
                  Zhen Zhang and
                  Yining Ye and
                  Bowen Li and
                  Ziwei Tang and
                  Jing Yi and
                  Yuzhang Zhu and
                  Zhenning Dai and
                  Lan Yan and
                  Xin Cong and
                  Yaxi Lu and
                  Weilin Zhao and
                  Yuxiang Huang and
                  Junxi Yan and
                  Xu Han and
                  Xian Sun and
                  Dahai Li and
                  Jason Phang and
                  Cheng Yang and
                  Tongshuang Wu and
                  Heng Ji and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {Tool Learning with Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.08354},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.08354},
  doi          = {10.48550/ARXIV.2304.08354},
  eprinttype    = {arXiv},
  eprint       = {2304.08354},
  timestamp    = {Fri, 01 Sep 2023 13:50:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-08354.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2307-04964,
  author       = {Rui Zheng and
                  Shihan Dou and
                  Songyang Gao and
                  Yuan Hua and
                  Wei Shen and
                  Binghai Wang and
                  Yan Liu and
                  Senjie Jin and
                  Qin Liu and
                  Yuhao Zhou and
                  Limao Xiong and
                  Lu Chen and
                  Zhiheng Xi and
                  Nuo Xu and
                  Wenbin Lai and
                  Minghao Zhu and
                  Cheng Chang and
                  Zhangyue Yin and
                  Rongxiang Weng and
                  Wensen Cheng and
                  Haoran Huang and
                  Tianxiang Sun and
                  Hang Yan and
                  Tao Gui and
                  Qi Zhang and
                  Xipeng Qiu and
                  Xuanjing Huang},
  title        = {Secrets of {RLHF} in Large Language Models Part {I:} {PPO}},
  journal      = {CoRR},
  volume       = {abs/2307.04964},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.04964},
  doi          = {10.48550/ARXIV.2307.04964},
  eprinttype    = {arXiv},
  eprint       = {2307.04964},
  timestamp    = {Fri, 27 Oct 2023 10:07:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-04964.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2305-18290,
  author       = {Rafael Rafailov and
                  Archit Sharma and
                  Eric Mitchell and
                  Stefano Ermon and
                  Christopher D. Manning and
                  Chelsea Finn},
  title        = {Direct Preference Optimization: Your Language Model is Secretly a
                  Reward Model},
  journal      = {CoRR},
  volume       = {abs/2305.18290},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.18290},
  doi          = {10.48550/ARXIV.2305.18290},
  eprinttype    = {arXiv},
  eprint       = {2305.18290},
  timestamp    = {Wed, 07 Jun 2023 14:31:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-18290.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nips/Ouyang0JAWMZASR22,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {NeurIPS 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:36 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2401-02954,
  author       = {Xiao Bi and
                  Deli Chen and
                  Guanting Chen and
                  Shanhuang Chen and
                  Damai Dai and
                  Chengqi Deng and
                  Honghui Ding and
                  Kai Dong and
                  Qiushi Du and
                  Zhe Fu and
                  Huazuo Gao and
                  Kaige Gao and
                  Wenjun Gao and
                  Ruiqi Ge and
                  Kang Guan and
                  Daya Guo and
                  Jianzhong Guo and
                  Guangbo Hao and
                  Zhewen Hao and
                  Ying He and
                  Wenjie Hu and
                  Panpan Huang and
                  Erhang Li and
                  Guowei Li and
                  Jiashi Li and
                  Yao Li and
                  Y. K. Li and
                  Wenfeng Liang and
                  Fangyun Lin and
                  Alex X. Liu and
                  Bo Liu and
                  Wen Liu and
                  Xiaodong Liu and
                  Xin Liu and
                  Yiyuan Liu and
                  Haoyu Lu and
                  Shanghao Lu and
                  Fuli Luo and
                  Shirong Ma and
                  Xiaotao Nie and
                  Tian Pei and
                  Yishi Piao and
                  Junjie Qiu and
                  Hui Qu and
                  Tongzheng Ren and
                  Zehui Ren and
                  Chong Ruan and
                  Zhangli Sha and
                  Zhihong Shao and
                  Junxiao Song and
                  Xuecheng Su and
                  Jingxiang Sun and
                  Yaofeng Sun and
                  Minghui Tang and
                  Bingxuan Wang and
                  Peiyi Wang and
                  Shiyu Wang and
                  Yaohui Wang and
                  Yongji Wang and
                  Tong Wu and
                  Y. Wu and
                  Xin Xie and
                  Zhenda Xie and
                  Ziwei Xie and
                  Yiliang Xiong and
                  Hanwei Xu and
                  R. X. Xu and
                  Yanhong Xu and
                  Dejian Yang and
                  Yuxiang You and
                  Shuiping Yu and
                  Xingkai Yu and
                  B. Zhang and
                  Haowei Zhang and
                  Lecong Zhang and
                  Liyue Zhang and
                  Mingchuan Zhang and
                  Minghua Zhang and
                  Wentao Zhang and
                  Yichao Zhang and
                  Chenggang Zhao and
                  Yao Zhao and
                  Shangyan Zhou and
                  Shunfeng Zhou and
                  Qihao Zhu and
                  Yuheng Zou},
  title        = {DeepSeek {LLM:} Scaling Open-Source Language Models with Longtermism},
  journal      = {CoRR},
  volume       = {abs/2401.02954},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.02954},
  doi          = {10.48550/ARXIV.2401.02954},
  eprinttype    = {arXiv},
  eprint       = {2401.02954},
  timestamp    = {Tue, 23 Jan 2024 15:39:22 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-02954.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2302-13971,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/ARXIV.2302.13971},
  eprinttype    = {arXiv},
  eprint       = {2302.13971},
  timestamp    = {Mon, 28 Aug 2023 21:26:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2307-09288,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2307-16789,
  author       = {Yujia Qin and
                  Shihao Liang and
                  Yining Ye and
                  Kunlun Zhu and
                  Lan Yan and
                  Yaxi Lu and
                  Yankai Lin and
                  Xin Cong and
                  Xiangru Tang and
                  Bill Qian and
                  Sihan Zhao and
                  Runchu Tian and
                  Ruobing Xie and
                  Jie Zhou and
                  Mark Gerstein and
                  Dahai Li and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world
                  APIs},
  journal      = {CoRR},
  volume       = {abs/2307.16789},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.16789},
  doi          = {10.48550/ARXIV.2307.16789},
  eprinttype    = {arXiv},
  eprint       = {2307.16789},
  timestamp    = {Tue, 07 Nov 2023 17:01:53 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-16789.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2302-04761,
  author       = {Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Roberto Dess{\`{\i}} and
                  Roberta Raileanu and
                  Maria Lomeli and
                  Luke Zettlemoyer and
                  Nicola Cancedda and
                  Thomas Scialom},
  title        = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  journal      = {CoRR},
  volume       = {abs/2302.04761},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.04761},
  doi          = {10.48550/ARXIV.2302.04761},
  eprinttype    = {arXiv},
  eprint       = {2302.04761},
  timestamp    = {Mon, 13 Feb 2023 14:23:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-04761.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2023t,
  title={T-eval: Evaluating the tool utilization capability step by step},
  author={Chen, Zehui and Du, Weihua and Zhang, Wenwei and Liu, Kuikun and Liu, Jiangning and Zheng, Miao and Zhuo, Jingming and Zhang, Songyang and Lin, Dahua and Chen, Kai and others},
  journal={arXiv preprint arXiv:2312.14033},
  year={2023}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca\_eval}}
}


@article{liu2023alignbench,
  title={Alignbench: Benchmarking chinese alignment of large language models},
  author={Liu, Xiao and Lei, Xuanyu and Wang, Shengyuan and Huang, Yue and Feng, Zhuoer and Wen, Bosi and Cheng, Jiale and Ke, Pei and Xu, Yifan and Tam, Weng Lam and others},
  journal={arXiv preprint arXiv:2311.18743},
  year={2023}
}

@article{ke2023critiquellm,
  title={Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation},
  author={Ke, Pei and Wen, Bosi and Feng, Zhuoer and Liu, Xiao and Lei, Xuanyu and Cheng, Jiale and Wang, Shengyuan and Zeng, Aohan and Dong, Yuxiao and Wang, Hongning and others},
  journal={arXiv preprint arXiv:2311.18702},
  year={2023}
}

@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}

@inproceedings{stienon2020learning,
  author = {Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
  title = {Learning to summarize from human feedback},
  booktitle=NeurIPS,
  year = 2020,
}

@inproceedings{pmlr-v162-ethayarajh22a,
  title={Understanding Dataset Difficulty with $$\backslash$mathcal $\{$V$\}$ $-Usable Information},
  author={Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle={International Conference on Machine Learning},
  pages={5988--6008},
  year={2022},
  organization={PMLR}
}

@article{cui2023ultrafeedback,
  title={Ultrafeedback: Boosting language models with high-quality feedback},
  author={Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Guanming and Zhu, Wei and Ni, Yuan and Xie, Guotong and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2310.01377},
  year={2023}
}

@article{DBLP:journals/corr/abs-2309-16609,
  author       = {Jinze Bai and
                  Shuai Bai and
                  Yunfei Chu and
                  Zeyu Cui and
                  Kai Dang and
                  Xiaodong Deng and
                  Yang Fan and
                  Wenbin Ge and
                  Yu Han and
                  Fei Huang and
                  Binyuan Hui and
                  Luo Ji and
                  Mei Li and
                  Junyang Lin and
                  Runji Lin and
                  Dayiheng Liu and
                  Gao Liu and
                  Chengqiang Lu and
                  Keming Lu and
                  Jianxin Ma and
                  Rui Men and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Chuanqi Tan and
                  Sinan Tan and
                  Jianhong Tu and
                  Peng Wang and
                  Shijie Wang and
                  Wei Wang and
                  Shengguang Wu and
                  Benfeng Xu and
                  Jin Xu and
                  An Yang and
                  Hao Yang and
                  Jian Yang and
                  Shusheng Yang and
                  Yang Yao and
                  Bowen Yu and
                  Hongyi Yuan and
                  Zheng Yuan and
                  Jianwei Zhang and
                  Xingxuan Zhang and
                  Yichang Zhang and
                  Zhenru Zhang and
                  Chang Zhou and
                  Jingren Zhou and
                  Xiaohuan Zhou and
                  Tianhang Zhu},
  title        = {Qwen Technical Report},
  journal      = {CoRR},
  volume       = {abs/2309.16609},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.16609},
  doi          = {10.48550/ARXIV.2309.16609},
  eprinttype    = {arXiv},
  eprint       = {2309.16609},
  timestamp    = {Wed, 18 Oct 2023 08:13:06 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-16609.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yue2024mmmu,
  title={Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark},
  author={Yue, Xiang and Zheng, Tianyu and Ni, Yuansheng and Wang, Yubo and Zhang, Kai and Tong, Shengbang and Sun, Yuxuan and Yin, Ming and Yu, Botao and Zhang, Ge and others},
  journal={arXiv preprint arXiv:2409.02813},
  year={2024}
}

@article{meng2024mmiu,
  title={MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models},
  author={Meng, Fanqing and Wang, Jin and Li, Chuanhao and Lu, Quanfeng and Tian, Hao and Liao, Jiaqi and Zhu, Xizhou and Dai, Jifeng and Qiao, Yu and Luo, Ping and others},
  journal={arXiv preprint arXiv:2408.02718},
  year={2024}
}

@inproceedings{zhang2025mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},
  booktitle=ECCV,
  pages={169--186},
  year={2025},
  organization={Springer}
}

@article{liu2024mmdu,
  title={MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs},
  author={Liu, Ziyu and Chu, Tao and Zang, Yuhang and Wei, Xilin and Dong, Xiaoyi and Zhang, Pan and Liang, Zijian and Xiong, Yuanjun and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2406.11833},
  year={2024}
}

@article{wang2024muirbench,
  title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
  author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
  journal={arXiv preprint arXiv:2406.09411},
  year={2024}
}

@article{fu2024blink,
  title={BLINK: Multimodal Large Language Models Can See but Not Perceive},
  author={Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},
  journal={arXiv preprint arXiv:2404.12390},
  year={2024}
}

@article{li2024r,
  title={R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions?},
  author={Li, Chunyi and Zhang, Jianbo and Zhang, Zicheng and Wu, Haoning and Tian, Yuan and Sun, Wei and Lu, Guo and Liu, Xiaohong and Min, Xiongkuo and Lin, Weisi and others},
  journal={arXiv preprint arXiv:2410.05474},
  year={2024}
}

@article{ma2024mmlongbench,
  title={Mmlongbench-doc: Benchmarking long-context document understanding with visualizations},
  author={Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and others},
  journal={arXiv preprint arXiv:2407.01523},
  year={2024}
}

@inproceedings{van2023document,
  title={Document understanding dataset and evaluation (dude)},
  author={Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and others},
  booktitle=ICCV,
  pages={19528--19540},
  year={2023}
}

@article{wang2023amber,
  title={Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
  author={Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
  journal={arXiv preprint arXiv:2311.07397},
  year={2023}
}

@article{zhang2024mme,
  title={MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?},
  author={Zhang, Yi-Fan and Zhang, Huanyu and Tian, Haochen and Fu, Chaoyou and Zhang, Shuangqing and Wu, Junfei and Li, Feng and Wang, Kun and Wen, Qingsong and Zhang, Zhang and others},
  journal={arXiv preprint arXiv:2408.13257},
  year={2024}
}

@article{wang2024measuring,
  title={Measuring multimodal mathematical reasoning with math-vision dataset},
  author={Wang, Ke and Pan, Junting and Shi, Weikang and Lu, Zimu and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2402.14804},
  year={2024}
}

@article{sun2024mm,
  title={MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification},
  author={Sun, Kai and Bai, Yushi and Qi, Ji and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2404.05091},
  year={2024}
}

@article{sun2024parrot,
  title={Parrot: Multilingual Visual Instruction Tuning},
  author={Sun, Hai-Long and Zhou, Da-Wei and Li, Yang and Lu, Shiyin and Yi, Chao and Chen, Qing-Guo and Xu, Zhao and Luo, Weihua and Zhang, Kaifu and Zhan, De-Chuan and others},
  journal={arXiv preprint arXiv:2406.02539},
  year={2024}
}

@article{tang2024mtvqa,
  title={MTVQA: Benchmarking Multilingual Text-Centric Visual Question Answering},
  author={Tang, Jingqun and Liu, Qi and Ye, Yongjie and Lu, Jinghui and Wei, Shu and Lin, Chunhui and Li, Wanqing and Mahmood, Mohamad Fitri Faiz Bin and Feng, Hao and Zhao, Zhen and others},
  journal={arXiv preprint arXiv:2405.11985},
  year={2024}
}

@article{wang2024needle,
  title={Needle In A Multimodal Haystack}, 
  author={Wang, Weiyun and Zhang, Shuibo and Ren, Yiming and Duan, Yuchen and Li, Tiantong and Liu, Shuo and Hu, Mengkang and Chen, Zhe and Zhang, Kaipeng and Lu, Lewei and Zhu, Xizhou and Luo, Ping and Qiao, Yu and Dai, Jifeng and Shao, Wenqi and Wang, Wenhai},
  journal={arXiv preprint arXiv:2406.07230},
  year={2024}
}

@inproceedings{tanaka2023slidevqa,
  title={Slidevqa: A dataset for document visual question answering on multiple images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  booktitle=AAAI,
  volume={37},
  number={11},
  pages={13636--13645},
  year={2023}
}

@article{rohrbach2018object,
  title={Object hallucination in image captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1809.02156},
  year={2018}
}

@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author={Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}

@article{xia2024docgenome,
  title={DocGenome: An Open Large-scale Scientific Document Benchmark for Training and Testing Multi-modal Large Language Models},
  author={Xia, Renqiu and Mao, Song and Yan, Xiangchao and Zhou, Hongbin and Zhang, Bo and Peng, Haoyang and Pi, Jiahao and Fu, Daocheng and Wu, Wenjie and Ye, Hancheng and others},
  journal={arXiv preprint arXiv:2406.11633},
  year={2024}
}

@misc{realworldqa,
  title = {Grok-1.5 Vision Preview: Connecting the digital and physical worlds with our first multimodal model.},
  author = {X.AI Corp.},
  year = {2024},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  pages={787--798},
  year={2014}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle=CVPR,
  pages={11--20},
  year={2016}
}

@article{yu2024mmvetv2,
  title={Mm-vet v2: A challenging benchmark to evaluate large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Ren, Linfeng and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2408.00765},
  year={2024}
}

@article{huang2024aesbench,
  title={Aesbench: An expert benchmark for multimodal large language models on image aesthetics perception},
  author={Huang, Yipo and Yuan, Quan and Sheng, Xiangfei and Yang, Zhichao and Wu, Haoning and Chen, Pengfei and Yang, Yuzhe and Li, Leida and Lin, Weisi},
  journal={arXiv preprint arXiv:2401.08276},
  year={2024}
}


@inproceedings{wu2024qbench,
    author = {Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and Lin, Weisi},
    title = {Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision},
    booktitle = {ICLR},
    year = {2024}
}


@article{zhang2024bench,
  title={A-Bench: Are LMMs Masters at Evaluating AI-generated Images?},
  author={Zhang, Zicheng and Wu, Haoning and Li, Chunyi and Zhou, Yingjie and Sun, Wei and Min, Xiongkuo and Chen, Zijian and Liu, Xiaohong and Lin, Weisi and Zhai, Guangtao},
  journal={arXiv preprint arXiv:2406.03070},
  year={2024}
}

@article{chen2024mmstar,
  title={Are We on the Right Way for Evaluating Large Vision-Language Models?},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
  journal={arXiv preprint arXiv:2403.20330},
  year={2024}
}


@article{li2023seed2,
  title={SEED-Bench-2: Benchmarking Multimodal Large Language Models},
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  journal={arXiv preprint arXiv:2311.17092},
  year={2023}
}

@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle=ICML,
  year={2022},
}


@article{jiang2024mantis,
  title={Mantis: Interleaved multi-image instruction tuning},
  author={Jiang, Dongfu and He, Xuan and Zeng, Huaye and Wei, Cong and Ku, Max and Liu, Qian and Chen, Wenhu},
  journal={arXiv preprint arXiv:2405.01483},
  year={2024}
}

@article{tito2023hier,
  title={Hierarchical multimodal transformers for multipage docvqa},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  journal={Pattern Recognition},
  volume={144},
  pages={109834},
  year={2023},
  publisher={Elsevier}
}

@article{fu2024video,
  title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
  author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
  journal={arXiv preprint arXiv:2405.21075},
  year={2024}
}

@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle=CVPR,
  pages={22195--22206},
  year={2024}
}

@article{fang2024mmbench,
  title={MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding},
  author={Fang, Xinyu and Mao, Kangrui and Duan, Haodong and Zhao, Xiangyu and Li, Yining and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2406.14515},
  year={2024}
}

@article{MLVU,
  title={MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}

@article{liu2024tempcompass,
  title={Tempcompass: Do video llms really understand videos?},
  author={Liu, Yuanxin and Li, Shicheng and Liu, Yi and Wang, Yuxiang and Ren, Shuhuai and Li, Lei and Chen, Sishuo and Sun, Xu and Hou, Lu},
  journal={arXiv preprint arXiv:2403.00476},
  year={2024}
}

@article{wu2024longvideobench,
  title={Longvideobench: A benchmark for long-context interleaved video-language understanding},
  author={Wu, Haoning and Li, Dongxu and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2407.15754},
  year={2024}
}

@inproceedings{wang2023v3det,
  title={V3det: Vast vocabulary visual detection dataset},
  author={Wang, Jiaqi and Zhang, Pan and Chu, Tao and Cao, Yuhang and Zhou, Yujie and Wu, Tong and Wang, Bin and He, Conghui and Lin, Dahua},
  booktitle=ICCV,
  pages={19844--19854},
  year={2023}
}


@article{2024docmatrix,
  title={Building and better understanding vision-language models: insights and future directions},
  author={Lauren{\c{c}}on, Hugo and Marafioti, Andr{\'e}s and Sanh, Victor and Tronchon, L{\'e}o},
  journal={arXiv preprint arXiv:2408.12637},
  year={2024}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle=ICCV,
  pages={4015--4026},
  year={2023}
}

@article{yang2024vript,
  title={Vript: A Video Is Worth Thousands of Words},
  author={Yang, Dongjie and Huang, Suyuan and Lu, Chengqiang and Han, Xiaodong and Zhang, Haoxin and Gao, Yan and Hu, Yao and Zhao, Hai},
  journal={arXiv preprint arXiv:2406.06040},
  year={2024}
}

@article{nan2024openvid,
  title={Openvid-1m: A large-scale high-quality dataset for text-to-video generation},
  author={Nan, Kepan and Xie, Rui and Zhou, Penghao and Fan, Tiehan and Yang, Zhenheng and Chen, Zhijie and Li, Xiang and Yang, Jian and Tai, Ying},
  journal={arXiv preprint arXiv:2407.02371},
  year={2024}
}



@article{xia2023structchart,
  title={StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding},
  author={Xia, Renqiu and Zhang, Bo and Peng, Haoyang and Ye, Hancheng and Yan, Xiangchao and Ye, Peng and Shi, Botian and Yan, Junchi and Qiao, Yu},
  journal={arXiv preprint arXiv:2309.11268},
  year={2023}
}

@dataset{Sujet-Finance-QA-Vision-100k,
  author = {Sujet AI, Allaa Boutaleb, Hamed Rahimi},
  title = {Sujet-Finance-QA-Vision-100k: A Large-Scale Dataset for Financial Document VQA},
  year = {2024},
  url = {https://huggingface.co/datasets/sujet-ai/Sujet-Finance-QA-Vision-100k}
}
@online{SoSoDocvqa,
  AUTHOR = {Loc SOKOUDJOU SONAGU, Yoann SOLA},
  URL = {https://huggingface.co/datasets/cmarkea/doc-vqa},
  YEAR = {2024},
  KEYWORDS = {NLP ; Multimodal}
}


@misc{doan2024vintern1befficientmultimodallarge,
      title={Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese}, 
      author={Khang T. Doan and Bao G. Huynh and Dung T. Hoang and Thuc D. Pham and Nhat H. Pham and Quan T. M. Nguyen and Bang Q. Vo and Suong N. Hoang},
      year={2024},
      eprint={2408.12480},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.12480}, 
}

@inproceedings{chen2024sharegpt4v,
        title={Sharegpt4v: Improving large multi-modal models with better captions},
        author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
        booktitle={European Conference on Computer Vision},
        pages={370--387},
        year={2024},
        organization={Springer}
      }
  
@article{chen2024sharegpt4video,
  title={Sharegpt4video: Improving video understanding and generation with better captions},
  author={Chen, Lin and Wei, Xilin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Lin, Bin and Tang, Zhenyu and others},
  journal={arXiv preprint arXiv:2406.04325},
  year={2024}
}

@article{Maaz2024VideoGPT+,
  title={VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad},
  journal={arXiv preprint arXiv:2406.09418},
  year={2024}
}


@inproceedings{Maaz2023VideoChatGPT,
    title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    booktitle=ACL,
    year={2024}
}



@article{wang2024mementos,
  title={Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences},
  author={Wang, Xiyao and Zhou, Yuhang and Liu, Xiaoyu and Lu, Hongjin and Xu, Yuancheng and He, Feihong and Yoon, Jaehong and Lu, Taixi and Bertasius, Gedas and Bansal, Mohit and others},
  journal={arXiv preprint arXiv:2401.10529},
  year={2024}
}

@article{jia2022egotaskqa,
  title={Egotaskqa: Understanding human tasks in egocentric videos},
  author={Jia, Baoxiong and Lei, Ting and Zhu, Song-Chun and Huang, Siyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3343--3360},
  year={2022}
}

@article{liu2020ntu,
  title={NTU RGB+D 120: A large-scale benchmark for 3D human activity understanding},
  author={Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={42},
  number={10},
  pages={2684--2701},
  year={2020}
}

@inproceedings{shao2019objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle=ICCV,
  pages={8430--8439},
  year={2019}
}

@article{kiela2020hateful,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  journal=NeurIPS,
  volume={33},
  pages={2611--2624},
  year={2020}
}

@inproceedings{acharya2019tallyqa,
  title={TallyQA: Answering complex counting questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle=AAAI,
  volume={33},
  pages={8076--8084},
  year={2019}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{honovich2022unnatural,
  title={Unnatural instructions: Tuning language models with (almost) no human labor},
  author={Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  journal={arXiv preprint arXiv:2212.09689},
  year={2022}
}

@inproceedings{zhu2016visual7w,
    title={Visual7w: Grounded question answering in images},
    author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
    booktitle=CVPR,
    pages={4995--5004},
    year={2016}
}

@article{jiao2024img,
  title={Img-diff: Contrastive data synthesis for multimodal large language models},
  author={Jiao, Qirui and Chen, Daoyuan and Huang, Yilun and Li, Yaliang and Shen, Ying},
  journal={arXiv preprint arXiv:2408.04594},
  year={2024}
}

@inproceedings{hessel2023androids,
  title={Do Androids Laugh at Electric Sheep? {Humor} ``Understanding''
         Benchmarks from {The New Yorker Caption Contest}},
  author={Hessel, Jack and Marasovi{\'c}, Ana and Hwang, Jena D. and Lee, Lillian
          and Da, Jeff and Zellers, Rowan and Mankoff, Robert and Choi, Yejin},
  booktitle=ACL,
  year={2023}
}

@misc{no_robots,
  author       = {Nazneen Rajani and Lewis Tunstall and Edward Beeching and Nathan Lambert and Alexander M. Rush and Thomas Wolf},
  title        = {No Robots},
  year         = {2023},
  howpublished = {Hugging Face repository, \url{https://huggingface.co/datasets/HuggingFaceH4/no\_robots}}
}


@article{yang2023longqlora,
  title={Longqlora: Efficient and effective method to extend context length of large language models},
  author={Yang, Jianxin},
  journal={arXiv preprint arXiv:2311.04879},
  year={2023}
}

@article{long-alpaca,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal=NeurIPS,
  volume={36},
  year={2024}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@misc{Firefly,
  author = {Jianxin Yang},
  title = {Firefly: A Chinese Conversational Large Language Model},
  year = {2023},
  howpublished = {\url{https://github.com/yangjianxin1/Firefly}},
}

@article{conover2023free,
  title={Free dolly: Introducing the worlds first truly open instruction-tuned llm},
  author={Conover, Mike and Hayes, Matt and Mathur, Ankit and Xie, Jianwei and Wan, Jun and Shah, Sam and Ghodsi, Ali and Wendell, Patrick and Zaharia, Matei and Xin, Reynold},
  journal={Company Blog of Databricks},
  year={2023}
}

@inproceedings{xu2024wizardlm,
  title={WizardLM: Empowering large pre-trained language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin},
  booktitle=ICLR,
  year={2024}
}

@article{li2024numinamath,
  title={Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions},
  author={Li, Jia and Beeching, Edward and Tunstall, Lewis and Lipkin, Ben and Soletskyi, Roman and Huang, Shengyi and Rasul, Kashif and Yu, Longhui and Jiang, Albert Q and Shen, Ziju and others},
  journal={Hugging Face repository},
  year={2024}
}

@article{mitra2024orcamath,
  title={Orca-math: Unlocking the potential of slms in grade school math},
  author={Mitra, Arindam and Khanpour, Hamed and Rosset, Corby and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2402.14830},
  year={2024}
}

@inproceedings{zhang2024inifinitymath,
  title={Infinitymath: A scalable instruction tuning dataset in programmatic mathematical reasoning},
  author={Zhang, Bo-Wen and Yan, Yan and Li, Lin and Liu, Guang},
  booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages={5405--5409},
  year={2024}
}

@article{opencodeinterpreter,
  title={Opencodeinterpreter: Integrating code generation with execution and refinement},
  author={Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2402.14658},
  year={2024}
}

@article{liu2024oryx,
  title={Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution},
  author={Liu, Zuyan and Dong, Yuhao and Liu, Ziwei and Hu, Winston and Lu, Jiwen and Rao, Yongming},
  journal={arXiv preprint arXiv:2409.12961},
  year={2024}
}


@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{xu2024magpie,
  title={Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing},
  author={Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Deng, Yuntian and Poovendran, Radha and Choi, Yejin and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2406.08464},
  year={2024}
}

@article{wang2024xcoder80k,
  title={How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data},
  author={Wang, Yejie and He, Keqing and Fu, Dayuan and Gongque, Zhuoma and Xu, Heyang and Chen, Yanxu and Wang, Zhexu and Fu, Yujia and Dong, Guanting and Diao, Muxi and others},
  journal={arXiv preprint arXiv:2409.03810},
  year={2024}
}

@article{yu2023paraphrasing,
  title={"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA},
  author={Yu, Yijiong},
  journal={arXiv preprint arXiv:2312.11193},
  year={2023}
}

@article{zhang2024longcite,
    title = {LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA},
    author={Jiajie Zhang and Yushi Bai and Xin Lv and Wanjun Gu and Danqing Liu and Minhao Zou and Shulin Cao and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li},
    journal={arXiv preprint arXiv:2409.02897},
    year={2024}
}

@misc{SlimOrca,
  title = {SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification},
  author = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  howpublished = {\url{https://https://huggingface.co/Open-Orca/SlimOrca}}
}

@article{suhr2018corpus,
  title={A corpus for reasoning about natural language grounded in photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  journal={arXiv preprint arXiv:1811.00491},
  year={2018}
}

@inproceedings{ranjan2021learning,
  title={Learning to count everything},
  author={Ranjan, Viresh and Sharma, Udbhav and Nguyen, Thu and Hoai, Minh},
  booktitle=CVPR,
  pages={3394--3403},
  year={2021}
}

@inproceedings{mao2017deepart,
  title={Deepart: Learning joint representations of visual arts},
  author={Mao, Hui and Cheung, Ming and She, James},
  booktitle={Proceedings of the ACM International Conference on Multimedia},
  pages={1183--1191},
  year={2017}
}

@inproceedings{huang2020movienet,
  title={MovieNet: A Holistic Dataset for Movie Understanding},
  author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
  booktitle=ECCV, 
  year={2020}
}

@article{hosu2020koniq,
  title={KonIQ-10k: An ecologically valid database for deep learning of blind image quality assessment},
  author={Hosu, Vlad and Lin, Hanhe and Sziranyi, Tamas and Saupe, Dietmar},
  journal={IEEE Transactions on Image Processing},
  volume={29},
  pages={4041--4056},
  year={2020},
  publisher={IEEE}
}

@inproceedings{van2018inaturalist,
  title={The inaturalist species classification and detection dataset},
  author={Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  booktitle=CVPR,
  pages={8769--8778},
  year={2018}
}

@misc{sujet-finance,
  author = {Sujet AI, Allaa Boutaleb, Hamed Rahimi},
  title = {Sujet-Finance-QA-Vision-100k: A Large-Scale Dataset for Financial Document VQA},
  year = {2024},
  howpublished = {\url{https://huggingface.co/datasets/sujet-ai/Sujet-Finance-QA-Vision-100k}}
}



@article{li2024multimodal,
  title={Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models},
  author={Li, Lei and Wang, Yuqi and Xu, Runxin and Wang, Peiyi and Feng, Xiachong and Kong, Lingpeng and Liu, Qi},
  journal={arXiv preprint arXiv:2403.00231},
  year={2024}
}

@article{liu2023mmc,
  title={MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning},
  author={Liu, Fuxiao and Wang, Xiaoyang and Yao, Wenlin and Chen, Jianshu and Song, Kaiqiang and Cho, Sangwoo and Yacoob, Yaser and Yu, Dong},
  journal={arXiv preprint arXiv:2311.10774},
  year={2023}
}

@article{li2024omnicorpus,
  title={OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text},
  author={Li, Qingyun and Chen, Zhe and Wang, Weiyun and Wang, Wenhai and Ye, Shenglong and Jin, Zhenjiang and Chen, Guanzhou and He, Yinan and Gao, Zhangwei and Cui, Erfei and others},
  journal={arXiv preprint arXiv:2406.08418},
  year={2024}
}

@article{yi2019clevrer,
  title={Clevrer: Collision events for video representation and reasoning},
  author={Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:1910.01442},
  year={2019}
}

@article{li2024gmai,
  title={GMAI-VL \& GMAI-VL-5.5 M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI},
  author={Li, Tianbin and Su, Yanzhou and Li, Wei and Fu, Bin and Chen, Zhe and Huang, Ziyan and Wang, Guoan and Ma, Chenglong and Chen, Ying and Hu, Ming and others},
  journal={arXiv preprint arXiv:2411.14522},
  year={2024}
}

@article{obeid2020chart,
  title={Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model},
  author={Obeid, Jason and Hoque, Enamul},
  journal={arXiv preprint arXiv:2010.09142},
  year={2020}
}

@article{zheng2024multimodal,
  title={Multimodal Table Understanding},
  author={Zheng, Mingyu and Feng, Xinwei and Si, Qingyi and She, Qiaoqiao and Lin, Zheng and Jiang, Wenbin and Wang, Weiping},
  journal={arXiv preprint arXiv:2406.08100},
  year={2024}
}

@inproceedings{
  anonymous2024cgbench,
  title={{CG}-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding},
  author={Anonymous},
  booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=le4IoZZHy1},
  note={under review}
}

@misc{chinese-ocr,
  title={Chinese-OCR},
  author={Beijing Anjie Zhihe Technology Co., Ltd.},
  howpublished={\url{https://huggingface.co/datasets/longmaodata/Chinese-OCR}},
  year={2024}
}

@article{veit2016coco,
  title={Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  author={Veit, Andreas and Matera, Tomas and Neumann, Lukas and Matas, Jiri and Belongie, Serge},
  journal={arXiv preprint arXiv:1601.07140},
  year={2016}
}

@inproceedings{he2018icpr2018,
  title={ICPR2018 contest on robust reading for multi-type web images},
  author={He, Mengchao and Liu, Yuliang and Yang, Zhibo and Zhang, Sheng and Luo, Canjie and Gao, Feiyu and Zheng, Qi and Wang, Yongpan and Zhang, Xin and Jin, Lianwen},
  booktitle={International Conference on Pattern Recognition},
  pages={7--12},
  year={2018}
}

@article{marti2002iam,
  title={The IAM-database: an English sentence database for offline handwriting recognition},
  author={Marti, U-V and Bunke, Horst},
  journal={International Journal on Document Analysis and Recognition},
  volume={5},
  pages={39--46},
  year={2002},
  publisher={Springer}
}

@inproceedings{kuang2023visual,
  title={Visual information extraction in the wild: practical dataset and end-to-end solution},
  author={Kuang, Jianfeng and Hua, Wei and Liang, Dingkang and Yang, Mingkun and Jiang, Deqiang and Ren, Bo and Bai, Xiang},
  booktitle=ICDAR,
  pages={36--53},
  year={2023},
  organization={Springer}
}

@inproceedings{huang2019icdar2019,
    title={Icdar2019 competition on scanned receipt ocr and information extraction},
    author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
    booktitle=ICDAR,
    pages={1516--1520},
    year={2019}
}

@article{doan2024vintern,
  title={Vintern-1B: An Efficient Multimodal Large Language Model for Vietnamese},
  author={Doan, Khang T and Huynh, Bao G and Hoang, Dung T and Pham, Thuc D and Pham, Nhat H and Nguyen, Quan and Vo, Bang Q and Hoang, Suong N},
  journal={arXiv preprint arXiv:2408.12480},
  year={2024}
}

@inproceedings{wang2020general,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle=CVPR,
  pages={10126--10135},
  year={2020}
}



@inproceedings{zheng2021global,
  title={Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context},
  author={Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={697--706},
  year={2021}
}

@inproceedings{yuan2022syntax,
  title={Syntax-aware network for handwritten mathematical expression recognition},
  author={Yuan, Ye and Liu, Xiao and Dikubab, Wondimu and Liu, Hui and Ji, Zhilong and Wu, Zhongqin and Bai, Xiang},
  booktitle=CVPR,
  pages={4553--4562},
  year={2022}
}

@misc{cyrillic,
  title={Cyrillic Handwriting Dataset},
  author={Konstantin Verner},
  howpublished = {\url{https://www.kaggle.com/datasets/constantinwerner/cyrillic-handwriting-dataset}},
  year={2020}
}

@article{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~}
}

@article{yang2023open,
  title={An open dataset for intelligent recognition and classification of abnormal condition in longwall mining},
  author={Yang, Wenjuan and Zhang, Xuhui and Ma, Bing and Wang, Yanqun and Wu, Yujia and Yan, Jianxing and Liu, Yongwei and Zhang, Chao and Wan, Jicheng and Wang, Yue and others},
  journal={Scientific Data},
  volume={10},
  number={1},
  pages={416},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{seo2015solving,
  title={Solving geometry problems: Combining text and diagram interpretation},
  author={Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali and Etzioni, Oren and Malcolm, Clint},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={1466--1476},
  year={2015}
}

@article{chen2022unigeo,
  title={Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression},
  author={Chen, Jiaqi and Li, Tong and Qin, Jinghui and Lu, Pan and Lin, Liang and Chen, Chongyu and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2212.02746},
  year={2022}
}

@article{chang2022mapqa,
  title={MapQA: A dataset for question answering on choropleth maps},
  author={Chang, Shuaichen and Palzer, David and Li, Jialin and Fosler-Lussier, Eric and Xiao, Ningchuan},
  journal={arXiv preprint arXiv:2211.08545},
  year={2022}
}

@article{lau2018dataset,
  title={A dataset of clinically generated visual questions and answers about radiology images},
  author={Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
  journal={Scientific data},
  volume={5},
  pages={1--10},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{zhang2023pmc,
  title={Pmc-vqa: Visual instruction tuning for medical visual question answering},
  author={Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lin, Weixiong and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2305.10415},
  year={2023}
}

@article{zhang2024mavis,
  title={Mavis: Mathematical visual instruction tuning},
  author={Zhang, Renrui and Wei, Xinyu and Jiang, Dongzhi and Zhang, Yichi and Guo, Ziyu and Tong, Chengzhuo and Liu, Jiaming and Zhou, Aojun and Wei, Bin and Zhang, Shanghang and others},
  journal={arXiv preprint arXiv:2407.08739},
  year={2024}
}


@article{dan2023educhat,
  title={Educhat: A large-scale language model-based chatbot system for intelligent education},
  author={Dan, Yuhao and Lei, Zhikai and Gu, Yiyang and Li, Yong and Yin, Jianghao and Lin, Jiaju and Ye, Linhao and Tie, Zhiyan and Zhou, Yougen and Wang, Yilei and others},
  journal={arXiv preprint arXiv:2308.02773},
  year={2023}
}
@article{chi2019complicated,
  title={Complicated Table Structure Recognition},
  author={Chi, Zewen and Huang, Heyan and Xu, Heng-Da and Yu, Houjin and Yin, Wanxuan and Mao, Xian-Ling},
  journal={arXiv preprint arXiv:1908.04729},
  year={2019}
}
@article{liu2024cmmmath,
  title={CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the Mathematics Reasoning of Large Multimodal Models},
  author={Liu, Wentao and Pan, Qianjun and Zhang, Yi and Liu, Zhuo and Wu, Ji and Zhou, Jie and Zhou, Aimin and Chen, Qin and Jiang, Bo and He, Liang},
  journal={arXiv preprint arXiv:2409.02834},
  year={2024}
}
@inproceedings{kim2019korean,
  title={Korean localization of visual question answering for blind people},
  author={Kim, Jin-Hwa and Lim, Soohyun and Park, Jaesun and Cho, Hansu},
  booktitle={SK T-Brain-AI for Social Good Workshop at NeurIPS},
  volume={2},
  year={2019}
}
@inproceedings{wang2021screen2words,
  title={Screen2words: Automatic mobile UI summarization with multimodal learning},
  author={Wang, Bryan and Li, Gang and Zhou, Xin and Chen, Zhourong and Grossman, Tovi and Li, Yang},
  booktitle={The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages={498--510},
  year={2021}
}

@article{laurenccon2024unlocking,
  title={Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Sanh, Victor},
  journal={arXiv preprint arXiv:2403.09029},
  year={2024}
}

@inproceedings{garcia2015overview,
  title={Overview of the ImageCLEF 2015 medical classification task},
  author={Garcia Seco De Herrera, Alba and M{\"u}ller, Henning and Bromuri, Stefano},
  booktitle={Working Notes of CLEF 2015--Cross Language Evaluation Forum, CEUR},
  volume={1391},
  year={2015},
  organization={CEUR Workshop Proceedings}
}

@article{wu2023towards,
  title={Towards generalist foundation model for radiology},
  author={Wu, Chaoyi and Zhang, Xiaoman and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
  journal={arXiv preprint arXiv:2308.02463},
  year={2023}
}

@inproceedings{liu2021slake,
  title={Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
  author={Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming},
  booktitle={2021 IEEE 18th International Symposium on Biomedical Imaging},
  pages={1650--1654},
  year={2021},
  organization={IEEE}
}

@article{zhang2024video,
  title={Video instruction tuning with synthetic data},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}

@inproceedings{singh2025benchmarking,
  title={Benchmarking Object Detectors with COCO: A New Path Forward},
  author={Singh, Shweta and Yadav, Aayan and Jain, Jitesh and Shi, Humphrey and Johnson, Justin and Desai, Karan},
  booktitle={European Conference on Computer Vision},
  pages={279--295},
  year={2025},
  organization={Springer}
}

@article{chen2024gui,
  title={GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents},
  author={Chen, Dongping and Huang, Yue and Wu, Siyuan and Tang, Jingyu and Chen, Liuyi and Bai, Yilin and He, Zhigang and Wang, Chenlong and Zhou, Huichi and Li, Yiqiang and others},
  journal={arXiv preprint arXiv:2406.10819},
  year={2024}
}

@article{zhang2024chemllm,
  title={Chemllm: A chemical large language model},
  author={Zhang, Di and Liu, Wei and Tan, Qian and Chen, Jingdan and Yan, Hang and Yan, Yuliang and Li, Jiatong and Huang, Weiran and Yue, Xiangyu and Zhou, Dongzhan and others},
  journal={arXiv preprint arXiv:2402.06852},
  year={2024}
}

@misc{FineVideo,
  title={FineVideo},
  author={Farr, Miquel and Marafioti, Andi and Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year={2024},
  howpublished={\url{https://huggingface.co/datasets/HuggingFaceFV/finevideo}},
}

@article{patraucean2024perception,
  title={Perception test: A diagnostic benchmark for multimodal video models},
  author={Patraucean, Viorica and Smaira, Lucas and Gupta, Ankush and Recasens, Adria and Markeeva, Larisa and Banarse, Dylan and Koppula, Skanda and Malinowski, Mateusz and Yang, Yi and Doersch, Carl and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yu2024rlaifv,
  title={RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness}, 
  author={Yu, Tianyu and Zhang, Haoye and Yao, Yuan and Dang, Yunkai and Chen, Da and Lu, Xiaoman and Cui, Ganqu and He, Taiwen and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
  journal={arXiv preprint arXiv:2405.17220},
  year={2024},
}


@article{bai2021uibert,
  title={Uibert: Learning generic multimodal representations for ui understanding},
  author={Bai, Chongyang and Zang, Xiaoxue and Xu, Ying and Sunkara, Srinivas and Rastogi, Abhinav and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2107.13731},
  year={2021}
}

@article{li2020widget,
  title={Widget captioning: Generating natural language description for mobile user interface elements},
  author={Li, Yang and Li, Gang and He, Luheng and Zheng, Jingjie and Li, Hong and Guan, Zhiwei},
  journal={arXiv preprint arXiv:2010.04295},
  year={2020}
}

@inproceedings{deka2017rico,
  title={Rico: A mobile app dataset for building data-driven design applications},
  author={Deka, Biplab and Huang, Zifeng and Franzen, Chad and Hibschman, Joshua and Afergan, Daniel and Li, Yang and Nichols, Jeffrey and Kumar, Ranjitha},
  booktitle={Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
  pages={845--854},
  year={2017}
}

@article{cheng2024seeclick,
  title={Seeclick: Harnessing gui grounding for advanced visual gui agents},
  author={Cheng, Kanzhi and Sun, Qiushi and Chu, Yougang and Xu, Fangzhi and Li, Yantao and Zhang, Jianbing and Wu, Zhiyong},
  journal={arXiv preprint arXiv:2401.10935},
  year={2024}
}

@inproceedings{zala2023hierarchical,
  title={Hierarchical video-moment retrieval and step-captioning},
  author={Zala, Abhay and Cho, Jaemin and Kottur, Satwik and Chen, Xilun and Oguz, Barlas and Mehdad, Yashar and Bansal, Mohit},
  booktitle=CVPR,
  pages={23056--23065},
  year={2023}
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal=NeurIPS,
  volume={36},
  pages={46212--46244},
  year={2023}
}

@inproceedings{azuma2022scanqa,
  title={Scanqa: 3d question answering for spatial scene understanding},
  author={Azuma, Daichi and Miyanishi, Taiki and Kurita, Shuhei and Kawanabe, Motoaki},
  booktitle=CVPR,
  pages={19129--19139},
  year={2022}
}

@article{chai2024amex,
  title={Amex: Android multi-annotation expo dataset for mobile gui agents},
  author={Chai, Yuxiang and Huang, Siyuan and Niu, Yazhe and Xiao, Han and Liu, Liang and Zhang, Dingyu and Gao, Peng and Ren, Shuai and Li, Hongsheng},
  journal={arXiv preprint arXiv:2407.17490},
  year={2024}
}

@article{rawles2024androidinthewild,
  title={Androidinthewild: A large-scale dataset for android device control},
  author={Rawles, Christopher and Li, Alice and Rodriguez, Daniel and Riva, Oriana and Lillicrap, Timothy},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lu2024gui,
  title={GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices},
  author={Lu, Quanfeng and Shao, Wenqi and Liu, Zitao and Meng, Fanqing and Li, Boxuan and Chen, Botong and Huang, Siyuan and Zhang, Kaipeng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2406.08451},
  year={2024}
}

@inproceedings{lieffects,
  title={On the Effects of Data Scale on UI Control Agents},
  author={Li, Wei and Bishop, William E and Li, Alice and Rawles, Christopher and Campbell-Ajala, Folawiyo and Tyamagundlu, Divya and Riva, Oriana},
  booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024}
}

@article{deng2024mind2web,
  title={Mind2web: Towards a generalist agent for the web},
  author={Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Sam and Wang, Boshi and Sun, Huan and Su, Yu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kapoor2025omniact,
  title={OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web},
  author={Kapoor, Raghav and Butala, Yash Parag and Russak, Melisa and Koh, Jing Yu and Kamble, Kiran and AlShikh, Waseem and Salakhutdinov, Ruslan},
  booktitle={European Conference on Computer Vision},
  pages={161--178},
  year={2025},
  organization={Springer}
}

@misc{agentsea_wave_ui,
  author       = {AgentSea},
  title        = {Wave-UI},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/agentsea/wave-ui}},
}

@misc{hezarai_parsynth_ocr_200k,
  author       = {Hezarai},
  title        = {Parsynth-OCR-200k},
  year         = {2024},
  howpublished = {\url{https://huggingface.co/datasets/hezarai/parsynth-ocr-200k}},
}


@article{wu2024star,
  title={Star: A benchmark for situated reasoning in real-world videos},
  author={Wu, Bo and Yu, Shoubin and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  journal={arXiv preprint arXiv:2405.09711},
  year={2024}
}

@article{li2024chemvlm,
  title={ChemVLM: Exploring the Power of Multimodal Large Language Models in Chemistry Area},
  author={Li, Junxian and Zhang, Di and Wang, Xunzhi and Hao, Zeying and Lei, Jingdi and Tan, Qian and Zhou, Cai and Liu, Wei and Yang, Yaotian and Xiong, Xinrui and others},
  journal={arXiv preprint arXiv:2408.07246},
  year={2024}
}

@inproceedings{ben2019vqa,
  title={Vqa-med: Overview of the medical visual question answering task at imageclef 2019},
  author={Ben Abacha, Asma and Hasan, Sadid A and Datla, Vivek V and Demner-Fushman, Dina and M{\"u}ller, Henning},
  booktitle={Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes},
  year={2019}
}

@article{hu2023medical,
  title={Medical-Diff-VQA: a large-scale medical dataset for difference visual question answering on chest x-ray images},
  author={Hu, Xinyue and Gu, L and An, Q and Zhang, M and Liu, L and Kobayashi, K and Harada, T and Summers, R and Zhu, Y},
  year={2023},
  journal={PhysioNet}
}

@article{he2020pathvqa,
  title={Pathvqa: 30000+ questions for medical visual question answering},
  author={He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal={arXiv preprint arXiv:2003.10286},
  year={2020}
}

@misc{pmccase,
  title = {PMC-CaseReport},
  howpublished = {\url{https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport}},
  author = {Wu, Chaoyi},
  year = {2023}
}

@inproceedings{tanaka2021visualmrc,
  title={Visualmrc: Machine reading comprehension on document images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  booktitle=AAAI,
  volume={35},
  pages={13878--13888},
  year={2021}
}

@article{dai202415m,
  title={15M Multimodal Facial Image-Text Dataset},
  author={Dai, Dawei and Li, YuTang and Liu, YingGe and Jia, Mingming and YuanHui, Zhang and Wang, Guoyin},
  journal={arXiv preprint arXiv:2407.08515},
  year={2024}
}

@inproceedings{rohrbach2015dataset,
  title={A dataset for movie description},
  author={Rohrbach, Anna and Rohrbach, Marcus and Tandon, Niket and Schiele, Bernt},
  booktitle=CVPR,
  pages={3202--3212},
  year={2015}
}

@inproceedings{yao2011human,
  title={Human action recognition by learning bases of action attributes and parts},
  author={Yao, Bangpeng and Jiang, Xiaoye and Khosla, Aditya and Lin, Andy Lai and Guibas, Leonidas and Fei-Fei, Li},
  booktitle={2011 International Conference on Computer Vision},
  pages={1331--1338},
  year={2011}
}

@article{shi2024chartmimic,
  title={ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation},
  author={Shi, Chufan and Yang, Cheng and Liu, Yaxin and Shui, Bo and Wang, Junjie and Jing, Mohan and Xu, Linran and Zhu, Xinyu and Li, Siheng and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2406.09961},
  year={2024}
}

@article{you2023ferret,
  title={Ferret: Refer and ground anything anywhere at any granularity},
  author={You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
  journal={arXiv preprint arXiv:2310.07704},
  year={2023}
}


@article{yu2024texthawk2,
  title={Texthawk2: A large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens},
  author={Yu, Ya-Qi and Liao, Minghui and Zhang, Jiwen and Wu, Jihao},
  journal={arXiv preprint arXiv:2410.05261},
  year={2024}
}

@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{rafailov2024dpo,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jung2024bco,
  title={Binary classifier optimization for large language model alignment},
  author={Jung, Seungjae and Han, Gunsoo and Nam, Daniel Wontae and On, Kyoung-Woon},
  journal={arXiv preprint arXiv:2404.04656},
  year={2024}
}

@article{chen2023amsp,
  title={Amsp: Reducing communication overhead of zero for efficient llm training},
  author={Chen, Qiaoling and Hu, Qinghao and Wang, Guoteng and Xiong, Yingtong and Huang, Ting and Chen, Xun and Gao, Yang and Yan, Hang and Wen, Yonggang and Zhang, Tianwei and others},
  journal={arXiv preprint arXiv:2311.00257},
  year={2023}
}

@article{gu2024loongtrain,
  title={Loongtrain: Efficient training of long-sequence llms with head-context parallelism},
  author={Gu, Diandian and Sun, Peng and Hu, Qinghao and Huang, Ting and Chen, Xun and Xiong, Yingtong and Wang, Guoteng and Chen, Qiaoling and Zhao, Shangchun and Fang, Jiarui and others},
  journal={arXiv preprint arXiv:2406.18485},
  year={2024}
}

@article{chen2024m3cot,
  title={M3CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought},
  author={Chen, Qiguang and Qin, Libo and Zhang, Jin and Chen, Zhi and Xu, Xiao and Che, Wanxiang},
  journal={arXiv preprint arXiv:2405.16473},
  year={2024}
}

@inproceedings{seo2015geos,
  title={Solving geometry problems: Combining text and diagram interpretation},
  author={Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali and Etzioni, Oren and Malcolm, Clint},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={1466--1476},
  year={2015}
}

@article{gao2023gllava,
  title={G-llava: Solving geometric problem with multi-modal large language model},
  author={Gao, Jiahui and Pi, Renjie and Zhang, Jipeng and Ye, Jiacheng and Zhong, Wanjun and Wang, Yufei and Hong, Lanqing and Han, Jianhua and Xu, Hang and Li, Zhenguo and others},
  journal={arXiv preprint arXiv:2312.11370},
  year={2023}
}

@inproceedings{huang2019sroie,
  title={Icdar2019 competition on scanned receipt ocr and information extraction},
  author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1516--1520},
  year={2019},
  organization={IEEE}
}

@article{chen2024internvl_1_5,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{ge2024v2pe,
  title={V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding},
  author={Ge, Junqi and Chen, Ziyi and Lin, Jintao and Zhu, Jinguo and Liu, Xihui and Dai, Jifeng and Zhu, Xizhou},
  journal={arXiv preprint arXiv:2412.09616},
  year={2024}
}

@article{chen2024internevo,
  title={Internevo: Efficient long-sequence large language model training via hybrid parallelism and redundant sharding},
  author={Chen, Qiaoling and Gu, Diandian and Wang, Guoteng and Chen, Xun and Xiong, YingTong and Huang, Ting and Hu, Qinghao and Jin, Xin and Wen, Yonggang and Zhang, Tianwei and others},
  journal={arXiv preprint arXiv:2401.09149},
  year={2024}
}

@article{shi2024mathv,
  title={Math-llava: Bootstrapping mathematical reasoning for multimodal large language models},
  author={Shi, Wenhao and Hu, Zhiqiang and Bin, Yi and Liu, Junhua and Yang, Yang and Ng, See-Kiong and Bing, Lidong and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2406.17294},
  year={2024}
}

@article{wang2025visualprm,
  title={Visualprm: An effective process reward model for multimodal reasoning},
  author={Wang, Weiyun and Gao, Zhangwei and Chen, Lianjie and Chen, Zhe and Zhu, Jinguo and Zhao, Xiangyu and Liu, Yangzhou and Cao, Yue and Ye, Shenglong and Zhu, Xizhou and others},
  journal={arXiv preprint arXiv:2503.10291},
  year={2025}
}

@article{snell2024test_time_scaling_efficient,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{mcaleese2024openai_critic,
  title={Llm critics help catch llm bugs},
  author={McAleese, Nat and Pokorny, Rai Michael and Uribe, Juan Felipe Ceron and Nitishinskaya, Evgenia and Trebacz, Maja and Leike, Jan},
  journal={arXiv preprint arXiv:2407.00215},
  year={2024}
}

@article{luo2024omegaprm,
  title={Improve mathematical reasoning in language models by automated process supervision},
  author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
  journal={arXiv preprint arXiv:2406.06592},
  volume={2},
  year={2024}
}

@inproceedings{liu2020learning,
  title={Learning to summarize from human feedback},
  author={Liu, Fei and others},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={583--592},
  year={2020}
}

@article{wang2023mathshepherd,
  title={Math-shepherd: Verify and reinforce llms step-by-step without human annotations},
  author={Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, RX and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Yu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2312.08935},
  year={2023}
}

@article{zhang2025qwen_prm,
  title={The lessons of developing process reward models in mathematical reasoning},
  author={Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2501.07301},
  year={2025}
}

@inproceedings{lightman2023prm800k,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{zhang2024mathverse,
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Gao, Peng and others},
  journal={arXiv preprint arXiv:2403.14624},
  year={2024}
}

@article{wang2024mathvision,
  title={Measuring multimodal mathematical reasoning with math-vision dataset},
  author={Wang, Ke and Pan, Junting and Shi, Weikang and Lu, Zimu and Zhan, Mingjie and Li, Hongsheng},
  journal={arXiv preprint arXiv:2402.14804},
  year={2024}
}

@article{zou2024dynamath,
  title={Dynamath: A dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models},
  author={Zou, Chengke and Guo, Xingang and Yang, Rui and Zhang, Junyu and Hu, Bin and Zhang, Huan},
  journal={arXiv preprint arXiv:2411.00836},
  year={2024}
}

@article{xiao2024logicvista,
  title={Logicvista: Multimodal llm logical reasoning benchmark in visual contexts},
  author={Xiao, Yijia and Sun, Edward and Liu, Tianyu and Wang, Wei},
  journal={arXiv preprint arXiv:2407.04973},
  year={2024}
}

@article{qiao2024wemath,
  title={We-math: Does your large multimodal model achieve human-like mathematical reasoning?},
  author={Qiao, Runqi and Tan, Qiuna and Dong, Guanting and Wu, Minhui and Sun, Chong and Song, Xiaoshuai and GongQue, Zhuoma and Lei, Shanglin and Wei, Zhe and Zhang, Miaoxuan and others},
  journal={arXiv preprint arXiv:2407.01284},
  year={2024}
}

@misc{qiao2025wemath,
      title={We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning}, 
      author={Runqi Qiao and Qiuna Tan and Peiqing Yang and Yanzi Wang and Xiaowan Wang and Enhui Wan and Sitong Zhou and Guanting Dong and Yuchen Zeng and Yida Xu and Jie Wang and Chong Sun and Chen Li and Honggang Zhang},
      year={2025},
      eprint={2508.10433},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.10433}, 
}

@article{bai2025qwen2_5,
  title={Qwen2.5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@misc{gemini2_0,
    title={Introducing gemini 2.0: our new ai model for the agentic era},
    author={Google Deepmind},
    howpublished = {\url{https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/}},
    year={2024}
}

@misc{gemini2_0pro,
    title={Gemini 2.0 is now available to everyone},
    author={Google Deepmind},
    howpublished = {\url{https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/}},
    year={202}
}

@misc{qvq-72b-preview,
    title = {QVQ: To See the World with Wisdom},
    url = {https://qwenlm.github.io/blog/qvq-72b-preview/},
    author = {Qwen Team},
    month = {December},
    year = {2024}
}
@misc{geminipro2.5,
  author       = {v DeepMind},
  title        = {Gemini 2.5 Pro},
  year         = {2025},
  howpublished = {\url{https://deepmind.google/technologies/gemini/pro/}},
}

@article{wu2024atlas,
  title={Os-atlas: A foundation action model for generalist gui agents},
  author={Wu, Zhiyong and Wu, Zhenyu and Xu, Fangzhi and Wang, Yian and Sun, Qiushi and Jia, Chengyou and Cheng, Kanzhi and Ding, Zichen and Chen, Liheng and Liang, Paul Pu and others},
  journal={arXiv preprint arXiv:2410.23218},
  year={2024}
}

@article{meng2025mm_eureka,
  title={MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale Reinforcement Learning},
  author={Meng, Fanqing and Du, Lingxiao and Liu, Zongkai and Zhou, Zhixiang and Lu, Quanfeng and Fu, Daocheng and Shi, Botian and Wang, Wenhai and He, Junjun and Zhang, Kaipeng and others},
  journal={arXiv preprint arXiv:2503.07365},
  year={2025}
}
@software{benallal2024smollmcorpus,
  author = {Ben Allal, Loubna and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  title = {SmolLM-Corpus},
  month = July,
  year = 2024,
  url = {https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus}
}


@article{acemath2024,
  title={AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling},
  author={Liu, Zihan and Chen, Yang and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint},
  year={2024}
}


@misc{scp116k,
      title={SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain}, 
      author={Dakuan Lu and Xiaoyu Tan and Rui Xu and Tianchu Yao and Chao Qu and Wei Chu and Yinghui Xu and Yuan Qi},
      year={2025},
      eprint={2501.15587},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.15587}, 
}

@article{qin2025ui,
  title={UI-TARS: Pioneering Automated GUI Interaction with Native Agents},
  author={Qin, Yujia and Ye, Yining and Fang, Junjie and Wang, Haoming and Liang, Shihao and Tian, Shizuo and Zhang, Junda and Li, Jiahao and Li, Yunxin and Huang, Shijue and others},
  journal={arXiv preprint arXiv:2501.12326},
  year={2025}
}

@misc{vgg_via,
  title = {VGG Image Annotator (VIA)},
  author = {Abhishek Dutta and Ankush Gupta and Andrew Zisserman},
  howpublished = {\url{https://www.robots.ox.ac.uk/~vgg/software/via/}},
}

@article{xu2024aguvis,
  title={Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction},
  author={Yiheng Xu and Zekun Wang and Junli Wang and Dunjie Lu and Tianbao Xie and Amrita Saha and Doyen Sahoo and Tao Yu and Caiming Xiong},
  year={2024},
  url={https://arxiv.org/abs/2412.04454}
}
@article{longva,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}
@article{zhang2024llavanext,
  title={Llava-next: A strong zero-shot video understanding model},
  author={Zhang, Y and Li, B and Liu, H and Lee, Y and Gui, L and Fu, D and Feng, J and Liu, Z and Li, C},
  year={2024}
}
@misc{gemmateam2025gemma3,
      title={Gemma 3 Technical Report}, 
      author={Gemma Team and Aishwarya Kamath and Johan Ferret and Shreya Pathak and Nino Vieillard and Ramona Merhej and Sarah Perrin and Tatiana Matejovicova and Alexandre Ram and Morgane Rivire and Louis Rouillard and Thomas Mesnard and Geoffrey Cideron and Jean-bastien Grill and Sabela Ramos and Edouard Yvinec and Michelle Casbon and Etienne Pot and Ivo Penchev and Gal Liu and Francesco Visin and Kathleen Kenealy and Lucas Beyer and Xiaohai Zhai and Anton Tsitsulin and Robert Busa-Fekete and Alex Feng and Noveen Sachdeva and Benjamin Coleman and Yi Gao and Basil Mustafa and Iain Barr and Emilio Parisotto and David Tian and Matan Eyal and Colin Cherry and Jan-Thorsten Peter and Danila Sinopalnikov and Surya Bhupatiraju and Rishabh Agarwal and Mehran Kazemi and Dan Malkin and Ravin Kumar and David Vilar and Idan Brusilovsky and Jiaming Luo and Andreas Steiner and Abe Friesen and Abhanshu Sharma and Abheesht Sharma and Adi Mayrav Gilady and Adrian Goedeckemeyer and Alaa Saade and Alex Feng and Alexander Kolesnikov and Alexei Bendebury and Alvin Abdagic and Amit Vadi and Andrs Gyrgy and Andr Susano Pinto and Anil Das and Ankur Bapna and Antoine Miech and Antoine Yang and Antonia Paterson and Ashish Shenoy and Ayan Chakrabarti and Bilal Piot and Bo Wu and Bobak Shahriari and Bryce Petrini and Charlie Chen and Charline Le Lan and Christopher A. Choquette-Choo and CJ Carey and Cormac Brick and Daniel Deutsch and Danielle Eisenbud and Dee Cattle and Derek Cheng and Dimitris Paparas and Divyashree Shivakumar Sreepathihalli and Doug Reid and Dustin Tran and Dustin Zelle and Eric Noland and Erwin Huizenga and Eugene Kharitonov and Frederick Liu and Gagik Amirkhanyan and Glenn Cameron and Hadi Hashemi and Hanna Klimczak-Pluciska and Harman Singh and Harsh Mehta and Harshal Tushar Lehri and Hussein Hazimeh and Ian Ballantyne and Idan Szpektor and Ivan Nardini and Jean Pouget-Abadie and Jetha Chan and Joe Stanton and John Wieting and Jonathan Lai and Jordi Orbay and Joseph Fernandez and Josh Newlan and Ju-yeong Ji and Jyotinder Singh and Kat Black and Kathy Yu and Kevin Hui and Kiran Vodrahalli and Klaus Greff and Linhai Qiu and Marcella Valentine and Marina Coelho and Marvin Ritter and Matt Hoffman and Matthew Watson and Mayank Chaturvedi and Michael Moynihan and Min Ma and Nabila Babar and Natasha Noy and Nathan Byrd and Nick Roy and Nikola Momchev and Nilay Chauhan and Noveen Sachdeva and Oskar Bunyan and Pankil Botarda and Paul Caron and Paul Kishan Rubenstein and Phil Culliton and Philipp Schmid and Pier Giuseppe Sessa and Pingmei Xu and Piotr Stanczyk and Pouya Tafti and Rakesh Shivanna and Renjie Wu and Renke Pan and Reza Rokni and Rob Willoughby and Rohith Vallu and Ryan Mullins and Sammy Jerome and Sara Smoot and Sertan Girgin and Shariq Iqbal and Shashir Reddy and Shruti Sheth and Siim Pder and Sijal Bhatnagar and Sindhu Raghuram Panyam and Sivan Eiger and Susan Zhang and Tianqi Liu and Trevor Yacovone and Tyler Liechty and Uday Kalra and Utku Evci and Vedant Misra and Vincent Roseberry and Vlad Feinberg and Vlad Kolesnikov and Woohyun Han and Woosuk Kwon and Xi Chen and Yinlam Chow and Yuvein Zhu and Zichuan Wei and Zoltan Egyed and Victor Cotruta and Minh Giang and Phoebe Kirk and Anand Rao and Kat Black and Nabila Babar and Jessica Lo and Erica Moreira and Luiz Gustavo Martins and Omar Sanseviero and Lucas Gonzalez and Zach Gleicher and Tris Warkentin and Vahab Mirrokni and Evan Senter and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and Yossi Matias and D. Sculley and Slav Petrov and Noah Fiedel and Noam Shazeer and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Jean-Baptiste Alayrac and Rohan Anil and Dmitry and Lepikhin and Sebastian Borgeaud and Olivier Bachem and Armand Joulin and Alek Andreev and Cassidy Hardin and Robert Dadashi and Lonard Hussenot},
      year={2025},
      eprint={2503.19786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19786}, 
}
@misc{zhu2025internvl3,
      title={InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models}, 
      author={Jinguo Zhu and Weiyun Wang and Zhe Chen and Zhaoyang Liu and Shenglong Ye and Lixin Gu and Hao Tian and Yuchen Duan and Weijie Su and Jie Shao and Zhangwei Gao and Erfei Cui and Xuehui Wang and Yue Cao and Yangzhou Liu and Xingguang Wei and Hongjie Zhang and Haomin Wang and Weiye Xu and Hao Li and Jiahao Wang and Nianchen Deng and Songze Li and Yinan He and Tan Jiang and Jiapeng Luo and Yi Wang and Conghui He and Botian Shi and Xingcheng Zhang and Wenqi Shao and Junjun He and Yingtong Xiong and Wenwen Qu and Peng Sun and Penglong Jiao and Han Lv and Lijun Wu and Kaipeng Zhang and Huipeng Deng and Jiaye Ge and Kai Chen and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},
      year={2025},
      eprint={2504.10479},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.10479}, 
}

@article{wang2025internvl3,
  title={Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency},
  author={Wang, Weiyun and Gao, Zhangwei and Gu, Lixin and Pu, Hengjun and Cui, Long and Wei, Xingguang and Liu, Zhaoyang and Jing, Linglin and Ye, Shenglong and Shao, Jie and others},
  journal={arXiv preprint arXiv:2508.18265},
  year={2025}
}

@article{dong2025scalable,
  title={Scalable vision language model training via high quality data curation},
  author={Dong, Hongyuan and Kang, Zijian and Yin, Weijie and Liang, Xiao and Feng, Chao and Ran, Jiao},
  journal={arXiv preprint arXiv:2501.05952},
  year={2025}
}


@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{chu2023mobilevlm,
  title={Mobilevlm: A fast, strong and open vision language assistant for mobile devices},
  author={Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
  journal={arXiv preprint arXiv:2312.16886},
  year={2023}
}

@article{chu2024mobilevlm,
  title={Mobilevlm v2: Faster and stronger baseline for vision language model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}

@article{wu2024mobilevlm,
  title={Mobilevlm: A vision-language model for better intra-and inter-ui understanding},
  author={Wu, Qinzhuo and Xu, Weikai and Liu, Wei and Tan, Tao and Liu, Jianfeng and Li, Ang and Luan, Jian and Wang, Bin and Shang, Shuo},
  journal={arXiv preprint arXiv:2409.14818},
  year={2024}
}

@inproceedings{you2024ferret,
  title={Ferret-ui: Grounded mobile ui understanding with multimodal llms},
  author={You, Keen and Zhang, Haotian and Schoop, Eldon and Weers, Floris and Swearngin, Amanda and Nichols, Jeffrey and Yang, Yinfei and Gan, Zhe},
  booktitle={European Conference on Computer Vision},
  pages={240--255},
  year={2024},
  organization={Springer}
}

@article{li2024ferret,
  title={Ferret-ui 2: Mastering universal user interface understanding across platforms},
  author={Li, Zhangheng and You, Keen and Zhang, Haotian and Feng, Di and Agrawal, Harsh and Li, Xiujun and Moorthy, Mohana Prasad Sathya and Nichols, Jeff and Yang, Yinfei and Gan, Zhe},
  journal={arXiv preprint arXiv:2410.18967},
  year={2024}
}
@misc{ONNX-Runtime,
  author       = {Microsoft},
  title        = {Accelerated edge machine learning},
  year         = {2023},
  howpublished = {\url{https://onnxruntime.ai/}},
}

@misc{llamacpp,
  author       = {Ggerganov},
  title        = {llama.cpp - LLM inference with minimal
setup and state-of-the-art performance on a wide range
of hardware},
  year         = {2023},
  howpublished = {\url{https:github.com/ggerganov/
llama.cpp/}},
}

@misc{MLC,
  author={MLC team},
  title={MLC-LLM - Universal LLM Deployment
Engine with ML Compilation},
  year={2023-2024},
  howpublished = {\url{https://github.com/
mlc-ai/mlc-llm/}},
}


@article{iyer2023automated,
  title={Automated Backend Allocation for Multi-Model, On-Device AI Inference},
  author={Iyer, Venkatraman and Lee, Sungho and Lee, Semun and Kim, Juitem Joonwoo and Kim, Hyunjun and Shin, Youngjae},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={7},
  number={3},
  pages={1--33},
  year={2023},
  publisher={ACM New York, NY, USA}
}
@article{jiang2020mnn,
  title={MNN: A universal and efficient inference engine},
  author={Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={1--13},
  year={2020}
}

@article{li2024transformer,
  title={Transformer-lite: High-efficiency deployment of large language models on mobile phone gpus},
  author={Li, Luchang and Qian, Sheng and Lu, Jie and Yuan, Lunxi and Wang, Rui and Xie, Qin},
  journal={arXiv preprint arXiv:2403.20041},
  year={2024}
}

@misc{genimi-nano,
  author       = {Deepmind},
  title        = {Gemini-nano},
  year         = {2024},
  howpublished = {\url{https://deepmind.google/models/gemini/nano/}},
}

@misc{apple-core-ML,
  author       = {Apple},
  title        = {On Device Llama 3.1 with Core ML},
  year         = {2024},
  howpublished = {\url{https://machinelearning.apple.com/research/core-ml-on-device-llama?utm_source=chatgpt.com}},
}

@article{li2025screenspot,
  title={Screenspot-pro: Gui grounding for professional high-resolution computer use},
  author={Li, Kaixin and Meng, Ziyang and Lin, Hongzhan and Luo, Ziyang and Tian, Yuchen and Ma, Jing and Huang, Zhiyong and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2504.07981},
  year={2025}
}

@article{li2023silkie,
  title={Silkie: Preference distillation for large visual language models},
  author={Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2312.10665},
  year={2023}
}


@article{zhou2024aligning,
  title={Aligning modalities in vision large language models via preference fine-tuning},
  author={Zhou, Yiyang and Cui, Chenhang and Rafailov, Rafael and Finn, Chelsea and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2402.11411},
  year={2024}
}

@article{dehghani2023patch,
  title={Patch npack: Navit, a vision transformer for any aspect ratio and resolution},
  author={Dehghani, Mostafa and Mustafa, Basil and Djolonga, Josip and Heek, Jonathan and Minderer, Matthias and Caron, Mathilde and Steiner, Andreas and Puigcerver, Joan and Geirhos, Robert and Alabdulmohsin, Ibrahim M and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={2252--2274},
  year={2023}
}

@online{kexuefm-8397,
    title={Transformer Upgrade Path: 4. Rotary Position Encoding for Two-Dimensional Positions},
    author={Su, Jianlin},
    year={2021},
    month={May},
    url={\url{https://kexue.fm/archives/8397}},
}

@misc{laurencon2024building,
      title={Building and better understanding vision-language models: insights and future directions.}, 
      author={Hugo Laurenon and Andrs Marafioti and Victor Sanh and Lo Tronchon},
      year={2024},
      eprint={2408.12637},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{sun2024generative,
  title={Generative multimodal models are in-context learners},
  author={Sun, Quan and Cui, Yufeng and Zhang, Xiaosong and Zhang, Fan and Yu, Qiying and Wang, Yueze and Rao, Yongming and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14398--14409},
  year={2024}
}

@misc{liu2024harnessingwebpageuistextrich,
      title={Harnessing Webpage UIs for Text-Rich Visual Understanding}, 
      author={Junpeng Liu and Tianyue Ou and Yifan Song and Yuxiao Qu and Wai Lam and Chenyan Xiong and Wenhu Chen and Graham Neubig and Xiang Yue},
      year={2024},
      eprint={2410.13824},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.13824}, 
}

@article{liu2022taisu,
  title={Taisu: A 166m large-scale high-quality dataset for chinese vision-language pre-training},
  author={Liu, Yulong and Zhu, Guibo and Zhu, Bin and Song, Qi and Ge, Guojing and Chen, Haoran and Qiao, GuanHui and Peng, Ru and Wu, Lingxiang and Wang, Jinqiao},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16705--16717},
  year={2022}
}

@article{jia2024leopard,
  title={Leopard: A vision language model for text-rich multi-image tasks},
  author={Jia, Mengzhao and Yu, Wenhao and Ma, Kaixin and Fang, Tianqing and Zhang, Zhihan and Ouyang, Siru and Zhang, Hongming and Jiang, Meng and Yu, Dong},
  journal={arXiv preprint arXiv:2410.01744},
  year={2024}
}

@article{hu2024mplug2,
  title={mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding},
  author={Hu, Anwen and Xu, Haiyang and Zhang, Liang and Ye, Jiabo and Yan, Ming and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2409.03420},
  year={2024}
}

@article{guo2024mammoth,
  title={Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale},
  author={Guo, Jarvis and Zheng, Tuney and Bai, Yuelin and Li, Bo and Wang, Yubo and Zhu, King and Li, Yizhi and Neubig, Graham and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2412.05237},
  year={2024}
}

@article{zhu2023multimodal,
  title={Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author={Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={8958--8974},
  year={2023}
}

@article{awadalla2024mint,
  title={Mint-1t: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens},
  author={Awadalla, Anas and Xue, Le and Lo, Oscar and Shu, Manli and Lee, Hannah and Guha, Etash and Shen, Sheng and Awadalla, Mohamed and Savarese, Silvio and Xiong, Caiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={36805--36828},
  year={2024}
}

@misc{Cauldron,
      title={What matters when building vision-language models?}, 
      author={Hugo Laurenon and Lo Tronchon and Matthieu Cord and Victor Sanh},
      year={2024},
      eprint={2405.02246},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yu2025opencsgchinesecorpusseries,
      title={OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training}, 
      author={Yijiong Yu and Ziyun Dai and Zekun Wang and Wei Wang and Ran Chen and Ji Pei},
      year={2025},
      eprint={2501.08197},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.08197}, 
}

@misc{lozhkov2024fineweb-edu,
    author       = { Lozhkov, Anton and Ben Allal, Loubna and von Werra, Leandro and Wolf, Thomas },  
    title        = { FineWeb-Edu: the Finest Collection of Educational Content }, 
    year         = 2024,  
    url          = { https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu },  
    doi          = { 10.57967/hf/2497 },
    publisher    = { Hugging Face }
}


@inproceedings{Huang2024OpenCoderTO,
  title = {OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models},
  author = {Siming Huang and Tianhao Cheng and Jason Klein Liu and Jiaran Hao and Liuyihan Song and Yang Xu and J. Yang and J. H. Liu and Chenchen Zhang and Linzheng Chai and Ruifeng Yuan and Zhaoxiang Zhang and Jie Fu and Qian Liu and Ge Zhang and Zili Wang and Yuan Qi and Yinghui Xu and Wei Chu},
  year = {2024},
  url = {https://arxiv.org/pdf/2411.04905}
}

@misc{allal2025smollm2smolgoesbig,
      title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Gabriel Martn Blzquez and Guilherme Penedo and Lewis Tunstall and Andrs Marafioti and Hynek Kydlek and Agustn Piqueres Lajarn and Vaibhav Srivastav and Joshua Lochner and Caleb Fahlgren and Xuan-Son Nguyen and Clmentine Fourrier and Ben Burtenshaw and Hugo Larcher and Haojun Zhao and Cyril Zakka and Mathieu Morlon and Colin Raffel and Leandro von Werra and Thomas Wolf},
      year={2025},
      eprint={2502.02737},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.02737}, 
}

@article{InfinityInstruct2024,
  title={Infinity Instruct},
  author={Beijing Academy of Artificial Intelligence (BAAI)},
  journal={arXiv preprint arXiv:2406.XXXX},
  year={2024}
}
@misc{ashkboos2024quarotoutlierfree4bitinference,
      title={QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs}, 
      author={Saleh Ashkboos and Amirkeivan Mohtashami and Maximilian L. Croci and Bo Li and Pashmina Cameron and Martin Jaggi and Dan Alistarh and Torsten Hoefler and James Hensman},
      year={2024},
      eprint={2404.00456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.00456}, 
}
@inproceedings{li2025bild,
  title={BiLD: Bi-directional Logits Difference Loss for Large Language Model Distillation},
  author={Li, Minchong and Zhou, Feng and Song, Xiaohui},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={1168--1182},
  year={2025}
}
@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

@article{visualwebinstruct,
  title     = {VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search},
  author    = {Jia, Yiming and Li, Jiachen and Yue, Xiang and Li, Bo and Nie, Ping and Zou, Kai and Chen, Wenhu},
  journal   = {arXiv preprint arXiv:2503.10582},
  year      = {2025}
}

@article{yue2024pangeafullyopenmultilingual,
  title={Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages},
  author={Xiang Yue and Yueqi Song and Akari Asai and Seungone Kim and Jean de Dieu Nyandwi and Simran Khanuja and Anjali Kantharuban and Lintang Sutawika and Sathyanarayanan Ramamoorthy and Graham Neubig},
  year={2024},
  journal={arXiv preprint arXiv:2410.16153},
  url={https://arxiv.org/abs/2410.16153}
}

@article{zhang20252,
  title={2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining},
  author={Zhang, Wenqi and Zhang, Hang and Li, Xin and Sun, Jiashuo and Shen, Yongliang and Lu, Weiming and Zhao, Deli and Zhuang, Yueting and Bing, Lidong},
  journal={arXiv preprint arXiv:2501.00958},
  year={2025}
}

@article{li2025megrez,
  title={Megrez-omni technical report},
  author={Li, Boxun and Li, Yadong and Li, Zhiyuan and Liu, Congyi and Liu, Weilin and Niu, Guowei and Tan, Zheyue and Xu, Haiyang and Yao, Zhuyu and Yuan, Tao and others},
  journal={arXiv preprint arXiv:2502.15803},
  year={2025}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@misc{yang2025ariauivisualgroundinggui,
      title={Aria-UI: Visual Grounding for GUI Instructions}, 
      author={Yuhao Yang and Yue Wang and Dongxu Li and Ziyang Luo and Bei Chen and Chao Huang and Junnan Li},
      year={2025},
      eprint={2412.16256},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2412.16256}, 
}

@article{zhang2024android,
  title={Android in the zoo: Chain-of-action-thought for gui agents},
  author={Zhang, Jiwen and Wu, Jihao and Teng, Yihua and Liao, Minghui and Xu, Nuo and Xiao, Xiao and Wei, Zhongyu and Tang, Duyu},
  journal={arXiv preprint arXiv:2403.02713},
  year={2024}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}
@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}
@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={22947--22970},
  year={2024}
}


@article{li2024eagle,
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2406.16858},
  year={2024}
}

@article{zhang2024learning,
  title={Learning harmonized representations for speculative sampling},
  author={Zhang, Lefan and Wang, Xiaodan and Huang, Yanhua and Xu, Ruiwen},
  journal={arXiv preprint arXiv:2408.15766},
  year={2024}
}

@inproceedings{fini2025multimodal,
  title={Multimodal autoregressive pre-training of large vision encoders},
  author={Fini, Enrico and Shukor, Mustafa and Li, Xiujun and Dufter, Philipp and Klein, Michal and Haldimann, David and Aitharaju, Sai and da Costa, Victor G Turrisi and B{\'e}thune, Louis and Gan, Zhe and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={9641--9654},
  year={2025}
}

@misc{deepscaler2025,
  title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL},
  author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Tianjun Zhang and Erran Li and Raluca Ada Popa and Ion Stoica},
  year={2025},
  howpublished={\url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}},
  note={Notion Blog}
}

@article{moshkov2025aimo2,
  title   = {AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset},
  author  = {Ivan Moshkov and Darragh Hanley and Ivan Sorokin and Shubham Toshniwal and Christof Henkel and Benedikt Schifferer and Wei Du and Igor Gitman},
  year    = {2025},
  journal = {arXiv preprint arXiv:2504.16891}
}

@article{ahmad2025opencodereasoning,
      title={OpenCodeReasoning: Advancing Data Distillation for Competitive Coding}, 
      author={Wasi Uddin Ahmad and Sean Narenthiran and Somshubra Majumdar and Aleksander Ficek and Siddhartha Jain and Jocelyn Huang and Vahid Noroozi and Boris Ginsburg},
      year={2025},
      eprint={2504.01943},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.01943}, 
}

@misc{guha2025openthoughtsdatarecipesreasoning,
  title={OpenThoughts: Data Recipes for Reasoning Models}, 
  author={Etash Guha and Ryan Marten and Sedrick Keh and Negin Raoof and Georgios Smyrnis and Hritik Bansal and Marianna Nezhurina and Jean Mercat and Trung Vu and Zayne Sprague and Ashima Suvarna and Benjamin Feuer and Liangyu Chen and Zaid Khan and Eric Frankel and Sachin Grover and Caroline Choi and Niklas Muennighoff and Shiye Su and Wanjia Zhao and John Yang and Shreyas Pimpalgaonkar and Kartik Sharma and Charlie Cheng-Jie Ji and Yichuan Deng and Sarah Pratt and Vivek Ramanujan and Jon Saad-Falcon and Jeffrey Li and Achal Dave and Alon Albalak and Kushal Arora and Blake Wulfe and Chinmay Hegde and Greg Durrett and Sewoong Oh and Mohit Bansal and Saadia Gabriel and Aditya Grover and Kai-Wei Chang and Vaishaal Shankar and Aaron Gokaslan and Mike A. Merrill and Tatsunori Hashimoto and Yejin Choi and Jenia Jitsev and Reinhard Heckel and Maheswaran Sathiamoorthy and Alexandros G. Dimakis and Ludwig Schmidt},
  year={2025},
  eprint={2506.04178},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2506.04178}, 
}

@software{NemotronPostTrainingDatasetV1,
      author = {Nathawani, Dhruv and Gitman, Igor and Majumdar, Somshubra and Bakhturina, Evelina and Sunil Mahabaleshwarkar, Ameya and and Zhang, Jian and Polak Scowcroft, Jane},
      title = {{Nemotron-Post-Training-Dataset-v1}},
      version = {1.0},
      publisher = {{NVIDIA}},
      year = {2025}, month = July,
      url = {https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1}
}

@article{jiang2025r,
  title={R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning},
  author={Jiang, Jie and Yang, Qi and Ni, Bolin and Xiang, Shiming and Hu, Han and Peng, Houwen},
  journal={arXiv preprint arXiv:2508.21113},
  year={2025}
}

@article{dong2025qianfan,
  title={Qianfan-VL: Domain-Enhanced Universal Vision-Language Models},
  author={Dong, Daxiang and Zheng, Mingming and Xu, Dong and Zhuang, Bairong and Zhang, Wenyu and Luo, Chunhua and Wang, Haoran and Zhao, Zijian and Li, Jie and Li, Yuxuan and others},
  journal={arXiv preprint arXiv:2509.18189},
  year={2025}
}
@inproceedings{wang2024allCRPE,
  title={The all-seeing project v2: Towards general relation comprehension of the open world},
  author={Wang, Weiyun and Ren, Yiming and Luo, Haowen and Li, Tiantong and Yan, Chenxiang and Chen, Zhe and Wang, Wenhai and Li, Qingyun and Lu, Lewei and Zhu, Xizhou and others},
  booktitle={European Conference on Computer Vision},
  pages={471--490},
  year={2024},
  organization={Springer}
}
@article{abouelenin2025phi,
  title={Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras},
  author={Abouelenin, Abdelrahman and Ashfaq, Atabak and Atkinson, Adam and Awadalla, Hany and Bach, Nguyen and Bao, Jianmin and Benhaim, Alon and Cai, Martin and Chaudhary, Vishrav and Chen, Congcong and others},
  journal={arXiv preprint arXiv:2503.01743},
  year={2025}
}
@article{yin2025sail,
  title={SAIL-VL2 Technical Report},
  author={Yin, Weijie and Ye, Yongjie and Shu, Fangxun and Liao, Yue and Kang, Zijian and Dong, Hongyuan and Yu, Haiyang and Yang, Dingkang and Wang, Jiacong and Wang, Han and others},
  journal={arXiv preprint arXiv:2509.14033},
  year={2025}
}