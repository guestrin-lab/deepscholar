\section{AndesUI Dataset}
\label{app:andesui_dataset}
In this section, we provide a comprehensive presentation of the AndesUI dataset construction pipeline, including the data collection process, human annotation, and data generation.

\textbf{Selection of APPs.}
We collected a total of 90 APPs, comprising 65 popular download APPs from the OPPO Software Store, covering a wide range of categories commonly used by users, along with 25 ColorOS pre-installed APPs. These APPs are listed in Table~\ref{tab:app_category}.

\textbf{Screenshot Data Collection.}
For each APP, we instructed annotators to capture screenshots of various diverse pages within the app, ensuring that each screenshot had distinct layouts and content. If two screenshots had similar layout structures but differed solely in text and images, they were classified as homogeneous interfaces. Our objective was to maximize diversity within the dataset while covering all typical interfaces of the app. Depending on the homogeneity degree, we collected between 1 and 10 screenshots for each heterogeneous page. For example, in the Xiaohongshu post interface, the display of different users' posts is similar enough to be regarded as a homogeneous page; however, since some posts include images while others do not, we aimed to collect additional screenshots from this homogeneous interface.

Throughout the screenshot collection process, we focused on capturing various atypical scenarios, including network interruptions and pop-ups (encompassing advertisement, log-in, confirmation, and phone pop-ups). For the training dataset, we collected a total of 10,747 screenshots from third-party apps and 2,255 screenshots from system pre-installed apps. In the testing set, there were a total of 455 screenshots. These screenshots were heterogeneous to reduce duplicate and similar pages. All detailed statistics of the dataset is shown in Table~\ref{tab:andesui_stat}

\textbf{Annotation of Widgets.}
Our objective was to provide annotations for all widgets present within each screenshot. This included delineating bounding boxes, identifying widget types, recording any text on the widgets (when available), and indicating whether they are clickable, among other details. For this process, we employed the VIA-2.0.12 tool~\cite{vgg_via}. Annotating all widgets manually from scratch is a labor-intensive endeavor; hence, we initially used Qwen2-VL-72B~\cite{wang2024qwen2vl} to generate preliminary annotations on each screenshot, converting these annotations into a JSON format compatible with VIA. Subsequent modifications and refinements were then carried out by annotators. On average, each interface resulted in 18 widgets. The training dataset contained a total of 226,901 widgets, while the testing dataset included 9,068 widgets. Examples of labeled widgets of screenshots are provided in Fig.~\ref{fig:ui_widget}.

\begin{figure}[htbp]
    \centering
    % 第一张子图
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Image_20250626212020.png}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    % 第二张子图
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Image_20250626212041.png}
        \label{fig:subfig2}
    \end{subfigure}
    \hfill
    % 第三张子图
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Image_20250626212056.png}
        \label{fig:subfig3}
    \end{subfigure}
    \caption{Examples of widget labels in the AndesUI dataset.}
    \label{fig:ui_widget}
\end{figure}

We needed to construct both basic and advanced data. Basic data includes grounding and referring data, while advanced data consists of comprehensive descriptive data and natural question-answer pairs. Basic data can be generated through programmatic means. In particular, for each widget, a single grounding data entry and a single referring data entry are generated. As an illustration, for a ``send'' widget with coordinates [3212, 1045, 3550, 2242], the associated grounding and referring data are
\begin{itemize}
    \item Question: ``Can you tell me the coordinates of the widget named 'send'?''
Answer: "<|box\_start|>(3212, 1047),(3550, 2242)<|box\_end|>"
\item Question: ``What is the widget located within the bounding box <|box\_start|>(3212, 1047),(3550, 2242)<|box\_end|>?''
Answer: ``send''
\end{itemize}
Consequently, the training dataset produced 226,901 data entries for referring and 185,968 for grounding. The test dataset included 7,194 grounding entries and 8,642 referring entries. This discrepancy occurs because a single page can contain several widgets sharing the same name, leading to fewer grounding entries. The questions were randomly selected from a seed library of questions. Initially, GPT-4 was employed to create 100 different question formulations. The bounding box coordinates underwent normalization and were then scaled by a factor of 10,000.

\textbf{Generation of Advanced Data.}
For comprehensive descriptive data, each screenshot was analyzed by aggregating the details of individual widgets. Subsequently, GPT-4 was utilized to generate a detailed description of the current page, including the theme, function, spatial arrangement of principal widgets, and a general usage guide for the interface.

For dataset creation involving natural question-answer pairs, we constructed several pairs for each screenshot. To achieve this, we initially utilized the information from each widget to instruct GPT-4 to formulate approximately ten question-answer pairs, emulating possible user inquiries during real-world application. The natural question-answer pairs were divided into four categories: descriptive questions, locating questions, interaction questions, and questions regarding natural scenes. They can also be classified by difficulty level: easy, medium, and hard. Questions classified as ``easy'' can be immediately answered, whereas ``hard'' questions might necessitate reasoning or multiple steps to resolve. Initially, GPT-4 was employed to generate several preliminary questions, which were then refined by annotators. Ultimately, we generated 107,688 natural question-answer pairs for the training set and constructed 1,181 pairs for the test set. Below is the system prompt used to guide GPT-4 in generating the natural question-answer pairs:

\textit{
You are an AI visual assistant capable of analyzing mobile screens. You will receive a screenshot from the \{app\_info\} app of the \{page\_description\} page, along with a string representation of a widget dictionary.
Each element in the dictionary is a dictionary that represents a UI widget, where the key is the widget number and the value contains information about the widget, including its bounding box coordinates, widget type, and widget description. The bounding box coordinates are represented as (x1, y1, x2, y2), with floating-point values ranging from 0 to 1.
Based on the provided text and coordinates, please design several simulated question-and-answer dialogues that represent interactions between the user and the system. These dialogues should focus on the user's potential actions on the screen (rather than perceptions).
The questions you create should be divided into three levels of difficulty: easy, medium, and hard. Easy questions can be answered directly from the widget dictionary. Medium and hard questions require some reasoning.
The questions can also be categorized into four content types: descriptive questions, locating questions, interaction questions, and natural scene questions. Here are four examples for reference; they may not be related to this image, and please do not restrict yourself to these few questions.
Examples of descriptive questions: Can you describe the function of widget\_23? / How many video list items are there in this screenshot?
Examples of locating questions: How do I access the creation page from the current page? / How can I view detailed information about the fourth video?
Examples of interaction questions: Can widget\_2 be swiped?
Examples of natural scene questions: How can I search for the latest movies? / How do I share the second video on social media?
For non-descriptive questions, you do not need to specify the type information of the widget in your responses.
When creating dialogues involving specific widgets, please strictly refer to the widget number (e.g., "widget\_3") rather than using the bounding box coordinates. This is necessary for me to use this data for function calls, so clear reference is required.
Your response format should be: [\{"User":"...","System":"...","Difficulty Level":"","Content Category":""\},...]
}