@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}






@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})













@inproceedings{zendel2022unifying,
  title={Unifying panoptic segmentation for autonomous driving},
  author={Zendel, Oliver and Sch{\"o}rghuber, Matthias and Rainer, Bernhard and Murschitz, Markus and Beleznai, Csaba},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{shin2023sdc,
  title={SDC-UDA: volumetric unsupervised domain adaptation framework for slice-direction continuous cross-modality medical image segmentation},
  author={Shin, Hyungseob and Kim, Hyeongyu and Kim, Sewon and Jun, Yohan and Eo, Taejoon and Hwang, Dosik},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{lee2020structure,
  title={Structure boundary preserving segmentation for medical image with ambiguous boundary},
  author={Lee, Hong Joo and Kim, Jung Uk and Lee, Sangmin and Kim, Hak Gu and Ro, Yong Man},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{goyal2022ifor,
  title={Ifor: Iterative flow minimization for robotic object rearrangement},
  author={Goyal, Ankit and Mousavian, Arsalan and Paxton, Chris and Chao, Yu-Wei and Okorn, Brian and Deng, Jia and Fox, Dieter},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{florence2020robot,
  title={Robot-supervised learning for object segmentation},
  author={Florence, Victoria and Corso, Jason J and Griffin, Brent},
  booktitle={ICRA},
  year={2020}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{cheng2021per,
  title={Per-pixel classification is not all you need for semantic segmentation},
  author={Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{zhou2022extract,
  title={Extract free dense labels from clip},
  author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{xu2023open,
  title={Open-vocabulary panoptic segmentation with text-to-image diffusion models},
  author={Xu, Jiarui and Liu, Sifei and Vahdat, Arash and Byeon, Wonmin and Wang, Xiaolong and De Mello, Shalini},
  booktitle={CVPR},
  pages={2955--2966},
  year={2023}
}

@inproceedings{liang2023open,
  title={Open-vocabulary semantic segmentation with mask-adapted clip},
  author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{yang2022lavt,
  title={Lavt: Language-aware vision transformer for referring image segmentation},
  author={Yang, Zhao and Wang, Jiaqi and Tang, Yansong and Chen, Kai and Zhao, Hengshuang and Torr, Philip HS},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{liu2023gres,
  title={Gres: Generalized referring expression segmentation},
  author={Liu, Chang and Ding, Henghui and Jiang, Xudong},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{wang2022cris,
  title={Cris: Clip-driven referring image segmentation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{xu2023bridging,
  title={Bridging vision and language encoders: Parameter-efficient tuning for referring image segmentation},
  author={Xu, Zunnan and Chen, Zhihong and Zhang, Yong and Song, Yibing and Wan, Xiang and Li, Guanbin},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yu2023zero,
  title={Zero-shot Referring Image Segmentation with Global-Local Context Features},
  author={Yu, Seonghoon and Seo, Paul Hongsuck and Son, Jeany},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{seo2020urvos,
  title={Urvos: Unified referring video object segmentation network with a large-scale benchmark},
  author={Seo, Seonguk and Lee, Joon-Young and Han, Bohyung},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{wu2022language,
  title={Language as queries for referring video object segmentation},
  author={Wu, Jiannan and Jiang, Yi and Sun, Peize and Yuan, Zehuan and Luo, Ping},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{li2023learning,
  title={Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples},
  author={Li, Guanghui and Gao, Mingqi and Liu, Heng and Zhen, Xiantong and Zheng, Feng},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{wu2023onlinerefer,
  title={OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation},
  author={Wu, Dongming and Wang, Tiancai and Zhang, Yuang and Zhang, Xiangyu and Shen, Jianbing},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{liu2024grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  booktitle={ECCV},
  year={2024}
}

@article{cheng2023segment,
  title={Segment and track anything},
  author={Cheng, Yangming and Li, Liulei and Xu, Yuanyou and Li, Xiaodi and Yang, Zongxin and Wang, Wenguan and Yang, Yi},
  journal={arXiv preprint arXiv:2305.06558},
  year={2023}
}

@article{li2023refsam,
  title={RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation},
  author={Li, Yonglin and Zhang, Jing and Teng, Xiao and Lan, Long},
  journal={arXiv preprint arXiv:2307.00997},
  year={2023}
}

@inproceedings{ye2019cross,
  title={Cross-modal self-attention network for referring image segmentation},
  author={Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Wang, Yang},
  booktitle={CVPR},
  year={2019}
}

@article{CMPC,
  title={Cross-modal progressive comprehension for referring segmentation},
  author={Liu, Si and Hui, Tianrui and Huang, Shaofei and Wei, Yunchao and Li, Bo and Li, Guanbin},
  journal={TPAMI},
  year={2021}
}

@article{PMINet,
  title={Progressive multimodal interaction network for referring video object segmentation},
  author={Ding, Zihan and Hui, Tianrui and Huang, Shaofei and Liu, Si and Luo, Xuan and Huang, Junshi and Wei, Xiaoming},
  journal={The 3rd Large-scale Video Object Segmentation Challenge},
  year={2021}
}

@inproceedings{YOFO,
  title={You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation},
  author={Li, Dezhuang and Li, Ruoqi and Wang, Lijun and Wang, Yifan and Qi, Jinqing and Zhang, Lu and Liu, Ting and Xu, Qingquan and Lu, Huchuan},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{LBDT,
  title={Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation},
  author={Ding, Zihan and Hui, Tianrui and Huang, Junshi and Wei, Xiaoming and Han, Jizhong and Liu, Si},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{MLRL,
  title={Multi-Level Representation Learning With Semantic Alignment for Referring Video Object Segmentation},
  author={Wu, Dongming and Dong, Xingping and Shao, Ling and Shen, Jianbing},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{MTTR,
  title={End-to-end referring video object segmentation with multimodal transformers},
  author={Botach, Adam and Zheltonozhskii, Evgenii and Baskin, Chaim},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{MANet,
  title={Multi-Attention Network for Compressed Video Referring Object Segmentation},
  author={Chen, Weidong and Hong, Dexiang and Qi, Yuankai and Han, Zhenjun and Wang, Shuhui and Qing, Laiyun and Huang, Qingming and Li, Guorong},
  booktitle={ACMMM},
  year={2022}
}

@inproceedings{miao2023spectrum,
  title={Spectrum-guided multi-granularity referring video object segmentation},
  author={Miao, Bo and Bennamoun, Mohammed and Gao, Yongsheng and Mian, Ajmal},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yuan2024losh,
  title={Losh: Long-short text joint prediction network for referring video object segmentation},
  author={Yuan, Linfeng and Shi, Miaojing and Yue, Zijie and Chen, Qijun},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yan2024referred,
  title={Referred by multi-modality: A unified temporal transformer for video object segmentation},
  author={Yan, Shilin and Zhang, Renrui and Guo, Ziyu and Chen, Wenchao and Zhang, Wei and Li, Hongyang and Qiao, Yu and Dong, Hao and He, Zhongjiang and Gao, Peng},
  booktitle={AAAI},
  year={2024}
}

@inproceedings{yan2023universal,
  title={Universal instance perception as object discovery and retrieval},
  author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Luo, Ping and Yuan, Zehuan and Lu, Huchuan},
  booktitle={CVPR},
  year={2023}
}

@article{ding2022vlt,
  title={Vlt: Vision-language transformer and query generation for referring segmentation},
  author={Ding, Henghui and Liu, Chang and Wang, Suchen and Jiang, Xudong},
  journal={TPAMI},
  year={2022}
}

@article{liang2023local,
  title={Local-global context aware transformer for language-guided video segmentation},
  author={Liang, Chen and Wang, Wenguan and Zhou, Tianfei and Miao, Jiaxu and Luo, Yawei and Yang, Yi},
  journal={TPAMI},
  year={2023}
}

@inproceedings{li2023robust,
  title={Robust Referring Video Object Segmentation with Cyclic Structural Consensus},
  author={Li, Xiang and Wang, Jinglu and Xu, Xiaohao and Li, Xiao and Raj, Bhiksha and Lu, Yan},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{han2023html,
  title={Html: Hybrid temporal-scale multimodal learning framework for referring video object segmentation},
  author={Han, Mingfei and Wang, Yali and Li, Zhihui and Yao, Lina and Chang, Xiaojun and Qiao, Yu},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{tang2023temporal,
  title={Temporal collection and distribution for referring video object segmentation},
  author={Tang, Jiajin and Zheng, Ge and Yang, Sibei},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{luo2024soc,
  title={SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation},
  author={Luo, Zhuoyan and Xiao, Yicheng and Liu, Yong and Li, Shuyan and Wang, Yitong and Tang, Yansong and Li, Xiu and Yang, Yujiu},
  booktitle={NeurIPS},
  year={2024}
}

@article{chen2023epcformer,
  title={EPCFormer: expression prompt collaboration transformer for universal referring video object segmentation},
  author={Chen, Jiajun and Lin, Jiacheng and Xiao, Zhiqiang and Fu, Haolong and Nai, Ke and Yang, Kailun and Li, Zhiyong},
  journal={arXiv preprint arXiv:2308.04162},
  year={2023}
}

@inproceedings{wu2023segment,
  title={Segment Every Reference Object in Spatial and Temporal Spaces},
  author={Wu, Jiannan and Jiang, Yi and Yan, Bin and Lu, Huchuan and Yuan, Zehuan and Luo, Ping},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{cheng2023tracking,
  title={Tracking anything with decoupled video segmentation},
  author={Cheng, Ho Kei and Oh, Seoung Wug and Price, Brian and Schwing, Alexander and Lee, Joon-Young},
  booktitle={ICCV},
  year={2023}
}

@article{ren2024grounded,
  title={Grounded sam: Assembling open-world models for diverse visual tasks},
  author={Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and others},
  journal={arXiv preprint arXiv:2401.14159},
  year={2024}
}

@inproceedings{he2024decoupling,
  title={Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation},
  author={He, Shuting and Ding, Henghui},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{zhu2024exploring,
  title={Exploring Pre-trained Text-to-Video Diffusion Models for Referring Video Object Segmentation},
  author={Zhu, Zixin and Feng, Xuelu and Chen, Dongdong and Yuan, Junsong and Qiao, Chunming and Hua, Gang},
  booktitle={ECCV},
  year={2024}
}

%% a2d %%
@inproceedings{hu2016segmentation,
  title={Segmentation from natural language expressions},
  author={Hu, Ronghang and Rohrbach, Marcus and Darrell, Trevor},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{gavrilyuk2018actor,
  title={Actor and action video segmentation from a sentence},
  author={Gavrilyuk, Kirill and Ghodrati, Amir and Li, Zhenyang and Snoek, Cees GM},
  booktitle={CVPR},
  year={2018}
}

@article{ye2021referring,
  title={Referring segmentation in images and videos with cross-modal self-attention network},
  author={Ye, Linwei and Rochan, Mrigank and Liu, Zhi and Zhang, Xiaoqin and Wang, Yang},
  journal={TPAMI},
  year={2021},
}

@inproceedings{wang2019asymmetric,
  title={Asymmetric cross-guided attention network for actor and action video segmentation from natural language query},
  author={Wang, Hao and Deng, Cheng and Yan, Junchi and Tao, Dacheng},
  booktitle={ICCV},
  year={2019}
}

@article{liu2021cross,
  title={Cross-modal progressive comprehension for referring segmentation},
  author={Liu, Si and Hui, Tianrui and Huang, Shaofei and Wei, Yunchao and Li, Bo and Li, Guanbin},
  journal={TPAMI},
  year={2021},
}

@article{liang2021clawcranenet,
  title={Clawcranenet: Leveraging object-level relation for text-based video segmentation},
  author={Liang, Chen and Wu, Yu and Luo, Yawei and Yang, Yi},
  journal={arXiv preprint arXiv:2103.10702},
  year={2021}
}

@inproceedings{khoreva2018video,
  title={Video object segmentation with referring expressions},
  author={Khoreva, Anna and Rohrbach, Anna and Schiele, Brent},
  booktitle={ECCV},
  year={2018}
}

@article{caelles20182018,
  title={The 2018 davis challenge on video object segmentation},
  author={Caelles, Sergi and Montes, Alberto and Maninis, Kevis-Kokitsi and Chen, Yuhua and Van Gool, Luc and Perazzi, Federico and Pont-Tuset, Jordi},
  journal={arXiv preprint arXiv:1803.00557},
  year={2018}
}

@inproceedings{xu2015can,
  title={Can humans fly? action understanding with multiple classes of actors},
  author={Xu, Chenliang and Hsieh, Shao-Hang and Xiong, Caiming and Corso, Jason J},
  booktitle={CVPR},
  pages={2264--2273},
  year={2015}
}

@inproceedings{jhuang2013towards,
  title={Towards understanding action recognition},
  author={Jhuang, Hueihan and Gall, Juergen and Zuffi, Silvia and Schmid, Cordelia and Black, Michael J},
  booktitle={ICCV},
  year={2013}
}

%% 2-1 related work
@article{MedSAM,
  title={Segment Anything in Medical Images},
  author={Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},
  journal={Nature Communications},
  year={2024}
}

@article{chen2023sam,
  title={SAM Fails to Segment Anything?--SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, Medical Image Segmentation, and More},
  author={Chen, Tianrun and Zhu, Lanyun and Ding, Chaotao and Cao, Runlong and Wang, Yan and Li, Zejian and Sun, Lingyun and Mao, Papa and Zang, Ying},
  journal={arXiv preprint arXiv:2304.09148},
  year={2023}
}

@article{wu2023medical,
  title={Medical sam adapter: Adapting segment anything model for medical image segmentation},
  author={Wu, Junde and Fu, Rao and Fang, Huihui and Liu, Yuanpei and Wang, Zhaowei and Xu, Yanwu and Jin, Yueming and Arbel, Tal},
  journal={arXiv preprint arXiv:2304.12620},
  year={2023}
}

@article{cheng2023sam,
  title={Sam-med2d},
  author={Cheng, Junlong and Ye, Jin and Deng, Zhongying and Chen, Jianpin and Li, Tianbin and Wang, Haoyu and Su, Yanzhou and Huang, Ziyan and Chen, Jilong and Jiang, Lei and others},
  journal={arXiv preprint arXiv:2308.16184},
  year={2023}
}

@article{chen2024rsprompter,
  title={RSPrompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model},
  author={Chen, Keyan and Liu, Chenyang and Chen, Hao and Zhang, Haotian and Li, Wenyuan and Zou, Zhengxia and Shi, Zhenwei},
  journal={TGRS},
  year={2024}
}

@inproceedings{SAMRS,
  title={SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model},
  author={Di Wang and Jing Zhang and Bo Du and Minqiang Xu and Lin Liu and Dacheng Tao and Liangpei Zhang},
  booktitle={NeurIPS},
  year={2024}
}

@article{sam-pt,
  title   = {Segment Anything Meets Point Tracking},
  author  = {Rajiƒç, Frano and Ke, Lei and Tai, Yu-Wing and Tang, Chi-Keung and Danelljan, Martin and Yu, Fisher},
  journal = {arXiv:2307.01197},
  year    = {2023}
}

@article{ravi2024sam,
  title={Sam 2: Segment anything in images and videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@article{yang2023track,
  title={Track anything: Segment anything meets videos},
  author={Yang, Jinyu and Gao, Mingqi and Li, Zhe and Gao, Shang and Wang, Fangjing and Zheng, Feng},
  journal={arXiv preprint arXiv:2304.11968},
  year={2023}
}

@inproceedings{zou2024segment,
  title={Segment everything everywhere all at once},
  author={Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{wang2023seggpt,
  title={SegGPT: Towards Segmenting Everything in Context},
  author={Wang, Xinlong and Zhang, Xiaosong and Cao, Yue and Wang, Wen and Shen, Chunhua and Huang, Tiejun},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{yang2022decoupling,
  title={Decoupling features in hierarchical propagation for video object segmentation},
  author={Yang, Zongxin and Yang, Yi},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{liu2023polyformer,
  title={PolyFormer: Referring image segmentation as sequential polygon generation},
  author={Liu, Jiang and Ding, Hui and Cai, Zhaowei and Zhang, Yuting and Satzoda, Ravi Kumar and Mahadevan, Vijay and Manmatha, R},
  booktitle={CVPR},
  year={2023}
}
%%

%% weakyl-supervised RVOS
@article{zhao2023learning,
  title={Learning Referring Video Object Segmentation from Weak Annotation},
  author={Zhao, Wangbo and Nan, Kepan and Zhang, Songyang and Chen, Kai and Lin, Dahua and You, Yang},
  journal={arXiv preprint arXiv:2308.02162},
  year={2023}
}

%% Datasets
@inproceedings{perazzi2016benchmark,
  title={A benchmark dataset and evaluation methodology for video object segmentation},
  author={Perazzi, Federico and Pont-Tuset, Jordi and McWilliams, Brian and Van Gool, Luc and Gross, Markus and Sorkine-Hornung, Alexander},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  year={2016}
}

@inproceedings{muller2018trackingnet,
  title={Trackingnet: A large-scale dataset and benchmark for object tracking in the wild},
  author={Muller, Matthias and Bibi, Adel and Giancola, Silvio and Alsubaihi, Salman and Ghanem, Bernard},
  booktitle={ECCV},
  year={2018}
}

@article{xu2018youtube,
  title={Youtube-vos: A large-scale video object segmentation benchmark},
  author={Xu, Ning and Yang, Linjie and Fan, Yuchen and Yue, Dingcheng and Liang, Yuchen and Yang, Jianchao and Huang, Thomas},
  journal={arXiv preprint arXiv:1809.03327},
  year={2018}
}

@inproceedings{fan2019lasot,
  title={Lasot: A high-quality benchmark for large-scale single object tracking},
  author={Fan, Heng and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Bai, Hexin and Xu, Yong and Liao, Chunyuan and Ling, Haibin},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{yang2019video,
  title={Video instance segmentation},
  author={Yang, Linjie and Fan, Yuchen and Xu, Ning},
  booktitle={ICCV},
  year={2019}
}

@article{huang2019got,
  title={Got-10k: A large high-diversity benchmark for generic object tracking in the wild},
  author={Huang, Lianghua and Zhao, Xin and Huang, Kaiqi},
  journal={TPAMI},
  year={2019}
}

@inproceedings{yu2020bdd100k,
  title={Bdd100k: A diverse driving dataset for heterogeneous multitask learning},
  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{pan2022wnet,
  title={Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networks},
  author={Pan, Wenwen and Shi, Haonan and Zhao, Zhou and Zhu, Jieming and He, Xiuqiang and Pan, Zhigeng and Gao, Lianli and Yu, Jun and Wu, Fei and Tian, Qi},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{zhou2022audio,
  title={Audio--visual segmentation},
  author={Zhou, Jinxing and Wang, Jianyuan and Zhang, Jiayi and Sun, Weixuan and Zhang, Jing and Birchfield, Stan and Guo, Dan and Kong, Lingpeng and Wang, Meng and Zhong, Yiran},
  booktitle={ECCV},
  year={2022}
}

@article{qi2022occluded,
  title={Occluded video instance segmentation: A benchmark},
  author={Qi, Jiyang and Gao, Yan and Hu, Yao and Wang, Xinggang and Liu, Xiaoyu and Bai, Xiang and Belongie, Serge and Yuille, Alan and Torr, Philip HS and Bai, Song},
  journal={IJCV},
  year={2022}
}

@inproceedings{hong2023lvos,
  title={Lvos: A benchmark for long-term video object segmentation},
  author={Hong, Lingyi and Chen, Wenchao and Liu, Zhongying and Zhang, Wei and Guo, Pinxue and Chen, Zhaoyu and Zhang, Wenqiang},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{ding2023mevis,
  title={MeViS: A large-scale benchmark for video segmentation with motion expressions},
  author={Ding, Henghui and Liu, Chang and He, Shuting and Jiang, Xudong and Loy, Chen Change},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{maaz2024video,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle={ACL},
  year={2024}
}

% IoU
@inproceedings{rezatofighi2019generalized,
  title={Generalized intersection over union: A metric and a loss for bounding box regression},
  author={Rezatofighi, Hamid and Tsoi, Nathan and Gwak, JunYoung and Sadeghian, Amir and Reid, Ian and Savarese, Silvio},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={ACL},
  year={2019}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}













@inproceedings{khoreva2017simple,
  title={Simple does it: Weakly supervised instance and semantic segmentation},
  author={Khoreva, Anna and Benenson, Rodrigo and Hosang, Jan and Hein, Matthias and Schiele, Bernt},
  booktitle={CVPR},
  year={2017}
}


@inproceedings{cao2023observation,
  title={Observation-centric sort: Rethinking sort for robust multi-object tracking},
  author={Cao, Jinkun and Pang, Jiangmiao and Weng, Xinshuo and Khirodkar, Rawal and Kitani, Kris},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{guo2024regiongpt,
  title={Regiongpt: Towards region understanding vision language model},
  author={Guo, Qiushan and De Mello, Shalini and Yin, Hongxu and Byeon, Wonmin and Cheung, Ka Chun and Yu, Yizhou and Luo, Ping and Liu, Sifei},
  booktitle={CVPR},
  year={2024}
}


% LLM-based RVOS
@article{zhu2023tracking,
  title={Tracking with human-intent reasoning},
  author={Zhu, Jiawen and Cheng, Zhi-Qi and He, Jun-Yan and Li, Chenyang and Luo, Bin and Lu, Huchuan and Geng, Yifeng and Xie, Xuansong},
  journal={arXiv preprint arXiv:2312.17448},
  year={2023}
}

@inproceedings{lai2024lisa,
  title={Lisa: Reasoning segmentation via large language model},
  author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{yan2024visa,
  title={Visa: Reasoning video object segmentation via large language models},
  author={Yan, Cilin and Wang, Haochen and Yan, Shilin and Jiang, Xiaolong and Hu, Yao and Kang, Guoliang and Xie, Weidi and Gavves, Efstratios},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{bai2024onetoken,
  title={One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos},
  author={Bai, Zechen and He, Tong and Mei, Haiyang and Wang, Pichao and Gao, Ziteng and Chen, Joya and Liu, Lei and Zhang, Zheng and Shou, Mike Zheng},
  booktitle={NeurIPS},
  year={2024}
}

@article{huang2024unleashing,
  title={Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation},
  author={Huang, Shaofei and Ling, Rui and Li, Hongyu and Hui, Tianrui and Tang, Zongheng and Wei, Xiaoming and Han, Jizhong and Liu, Si},
  journal={arXiv preprint arXiv:2408.15876},
  year={2024}
}

% Github
@misc{ren2024grounded2,
  title={Grounded SAM 2},
  author={Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and others},
  howpublished = "\url{https://github.com/IDEA-Research/Grounded-SAM-2}",
  year={2024}
}

@article{wang2023modelscope,
  title={Modelscope text-to-video technical report},
  author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},
  journal={arXiv preprint arXiv:2308.06571},
  year={2023}
}
