


\input{ytvos+davis}

% \input{Figures/fig4}

% \input{Figures/fig3}


% \input{Figures/fig1}
% \input{Figures/fig2}




% \input{Tables/a2d}

% \input{Tables/jhmdb}


% \input{figures/vis_davis}
% \input{figures/vis_ytvos}

\section{Experiments}
\label{sec:exp_sec4}
\subsection{Datasets and Implementation Details}


\paragraph{Datasets.} We conduct experiments on RVOS benchmark datasets: Refer-Youtube-VOS~\citep{seo2020urvos} and Refer-DAVIS$_{17}$~\citep{khoreva2018video}. Refer-Youtube-VOS is a large-scale dataset for RVOS, with $3,975$ videos, $7,451$ objects, and $27,899$ expressions. Refer-DAVIS$_{17}$ is augmented from the popular video object segmentation dataset, DAVIS$_{17}$~\citep{caelles20182018}. It contains $90$ videos ($60$ for training and $30$ for testing) with more than $1,500$ expressions. Since the annotations of the Ref-Youtube-VOS validation set are not publicly released, we evaluate the results on the official server. As for Ref-DAVIS$_{17}$, we use the official code for evaluation.
\vspace{-3mm}
\paragraph{Implementation Details.} For the image encoder, we use the CLIP image encoder pretrained from~\citet{guo2024regiongpt} followed by a two-layer MLP. As for the text encoder, we use the pretrained CLIP text encoder with a two-layer MLP. As for the Transformer, we use a one-layer Transformer encoder layer. We only train the MLP's and Transformer's parameters. We use the learning rate of $0.0001$ and train for $50$ epochs with the Adam optimizer. All models are implemented in PyTorch and trained on NVIDIA H100 GPUs.
\label{sec:implementation}
\vspace{-3mm}

\subsection{Quantitative and Qualitative Results}
We compare our \nameshort~framework with the following three types of methods for the RVOS task:
\begin{itemize}[leftmargin=*]
    \item Standard RVOS approaches: MTTR~\citep{MTTR}, ReferFormer~\citep{wu2022language}, $\text{R}^2\text{-VOS}$~\citep{li2023robust}, HTML~\citep{han2023html}, OnlineRefer~\citep{wu2023onlinerefer}, SgMg~\citep{miao2023spectrum}, TempCD~\citep{tang2023temporal}, and RefSAM~\citep{li2023refsam}.
    % SOC~\citep{luo2024soc}, LoSh~\citep{yuan2024losh}, DsHmp~\citep{he2024decoupling}.
    \item Large-scale training approaches: UniNEXT~\citep{yan2023universal}, DEVA~\citep{cheng2023tracking}, UniRef~\citep{wu2023segment}, TrackGPT~\citep{zhu2023tracking}, LISA~\citep{lai2024lisa}, VISA~\citep{yan2024visa}, VD-IT~\citep{zhu2024exploring}, and VideoLISA~\citep{bai2024onetoken}.
    \item Efficient tuning approaches: WRVOS~\citep{zhao2023learning}, Grounded-SAM~\citep{ren2024grounded}, and Grounded-SAM2~\citep{ren2024grounded2}.
\end{itemize}

\input{fig4}

\input{fig3}

In Table~\ref{tab:quantitative}, we first provide quantitative results on Ref-YouTube-VOS and Ref-DAVIS17. We see that our \nameshort~framework employs \nameselect~to select the track for prompting SAM, resulting in $65.5\%$ and $71.0\%$ in $\mathcal{J}\&\mathcal{F}$ on the two datasets, respectively. For the judo case in Figure~\ref{fig:visual_prompt3}, we see that \nameselect~is able to select the reference proposal for the referred object and produces similar results with the ground truth. As for the difficult eyeglasses case in Figure~\ref{fig:visual_prompt4}, our framework also performs better than Grounded-SAM2, demonstrating the effectiveness of our method. 





% \input{Figures/fig2}
\subsection{Ablation Studies}

\paragraph{Candidate Track Variants.} In Table~\ref{table:candidate}, we additionally provide the results of candidate track variants. By comparing the first and second rows, we see that if we instead use Top-5 proposals from the finetuned detector to produce tracks, a $5.5\%$ performance drop in $\mathcal{J}\&\mathcal{F}$ would be observed. This is because the finetuned detector would lose diversity and produce repetitive box proposals, making the subsequent tracking meaningless. As a result, we take the Top-5 proposals from the pretrained Grounding DINO plus the Top-1 proposal from the finetuned one to produce candidate tracks from OC-SORT. Instead of simply selecting one best candidate track, another alternative is to greedily select and merge the tracks according to the mIoUs. Nevertheless, the resulting $\mathcal{J}\&\mathcal{F}$ is only $1.2\%$ higher in the third row. Hence, we simply consider the candidates individually.

\paragraph{Number of Proposals.} In Table~\ref{table:num}, we additionally provide the results when varying the number of proposals K from the pretrained Grounding DINO, and we see that the $\mathcal{J}\&\mathcal{F}$ saturates when K$\geq$5. Therefore, we simply set K to 5 for efficiency. 

\input{candidate}
\input{new_table_2}
% \paragraph{More Qualitative Results.} From Figure~\ref{fig:visual_prompt2}, we see that the candidate track with the highest confidence score (c) performs significantly worse than the one with the highest mIoU (e). This further supports that high-quality candidate tracks could not be easily identified.


% \input{Tables/setting}
% \subsection{Setting and Efficiency Comparisons}
% % \input{Tables/setting}
\input{parameter}

% In Table~\ref{tab:setting}, we compare the setting of our proposed \nameshort~framework with recent RVOS methods. From this table, we see that WRVOS~\cite{zhao2023learning} attempts to address RVOS from box-level weak supervision plus the ground-truth mask for the first frame, while OnlineRefer~\cite{wu2023onlinerefer} extends ReferFormer~\cite{wu2022language} with query propagation to handle ongoing videos under the online setting. However, these methods require end-to-end training for vision-language models, which could be computationally expensive and time-consuming. On the other hand, assuming that additional video data are accessible, DEVA~\cite{cheng2023tracking} decouples RVOS into image segmentation and temporal propagation to increase the scalability. Compared to these works, our proposed \nameshort~framework decouples RVOS into proposal generation and prompted segmentation with no need for additional video data for training. In this decoupled manner, our framework can learn proper prompts from weak supervision for foundation segmentation models and could also be applied to online settings. 

\paragraph{Efficiency Comparisons.} In Table~\ref{tab:parameter}, 
% In Table~\ref{tab:setting}, 
we also provide efficiency comparisons with recent works. We see that the number of trainable parameters of our method is over $2$ times fewer than DEVA. This is because our proposed \nameshort~framework learns to prompt foundation models for efficient adaptation instead of training a vision-language model end-to-end. Together with the quantitative comparisons in Table~\ref{tab:quantitative}, we validate that our proposed \nameshort~framework is preferable in terms of performance, setting, and efficiency.