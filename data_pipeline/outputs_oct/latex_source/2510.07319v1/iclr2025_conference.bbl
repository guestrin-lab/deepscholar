\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2024)Bai, He, Mei, Wang, Gao, Chen, Liu, Zhang, and Shou]{bai2024onetoken}
Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike~Zheng Shou.
\newblock One token to seg them all: Language instructed reasoning segmentation in videos.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Botach et~al.(2022)Botach, Zheltonozhskii, and Baskin]{MTTR}
Adam Botach, Evgenii Zheltonozhskii, and Chaim Baskin.
\newblock End-to-end referring video object segmentation with multimodal transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Caelles et~al.(2018)Caelles, Montes, Maninis, Chen, Van~Gool, Perazzi, and Pont-Tuset]{caelles20182018}
Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis, Yuhua Chen, Luc Van~Gool, Federico Perazzi, and Jordi Pont-Tuset.
\newblock The 2018 davis challenge on video object segmentation.
\newblock \emph{arXiv preprint arXiv:1803.00557}, 2018.

\bibitem[Cao et~al.(2023)Cao, Pang, Weng, Khirodkar, and Kitani]{cao2023observation}
Jinkun Cao, Jiangmiao Pang, Xinshuo Weng, Rawal Khirodkar, and Kris Kitani.
\newblock Observation-centric sort: Rethinking sort for robust multi-object tracking.
\newblock In \emph{CVPR}, 2023.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko]{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In \emph{ECCV}, 2020.

\bibitem[Chen et~al.(2024)Chen, Liu, Chen, Zhang, Li, Zou, and Shi]{chen2024rsprompter}
Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi.
\newblock Rsprompter: Learning to prompt for remote sensing instance segmentation based on visual foundation model.
\newblock \emph{TGRS}, 2024.

\bibitem[Chen et~al.(2023)Chen, Zhu, Ding, Cao, Wang, Li, Sun, Mao, and Zang]{chen2023sam}
Tianrun Chen, Lanyun Zhu, Chaotao Ding, Runlong Cao, Yan Wang, Zejian Li, Lingyun Sun, Papa Mao, and Ying Zang.
\newblock Sam fails to segment anything?--sam-adapter: Adapting sam in underperformed scenes: Camouflage, shadow, medical image segmentation, and more.
\newblock \emph{arXiv preprint arXiv:2304.09148}, 2023.

\bibitem[Cheng et~al.(2021)Cheng, Schwing, and Kirillov]{cheng2021per}
Bowen Cheng, Alex Schwing, and Alexander Kirillov.
\newblock Per-pixel classification is not all you need for semantic segmentation.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Cheng et~al.(2023{\natexlab{a}})Cheng, Oh, Price, Schwing, and Lee]{cheng2023tracking}
Ho~Kei Cheng, Seoung~Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee.
\newblock Tracking anything with decoupled video segmentation.
\newblock In \emph{ICCV}, 2023{\natexlab{a}}.

\bibitem[Cheng et~al.(2023{\natexlab{b}})Cheng, Ye, Deng, Chen, Li, Wang, Su, Huang, Chen, Jiang, et~al.]{cheng2023sam}
Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et~al.
\newblock Sam-med2d.
\newblock \emph{arXiv preprint arXiv:2308.16184}, 2023{\natexlab{b}}.

\bibitem[Cheng et~al.(2023{\natexlab{c}})Cheng, Li, Xu, Li, Yang, Wang, and Yang]{cheng2023segment}
Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi~Yang.
\newblock Segment and track anything.
\newblock \emph{arXiv preprint arXiv:2305.06558}, 2023{\natexlab{c}}.

\bibitem[Ding et~al.(2023)Ding, Liu, He, Jiang, and Loy]{ding2023mevis}
Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen~Change Loy.
\newblock Mevis: A large-scale benchmark for video segmentation with motion expressions.
\newblock In \emph{ICCV}, 2023.

\bibitem[Fan et~al.(2019)Fan, Lin, Yang, Chu, Deng, Yu, Bai, Xu, Liao, and Ling]{fan2019lasot}
Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge~Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
\newblock Lasot: A high-quality benchmark for large-scale single object tracking.
\newblock In \emph{CVPR}, 2019.

\bibitem[Goyal et~al.(2022)Goyal, Mousavian, Paxton, Chao, Okorn, Deng, and Fox]{goyal2022ifor}
Ankit Goyal, Arsalan Mousavian, Chris Paxton, Yu-Wei Chao, Brian Okorn, Jia Deng, and Dieter Fox.
\newblock Ifor: Iterative flow minimization for robotic object rearrangement.
\newblock In \emph{CVPR}, 2022.

\bibitem[Guo et~al.(2024)Guo, De~Mello, Yin, Byeon, Cheung, Yu, Luo, and Liu]{guo2024regiongpt}
Qiushan Guo, Shalini De~Mello, Hongxu Yin, Wonmin Byeon, Ka~Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu.
\newblock Regiongpt: Towards region understanding vision language model.
\newblock In \emph{CVPR}, 2024.

\bibitem[Han et~al.(2023)Han, Wang, Li, Yao, Chang, and Qiao]{han2023html}
Mingfei Han, Yali Wang, Zhihui Li, Lina Yao, Xiaojun Chang, and Yu~Qiao.
\newblock Html: Hybrid temporal-scale multimodal learning framework for referring video object segmentation.
\newblock In \emph{ICCV}, 2023.

\bibitem[He \& Ding(2024)He and Ding]{he2024decoupling}
Shuting He and Henghui Ding.
\newblock Decoupling static and hierarchical motion perception for referring video segmentation.
\newblock In \emph{CVPR}, 2024.

\bibitem[Hong et~al.(2023)Hong, Chen, Liu, Zhang, Guo, Chen, and Zhang]{hong2023lvos}
Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang.
\newblock Lvos: A benchmark for long-term video object segmentation.
\newblock In \emph{ICCV}, 2023.

\bibitem[Hu et~al.(2016)Hu, Rohrbach, and Darrell]{hu2016segmentation}
Ronghang Hu, Marcus Rohrbach, and Trevor Darrell.
\newblock Segmentation from natural language expressions.
\newblock In \emph{ECCV}, 2016.

\bibitem[Huang et~al.(2019)Huang, Zhao, and Huang]{huang2019got}
Lianghua Huang, Xin Zhao, and Kaiqi Huang.
\newblock Got-10k: A large high-diversity benchmark for generic object tracking in the wild.
\newblock \emph{TPAMI}, 2019.

\bibitem[Khoreva et~al.(2017)Khoreva, Benenson, Hosang, Hein, and Schiele]{khoreva2017simple}
Anna Khoreva, Rodrigo Benenson, Jan Hosang, Matthias Hein, and Bernt Schiele.
\newblock Simple does it: Weakly supervised instance and semantic segmentation.
\newblock In \emph{CVPR}, 2017.

\bibitem[Khoreva et~al.(2018)Khoreva, Rohrbach, and Schiele]{khoreva2018video}
Anna Khoreva, Anna Rohrbach, and Brent Schiele.
\newblock Video object segmentation with referring expressions.
\newblock In \emph{ECCV}, 2018.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson, Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock In \emph{ICCV}, 2023.

\bibitem[Lai et~al.(2024)Lai, Tian, Chen, Li, Yuan, Liu, and Jia]{lai2024lisa}
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.
\newblock Lisa: Reasoning segmentation via large language model.
\newblock In \emph{CVPR}, 2024.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Gao, Liu, Zhen, and Zheng]{li2023learning}
Guanghui Li, Mingqi Gao, Heng Liu, Xiantong Zhen, and Feng Zheng.
\newblock Learning cross-modal affinity for referring video object segmentation targeting limited samples.
\newblock In \emph{ICCV}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Wang, Xu, Li, Raj, and Lu]{li2023robust}
Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, and Yan Lu.
\newblock Robust referring video object segmentation with cyclic structural consensus.
\newblock In \emph{ICCV}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Zhang, Teng, and Lan]{li2023refsam}
Yonglin Li, Jing Zhang, Xiao Teng, and Long Lan.
\newblock Refsam: Efficiently adapting segmenting anything model for referring video object segmentation.
\newblock \emph{arXiv preprint arXiv:2307.00997}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Ding, and Jiang]{liu2023gres}
Chang Liu, Henghui Ding, and Xudong Jiang.
\newblock Gres: Generalized referring expression segmentation.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Ding, Cai, Zhang, Satzoda, Mahadevan, and Manmatha]{liu2023polyformer}
Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi~Kumar Satzoda, Vijay Mahadevan, and R~Manmatha.
\newblock Polyformer: Referring image segmentation as sequential polygon generation.
\newblock In \emph{CVPR}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2024)Liu, Zeng, Ren, Li, Zhang, Yang, Li, Yang, Su, Zhu, et~al.]{liu2024grounding}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
\newblock In \emph{ECCV}, 2024.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{CVPR}, 2015.

\bibitem[Luo et~al.(2024)Luo, Xiao, Liu, Li, Wang, Tang, Li, and Yang]{luo2024soc}
Zhuoyan Luo, Yicheng Xiao, Yong Liu, Shuyan Li, Yitong Wang, Yansong Tang, Xiu Li, and Yujiu Yang.
\newblock Soc: Semantic-assisted object cluster for referring video object segmentation.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Ma et~al.(2024)Ma, He, Li, Han, You, and Wang]{MedSAM}
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo~Wang.
\newblock Segment anything in medical images.
\newblock \emph{Nature Communications}, 2024.

\bibitem[Maaz et~al.(2024)Maaz, Rasheed, Khan, and Khan]{maaz2024video}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision and language models.
\newblock In \emph{ACL}, 2024.

\bibitem[Mao et~al.(2016)Mao, Huang, Toshev, Camburu, Yuille, and Murphy]{mao2016generation}
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan~L Yuille, and Kevin Murphy.
\newblock Generation and comprehension of unambiguous object descriptions.
\newblock In \emph{CVPR}, 2016.

\bibitem[Miao et~al.(2023)Miao, Bennamoun, Gao, and Mian]{miao2023spectrum}
Bo~Miao, Mohammed Bennamoun, Yongsheng Gao, and Ajmal Mian.
\newblock Spectrum-guided multi-granularity referring video object segmentation.
\newblock In \emph{ICCV}, 2023.

\bibitem[Muller et~al.(2018)Muller, Bibi, Giancola, Alsubaihi, and Ghanem]{muller2018trackingnet}
Matthias Muller, Adel Bibi, Silvio Giancola, Salman Alsubaihi, and Bernard Ghanem.
\newblock Trackingnet: A large-scale dataset and benchmark for object tracking in the wild.
\newblock In \emph{ECCV}, 2018.

\bibitem[Perazzi et~al.(2016)Perazzi, Pont-Tuset, McWilliams, Van~Gool, Gross, and Sorkine-Hornung]{perazzi2016benchmark}
Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van~Gool, Markus Gross, and Alexander Sorkine-Hornung.
\newblock A benchmark dataset and evaluation methodology for video object segmentation.
\newblock In \emph{CVPR}, 2016.

\bibitem[Qi et~al.(2022)Qi, Gao, Hu, Wang, Liu, Bai, Belongie, Yuille, Torr, and Bai]{qi2022occluded}
Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip~HS Torr, and Song Bai.
\newblock Occluded video instance segmentation: A benchmark.
\newblock \emph{IJCV}, 2022.

\bibitem[Rajič et~al.(2023)Rajič, Ke, Tai, Tang, Danelljan, and Yu]{sam-pt}
Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu.
\newblock Segment anything meets point tracking.
\newblock \emph{arXiv:2307.01197}, 2023.

\bibitem[Ravi et~al.(2024)Ravi, Gabeur, Hu, Hu, Ryali, Ma, Khedr, R{\"a}dle, Rolland, Gustafson, et~al.]{ravi2024sam}
Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R{\"a}dle, Chloe Rolland, Laura Gustafson, et~al.
\newblock Sam 2: Segment anything in images and videos.
\newblock \emph{arXiv preprint arXiv:2408.00714}, 2024.

\bibitem[Ren et~al.(2024{\natexlab{a}})Ren, Liu, Zeng, Lin, Li, Cao, Chen, Huang, Chen, Yan, et~al.]{ren2024grounded}
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He~Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et~al.
\newblock Grounded sam: Assembling open-world models for diverse visual tasks.
\newblock \emph{arXiv preprint arXiv:2401.14159}, 2024{\natexlab{a}}.

\bibitem[Ren et~al.(2024{\natexlab{b}})Ren, Liu, Zeng, Lin, Li, Cao, Chen, Huang, Chen, Yan, et~al.]{ren2024grounded2}
Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He~Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et~al.
\newblock Grounded sam 2.
\newblock \url{https://github.com/IDEA-Research/Grounded-SAM-2}, 2024{\natexlab{b}}.

\bibitem[Seo et~al.(2020)Seo, Lee, and Han]{seo2020urvos}
Seonguk Seo, Joon-Young Lee, and Bohyung Han.
\newblock Urvos: Unified referring video object segmentation network with a large-scale benchmark.
\newblock In \emph{ECCV}, 2020.

\bibitem[Shin et~al.(2023)Shin, Kim, Kim, Jun, Eo, and Hwang]{shin2023sdc}
Hyungseob Shin, Hyeongyu Kim, Sewon Kim, Yohan Jun, Taejoon Eo, and Dosik Hwang.
\newblock Sdc-uda: volumetric unsupervised domain adaptation framework for slice-direction continuous cross-modality medical image segmentation.
\newblock In \emph{CVPR}, 2023.

\bibitem[Tang et~al.(2023)Tang, Zheng, and Yang]{tang2023temporal}
Jiajin Tang, Ge~Zheng, and Sibei Yang.
\newblock Temporal collection and distribution for referring video object segmentation.
\newblock In \emph{ICCV}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2024)Wang, Zhang, Du, Xu, Liu, Tao, and Zhang]{SAMRS}
Di~Wang, Jing Zhang, Bo~Du, Minqiang Xu, Lin Liu, Dacheng Tao, and Liangpei Zhang.
\newblock Samrs: Scaling-up remote sensing segmentation dataset with segment anything model.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Yuan, Chen, Zhang, Wang, and Zhang]{wang2023modelscope}
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
\newblock Modelscope text-to-video technical report.
\newblock \emph{arXiv preprint arXiv:2308.06571}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhang, Cao, Wang, Shen, and Huang]{wang2023seggpt}
Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang.
\newblock Seggpt: Towards segmenting everything in context.
\newblock In \emph{ICCV}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Wang, Zhang, Zhang, and Shen]{wu2023onlinerefer}
Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen.
\newblock Onlinerefer: A simple online baseline for referring video object segmentation.
\newblock In \emph{ICCV}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2022)Wu, Jiang, Sun, Yuan, and Luo]{wu2022language}
Jiannan Wu, Yi~Jiang, Peize Sun, Zehuan Yuan, and Ping Luo.
\newblock Language as queries for referring video object segmentation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Jiang, Yan, Lu, Yuan, and Luo]{wu2023segment}
Jiannan Wu, Yi~Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo.
\newblock Segment every reference object in spatial and temporal spaces.
\newblock In \emph{ICCV}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{c}})Wu, Fu, Fang, Liu, Wang, Xu, Jin, and Arbel]{wu2023medical}
Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei Wang, Yanwu Xu, Yueming Jin, and Tal Arbel.
\newblock Medical sam adapter: Adapting segment anything model for medical image segmentation.
\newblock \emph{arXiv preprint arXiv:2304.12620}, 2023{\natexlab{c}}.

\bibitem[Xu et~al.(2018)Xu, Yang, Fan, Yue, Liang, Yang, and Huang]{xu2018youtube}
Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang.
\newblock Youtube-vos: A large-scale video object segmentation benchmark.
\newblock \emph{arXiv preprint arXiv:1809.03327}, 2018.

\bibitem[Xu et~al.(2023)Xu, Chen, Zhang, Song, Wan, and Li]{xu2023bridging}
Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, and Guanbin Li.
\newblock Bridging vision and language encoders: Parameter-efficient tuning for referring image segmentation.
\newblock In \emph{ICCV}, 2023.

\bibitem[Yan et~al.(2023)Yan, Jiang, Wu, Wang, Luo, Yuan, and Lu]{yan2023universal}
Bin Yan, Yi~Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu.
\newblock Universal instance perception as object discovery and retrieval.
\newblock In \emph{CVPR}, 2023.

\bibitem[Yan et~al.(2024{\natexlab{a}})Yan, Wang, Yan, Jiang, Hu, Kang, Xie, and Gavves]{yan2024visa}
Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, and Efstratios Gavves.
\newblock Visa: Reasoning video object segmentation via large language models.
\newblock In \emph{ECCV}, 2024{\natexlab{a}}.

\bibitem[Yan et~al.(2024{\natexlab{b}})Yan, Zhang, Guo, Chen, Zhang, Li, Qiao, Dong, He, and Gao]{yan2024referred}
Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu~Qiao, Hao Dong, Zhongjiang He, and Peng Gao.
\newblock Referred by multi-modality: A unified temporal transformer for video object segmentation.
\newblock In \emph{AAAI}, 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2023)Yang, Gao, Li, Gao, Wang, and Zheng]{yang2023track}
Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng.
\newblock Track anything: Segment anything meets videos.
\newblock \emph{arXiv preprint arXiv:2304.11968}, 2023.

\bibitem[Yang et~al.(2019)Yang, Fan, and Xu]{yang2019video}
Linjie Yang, Yuchen Fan, and Ning Xu.
\newblock Video instance segmentation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Yang et~al.(2022)Yang, Wang, Tang, Chen, Zhao, and Torr]{yang2022lavt}
Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip~HS Torr.
\newblock Lavt: Language-aware vision transformer for referring image segmentation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yang \& Yang(2022)Yang and Yang]{yang2022decoupling}
Zongxin Yang and Yi~Yang.
\newblock Decoupling features in hierarchical propagation for video object segmentation.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Yu et~al.(2020)Yu, Chen, Wang, Xian, Chen, Liu, Madhavan, and Darrell]{yu2020bdd100k}
Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell.
\newblock Bdd100k: A diverse driving dataset for heterogeneous multitask learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Yu et~al.(2016)Yu, Poirson, Yang, Berg, and Berg]{yu2016modeling}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In \emph{ECCV}, 2016.

\bibitem[Yu et~al.(2023)Yu, Seo, and Son]{yu2023zero}
Seonghoon Yu, Paul~Hongsuck Seo, and Jeany Son.
\newblock Zero-shot referring image segmentation with global-local context features.
\newblock In \emph{CVPR}, 2023.

\bibitem[Yuan et~al.(2024)Yuan, Shi, Yue, and Chen]{yuan2024losh}
Linfeng Yuan, Miaojing Shi, Zijie Yue, and Qijun Chen.
\newblock Losh: Long-short text joint prediction network for referring video object segmentation.
\newblock In \emph{CVPR}, 2024.

\bibitem[Zendel et~al.(2022)Zendel, Sch{\"o}rghuber, Rainer, Murschitz, and Beleznai]{zendel2022unifying}
Oliver Zendel, Matthias Sch{\"o}rghuber, Bernhard Rainer, Markus Murschitz, and Csaba Beleznai.
\newblock Unifying panoptic segmentation for autonomous driving.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Nan, Zhang, Chen, Lin, and You]{zhao2023learning}
Wangbo Zhao, Kepan Nan, Songyang Zhang, Kai Chen, Dahua Lin, and Yang You.
\newblock Learning referring video object segmentation from weak annotation.
\newblock \emph{arXiv preprint arXiv:2308.02162}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Cheng, He, Li, Luo, Lu, Geng, and Xie]{zhu2023tracking}
Jiawen Zhu, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Bin Luo, Huchuan Lu, Yifeng Geng, and Xuansong Xie.
\newblock Tracking with human-intent reasoning.
\newblock \emph{arXiv preprint arXiv:2312.17448}, 2023.

\bibitem[Zhu et~al.(2024)Zhu, Feng, Chen, Yuan, Qiao, and Hua]{zhu2024exploring}
Zixin Zhu, Xuelu Feng, Dongdong Chen, Junsong Yuan, Chunming Qiao, and Gang Hua.
\newblock Exploring pre-trained text-to-video diffusion models for referring video object segmentation.
\newblock In \emph{ECCV}, 2024.

\bibitem[Zou et~al.(2024)Zou, Yang, Zhang, Li, Li, Wang, Wang, Gao, and Lee]{zou2024segment}
Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong~Jae Lee.
\newblock Segment everything everywhere all at once.
\newblock In \emph{NeurIPS}, 2024.

\end{thebibliography}
