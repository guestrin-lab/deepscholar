\documentclass[journal, 11pt, onecolumn]{IEEEtran}
% \IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

% \overrideIEEEmargins                                      % Needed to meet printer requirements.
\usepackage{xparse}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% \theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
% % 
% \theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{csquotes}

\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{calc}
\usetikzlibrary{positioning}
\usetikzlibrary {shapes.geometric}
\usetikzlibrary{shapes.misc}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usepgflibrary{shapes.arrows}
\usepgflibrary{shapes.symbols}

\usepackage[noadjust]{cite}
% Link citations
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[colorlinks,citecolor=black,linkcolor=black]{hyperref}
\usepackage[capitalise]{cleveref}
\crefname{equation}{}{}
\Crefname{equation}{}{}

\usepackage{censor}

\newcommand{\citet}[1]{{\color{red}TODO}~\cite{#1}}

\NewDocumentCommand{\legendline}{m}{{%
  \hspace*{-.2mm}\begin{tikzpicture}%
    \draw[line width=2pt, #1] (0.0cm,0.0cm) -- (0.3cm,0.2cm);
  \end{tikzpicture}\hspace*{-.2mm}%
}}

\newcommand\transp[1]{{%
    #1^{\mkern-1.5mu\mathsf{T}}
}}

\usepackage{xfrac}

\newcommand{\lfd}{LfD}
\newcommand{\ourframework}{sNMODE}

\newcommand{\relu}[1]{\operatorname{ReLU}{(#1)}}

\newcommand{\Reals}{\mathbb{R}}
\newcommand{\RealsNonNeg}{\Reals_{\geq 0}}
\newcommand{\manif}{\mathcal{M}}
\newcommand{\sphere}{S^2}
\newcommand{\unitquat}{S^3}
\newcommand{\spdtwo}{\mathcal{S}^{2}_{++}}
\newcommand{\distance}[1]{d_{#1}}
\newcommand{\tangentsp}[1]{\mathcal{T}_{#1}\mathcal{M}}
\newcommand{\tangentsptwo}[2]{\mathcal{T}_{#1}#2}
\newcommand{\innerprod}[3]{{\langle#2,#3\rangle}_{#1}}
\newcommand{\norm}[2]{{\|#2\|}_{#1}}
% \newcommand{\Innerprod}[3]{{\left\langle#2,#3\right\rangle}_{#1}}
\newcommand{\liederiv}[2]{{\mathcal{L}_{#2}\;\!\!#1}}
\newcommand{\grad}[2]{\nabla_{#1}^{#2}}
\newcommand{\refpoint}{x_{r}}
\newcommand{\approxsol}{\tilde{\xi}}
\newcommand{\totangent}[1]{\operatorname{projt}_{#1}}

% colours from: https://jfly.uni-koeln.de/color/
\definecolor{Colors-A}{RGB}{230,159,0}  % orange
\definecolor{Colors-C}{RGB}{86,180,233}  % sky-blue
\definecolor{Colors-B}{RGB}{0,158,115}  % bluish green
\definecolor{Colors-D}{RGB}{213,94,0}  % vermilion (red)
\definecolor{Colors-E}{RGB}{204,121,167}  % reddish purple
\definecolor{Colors-F}{RGB}{0,114,178}  % blue
\definecolor{Colors-G}{RGB}{240,228,66}  % yellow 
%
\definecolor{bettergreen}{RGB}{0,127,93}  % dark bluish green
\definecolor{betterred}{RGB}{165,71,0}  % dark vermilion (red)
\definecolor{betterorange}{RGB}{178,121,0}  % dark orange
%
\definecolor{pltblue}{RGB}{31,119,180}
\definecolor{pltorange}{RGB}{255,127,14}
\definecolor{brewergreen}{RGB}{35,139,69}
\definecolor{robotviolet}{RGB}{94,23,235}

\newcommand{\TODO}[1]{{\color{red}\bfseries TODO: #1}}


\title{{\LARGE\bfseries%
  Stable Robot Motions on Manifolds: Learning\\ Lyapunov-Constrained Neural Manifold ODEs
}}

\author{%
    {David Boetius$^{1}$}, 
    {Abdelrahman Abdelnaby$^{1}$}, 
    {Ashok Kumar$^{2}$},\\
    {Stefan Leue$^{1}$},
    {Abdalla Swikir$^{2}$} and 
    {Fares J. Abu-Dakka$^{3}$}%
    \thanks{%
        $^{1}$David Boetius, Abdelrahman Abdelnaby, and Stefan Leue are with the Department of Computer and Information Sciences University of Konstanz, 78467 Konstanz, Germany {\tt\small \{david.boetius, abdelrahman.abdelnaby, stefan.leue\}@uni-konstanz.de}
    }
    \thanks{%
        $^{2}$Ashok Kumar and Abdalla Swikir are with the Department of Robotics, Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, AE {\tt\small \{ashok.kumar,abdalla.swikir\}@mbzuai.ac.ae}
    }
    \thanks{%
        $^{3}$Fares J. Abu-Dakka is with the Mechanical Engineering Department, New York University Abu Dhabi, Abu Dhabi, AE {\tt\small fa2656@nyu.edu}
    }
}

\begin{document}
\maketitle

\thispagestyle{empty}
\pagestyle{empty}

%===============================================================================

% Notation:
% \mathcal{M}, \mathcal{S}, ... -> manifolds
% n -> manifold dimension
% m -> ambient space dimension
% U, A -> sets
% x,y,z -> Points on a manifold
% u,v,w -> tangent vectors
% f,g,h -> vector fields
% \xi, \zeta -> trajectories/solutions
% T -> duration of a trajectory
% t -> time argument
% x_e -> equilibrium point
% F,G,H -> scalar functions Manifold -> Reals
% \gamma -> geodesic

%===============================================================================

\begin{abstract}
    Learning stable dynamical systems from data is crucial for safe and reliable robot motion planning and control. However, extending stability guarantees to trajectories defined on Riemannian manifolds poses significant challenges due to the manifold's geometric constraints. To address this, we propose a general framework for learning stable dynamical systems on Riemannian manifolds using neural ordinary differential equations. Our method guarantees stability by projecting the neural vector field evolving on the manifold so that it strictly satisfies the Lyapunov stability criterion, ensuring stability at every system state. By leveraging a flexible neural parameterisation for both the base vector field and the Lyapunov function, our framework can accurately represent complex trajectories while respecting manifold constraints by evolving solutions directly on the manifold.
    We provide an efficient training strategy for applying our framework and demonstrate its utility by solving Riemannian LASA datasets on the unit quaternion (\(\unitquat\)) and symmetric positive-definite matrix manifolds, as well as robotic motions evolving on~\(\Reals^3 \times \unitquat\).
    We demonstrate the performance, scalability, and practical applicability of our approach through extensive simulations and by learning robot motions in a real-world experiment.
\end{abstract}

% Two or three meaningful keywords should be added here
% \keywords{Neural ODEs, Lyapunov Stability, Riemannian Manifolds} 

%===============================================================================

\section{Introduction}
\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        \node (Left) at (0,0) {};
        \node (Right) at ($(.55\linewidth,0)-(.25cm,0)$) {};
        
        % \fill[robotviolet] ($(Left) - (-.5cm,1.6105cm)$) rectangle ($(Right) + (-.5cm,1.325cm)$);

        \node[below right=-1.45cm and 0cm of Left.west] (demos) {\includegraphics[width=2.95cm]{figures/cover-figure/demo-collection.png}};
        \node[below left=-1.45cm and 0cm of Right.east] (robot) {\includegraphics[width=2.95cm]{figures/cover-figure/robot-execution.png}};
        
        \draw[Violet, line width=3pt,{Circle[width=3mm,length=3mm]}-] ($(Left) + (1.6cm,-1.2cm)$) -- ($(Left)!.5!(Right) - (0,0cm)$);
        \draw[Violet, line width=6pt,-{Triangle[width=4mm,length=4mm]}] ($(Left)!.5!(Right) - (0,0cm)$) to[bend left] ($(Right) - (2cm,-.85cm)$);
        \fill[Violet] ($(Left)!.5!(Right) - (0,.2cm)$) circle (1.6cm);
        
        \node[
            name=sNMODE, 
        ] at ($(Left)!.5!(Right) - (.04,.21cm)$) {
            \includegraphics[width=3.5cm]{figures/cover-figure/cover-figure-manifold-illustration-on-robot.png}
        };
        % \node[white] at ($(Left)!.5!(Right) - (-1cm,1.3cm)$) {\large\(\manif\)};
    \end{tikzpicture}
    \caption{%
        \textbf{Learning Stable Motions on Riemannian Manifolds using \ourframework{}}.
        A user demonstrates position-orientation motions (\emph{Left}), which lie on a Riemannian manifold (\emph{Centre});
        \ourframework{} learns a stable vector field~\legendline{pltorange} that enables the robot to autonomously perform the motion (\emph{Right}).
    }\label{fig:visual-abstract}
\end{figure}


\noindent
The deployment of robotic systems in complex real-world scenarios demands models that are highly expressive and rigorously stable. Learning from demonstrations enables non-expert users to teach robots new skills by providing example trajectories~\cite{Schaal1999,Ravichandar2020Recent,WangSaverianoAbuDakka2022}. 
However, purely data-driven methods frequently lack formal stability guarantees required, in particular, in safety-critical domains. 
% In safety-critical domains, such as surgical robotics, even small deviations from a planned motion can have catastrophic consequences~\cite{RobotsInSurgery}.

Robotic state variables, such as orientation, stiffness, or inertia, naturally reside on smooth non-Euclidean spaces that are inherently structured as Riemannian manifolds~\cite{WangSaverianoAbuDakka2022,AlesEtAl2014,Zeestraten2018,KoutrasDoulgeri2019,ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023}.
To ensure reliable task execution, learned trajectories must accurately capture complex motion patterns on Riemannian manifolds while reliably steering the system toward a desired equilibrium state. Stable dynamical systems offer a principled solution by guaranteeing trajectory convergence and robustness to perturbations, significantly enhancing reliability in practice~\cite{IjspeertEtAl2013,LemmeEtAl2014,KhansariZadehBillard11,NeumannSteil2015,PerrinSchlehuberCaissier2016,BlocherSaverianoDongheui2017,DuanEtAl2019,NawazEtAl2024}. 

Recently, neural ordinary differential equations (NODEs)~\cite{NODEs} have emerged as an expressive framework for modelling continuous-time dynamical systems.
NODEs offer the flexibility to represent intricate trajectories effectively~\cite{LemmeEtAl2014,GruenbacherEtAl2021,WhiteEtAl2023,NawazEtAl2024,SochopoulosGiengerVijayakumar2024}, also in high-dimensional spaces~\cite{AuddyEtAl2023Scalable}. 
While stability of NODEs has been studied extensively within Euclidean spaces~\cite{ManekKolter2019,KangEtAl2021,LuoEtAl2025}, stability guarantees for NODEs on Riemannian manifolds remain an underexplored area.
Without stability guarantees, learned dynamics may drift, oscillate, or exhibit unpredictable behaviour, undermining both safety and performance.

In this paper, we introduce stable neural manifold ordinary differential equations (\ourframework{})~---~a novel stability framework that combines Lyapunov-based stability with NODEs evolving on arbitrary Riemannian manifolds.
Our approach jointly learns an expressive neural manifold vector field and a neural Lyapunov function. 
Projecting the vector field using a Lyapunov-based corrective term guarantees exponential stability towards a predefined equilibrium state~\cite{ManekKolter2019}.
% 
We uncover and resolve a significant issue in several stable NODE frameworks~\cite{ManekKolter2019,SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Scalable} that invalidates their stability guarantees and causes numerical instability in practice. 

For practically applying our framework, we devise an efficient training strategy that divides training \ourframework{}s into pretraining a base vector field on demonstrations, pretraining a Lyapunov function, and fine-tuning the composed system under the stability projection.
% Our training strategy significantly enhances training efficiency. 
Our \ourframework{}s reliably replicate demonstration trajectories while providing strong stability guarantees.
They outperform existing approaches~\cite{ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023} in terms of efficiency, respectively, performance and scalability.
In summary, our main contributions are
\begin{enumerate}
  \item proposing \ourframework{}, a novel manifold-aware Lyapunov projection method that ensures stability on Riemannian manifolds without imposing restrictive assumptions on the manifold or model structure,
  \item resolving a critical stability issue in previous frameworks~\cite{ManekKolter2019,SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Scalable} that invalidates theoretical stability guarantees and causes numerical instability in practice,
  \item providing an efficient training strategy for learning stable robot motions from demonstrations,
  \item addressing a fundamental limitation of the Riemannian LASA dataset~\cite{SaverianoAbuDakkaKyrki2023} regarding data realism, and
  \item performing an extensive experimental evaluation on several Riemannian manifolds, both in simulation and on a real-world robot.
\end{enumerate}

%===============================================================================

\section{Related Work}\label{sec:related-work}
\newcommand{\ck}{\textcolor{bettergreen}{\(\pmb{\checkmark}\)}}
\newcommand{\no}{\textcolor{betterred}{\(\pmb{\times}\)}}
\newcommand{\na}{--}
\begin{table}[b]
    \centering
    \caption{\textbf{Comparison with Existing LfD Frameworks}.}
    \label{tab:related-work-comparison}
    \begin{tabular}{>{\centering}p{.13\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering}p{.18\columnwidth} @{} >{\centering\arraybackslash}p{.12\columnwidth}}
        \ \newline\textbf{Reference} & \textbf{Riemannian}\newline\textbf{Manifold} & \ \textbf{Stability}\newline\textbf{Guarantee} & \ \textbf{Learned}\newline\textbf{Vector Field} & \textbf{Neural}\newline\textbf{ODE} \\ \midrule
        
        \cite{AuddyEtAl2023Continual}
        & \no & \no & \ck &\ck \\
        
        \cite{SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Scalable}
        & \no & \(\textcolor{betterorange}{\pmb{\checkmark}}^{\;\!\!\ast}\) & \ck &\ck \\
        
        \cite{LemmeEtAl2014,NawazEtAl2024}
        & \no & \ck & \ck & \ck \\

        \cite{KhansariZadehBillard11}
        & \no & \ck & \ck & \no \\
        
        \cite{WangSaverianoAbuDakka2022}
        & \ck & \no & \ck & \ck \\
        
        \cite{SaverianoAbuDakkaKyrki2023}
        & \ck & \ck & \no &\no \\
        
        \cite{ZhangBeikMohammadiRozo2022}
        & \ck & \(\textcolor{betterorange}{\pmb{\checkmark}}^{\;\!\!\dagger}\)  & \no & \ck \\
        
        \textbf{Ours}
        & \ck & \ck &\ck & \ck \\
    \end{tabular}
    
    {\footnotesize \({}^\ast\)Affected by the equilibrium point issue that we address in this paper.}\\
    {\footnotesize \({}^\dagger\)Unlike the other approaches, the equilibrium point is not user-defined.}
\end{table}
In this section, we review existing works related to stable neural ODEs (NODEs) on Riemannian manifolds. \Cref{tab:related-work-comparison} provides a concise comparison of our \ourframework{} framework to existing learning from demonstrations (LfD) frameworks.
We first discuss approaches in Euclidean space.

\paragraph{NODEs and Stability}
NODEs~\cite{NODEs} are a powerful paradigm for modelling complex continuous-time dynamical systems, from image processing~\cite{NODEs,KangEtAl2021,LuoEtAl2025} to learning robotic skills~\cite{LemmeEtAl2014,WhiteEtAl2023,NawazEtAl2024,SochopoulosGiengerVijayakumar2024,AuddyEtAl2023Continual,AuddyEtAl2023Scalable}.
%
Stability in NODEs is addressed using Lyapunov functions~\cite{ManekKolter2019,AuddyEtAl2023Scalable,KangEtAl2021,SochopoulosGiengerVijayakumar2024,LuoEtAl2025} and contraction metrics~\cite{KhansariZadehBillard11, NawazEtAl2024}. 

However, these stability frameworks are fundamentally limited to Euclidean spaces and cannot handle data evolving on more general Riemannian manifolds. Additionally, we identify a critical equilibrium consistency issue in~\cite{ManekKolter2019} that invalidates theoretical stability guarantees and causes numerical instability in practice. This issue also affects subsequent works~\cite{AuddyEtAl2023Scalable,SochopoulosGiengerVijayakumar2024} that build upon~\cite{ManekKolter2019}. We address this critical issue in \cref{sec:main-stability-issue}.
%While providing stability guarantees, these approaches can not handle data lying on more general Riemannian manifolds. Additionally, there is a significant issue in~\cite{ManekKolter2019} that invalidates the stability guarantees and creates numerical instability in practice. This also applies to~\cite{AuddyEtAl2023Scalable,SochopoulosGiengerVijayakumar2024}, which build upon~\cite{ManekKolter2019}. We discuss this issue in detail in \cref{sec:main-stability-issue}.
% 
% Existing frameworks for stable Euclidean neural ODEs often exhibit limited expressivity.
% For instance,~\cite{LemmeEtAl2014} induces stability by restricting the last-layer weights, significantly limiting the flexibility of their models.
% Similarly,~\cite{NawazEtAl2024} relies on manually designed Lyapunov functions, significantly constraining the model’s adaptability and practical scalability. 
% In contrast, we leverage learned neural Lyapunov functions, preserving expressive modelling capabilities while ensuring rigorous stability guarantees.

\paragraph{NODEs on Riemannian Manifolds}
Neural Manifold ODEs (NMODEs)~\cite{DynamicChartMethod} provide an expressive framework for learning dynamical systems while respecting manifold geometry. 
While~\cite{DynamicChartMethod} employs dynamic coordinate charts to solve NMODEs,~\cite{WangSaverianoAbuDakka2022} projects the manifold data into a single tangent space.
% We experimentally compare these approaches for solving NMODEs in \cref{sec:experiments-mode-solvers}.
% They employ coordinate charts and local parameterisations to solve ODEs on manifolds, though computational complexity remains a challenge.
% In~\cite{WangSaverianoAbuDakka2022}, non-Euclidean data is projected to a tangent space where a neural ODE operates, and the results are then projected back to the Riemannian manifold.
% However, this tangent space approximation significantly distorts learned trajectories.
Both~\cite{DynamicChartMethod} and~\cite{WangSaverianoAbuDakka2022} lack stability guarantees.
%We discuss~\cite{ZhangBeikMohammadiRozo2022}, which employs NMODEs to obtain stability guarantees below.

\paragraph{Stable Dynamical Systems on Riemannian Manifolds} 
% Learning stable dynamical systems on Riemannian manifolds is an underexplored area. 
% Existing frameworks that learn stable dynamical systems on Riemannian manifolds learn a smooth bijective mapping (diffeomorphism) between two manifolds~\cite{SaverianoAbuDakkaKyrki2023,ZhangBeikMohammadiRozo2022}. While this can be a viable approach, it is only practical if the learned diffeomorphism can be evaluated quickly, as it has to be evaluated for every time step of each predicted trajectory. Since~\cite{ZhangBeikMohammadiRozo2022} use NMODEs to learn the diffeomorphism,~\cite{ZhangBeikMohammadiRozo2022} is not practically viable, as evaluating an NMODE is costly. Using a Gaussian Mixture Models (GMM) instead, as applied by~\cite{SaverianoAbuDakkaKyrki2023}, offers a practical alternative, as they are fast to evaluate. However, GMMs scale poorly to higher-dimensional spaces, unlike NMODEs. Instead of learning a diffeomorphism, we directly use an NMODE to learn a stable vector field.
%Zhang et al.~\cite{ZhangBeikMohammadiRozo2022} propose learning Neural MODE-based diffeomorphisms that inherit stability from hand-crafted geodesic flows in latent space. While their method shows high expressiveness and strong performance on complex product manifolds ($\mathbb{R}^3 \times \mathcal{S}^3$), it has critical limitations that our method resolves: (1) it requires costly Neural MODE integration at each inference step, complicating real-time control due to the need for evaluating both the diffeomorphism and its pullback operator.(2) The two-stage process (latent stable system → diffeomorphism → target manifold) can accumulate approximation errors, whereas our direct learning eliminates this intermediate step. In contrast, Saveriano et al.~\cite{SaverianoAbuDakkaKyrki2023} utilize Gaussian Mixture Models (GMMs) for faster evaluation but struggle with scalability to higher dimensions. Unlike these approaches, ours directly learns stable vector fields on the target manifold with NMODEs, ensuring stronger stability, better computational efficiency, and robust training while preserving the expressiveness of neural approaches for high-dimensional manifold learning.
%
Learning stable dynamical systems on Riemannian manifolds combines differential geometry, control theory, and machine learning challenges. Recent approaches~\cite{ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023} focus on learning diffeomorphic mappings to transform stable canonical dynamics into complex behaviours using NMODEs and Gaussian mixture models (GMMs).

RSDS~\cite{ZhangBeikMohammadiRozo2022} uses NMODEs to construct diffeomorphisms for Riemannian stable dynamical systems, providing theoretical stability guarantees through pullback operations. However, this method is costly, as it solves an NMODE at each time step, hindering real-time applications. 
Additionally, a fundamental issue in~\cite{ZhangBeikMohammadiRozo2022} is that the equilibrium of the learned diffeomorphism is assumed to automatically correspond to the desired equilibrium point, without enforcing this constraint during training. While~\cite{ZhangBeikMohammadiRozo2022} ensures convergence to \emph{some} equilibrium point, users have no control over the location of this equilibrium point.

In contrast, SDS-RM~\cite{SaverianoAbuDakkaKyrki2023} employs GMMs to effectively learn diffeomorphisms. Compared to RSDS, SDS-RM ensures computational efficiency while guaranteeing convergence to a user-defined equilibrium point. Nevertheless, GMMs struggle with scalability in high-dimensional manifolds~\cite{abu-dakka2018force} and oftentimes fail to capture the complex nonlinear dynamics that neural networks handle well.

Alternative approaches~\cite{ravichandar2019learning,mukadam2020riemannian} explore contraction theory on manifolds~\cite{ravichandar2019learning} and Riemannian motion policies~\cite{mukadam2020riemannian}, but focus on tracking a reference trajectory rather than learning stable dynamics from demonstrations.

\paragraph{Positioning Our Approach}
Our work addresses key limitations in current methods by merging the expressiveness of NODEs with stability guarantees on Riemannian manifolds.
Unlike diffeomorphism-based methods~\cite{ZhangBeikMohammadiRozo2022,SaverianoAbuDakkaKyrki2023}, we learn stable vector fields on manifolds directly, avoiding expensive inverse mappings. 
% We address the equilibrium consistency problem in~\cite{ManekKolter2019} using explicit constraints. 
Our approach retains the computational benefits of NODEs while offering the geometric awareness crucial for robotic applications involving orientation, impedance, or other manifold-valued quantities.
%Our approach both guarantees stability and maintains Riemannian manifold constraints, while being fast and scalable to high-dimensional data, as we demonstrate in \cref{sec:experiments}.

% Among those,~\cite{SaverianoAbuDakkaKyrki2023} rely on Gaussian Mixture Models (GMMs) instead of neural ODEs.
% As we demonstrate in \cref{sec:experiments} GMMs on Riemannian manifolds scale poorly to higher-dimensional spaces, unlike neural ODEs.
% While~\cite{ZhangBeikMohammadiRozo2022} use neural ODEs, they use neural ODEs to learn the diffeomorphism.
% In contrast, we directly learn the vector field of a dynamical system using a neural ODE.
% While~\cite{ZhangBeikMohammadiRozo2022} requires solving a neural ODE on a Riemannian manifold for each time step of a trajectory, our method only requires solving a single neural ODE per trajectory.
% Concretely, simulating a trajectory with~\(1000\) time steps requires~\cite{ZhangBeikMohammadiRozo2022} to solve~\(1000\) NMODEs, while we only solve one.
% \TODO{Concrete inference times from Abdelrahman.}
% 
% Particularly,~\cite{ZhangBeikMohammadiRozo2022} introduces stability in manifold-valued dynamical systems by employing NMODEs to learn diffeomorphisms mapping complex trajectories to predefined stable reference paths (e.g., geodesics). Although effective, this diffeomorphic approach incurs significant computational overhead during inference, as each trajectory step demands solving an individual NMODE.



    
%===============================================================================

\section{Background}\label{sec:background}
This section presents the mathematical foundations of Riemannian manifolds, Lyapunov stability, and numerical methods for solving ODEs on Riemannian manifolds.

\subsection{Riemannian Manifolds}\label{sec:riemannian-manifolds}
An~\(n\)-dimensional \emph{manifold}~\(\manif\) is a topological space that locally resembles Euclidean space in the neighbourhood of each point~\(x \in \manif\).
Each manifold is embedded in an Euclidean \emph{ambient space}~\(\Reals^m \supseteq \manif\),~\(m \geq n\).
Each manifold is defined by a collection of charts~\({(\varphi_x: U_x \to U_x')}_{x \in A}\) with index set~\(A\), where each~\(\varphi_x\) is a homomorphism between the open sets~\(U_x \subseteq \manif\) and~\(U_x' \subseteq \Reals^n\).
A manifold is \emph{smooth} if each \emph{transition map}~\(\varphi_x \circ \varphi_y^{-1}\) is smooth for overlapping charts~\(x, y \in A\).
% Smoothness is ensured by requiring all \emph{transition map}~\(\varphi_x \circ \varphi_y^{-1}\) for overlapping charts~\(x, y \in A\) are smooth (i.e., \(C^\infty\)).
%
Each point~\(x \in \manif\) on a smooth manifold corresponds to an~\(n\)-dimensional \emph{tangent space}~\(\tangentsp{x} \subseteq \Reals^m\), which contains the velocities of curves passing through~\(x\). 
% For manifolds embedded in an ambient Euclidean space~\(\Reals^m \supseteq \manif\) with~\(m \geq n\), the tangent space~\(\tangentsp{x} \subseteq \Reals^m\) forms an~\(n\)-dimensional linear subspace.
The \emph{tangent bundle}~\(\tangentsp{}= \bigcup_{x \in \manif} \tangentsp{x}\) is the disjoint union of all tangent spaces.
The orthogonal projection~\(\totangent{x}: \Reals^m \to \tangentsp{x}\) maps vectors from the ambient space to the tangent space at~\(x\).
%
% \textbf{Notation.} In this paper, we use~\(x, y, z\) for points on the manifold,~\(u, v, w\) for tangent vectors and~\(a, b, c\) for points in the ambient space.

A \emph{Riemannian manifold}~\((\manif, g)\) is a smooth manifold equipped with a \emph{Riemannian metric}~\(g\), which assigns to each tangent space~\(\tangentsp{x}\) a positive-definite \emph{inner product}~\(\innerprod{x}{\cdot}{\cdot}: \tangentsp{x} \times \tangentsp{x} \to \Reals\) that varies smoothly with~\(x\).
The induced norm is~\(\norm{x}{u} = \sqrt{\innerprod{x}{u}{u}}\).
%
The Riemannian metric enables the definition of geometric concepts. 
In particular, the \emph{length} of a piecewise differentiable curve~\(\xi: [a, b] \to \manif\) is~\(\ell(\xi) = \int_a^b \norm{\xi(t)}{\dot{\xi}(t)}dt\), where~\(\dot{\xi}(t) = \frac{d \xi(s)}{d s}|_{s=t}\) is the velocity of~\(\xi\).
The \emph{geodesic}~\(\gamma_{x,y}: [0, 1] \to \manif\) is the shortest curve with~\(\gamma_{x,y}(0) = x\),~\(\gamma_{x,y}(1) = y\), and constant velocity~\(\norm{\gamma(t)}{\dot{\gamma}(t)}, \forall t \in [0, 1]\).
The Riemannian \emph{distance} between~\(x,y \in \manif\) is~\(\distance{\manif}(x, y) = \ell(\gamma_{x,y})\).
% 
The \emph{exponential map}~\(\exp_x: \tangentsp{x} \to \manif\) is defined as~\(\exp_x(u) = \gamma_{x}^u(1)\), where~\(\gamma_x^u\) is the unique geodesic satisfying~\(\gamma_x^u(0) = x\) and~\(\dot{\gamma}_x^u(0) = u\). Its inverse, the \emph{logarithmic map}~\(\log_x: \manif \to \tangentsp{x}\), exists locally and satisfies~\(\exp_x(\log_x(y)) = y\) for~\(y\) sufficiently close to~\(x\).

\paragraph{Differential Equations on Manifolds} 
A \emph{manifold ordinary differential equation (MODE)} is specified by a vector field~\(f: \manif \to \tangentsp{}\) such that~\(f(x) \in \tangentsp{x}\) for all~\(x \in \manif\).
The solution of a MODE is the curve~\(\xi : [0, T] \to \manif\) that satisfies the initial value problem
\begin{equation}
    \frac{d\xi(t)}{dt} = f(\xi(t)) \in \tangentsp{\xi(t)}, \quad \xi(0) = x_0, \quad \xi(t) \in \manif.\label{eqn:mode}
\end{equation}
If~\(f\) is parameterised by a neural network, \cref{eqn:mode} is a \emph{neural manifold ordinary differential equation (NMODE)}.

% In this paper, we study the \emph{stability} of NMODEs.
\paragraph{Lie Derivative} 
% The \emph{Lie derivative} provides a directional derivative of scalar functions along vector fields. 
The \emph{Lie derivative} of a smooth function~\(F: \manif \to \Reals\) with respect to the vector field~\(f: \manif \to \tangentsp{}\) provides a directional derivative
\begin{equation*}    
  \liederiv{F}{f}(x) = \frac{d}{dt} \left. F(\xi_x(t)) \right\rvert_{t=0} = \innerprod{x}{\grad{x}{\manif} F(x)}{f(x)}
\end{equation*}
where~\(\xi_x\) is the solution of \cref{eqn:mode} with~\(\xi_x(0) = x\) and \(\grad{x}{\manif} F(x) \in \tangentsp{x}\) is the Riemannian gradient of~\(F\) at~\(x\).
% 
Below, we provide an example of a Riemannian manifold: the unit quaternion manifold.
We refer to~\cite{SaverianoAbuDakkaKyrki2023} and~\cite{DynamicChartMethod} for further examples of Riemannian manifolds.

\begin{example}
    The set of unit quaternions~\(\unitquat = \{x \in \Reals^4 \mid \|x\|_2 = 1\}\) equipped with the Euclidean inner product~\(\innerprod{z}{x}{y} = \transp{x}y\) forms a three-dimensional Riemannian manifold embedded in~\(\Reals^4\). The exponential map~\(\exp_x: \tangentsptwo{x}{\unitquat} \to \unitquat\) is given by~\(\exp_x(u) = \cos(\|u\|)x + \sin(\|u\|)\frac{u}{\|u\|}\) for~\(u \neq 0\) and~\(\exp_x(0) = x\). The Lie derivative on~\(\unitquat\) is~\(\liederiv{F}{f}(x) = \transp{\grad{x}{\unitquat} F(x)} f(x)\), where~\(\grad{x}{\unitquat}F(x) = \totangent{x}(\grad{x}{\Reals^4}F(x)) = \grad{x}{\Reals^4}F(x) - x\innerprod{x}{\grad{x}{\Reals^4}F(x)}{x}\).
\end{example}
% If desired, time dependence can be expressed by augmenting the state space~\(\manif\) by a real-valued time variable.


\subsection{Stability}\label{sec:stability-background}
In this paper, we are primarily concerned with exponential stability on the Riemannian manifold~\(\manif\).
Since some manifolds, such as~\(\unitquat\), do not permit global stability due to the Pointcar{\'e}-Hopf Theorem~\cite{milnor1997topology}, we consider quasi-global stability, which allows additional unstable equilibrium points.
% Lyapunov functions allow us to prove stability.

\begin{defn}[Quasi-Global Stability]\label{defn:stability}
    Let~\(f: \manif \to \tangentsp{}\) be a vector field.
    A point~\(x_e\) is an \emph{equilibrium point} if~\(f(x_e) = 0\).
    It is a \emph{stable equilibrium point} if there exist~\(\alpha \geq 0, \beta > 0\) such that~\(\distance{\manif}(\xi(t), x_e) \leq \beta e^{-\alpha t}\distance{\manif}(x_0, x_e)\) for every~\(x_0 \in \manif \setminus E\), where~\(\xi\) is the solution of \cref{eqn:mode} and~\(E \subset \manif\) is a finite set.
    If~\(\alpha > 0\),~\(x_e\) is \emph{\(\alpha\)-exponentially stable}.
    % We call~\(f\) \emph{(exponentially) stable} if it has an (exponentially) stable equilibrium point.
\end{defn}
%
% On some Riemannian manifolds~\(\manif'\), all vector fields~\(f: \manif \to \tangentsp{}\) are guaranteed to have more than one equilibrium point~\(x_e \in \manif\) due to the Pointcar{\'e}-Hopf theorem~\cite{ZhangBeikMohammadiRozo2022}.
% Since any trajectory starting at~\(x_e\) will remain stationary, such manifolds~\(\manif'\) can not

\begin{theorem}[Lyapunov Function]\label{thm:lyapunov-fn}
    Let~\(f: \manif \to \tangentsp{}\) with equilibrium point~\(x_e\).
    Let~\(V: \manif \to \RealsNonNeg\) be a continuously differentiable function with~\(V(x_e) = 0\) and~\(V(x) > 0\) for~\(x \in \manif \setminus E\), where~\(E \subset \manif\) is finite.
    If there exist~\(\alpha \geq 0, c_1, c_2, p > 0\), such that~\(\liederiv{V}{f}(x) \leq -\alpha V(x)\) and~\(c_1 \distance{\manif}(x, x_e)^p \leq V(x) \leq c_2 \distance{\manif}(x, x_e)^p\) for all~\(x \in \manif\),~\(x_e\) is a stable equilibrium point.
    If~\(\alpha > 0\),~\(x_e\) is \(\alpha\)-exponentially stable.
\end{theorem}
We refer to~\cite{LyapunovFn} for a proof of \cref{thm:lyapunov-fn}.
% Before presenting our projection approach for ensuring stability in \cref{sec:main}, we introduce several methods for numerically solving MODEs.

\subsection{Numerical Solutions of MODEs}\label{sec:solve-mode}
For training NMODEs, we require a method for numerically solving MODEs.
A baseline approach for solving the MODE \cref{eqn:mode} is the \emph{tangent space method (TS)} ~\cite{WangSaverianoAbuDakka2022}.
TS constructs an approximate numerical solution~\(\approxsol_{0:N}\) of \cref{eqn:mode} by computing a numerical solution~\(\tilde{\zeta}_{0:N}\) of the ODE
\begin{equation*}
    \frac{d \zeta(t)}{dt} = (\totangent{\refpoint} \circ f \circ \exp_{\refpoint})(\zeta(t)), \quad
    \zeta(0) = \log_{\refpoint}(x_0),
    % \zeta(t) \in \tangentsp{x_r},
\end{equation*}
which evolves in the tangent space of a reference point~\(\refpoint \in \manif\).
The numerical solution~\(\tilde{\zeta}\) is computed using a standard technique for solving ODEs, such as the Euler method, or a Runge-Kutta method.
By projecting~\(\tilde{\zeta}\) to the manifold, we obtain the numerical solution of~\cref{eqn:mode} as~\(\approxsol_i = \exp_{\refpoint}(\tilde{\zeta}_i) \in \manif\) for each~\(i \in \{0, \ldots, N\}\).
In our setting, the equilibrium point~\(x_e\) is a natural choice for the reference point~\(\refpoint\).
% \begin{equation}
%     \begin{IEEEeqnarraybox}[][c]{c"c}
%         \frac{d \tilde{\zeta}(t)}{dt} = \totangent{\refpoint}(f(\tilde{\zeta}(t))), &
%         \tilde{\zeta}(t) \in \tangentsp{\refpoint}, \\
%         \tilde{\zeta}(0) = \log_{\refpoint}(x_0), & 
%         \approxsol(t) = \exp_{\refpoint}(\tilde{\zeta}(t))
%     \end{IEEEeqnarraybox}\label{eqn:ts-method}
% \end{equation}

By evolving in only one tangent space, TS can introduce significant approximation error. 
The \emph{exp-step method (Exp)} reduces this approximation error by switching the reference point frequently.
% Let~\(\delta_t = T/N\) be the step size of the \(\exp\)-step method.
% The approximate solution~\(\approxsol_{0:N}\) computed by the \(\exp\)-step method is a chain of short curves~\(\approxsol_i: [0, \delta_t] \to \manif\) with~\(\approxsol_1(0) = x_0\) and~\(\approxsol_{i+1}(0) = \approxsol_{i}(\delta_t)\) for~\(i \in [1, \lfloor {T}/{\delta_t} \rfloor]\).
% Similarly to \cref{eqn:ts-method}, each~\(\approxsol_i(t) = \exp_{\approxsol_i(0)}(\zeta_i(t))\) where~\(\zeta_i\) is trajectory in Euclidean space defined by
The steps of the approximate solution~\(\approxsol_{0:N}\) computed by Exp are~\(\approxsol_0 = x_0\) and~\(\approxsol_{i+1} = \exp_{\approxsol_{i}}(\hat{\zeta}_i')\) for~\(i \in \{0, \ldots, N-1\}\), where~\(\tilde{\zeta}_i'\) is the endpoint of a numerical solution of the ODE
% \approxsol_i(t) = \exp_{\approxsol_i(0)}(\zeta_i(t))%
% \begin{IEEEeqnarray*}{c}
%     \frac{d \zeta_i(t)}{dt} = (\totangent{\approxsol_i(0)} \circ f \circ \exp_{\approxsol_i(0)})(\zeta_i(t)), \\
%     \zeta_i(t) \in \tangentsp{\approxsol_i(0)} \qquad
%     \zeta_i(0) = \log_{\approxsol_i(0)}(x_0),
% \end{IEEEeqnarray*}
\begin{equation*}
    \frac{d \zeta_i(t)}{dt} = (\totangent{\approxsol_i} \circ f \circ \exp_{\approxsol_i})(\zeta_i(t)), \quad
    \zeta_i(0) = 0,
\end{equation*}
over the the time interval~\(t \in [(i-1)T/N, iT/N]\).

Finally, the \emph{dynamic-chart method (DC)}~\cite{DynamicChartMethod} leverages charts instead of tangent spaces to transfer Euclidean ODE solutions to the manifold. 
Let~\(\varphi_0: U_0 \to V_0\) be a chart of~\(\manif\) with~\(x_0 \in U_0\).
DC solves an ODE in~\(V_0\) until the solution approaches the boundary of~\(V_0\).
Once this is the case, the method switches to an adjacent chart~\(\varphi_1: U_1 \to V_1\) and solves another ODE, switching the chart again when the solution of the ODE approaches the boundary of~\(V_1\).
The ODE that is solved for the chart~\(\varphi_i : U_i \to V_i\) is
\begin{equation*}
    \frac{d\zeta_i(t)}{dt} = (D_{\varphi_i^{-1}(\zeta_i(t))}^{\manif}\varphi_i \circ f \circ \varphi_i^{-1})(\zeta_i(t)), \quad 
    \zeta_i(t) \in V_i,
\end{equation*}
where~\(D_{x}^{\manif} \varphi_i: \tangentsp{x} \to \Reals^n\) is the pushforward of~\(\varphi_i\) with respect to~\(x\) and the initial point~\(\zeta_i(0)\) is the end point of the trajectory in the previous chart~\(\varphi_{i-1}\), respectively,~\(x_0\) for~\(i=0\).
The steps of the overall numerical solution are~\(\approxsol_0 = x_0\) and~\(\approxsol_i = \varphi_i^{-1}(\tilde{\zeta}_i'')\) for~\(i \in \{1, \ldots, N\}\) where~\(\tilde{\zeta}_i''\) is the endpoint of the trajectory in the chart~\(\varphi_{i-1}\).
We refer to~\cite{DynamicChartMethod} for more details on DC.

%===============================================================================

\section{Stable Neural Manifold ODEs}\label{sec:main}
% \begin{figure*}
%     \begin{tikzpicture}[
%         stepshape/.style={
%             shape=rectangle, 
%             draw=black, fill=white, 
%             line width=1.75pt, 
%             minimum height=3cm, 
%             minimum width=3.82cm,
%             align=center,
%         },
%         steparrow/.style={
%             single arrow, shape border rotate=0,
%             draw=black, thin,
%             minimum width=.5cm,
%             single arrow tip angle=90,
%             single arrow head extend=.125cm,
%         },
%     ]
%         \node (Left) at (0,0) {};
%         \node (Right) at ($(\linewidth,0)-(.25cm,0)$) {};
%         
%         \node[
%             stepshape,
%             right=2.85cm of Left.west,
%             name=step1, 
%         ] {
%             Pretrain\\[-.05cm]
%             \textbf{Base NMODE}\\[-.2cm]
%             \includegraphics[width=3cm]{figures/cover-figure/cover-figure-manifold-illustration-base-nmode.png}
%             \vspace*{-.3cm}
%         };
%         
%         \node[
%             stepshape,
%             left=2.85cm of Right.east,
%             name=step3, 
%         ] {
%             Finetune under\\[-.05cm]
%             \textbf{Stability Projection}\\[-.2cm]
%             \includegraphics[width=3cm]{figures/cover-figure/cover-figure-manifold-illustration-stable-nmode.png}
%             \vspace*{-.3cm}
%         };
% 
%         \node[
%             steparrow, fill=pltorange,
%             minimum height=5cm,
%             below right=1cm and 2cm of step1.east,
%         ] {};
%         
%         \node[
%             stepshape,
%             right=.2cm of step1.east,
%             name=step2, 
%         ] {
%             Pretrain\\[-.05cm]
%             \textbf{Lyapunov Function}\\[-.2cm]
%             \includegraphics[width=3cm]{figures/cover-figure/cover-figure-manifold-illustration-lyapunov-fn.png}
%             \vspace*{-.3cm}
%         };
%         
%         \node[
%             steparrow, fill=pltorange,
%             minimum height=.9cm,
%             below right=0cm and -.05cm of step1.east,
%         ] {};
%         \node[
%             steparrow, fill=brewergreen,
%             minimum height=.9cm,
%             below right=0cm and -.05cm of step2.east,
%         ] {};
% 
%         \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step1.north west] {\huge\textbf{1.}};
%         \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step2.north west] {\huge\textbf{2.}};
%         \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step3.north west] {\huge\textbf{3.}};
%     
%         \node[below right=-1.45cm and 0cm of Left.west] (demos) {\includegraphics[width=2.95cm]{figures/cover-figure/demo-collection.png}};
%         \node[below left=-1.45cm and 0cm of Right.east] (robot) {\includegraphics[width=2.95cm]{figures/cover-figure/robot-execution.png}};
%         
%         \node[below left=-.025cm of demos.north east] (data) {\includegraphics[width=1.5cm]{figures/cover-figure/cover-figure-manifold-illustration-data-thumb.png}};
%         % \node[below left=-.05cm of robot.north east] (nmode-on-robot) {\includegraphics[width=1.6cm]{figures/cover-figure/cover-figure-manifold-illustration-on-robot-thumb.png}};
% 
%     \end{tikzpicture}
%     \caption{
%         \textbf{Our \ourframework{} Framework}.
%         \emph{Left}: We learn a stable dynamical system from human demonstrations on Riemannian manifolds in three stages: \emph{1.} learning a base vector field from the demonstrations, \emph{2.} learning a Lyapunov function, and \emph{3.} combining both into a \ourframework{}.
%         \emph{Right}: Using the \ourframework{}, a robot can perform the demonstrated task also from starting poses not contained in the demonstrations.
%     }\label{fig:cover}
% \end{figure*}
\begin{figure}
    \centering
    \begin{tikzpicture}[
        stepshape/.style={
            shape=rectangle, 
            draw=black, fill=white, 
            line width=1.75pt, 
            minimum height=3cm, 
            minimum width=1cm,
            align=center,
        },
        steparrow/.style={
            single arrow, shape border rotate=0,
            draw=black, thin,
            minimum width=.5cm,
            single arrow tip angle=90,
            single arrow head extend=.125cm,
        },
    ]
        \node (Left) at (0,0) {};
        \node (Right) at ($(.5\linewidth,0)-(.25cm,0)$) {};
        
        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=5.5em, 
            align=left, anchor=south west,
        ] (box1) at (0, 0) {};
        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=7em, 
            align=left, anchor=south west,
        ] (box2) at (.25\linewidth, 0) {};
        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=8.5em, 
            align=left, anchor=south west,
        ] (box3) at (.5\linewidth, 0) {};

        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=3em, 
            align=left, anchor=south west,
        ] (stage1) at (0, 0) {%
            \Large\textbf{1.}
            \normalsize
            Pretrain Base NMODE\!
        };
        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=4.5em, 
            align=left, anchor=south west,
        ] (stage2) at (.25\linewidth, 0) {%
            \Large\textbf{2.}
            \normalsize
            \begin{tabular}{l@{}}
                Pretrain
                Lyapunov\\
                Function
            \end{tabular}
        };
        \node[
            draw=black, rectangle, line width=1pt, 
            minimum width=.25\linewidth, minimum height=6em, 
            align=left, anchor=south west,
        ] (stage3) at (.5\linewidth, 0) {%
            \Large\textbf{3.}
            \normalsize
            \begin{tabular}{l@{}}
                Finetune
                under\\
                Stability
                Projection
            \end{tabular}
        };
        
        \node[
            above right=-.3cm and 0.55cm of stage1.north west,
            % below right=-.33cm and -.34cm of box1.north west,
            name=img1, 
        ]{%
            \includegraphics[width=3.25cm]{figures/cover-figure/cover-figure-manifold-illustration-base-nmode.png}
        };
        % \node[
        %     steparrow, fill=pltorange,
        %     minimum height=3.5cm,
        %     below right=-.8cm and .95cm of img1.south east,
        % ] {};
        \node[
            above right=-.3cm and 0.55cm of stage2.north west,
            % below right=-.33cm and -.37cm of box2.north west,
            name=img2, 
        ]{%
            \includegraphics[width=3.25cm]{figures/cover-figure/cover-figure-manifold-illustration-lyapunov-fn.png}
        };
        \node[
            above right=-.3cm and 0.55cm of stage3.north west,
            % below right=-.33cm and -.39cm of box3.north west,
            name=img3, 
        ]{%
            \includegraphics[width=3.25cm]{figures/cover-figure/cover-figure-manifold-illustration-stable-nmode.png}
        };

        % \node[
        %     steparrow, fill=pltorange,
        %     minimum height=.66cm,
        %     below right=-3.1cm and -.65cm of img1.south east,
        % ] {};
        % \node[
        %     steparrow, fill=brewergreen,
        %     minimum height=.66cm,
        %     below right=-3.1cm and -.65cm of img2.south east,
        % ] {};
        % 
        % \node[
        %     steparrow, fill=pltblue,
        %     minimum height=.7cm,
        %     anchor=south west,
        %     shape border rotate=270,
        % ] (data1) at (.15cm,3.95cm) {};
        % \node[above right=-.25cm and -.05cm of data1] {\footnotesize{}Demonstrations};
        % 
        % \node[
        %     steparrow, fill=pltblue,
        %     minimum height=.7cm,
        %     anchor=south west,
        %     shape border rotate=270,
        % ] (data2) at (3.05cm,4.5cm) {};
        % \node[
        %     steparrow, fill=pltblue,
        %     minimum height=.7cm,
        %     anchor=south west,
        %     shape border rotate=0,
        % ] (data3) at (5.33cm,4.65cm) {};
        
        % \node[
        %     stepshape,
        %     right=.05cm of step1.east,
        %     name=step2, 
        % ] {
        %     % Pretrain\\[-.05cm]
        %     % \textbf{Lyapunov Function}\\[-.2cm]
        %     \includegraphics[width=2.5cm]{figures/cover-figure/cover-figure-manifold-illustration-lyapunov-fn.png}
        %     \vspace*{-.3cm}
        % };
        
        % \node[
        %     steparrow, fill=pltorange,
        %     minimum height=.9cm,
        %     below right=0cm and -.05cm of step1.east,
        % ] {};
        % \node[
        %     steparrow, fill=brewergreen,
        %     minimum height=.9cm,
        %     below right=0cm and -.05cm of step2.east,
        % ] {};

        % \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step1.north west] {\huge\textbf{1.}};
        % \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step2.north west] {\huge\textbf{2.}};
        % \node[draw=black, fill=white, line width=2pt, align=center, above left=-.5cm of step3.north west] {\huge\textbf{3.}};
    \end{tikzpicture}
    \caption{
        \textbf{Our \ourframework{} Three-Stage Training Strategy}.
        The NMODEs \legendline{pltorange} and the Lyapunov function \legendline{brewergreen} are trained on the human demonstrations \legendline{pltblue}.
    }\label{fig:framework}
\end{figure}

In this section, we introduce~\ourframework{}, a novel expressive framework for learning quasi-globally exponentially stable neural ODEs defined on Riemannian manifolds. 
Our approach leverages the Lyapunov stability principles and generalises the successful stability projection proposed in~\cite{ManekKolter2019} from Euclidean space to arbitrary Riemannian manifolds. 
However, we find that~\cite{ManekKolter2019} has a critical issue that invalidates the theoretical stability guarantees and leads to numerical instability in practice.
We address this issue in our NMODE architecture, introduced in \cref{sec:nmode-arch}.
\Cref{sec:nmode-stability} contains our stability proof and \cref{sec:main-stability-issue} discusses the implications of the stability issue in~\cite{ManekKolter2019} in detail.
Before progressing to our experiments, we introduce our efficient training strategy in \cref{sec:nmode-train}, also visualised in \cref{fig:framework}.
% 
% Additionally, we resolve the critical issue of equilibrium points shifting during training, which invalidates the stability guarantees of~\citet{ManekKolter2019}.
% We first introduce our NMODE architecture in \cref{sec:nmode-arch} before proving the stability of our NMODEs in \cref{sec:nmode-stability}.

\subsection{NMODE Architecture}\label{sec:nmode-arch}
Let~\(x_e\) be the desired equilibrium point of the \ourframework{} to train.
Our approach is based on a neural base vector field~\(g: \manif \to \tangentsp{}\) and a positive definite neural Lyapunov function~\(V: \Reals^m \to \Reals\).
For obtaining a well-defined NMODE, we also require that~\(\grad{x}{\manif} V(x) \neq 0\) for every~\(x \in \manif \setminus \{x_e\}\).
This is ensured by combining a convex neural network~\cite{ICNN} with an invertible feature transformation~\cite{ManekKolter2019}.

\paragraph{Base Vector Field}
We first describe the architecture of our base vector field.
Let~\(h: \Reals^m \to \Reals^m\) be any smooth neural network.
Our base vector field~\(g: \manif \to \tangentsp{}\) is
\begin{equation}
    g(x) = \totangent{x}(h(x) - h(x_e)).\label{eqn:base-deriv}
\end{equation}
This definition ensures~\(g(x) \in \tangentsp{x}\) and~\(g(x_e) = 0\), so that~\(x_e\) is an equilibrium point of~\(g\).
When not enforcing~\(g(x_e) = 0\),~\(g\) may learn an incorrect equilibrium point or may not even possess an equilibrium point.
The stability issue in~\cite{ManekKolter2019} is failing to enforce~\(g(x_e) = 0\).
The implications of this are discussed in detail in \cref{sec:main-stability-issue}.

\paragraph{Lyapunov Function}
We define the candidate Lyapunov function~\(V: \manif \to \RealsNonNeg\), similarly to~\cite{ManekKolter2019}, as
\begin{equation}
   V(x) = \sigma(C(F(x)) - C(F(x_e))) + \varepsilon \distance{\manif}(x, x_e)^2,\label{eqn:neural-lyafn}
\end{equation}
where~\(F: \Reals^m \to \Reals^{m'}\) is a smooth invertible Lipschitz neural network~\cite{Lipschitz1,Lipschitz2},~\(C: \Reals^{m'} \to \Reals\) is a smooth convex neural network,~\(\varepsilon > 0\), and~\(\sigma\) is the smoothed ReLU function~\cite{ManekKolter2019}.
% \begin{equation*}
%     \sigma(x) = \begin{cases}
%         0 & x \leq 0 \\
%         \frac{x^2}{2} & 0 < x < 1 \\
%         x - \frac{1}{2} &  x \geq 1.
%     \end{cases}
% \end{equation*}
Eq.~\eqref{eqn:neural-lyafn} ensures~\(V(x_e) = 0\) and~\(V(x) > 0\) for~\(x \in \Reals^m \setminus \{x_e\}\), while the invertibility of~\(F\) and the convexity of~\(C\) ensure that~\(\grad{x}{\manif} V(x) \neq 0\) for~\(x \in \Reals^m \setminus \{x_e\}\)~\cite{ManekKolter2019}.
Concretely, we use~\(F(x) = \transp{[H(x), x]}\), where~\(H: \Reals^m \to \Reals^{k'}\) is a smooth neural network and invertibility is ensured by carrying over the input~\(x\) to the output.
Following \cite{ManekKolter2019}, we use an input convex neural network (ICNN)~\cite{ICNN} for~\(C\).

\paragraph{Stable Vector Field}
Given the base vector field~\(g\), the Lyapunov function~\(V\), and the desired exponential stability rate~\(\alpha \geq 0\), we can now define the vector field~\(f: \manif \to \tangentsp{}\) of our \ourframework{} as
\begin{align}
    f(x) = g(x) - \grad{x}{\manif}V(x)\frac{\relu{\liederiv{V}{g}(x) + \alpha V(x)}}{\norm{x}{\grad{x}{\manif} V(x)}^2},\label{eqn:stable-nmode}
\end{align}
where we use the convention~\(0 \cdot \frac{x}{0} = 0\) when~\(\grad{x}{\manif}V(x) = 0\).
For isolated points,~\(\grad{x}{\manif} V(x)\) can be undefined since~\(\grad{x}{\manif} \distance{\manif}(x, x_e)\) can be undefined on manifolds that only permit quasi-global stability, instead of global stability.
In this case, we let~\(f(x) = 0\).

\subsection{Stability Guarantee}\label{sec:nmode-stability}
In this section, we prove that the \ourframework{}~\eqref{eqn:stable-nmode} is stable.

\begin{theorem}\label{thm:nmode-stable}
    Let~\(x_e\),~\(f\), and~\(\alpha\) be as in \cref{sec:nmode-arch}.
    The point~\(x_e\) is a stable equilibrium point of \cref{eqn:stable-nmode}.
    If~\(\alpha > 0\),~\(x_e\) is \(\alpha\)-exponentially stable.
\end{theorem}

\begin{proof}
  Let~\(x_e\),~\(f\),~\(g\),~\(V\), and~\(\alpha\) be as in \cref{sec:nmode-arch}.
  % The arguments of~\citet{ManekKolter2019} for showing that~\(f\) from \cref{eqn:stable-nmode} is well defined transfer to our setting.
  % We refer to \citet{ManekKolter2019} for more details.
  Since~\(V(x_e) = 0\) and~\(g(x_e) = \totangent{x_e}(0) = 0\),~\(x_e\) is an equilibrium point of~\(f\).
  We now show that~\(V\) satisfies \cref{thm:lyapunov-fn}.
  \Cref{thm:nmode-stable} then follow from \cref{thm:lyapunov-fn} immediately.
  First, we have~\(V(x_e) = \sigma(C(F(x_e)) - C(F(x_e)) + \varepsilon\distance{\manif}(x_e, x_e)^2 = \sigma(0) + \varepsilon\distance{\manif}(x_e, x_e)^2 = 0\).
  Second,~\(V(x) > \varepsilon\distance{\manif}(x, x_e)^2 > 0\) for~\(x \in \manif \setminus (\{x_e\} \cup E)\), where~\(E\) are the finitely many points there~\(\grad{x}{\manif} V(x)\) is undefined.
  This also shows that~\(V(x) \geq c_1\distance{\manif}(x, x_e)^2\) for~\(c_1 = \varepsilon > 0\).
  The upper bound~\(V(x) \leq c_2 \distance{\manif}(x, x_e)^2\) follows from~\(F\),~\(\sigma\),and~\(C\) being Lipschitz~\cite{ManekKolter2019}.
  It remains to show that~\(\liederiv{V}{f}(x) \leq - \alpha V(x)\)
  % We have
  \begin{IEEEeqnarray*}{Cl}
      & \liederiv{V}{f}(x) \\
      =& \innerprod{x}{\grad{x}{\manif}V(x)}{f(x)} \\
      & {}- \innerprod{x}{\grad{x}{\manif}V(x)}{\grad{x}{\manif}V(x)}\frac{\relu{\liederiv{V}{g}(x) + \alpha V(x)}}{\norm{x}{\grad{x}{\manif} V(x)}^2} \\
      % &= \liederiv{V}{g}(x) - \norm{x}{\grad{x}{\manif} V(x)}^2 \frac{\relu{\liederiv{V}{g}(x) + \alpha V(x)}}{\norm{x}{\grad{x}{\manif} V(x)}^2} \\
      =& \liederiv{V}{g}(x) - \relu{\liederiv{V}{g}(x) + \alpha V(x)} \leq -\alpha V(x).
  \end{IEEEeqnarray*}
  The last equation is established by differentiating two cases based on whether~\(\relu{\liederiv{V}{g}(x) + \alpha V(x)}\) is positive or non-positive.
  This establishes \cref{thm:nmode-stable}.
\end{proof}

\subsection{Stability Issue in~\texorpdfstring{\cite{ManekKolter2019}}{Manek \& Kolter (2019)}}\label{sec:main-stability-issue}
The issue in \cite{ManekKolter2019} is that~\cite{ManekKolter2019} does not ensure the desired equilibrium point~\(x_e\) is actually an equilibrium point of the NODE~\(f\).
This issue has carried over to later works~\cite{AuddyEtAl2023Scalable,SochopoulosGiengerVijayakumar2024}, which build upon~\cite{ManekKolter2019}.
What may appear as a minor technical detail has, in fact, significant theoretical and practical implications that we discuss in this section.

The framework of~\cite{ManekKolter2019} is a special case of our framework when the Riemannian manifold is Euclidean space, except for one difference: In~\cite{ManekKolter2019},~\(g\) from \cref{eqn:base-deriv} is unconstrained, such that we may have~\(g(x_e) \neq 0\).
In consequence, the stabilised vector field~\(f\) may also have~\(f(x_e) \neq 0\) in \cite{ManekKolter2019}.
Since~\(f(x_e) = 0\) is a prerequisite for stability, strictly speaking,~\cite{ManekKolter2019} does not offer stability guarantees. 
Effectively,~\cite{ManekKolter2019} enforces convergence to point where~\(f(x_e)\) can be non-zero. 
This leads to numerical issues in practice, since~\(f\) is discontinuous if~\(f(x_e) \neq 0\).
A numerical solver encountering a point~\(x\) that is equal to~\(x_e\) up to machine precision, observes a rapid jump in the vector field~\(f(x)\), which drives away the numerical solution from the intended equilibrium point~\(x_e\).
This is visualised in \cref{fig:numerical-issues}.

In practice, not enforcing~\(g(x_e) = 0\) leads to some training runs aborting due to numerical instability.
If this does not occur, the equilibrium point appears to be shifting during training.
We observed this behaviour in our preliminary experiments, which led us to investigate the causes of this in~\cite{ManekKolter2019}.
In a successful training run, the issues in~\cite{ManekKolter2019} can be masked if the vector field learns to produce~\(f(x_e) \approx 0\).
% We believe this to be the reason that the stability issues in~\cite{ManekKolter2019} have not been discussed in the literature yet, despite the popularity of~\cite{ManekKolter2019}.

% An overlooked limitation of existing general stability frameworks~\cite{ManekKolter2019} is equilibrium consistency between the vector field and the learned Lyapunov function. These frameworks constrain the learned neural Lyapunov functions but leave the base vector field~\(f\) unconstrained, causing equilibrium drift during training and ultimately invalidating theoretical stability proofs.
% % Concretely,~\citet{ManekKolter2019} do not ensure that the desired target location~\(x_e\) is actually an equilibrium point of their learned vector field~\(f\).
% Concretely, while the Lyapunov functions of~\cite{ManekKolter2019} provides a stability proof \emph{if}the desired target location~\(x_e\) is an equilibrium point of~\(f\), the approach does not ensure that~\(x_e\) is an equilibrium point of~\(f\).
% % For this reason, the actual equilibrium point may not be the desired equilibrium point.
% % We found that, practically, the equilibrium point moves during training. 
% If learning does not recover~\(x_e\) as the equilibrium point precisely,~\cite{ManekKolter2019} does not guarantee stability.
% % We transfer the ideas of~\citet{ManekKolter2019} to Riemannian manifolds, while addressing the issue of moving equilibrium points.
% Our proposed method specifically addresses this critical shortcoming, ensuring equilibrium point invariance throughout training by explicitly constraining the equilibrium points of our vector fields.

\begin{figure}
  \centering
  \begin{subfigure}[t]{.49\linewidth}
      \centering
      \includegraphics[width=4cm]{figures/without-correction.png}
      \caption{Numerical issues in~\cite{ManekKolter2019}.}
  \end{subfigure}%
  \begin{subfigure}[t]{.49\linewidth}
      \centering
      \includegraphics[width=4cm]{figures/with-correction.png}
      \caption{With Our Correction.}
  \end{subfigure}
  \caption{%
    \textbf{Numerical solutions of a vector field stabilised using (a)~\cite{ManekKolter2019} and (b) our approach}.
    In this figure,~\(f(x_e) \neq 0\) when using~\cite{ManekKolter2019} as described in~\cref{sec:main-stability-issue}.
    This leads to numerical instability in~\cite{ManekKolter2019}, which is addressed by our correction.
    In this figure,~\(\manif = \Reals^2\) and~\(V(x) = \distance{\manif}(x, x_e)\).
  }
  \label{fig:numerical-issues}
\end{figure}

\subsection{Training Strategy}\label{sec:nmode-train}
We present an efficient training strategy for fitting \ourframework{}s to a set of motion demonstrations.
Training \cref{eqn:stable-nmode} using gradient descent requires computing a costly second derivative of~\(V\), since it requires computing the gradient of~\(\grad{x}{\manif}V(x)\).
% Since computing \cref{eqn:stable-nmode} requires computing the gradient of the Lyapunov function~\(V\), training \cref{eqn:stable-nmode} using a gradient-descent-based algorithm requires computing a second derivative of~\(V\).
% While this is not a hindrance for recent automatic differentiation frameworks, computing the second derivative makes training \cref{eqn:stable-nmode} more expensive than training an NMODE not containing a gradient, such as, for example, \cref{eqn:base-deriv}.
In practice, we found training \cref{eqn:stable-nmode} to require at least twice as much time as training \cref{eqn:base-deriv}, which does not require computing a second derivative.
To train more efficiently, we split the training into three stages, where the first stage has, in turn, two parts:
% Additionally, we found that training \cref{eqn:stable-nmode} directly requires a large number of epochs to develop a good fit when lacking a strong initialisation of the base vector field~\(g\) and the Lyapunov function~\(V\).
% 
% To address these issues, we provide a three-stage training recipe that accelerates learning stable NMODEs using our framework.
% The three stages are
\begin{enumerate}
    \item fit the base vector field~\(g\) to the demonstrations by
    \begin{enumerate}
        \item pretraining on estimated tangents and
        \item fit~\(g\) using multiple shooting~\cite{MultipleShootingNODE};
    \end{enumerate}
    \item train the Lyapunov function~\(V\) to descend along the training trajectories and the solutions of~\(g\); and
    \item fine-tune the assembled stable vector field~\(f\) on the demonstrations.
\end{enumerate}
While the first and second stages find a strong initialisation of~\(g\) and~\(V\), the third stage smoothes out kinks introduced by the stability projection.
%
In the first stage, we first pretrain on estimated tangent vectors and then leverage multiple shooting~\cite{MultipleShootingNODE} to further speed up training. 
In the following, we define each loss function for a single human demonstration~\(x_{0:N} \in \manif^{N}\) of the motion we want to learn.
In practice, we average each loss function over a small batch of demonstrations.

For the first stage, we estimate the tangent vectors as~\(u_i = \log_{x_i}(x_{i+1})\) for~\(i \in \{0, N-1\}\) and pretrain by minimising the mean squared error (MSE) between~\(g(x_{i})\) and~\(u_{i}\)
\begin{equation*}
    \mathcal{L}_{\text{tangents}} = \frac{1}{N-1}\sum_{i=1}^{N-1} (g(x_{i}) - u_i)^2.
\end{equation*}
% This step to produces good initialisations for training the NMODE solutions to match the demonstration using multiple shooting.
Next, multiple shooting divides the demonstration~\(x_{0:N}\) into~\(K\) shorter fragments~\(x_{0:\hat{N}}^{(k)}\) of length~\(\hat{N} \in \{2, \ldots, N\}\) where~\(k \in \{1, \ldots, K\}\) and trains on all fragments in parallel.
Further, it adds a term to the loss function that penalises fragment solutions whose endpoint is not equal to the starting point of the next fragment.
We use a manifold-aware MSE loss together with the multiple shooting penalty
\begin{IEEEeqnarray}{rCl}
    \mathcal{L}_{\text{MS}} &=& 
    \frac{1}{K\hat{N}}\sum_{k=1}^K\sum_{i=0}^{\hat{N}} 
        \distance{\manif}(\approxsol_i^{(k)}, x_i^{(k)})^2 \nonumber\\
    &&{}+ \frac{\lambda_{\text{MS}}}{K-1}\sum_{k=1}^{K-1}\distance{\manif}(\approxsol_{\hat{N}}^{(k)}, \approxsol_{0}^{(k+1)}),\label{eqn:multishoot-loss}
\end{IEEEeqnarray}
where~\(\approxsol_{0:\hat{N}}^{(k)}\) is a numerical solution of the NMODE with vector field~\(g\) for the fragment~\(x_{0:\hat{N}}^{(k)}\) and~\(\lambda_\text{MS} \in \RealsNonNeg\) is a hyperparameter.
During training, we linearly increase the shot length~\(\hat{T}\) within a predefined interval.
Training with longer shots is more costly but necessary to learn high-level features of the trajectories.

For the second stage, we train~\(V\) to decay exponentially along the original training trajectories and the NMODE solutions for~\(g\) that were learned in stage one.
% The fitted training trajectories are the solutions of the base NMODE with vector field~\(g\) for the initial state of each training trajectory.
Concretely, we train~\(V\) to minimize the the loss function
\begin{IEEEeqnarray*}{rCl}
    \mathcal{L}_V &=& 
    \frac{1}{N}\sum_{i=1}^{N} \relu{V(\hat{x}_i) - e^{-\alpha t\delta_t}V(x_0)},
\end{IEEEeqnarray*}
where~\(\hat{x}_{0:N}\) is either a human demonstration, or a solution for~\(g\).
In practice, we again average~\(\mathcal{L}_V\) over a small batch of both demonstrations and solutions of~\(g\).

In the third stage, we fine-tune the assembled \ourframework{} vector field~\(f\) using the multiple shooting technique also applied in stage one.
The loss function is \eqref{eqn:multishoot-loss} with the only change that~\(\approxsol_{0:\hat{N}}^{(k)}\) are approximate solutions for~\(f\) instead of~\(g\).
% \Cref{sec:experiments} compares our three-stage recipe to training~\(f\) directly without strong initialization.

%===============================================================================

\section{Experiments}\label{sec:experiments}
In this section, we compare our \ourframework{} framework to the existing robotic motion learning frameworks RSDS~\cite{ZhangBeikMohammadiRozo2022} and SDS-RM~\cite{SaverianoAbuDakkaKyrki2023} and demonstrate the practical applicability of our approach to real-world robot tasks.
Additionally, we compare different NMODE solvers from \cref{sec:solve-mode} and compare our three-stage training strategy to training~\eqref{eqn:stable-nmode} directly without a strong initialisation.
% We use RoboTask9~\cite{AuddyEtAl2023Continual} and develop enhanced Riemannian Lasa datasets~\cite{SaverianoAbuDakkaKyrki2023} as described in \cref{sec:experiments-datasets}.

\paragraph*{Evaluation Details}
Following previous work~\cite{Lasa,SaverianoAbuDakkaKyrki2023,AuddyEtAl2023Continual}, we train on all trajectories and report training losses.
We use AdamW~\cite{AdamWTrain} for training, the Euler method for solving ODEs, and optimise learning rates, training duration, and shot lengths for multiple shooting by running~\(100\) \texttt{Optuna}~\cite{Optuna} trials for each NMODE.
All hyperparameters are available online\footnote{\url{https://drive.google.com/file/d/1WZlMBKk4kJngwMAxgGIs09xvDWu2HSgU/}} alongside our code.
% 
% \paragraph*{Implementation \& Hardware}
% By implementing all manifold operations, such as~\(\distance{\manif}\) and~\(\totangent{x}\), in an automatic differentiation framework, such as \texttt{PyTorch}~\cite{PyTorch}, we can implement numerical NMODE solvers using a neural ODE package, such as \texttt{torchdiffeq}~\cite{torchdiffeq}.
% This allows leveraging standard neural network training algorithms~\cite{AdamTrain} for training stable vector fields to fit a set of training trajectories.
We implement our approach based on \texttt{PyTorch}~\cite{PyTorch} and \texttt{torchdiffeq}~\cite{torchdiffeq}.
Our experiments were run on an Ubuntu 24.04 compute server with an AMD Ryzen Threadripper 3960X CPU.
% We did not use a GPU in our experiments.

\subsection{Datasets}\label{sec:experiments-datasets}
\begin{figure}
    \centering
    \begin{tabular}{@{}c@{}c@{}c@{}c@{}}
        % \includegraphics[width=.15\textwidth]{figures/NewUQLasa-GShape.pdf} &
        % \includegraphics[width=.15\textwidth]{figures/NewUQLasa-NShape.pdf} &
        % \includegraphics[width=.15\textwidth]{figures/NewUQLasa-WShape.pdf}
        \multicolumn{2}{c}{\textbf{Original~\cite{SaverianoAbuDakkaKyrki2023}}} & 
        \multicolumn{2}{c}{\textbf{Improved (Ours)}} \\ \cmidrule(lr){1-2}\cmidrule(lr){3-4}
        \includegraphics[width=.2125\textwidth]{figures/UQLasa-WShape-modeint_project-UQLasa-WShape-modeint_project-2025-09-14_16-51-39_plot.pdf} &
        \includegraphics[width=.2125\textwidth]{figures/UQLasa-GShape-modeint_project-UQLasa-GShape-modeint_project-2025-09-14_16-48-38_plot.pdf} &
        \includegraphics[width=.2125\textwidth]{figures/NewUQLasa-WShape-modeint_project-NewUQLasa-WShape-modeint_project-2025-09-08_23-53-10_plot.pdf} & 
        \includegraphics[width=.2125\textwidth]{figures/NewUQLasa-GShape-modeint_project-NewUQLasa-GShape-modeint_project-2025-09-08_23-46-00_plot.pdf} \\
        \texttt{WShape} & \texttt{GShape} & \texttt{WShape} & \texttt{GShape}
    \end{tabular}
    \caption{\textbf{Unit Quaternion LASA Datasets.}
        In our improved dataset, the LASA shapes are visibly distorted by the manifold geometry, while the original dataset maintains the Euclidean geometry.
        The unit quaternions are displayed in an axis-angle representation.
        The stable NMODEs solutions~\legendline{pltorange} follow the demonstrations~\legendline{pltblue} from both datasets.
    }\label{fig:new-riemannian-lasa-results}
\end{figure}

We use the RoboTasks9 dataset~\cite{AuddyEtAl2023Scalable} and create improved versions of the unit quaternion (\(\unitquat\)) and~\(2\times 2\) symmetric positive-definite matrix (\(\spdtwo\)) Riemannian LASA datasets~\cite{SaverianoAbuDakkaKyrki2023}, where~\(\spdtwo\) is equipped with the affine-invariant metric.
RoboTasks9 contains demonstrations for nine robot tasks, such as opening a box or shelving a bottle.
Each task has nine demonstrations of~\(1000\) samples of a robot's end effector pose that lie on the~\(\Reals^3 \times \unitquat\) manifold.

Saveriano et al.~\cite{SaverianoAbuDakkaKyrki2023} construct Riemannian versions of the LASA datase~\cite{Lasa} on~\(\unitquat\) and~\(\spdtwo\) by embedding each 2D LASA trajectory into a fixed tangent space at the goal and then projecting it onto the manifold using the exponential map.
This approach yields trajectories that (\emph{i}) lie in a small neighbourhood of the goal~---~exhibiting minimal curvature~---~and (\emph{ii}) these datasets provide an unrealistic advantage to approaches working in a single tangent space, since such approaches can directly reconstruct the Euclidean data. 
% 
To address these shortcomings, we propose an enhanced transfer procedure. First, as in~\cite{SaverianoAbuDakkaKyrki2023}, we form four~\(\Reals^3\) trajectories for each of the 30 LASA shapes. We estimate velocities by~\(v_i = x_{i+1} - x_i\) for ~\(i \in \{0, \ldots, N-1\}\) and interpolate them linearly to obtain a continuous velocity field~\(v: [0, N] \to \Reals^3\). We then amplify~\(v\) by a factor of~\(20\) to ensure substantial manifold coverage. Finally, we integrate the scaled field~\(d\xi(t)/dt = -20v(t)\) on~\(\unitquat\) and~\(\spdtwo\) starting from a fixed goal point, using the dynamic‐chart method with a fourth‐order Runge–Kutta solver, to generate Riemannian trajectories with significant curvature. The resulting datasets consist of four demonstrations of~\(1000\) samples for each LASA shape, all terminating at a common target. Example trajectories from our~\(\unitquat\) dataset are shown in \Cref{fig:new-riemannian-lasa-results}.
%we group the two-dimensional Euclidean Lasa demonstrations to obtain four~\(\Reals^3\) trajectories for each shape.
%If~\(x_{0:N}\) is such a trajectory,~\(v_i = x_{i+1} - x_i\) provides a velocity estimate for each time step~\(i \in \{0, \ldots, N-1\}\).
%Linear interpolation between these velocities yields a continuous function~\(v: [0, N] \to \Reals^3\).
%To cover a larger fraction of the manifold, we scale up~\(v\) by~\(20\).
%Finally, we solve~\(d\xi(t)/dt = -20v(t)\) on~\(\unitquat\) and~\(\spdtwo\) starting from a fixed goal location to obtain Riemannian data using the dynamic-chart method with the fourth-order Runge-Kutta method as ODE solver.
%Overall, our~\(\unitquat\) and~\(\spdtwo\) Lasa datasets contain four demonstrations of~\(1000\) samples for each of the~\(30\) Lasa shapes, all ending in a common target state.
% Our trajectories exhibit significant curvature and are more difficult to fit than the original Riemannian Lasa datasets.
%\Cref{fig:new-riemannian-lasa-results} contains example trajectories from our~\(\unitquat\) Lasa dataset.

% 
% We use the unit quaternion (\(\unitquat\)) and~\(2 \times 2\) symmetric positive-definite matrix (\(\spdtwo\)) Riemannian Lasa datasets~\cite{SaverianoAbuDakkaKyrki2023} and the RoboTasks9 dataset~\cite{AuddyEtAl2023Scalable}, which contains data from the~\(\Reals^3 \times \unitquat\) position-orientation manifold.
% Both~\(\unitquat\) Lasa and~\(\spdtwo\) Lasa contain demonstrations for~\(30\) shapes derived from the popular Euclidean Lasa dataset~\cite{Lasa}.
% Each shape has four demonstrations of~\(1000\) samples ending in a common target location, which serves as the equilibrium point of our stable NMODEs.
% RoboTasks9 contains human demonstrations for nine real-world robot tasks, each with nine demonstrations of~\(1000\) samples of a robot's end effector position and orientation.


% \subsection{Datasets}
% Throughout our experiments, we use Riemannian Lasa datasets~\cite{SaverianoAbuDakkaKyrki2023}.
% The Lasa dataset~\cite{Lasa} is a popular dataset for learning from demonstrations containing demonstrations of handwriting letters.
% All Lasa trajectories contain~\(1000\) evenly spaced samples and end at a common target location, which serves as the equilibrium point of our stable NMODEs.
% 
% Our first dataset is a Lasa dataset on the sphere manifold~\(\sphere\).
% For this dataset, we interpret the normalized Euclidean Lasa trajectories~\cite{Lasa} as longitude and latitude parameters of trajectories on~\(\sphere\).
% \Cref{fig:lasa} contains example trajectories for this~\(\sphere\)~Lasa dataset.
% As a second dataset, we use the Unit Quaternion Lasa dataset~\cite{SaverianoAbuDakkaKyrki2023}, which recombines several Euclidean Lasa trajectories to obtain trajectories on the unit quaternion manifold~\(\unitquat\).
% While~\(\sphere\)~Lasa contains seven trajectories, of which we use four for training and three as a held-out test dataset,~\(\unitquat\)~Lasa contains four trajectories, of which we use two for training and two for testing.
% 
% Our goal when learning a Riemannian Lasa trajectory is minimizing the mean squared distance~\(\frac{1}{N} \sum_{i=0}^{N}{\distance{\manif}(\xi_i, \hat{\xi}_i)}^2\) between the dataset trajectory~\(\xi_{0:N} \in \manif^{N+1}\) and the solution of a NMODE~\(\hat{\xi}_{0:N} \in \manif^{N+1}\) for the initial state~\(\xi_0\).
% We primarily use the W-shape Lasa dataset (Lasa-W), as it is one of the most intricate trajectories in the Lasa collection. 
% \Cref{app:lasa-shapes} provides training results for additional Lasa shapes.

\subsection{Numerical NMODE Solvers}\label{sec:experiments-mode-solvers}
% Which numerical MODE solver to use is an additional important hyperparameter when training NMODEs.
We compare the three numerical NMODE solvers from \cref{sec:solve-mode} on our Riemannian LASA datasets when using our three-stage training strategy.
For this comparison, we select five LASA shapes each from~\(\unitquat\) LASA and~\(\spdtwo\) LASA that we found to be challenging to fit in a preliminary evaluation.
Perhaps surprisingly, using the dynamic chart method (DC) does not outperform the tangent space (TS) and exp-step (Exp) approaches, although DC is considerably more accurate than TS and Exp~\cite{DynamicChartMethod}.

\Cref{tab:comparison-lasa} summarises the results of this experiment.
% One might expect the dynamic chart method (DC) to consistently outperform the tangent space (TS) and Exp-step (Exp) approaches, since DC provides considerably more accurate solutions than TS and Exp~\cite{DynamicChartMethod}.
Our results show that the \ourframework{}s learn to mitigate the inaccuracy of TS and Exp, so that TS and Exp match the performance of DC and even outperform it in many cases. 
This is possible, since TS and Exp use fewer highly non-linear manifold operations than DC, which makes it easier to train NMODEs solved using TS and Exp.
% This, in turn, benefits hyperparameter tuning.
% Runtimes
% TS
% -------------------------------
% UQ stable_nmode inference time:
% count    100.000000
% mean       0.194758
% std        0.004415
% min        0.189061
% 25%        0.189692
% 50%        0.196060
% 75%        0.198711
% max        0.209700
% SPD2 stable_nmode inference time:
% count    100.000000
% mean       0.381811
% std        0.002762
% min        0.380022
% 25%        0.380664
% 50%        0.380920
% 75%        0.383009
% max        0.405954
%
% Exp-Step
% -------------------------------
% UQ stable_nmode inference time:
% count    100.000000
% mean       0.223465
% std        0.004634
% min        0.219065
% 25%        0.219734
% 50%        0.220332
% 75%        0.228053
% max        0.240230
% SPD2 stable_nmode inference time:
% count    100.000000
% mean       0.451248
% std        0.002522
% min        0.449940
% 25%        0.450593
% 50%        0.450834
% 75%        0.451207
% max        0.474962
% 
% Dynamic Chart
% -------------------------------
% UQ stable_nmode inference time:
% count    100.000000
% mean       0.395395
% std        0.006024
% min        0.389781
% 25%        0.390258
% 50%        0.393470
% 75%        0.397942
% max        0.412919
% SPD2 stable_nmode inference time:
% count    100.000000
% mean       0.862146
% std        0.003524
% min        0.858189
% 25%        0.859742
% 50%        0.860897
% 75%        0.864478
% max        0.886955
%
% Additionally, TS and Exp allow for faster training.
% Accross~\(100\) repetitions, solving a stable NMODE on~\(\unitquat\) for~\(100\) time steps has a median runtime of~\(0.20\)s with TS,~\(0.22\)s with Exp, and~\(0.39\)s with DC.
% On~\(\spdtwo\), TS requires~\(0.38\)s, Exp requires~\(0.45\)s, and DC requires~\(0.86\)s\footnote{In all cases, the inter-quartile range (IQR) is within~\(0.01\)s of the median.}.
At the same time, TS and Exp allow for faster training.
% Overall, we find that the faster TS and Exp are preferable over DC for training NMODEs, both in terms of runtime and achieved performance.
In our remaining experiments, we use Exp on~\(\unitquat\) and~\(\Reals^3 \times \unitquat\) and TS on~\(\spdtwo\).
\Cref{fig:new-riemannian-lasa-results} visualises a selection of our results on~\(\unitquat\) LASA.

\subsection{Training Strategies}\label{sec:experiments-training-strategies}
We compare our three-stage training strategy from \cref{sec:nmode-train} to training \cref{eqn:stable-nmode} directly from a random parameter initialisation (direct training).
The comparison is based on the five shapes from our~\(\unitquat\) LASA dataset also used in \cref{sec:experiments-mode-solvers}.
We use the Exp-Step method in this experiment.

We first compare the time requirements of training the base NMODE~\eqref{eqn:base-deriv} to training the \ourframework{}~\eqref{eqn:stable-nmode}.
Across~\(100\) repetitions, computing gradients of an NMODE solution with~\(100\) time steps has a median runtime of~\(0.18\)s for \cref{eqn:base-deriv} compared to~\(0.85\)s for \cref{eqn:stable-nmode}\footnote{The inter-quartile ranges (IQR) are within~\(0.004\)s of the median.}.
This difference allows our three-stage training strategy to significantly outpace direct training.
Concretely, our three-stage strategy achives a~\(1.99\)-fold speedup (IQR:~\(1.54\)--\(2.47\)) with a median runtime of~\(135\)s (IQR:~\(131\)s--\(182\)s) compared to~\(364\)s (IQR:~\(207\)s--\(502\)s) for direct training with indepently tuned hyperparameters.
At the same time, both strategies achieve comparable loss values: the median loss of three-stage training is~\(0.11\) (IQR:~\(0.11\)--\(0.18\)), while direct training achieves~\(0.14\) (IQR:~\(0.10\)--\(0.15\)).

\begin{table}
    \centering
    \caption{%
        \textbf{Results for Our Improved Riemannian LASA Datasets}.
        This table reports the root mean squared distance between the human demonstrations and the final learned solutions for different approaches.
        % A comparison of fitting performance and training runtime on our~\(\unitquat\) and~\(\spdtwo\)~Lasa datasets.
        We compare the tangent space (TS), exp-step (Exp), and dynamic chart (DC) methods for numerically solving NMODEs.
        % \enquote{\texttt{Multi}\hspace{1mm}\texttt{4}} and \enquote{\texttt{Multi }\hspace{1mm}\texttt{3}} abbreviate the correspoding \texttt{Multi Model} Lasa shapes.
        % \enquote{Three-Stage Training} refers to the training strategy from \cref{sec:nmode-train}, while \enquote{Direct Training} refers to training \cref{eqn:stable-nmode} directly from a random initialisation. 
    }\label{tab:comparison-lasa}
    \small
    \begin{tabular}{p{1.84cm}@{}ccccc@{\hspace{1em}}ccccc}
        % \toprule
        & \multicolumn{5}{c}{\(\unitquat\) (Unit Quaternions)} 
        & \multicolumn{5}{c}{\(\spdtwo\) (Symmetrix Positive-Definite \(2 \times 2\) Matrices)} \\ \cmidrule(lr){2-6}\cmidrule(lr){7-11}
        & \texttt{GShape}\hspace{-2mm} & \texttt{NShape}\hspace{-2mm} & \texttt{WShape} & \hspace{-2.5mm}\texttt{BendedLine}\hspace{-2.5mm} & \texttt{Multi}\hspace{1mm}\texttt{4}
        & \texttt{CShape}\hspace{-2mm} & \texttt{Sshape}\hspace{-2mm} & \texttt{Zshape}\hspace{-2mm} & \texttt{Leaf}\hspace{1mm}\texttt{1}\hspace{-2mm} & \texttt{Multi}\hspace{1mm}\texttt{3} \\ \midrule
        \multicolumn{10}{l}{\textbf{\ourframework{} (Ours)}} \\
        1. TS  & \(\mathbf{0.11}\) & \(0.19\)          & \(\mathbf{0.06}\) & \(0.41\)          & \(0.15\)
               & \(\mathbf{0.0064}\) & \(        0.0056 \) & \(\mathbf{0.0061}\) & \(        0.0047 \) & \(\mathbf{0.0018}\)
        \\
        2. Exp & \(\mathbf{0.11}\) & \(\mathbf{0.18}\) & \(0.10\)          & \(0.33\)          & \(\mathbf{0.11}\)
               & \(0.0068\)          & \(\mathbf{0.0049}\) & \(        0.0079 \) & \(\mathbf{0.0037}\) & \(        0.0020 \)
        \\
        3. DC  & \(0.20\)          & \(0.19\)          & \(0.10\)          & \(\mathbf{0.27}\) & \(0.15\)
               & \(        0.0103 \) & \(        0.0079 \) & \(        0.0090 \) & \(        0.0077 \) & \(        0.0047 \) 
        \\ \midrule
        % \multicolumn{6}{l}{\textbf{Direct Training}} \\
        % Exp & \(0.14\)    & --                      & \(0.07\)          & \(0.17\)          & \(0.15\)
        %     &  -- & -- & -- & -- & --
        % \\ \midrule
        \textbf{SDS-RM}~\cite{SaverianoAbuDakkaKyrki2023} 
            & \(1.32\)           & \(1.37\)         & \(1.39\)          & \(1.39\)           & \(0.93\)
            & \(0.0146\)         & \(0.0071\)       & \(0.0159\)        & \(0.0091\)         & \(0.0075\)
        \\
    \end{tabular}
\end{table}

% \begin{table}
%     \centering
%     \caption{
%         \textbf{Comparison of Numerical NMODE Solvers and Training Strategies}.
%         A comparison of fitting performance and training runtime on~\(\unitquat\)~Lasa and~\(\spdtwo\)~Lasa.
%         The Tangent Space, Exp-Step, and Dynamic Chart Method numerical NMODE solvers are abbreviated as \enquote{TS}, \enquote{Exp}, and \enquote{DC}, respectively.
%         \enquote{Three-Stage Training} refers to the training strategy from \cref{sec:nmode-train}, while \enquote{Direct Training} refers to training \cref{eqn:stable-nmode} directly from a random initialisation. 
%         While \enquote{Train Time} is the overall duration of training, \enquote{Loss} is the root mean squared distance between the human demonstrations and the final learned solutions.
%         No loss is available for DC on \(\spdtwo\) due to timeouts.
%     }\label{tab:comparison-lasa}
%     \small
%     \begin{tabular}{lrrrr}
%         \toprule
%         \multicolumn{1}{r}{\textbf{Mean:}}& \multicolumn{2}{c}{\textbf{Manifold RMSE}} & \multicolumn{2}{c}{\textbf{Train Time}} \\ \cmidrule(lr){2-3}\cmidrule(lr){4-5}
%         & \multicolumn{1}{c}{\(\unitquat\)} & \multicolumn{1}{c}{\(\spdtwo\)} & \multicolumn{1}{c}{\(\unitquat\)} & \multicolumn{1}{c}{\(\spdtwo\)} \\ \midrule
%         \multicolumn{5}{l}{\textbf{Three-Stage Training}} \\
%         1. TS & 
%         \(0.184 \pm 0.135\) & \(2.316 \pm 0.674\) & -- & --
%         \\
%         2. Exp & 
%         \(0.167 \pm 0.100\) & \(1.518 \pm 0.447\) & -- & --
%         \\
%         3. DC & 
%         \(0.182 \pm 0.063\) & \(1.618 \pm 1.045\) & -- & --
%         \\ \midrule
%         \multicolumn{5}{l}{\textbf{Direct Training}} \\
%         % 1. TS & -- & -- & -- & -- \\
%         2. Exp & -- & -- & -- & --
%     \end{tabular}
% \end{table}

% \begin{figure*}
%     \centering
%     \begin{tabular}{m{1em} m{4cm} @{} m{4cm} @{} m{4cm} @{} m{4cm}}
%      & \multicolumn{3}{c}{\textbf{Three-Stage Training Strategy}} & \textbf{Direct Training} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-5}
%      & \textbf{Tangent Space} & \textbf{Exp-Step} & \textbf{Dynamic-Chart} & \textbf{Exp-Step}
%      \\
%      \(\unitquat\) & 
%      \includegraphics[clip, trim=1.5cm 3cm 0cm 2.5cm, height=2.5cm]{figures/UQLasa-Sharpc-modeint_ts-200-UQLasa-Sharpc-modeint_ts-2025-08-15_06-29-14_plot.pdf}
%      & 
%      \includegraphics[clip, trim=1.5cm 3cm 0cm 2.5cm, height=2.5cm]{figures/UQLasa-Sharpc-modeint_project-200-UQLasa-Sharpc-modeint_project-2025-08-15_06-31-50_plot.pdf}
%      &
%      \includegraphics[clip, trim=1.5cm 3cm 0cm 2.5cm, height=2.5cm]{figures/UQLasa-Sharpc-modeint_dynamic_chart-200-UQLasa-Sharpc-modeint_dynamic_chart-2025-08-16_11-45-44_plot.pdf}
%      & 
%      \includegraphics[clip, trim=1.5cm 3cm 0cm 2.5cm, height=2.5cm]{figures/UQLasa-Sharpc-modeint_project-0-UQLasa-Sharpc-skip_pretraining-2025-08-15_18-02-42_plot.pdf}
%      %\includegraphics[width=.22\linewidth]{figures/SphereLasa-modeint_project-skip_pretraining-2025-04-30_22-19-04_test_trajectories.pdf}
%      % \\[2pt]
%      % Loss:~\(162 \times 10^{-3}\) & Loss:~\(76 \times 10^{-3}\) & Loss:~\(39 \times 10^{-3}\) & Loss:~\(140 \times 10^{-3}\)
%      % \\[4pt]
%      % Inference:~\(0.13\)s & Inference:~\(0.28\)s & Inference:~\(0.17\)s & Inference:~\(0.17\)s
%      % base vector field inference: 0.0184s & 0.0497s & 0.0575
%      % \\
%      % \(\unitquat\)
%      % &
%      % \includegraphics[width=\linewidth]{figures/SO3Lasa-modeint_ts-2025-04-29_11-20-40_test_trajectories.pdf}
%      % \\[2pt] &
%      % Loss: XYZ & Loss: XYZ & Loss: XYZ  
%      % \\[4pt] &
%      % Inference:~\(0.15\)s & Inference:~\(0.19\)s & Inference:~\(0.28\)s
%      % base vector field inference: 0.0179s & 0.0487s & 0.0575
%     \end{tabular}\\[10pt]
%     
%     \raisebox{1.75pt}{\tikz{\draw [fill=pltblue,draw=pltblue] (0,0) rectangle (1,0.05);}}~Human Demonstrations
%     \qquad
%     \raisebox{1.75pt}{\tikz{\draw [fill=pltorange,draw=pltorange] (0,0) rectangle (1,0.05);}}~NMODE~Solutions
%     
%     \caption{
%       A comparison of different training approaches and numerical solvers for training stable NMODEs on the~\(\sphere\) LASA-W dataset.
%       The \enquote{Three-Stage Training Strategy} is introduced in \cref{sec:nmode-train}, while \enquote{Direct Training} refers to training \cref{eqn:stable-nmode} directly, without a strong initialisation.
%       The \enquote{Tangent Space}, \enquote{Dynamic-Chart}, and \enquote{Exp-Step} numerical NMODE solvers are introduced in \cref{sec:solve-mode}.
%       \enquote{Loss} is the mean squared distance between the test set trajectories and the NMODE solutions.
%       \enquote{Inference} refers to the inference time for simulating~\(100\) time steps using the respective numerical solver.
%     }\label{fig:lasa}
% \end{figure*}

% \begin{wrapfigure}{R}{.4\textwidth}
%     \centering
%     \vspace*{-.75cm}
%     \includegraphics[width=\linewidth]{figures/lasa-robot.png}
%     \caption{A robot arm follows a learned N-shaped trajectory on the~\(\sphere\) manifold. A video is provided in the supplementary material.}
%     \label{fig:robot-lasa}
%     \vspace*{-1cm}
% \end{wrapfigure}

% \subsection{Robot Simulation}
% Lastly, we perform a simulation experiment where a robotic arm follows a learned stable motion. 
% For this experiment, use the Elephant Robotics MyCobot 320, a robotic arm with six degrees of freedom that we simulate using the \texttt{PyBullet} physics engine~\cite{coumans2021}. 
% The stable motion the robot performs is the~\(\sphere\) Lasa-N shape learned using our three-stage training recipe while leveraging the exp-step NMODE solver.
% \Cref{fig:robot-lasa} displays the result of our experiment, demonstrating that our approach is applicable for learning robotic motions.
% % The supplementary material contains a video of this robotic motion.

\subsection{Comparison with RSDS\texorpdfstring{~\cite{ZhangBeikMohammadiRozo2022}}{}}
% Among those,~\cite{SaverianoAbuDakkaKyrki2023} rely on Gaussian Mixture Models (GMMs) instead of neural ODEs.
% As we demonstrate in \cref{sec:experiments} GMMs on Riemannian manifolds scale poorly to higher-dimensional spaces, unlike neural ODEs.
% While~\cite{ZhangBeikMohammadiRozo2022} use neural ODEs, they use neural ODEs to learn the diffeomorphism.
% In contrast, we directly learn the vector field of a dynamical system using a neural ODE.
% While~\cite{ZhangBeikMohammadiRozo2022} requires solving a neural ODE on a Riemannian manifold for each time step of a trajectory, our method only requires solving a single neural ODE per trajectory.
% Concretely, simulating a trajectory with~\(1000\) time steps requires~\cite{ZhangBeikMohammadiRozo2022} to solve~\(1000\) NMODEs, while we only solve one.
% \begin{table}
%     \centering
%     \caption{
%         \textbf{Runtime Comparison with~\cite{ZhangBeikMohammadiRozo2022}}.
%         We report the median runtime of computing one tangent vector and the gradient of a tangent vector using our approach and~\cite{ZhangBeikMohammadiRozo2022} from~\(100\) repetitions.
%         Our approach is faster by one order of magnitude during deployment and by two orders of magnitude during training.
%     }\label{tab:runtime-compare}
%     \begin{tabular}{ccc}
%         & \multicolumn{2}{c}{\textbf{Runtime}} \\ \cmidrule(lr){2-3}
%         \textbf{Approach} & \textbf{Tangent} & \textbf{Gradient} \\ \midrule
%         \cite{ZhangBeikMohammadiRozo2022}
%         & \(0.053\)s & \(0.161\)s
%         \\
%         \textbf{Ours}
%         & \(\mathbf{0.002}\)\textbf{s} & \(\mathbf{0.005}\)\textbf{s}
%     \end{tabular}
% \end{table}
We perform a runtime comparison with RSDS~\cite{ZhangBeikMohammadiRozo2022}, demonstrating the computational efficiency of our approach.
Specifically, we compare the runtime of computing a tangent vector of the~\(\Reals^3 \times \unitquat\) manifold using RSDS and~\ourframework{}.
Since no code is available for RSDS, we recreated its model architecture.
% Computing one tangent vector corresponds to one control step when executing a motion on a robot.
Over~\(100\) repetitions, our approach computes tangent vectors in a median runtime of~\(0.002\)s versus~\(0.053\)s and gradients in~\(0.005\)s versus~\(0.161\)s, being one order of magnitude faster than RSDS during deployment and two orders of magnitude faster during training\footnote{All runtime measurements have an IQR within~\(0.0005\)s of the median.}.
Since the training process is discussed only sparsely in~\cite{ZhangBeikMohammadiRozo2022}, we were unable to perform a more detailed comparison with RSDS.

\subsection{Comparison with SDS-RM\texorpdfstring{~\cite{SaverianoAbuDakkaKyrki2023}}{}}
We compare our approach to SDS-RM~\cite{SaverianoAbuDakkaKyrki2023} on our improved Riemannian LASA datasets.
\Cref{tab:comparison-lasa} compares the performance of our \ourframework{}s to the Gaussian mixture models (GMMs) learned by SDS-RM.
For SDS-RM, we manually tune the number of GMM components and report the best result from five random restarts for each dataset.
Across the board, our \ourframework{}s outperform the GMMs learned by SDS-RM.
This demonstrates that our approach is more suitable for learning complex shapes that span Riemannian manifolds than SDS-RM.
Additionally, GMMs are known to scale poorly to high-dimensional manifolds~\cite{abu-dakka2018force}, while~\ourframework{} scales linearly in the manifold dimension, as demonstrated in the next section.

\subsection{Jointly Learning Stable Motions of Several Manipulators}
In this section, we study learning a joint motion of several manipulators based on the RoboTasks9 dataset.
This demonstrates that our approach is suitable for highly complex tasks that require up to nine manipulators and shows that the training time of our approach scales linearly in the manifold dimension.
% 
To obtain a dataset of multi-manipulator demonstrations, we combine tasks from the RoboTasks9 dataset~\cite{AuddyEtAl2023Scalable} in a fixed order.
% For example, for two joint tasks, our demonstrations contain two manipulators that synchronously open a box and shelve a bottle.
% For three tasks, another manipulator is added that stacks a plate.
% For~\(L \in \{2, \ldots, 9\}\) tasks, the demonstrations evolve on~\({(\Reals^3 \times \unitquat)}^L\), which has~\(6L\) dimensions.
\Cref{fig:manytasks} presents our results on this dataset, demonstrating that our approach can effectively learn up to nine tasks while the training time scales linearly in the manifold dimension.
% As apparent from \cref{fig:manytasks-runtime}, the training time of our approach scales linearly in the manifold dimension.

\begin{figure*}
    % tasks = (
    %     "openbox",
    %     "bottle2shelf",
    %     "platestandingtolying",
    %     "pouring",
    %     "matfolding",
    %     "obstaclerotate",
    %     "pan2stove",
    %     "scoop",
    %     "wineglass",
    % )
    \centering
    \begin{tabular}{l@{\hspace{3mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c@{\hspace{4mm}}c}
         %\multicolumn{1}{c}{\hspace{-.45cm}1} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
           \includegraphics[width=1.45cm]{figures/RoboTasks/openbox.png}               \clap{\hspace*{3.5mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/bottle2shelf.png}          \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/platestandingtolying.png}  \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/pouring.png}               \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/matfolding.png}            \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/obstaclerotate.png}        \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/pan2stove.png}             \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/scooping.png}              \clap{\hspace*{3mm}\raisebox{.45cm}{\(\times\)}}
         & \includegraphics[width=1.45cm]{figures/RoboTasks/wineglass.png}
         \\[-.45em]
         \scriptsize Open Box & \scriptsize Shelve Bottle & \scriptsize Stack Plate & \scriptsize Pouring & \scriptsize Fold Mat & \scriptsize Navigate & \scriptsize Pan on Stove & \scriptsize Scooping & \scriptsize Glass Upright \\ 
         \textbf{\#Tasks} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} \\ 
         \midrule
         \textbf{Loss}       &  \(0.05\) &  \(0.12\) &  \(0.19\) &  \(0.21\) &  \(0.31\) &  \(0.29\) &  \(0.26\) &  \(0.26\) \\
         \textbf{Train Time} & \(1429\)s & \(1636\)s & \(1840\)s & \(1998\)s & \(2213\)s & \(2416\)s & \(2612\)s & \(2794\)s \\ \midrule
         \textbf{Dimension}
         & \(12\) & \(18\) & \(24\) & \(30\) & \(36\) & \(42\) & \(48\) & \(54\) \\
    \end{tabular}
    \caption{%
        \textbf{Multi-Manipulator Results}.
        We train stable NMODEs to jointly perform~\(2\)--\(9\) tasks controlling multiple manipulators simultaneously.
        \enquote{Train Time} is the duration of training, \enquote{Loss} is the root mean squared distance between the demonstrations and the learned solutions, and \enquote{Dimension} is the dimension of the manifold.
        While the loss is independent of problem size, the runtime of our approach scales linearly with the problem size.
        The task illustrations are taken from~\cite{AuddyEtAl2023Continual}.
    }\label{fig:manytasks}
\end{figure*}


% \begin{figure}
%     \centering
%     \begin{tikzpicture}
%         \begin{axis}[
%           width=\linewidth, height=3.5cm,
%           xmin=10, xmax=56,
%           xtick={12,18,24,30,36,42,48,54},
%           ytick={1200,1800,2400},
%           yticklabels={20,30,40},
%           xlabel={Manifold Dimension},
%           ylabel={time (min)},
%         ]
%             \addplot [ultra thick, mark=*] coordinates {
%                 (12, 1429)   (18, 1636)   (24, 1840)   (30, 1998)   (36, 2213)   (42, 2416)   (48, 2612)   (54, 2794)
%             };
%         \end{axis}
%     \end{tikzpicture}
%     \caption{
%         \textbf{ManyTasks Training Scalability}.
%         % We train stable NMODEs to jointly perform~\(2\)--\(9\) tasks from RoboTasks9~\cite{AuddyEtAl2023Scalable}, controlling multiple manipulators simultaneously.
%         The duration of training (\enquote{time}) of our approach scales linearly with the manifold dimension.
%     }\label{fig:manytasks-runtime}
% \end{figure}

\subsection{Real-World Robot Experiment}
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/realExp.png}
%     \put(-215,110){\color{red}\textbf{Start}}
%     \put(-215,100){\color{red}\textbf{Point}}
%     \put(-45,80){\color{red}\textbf{Goal}}
%     \put(-45,70){\color{red}\textbf{Point}}
%     \caption{Book Shelving Task. \emph{Top-Row}: A librarian teaches the robot how to place a lying book upright on a shelf. \emph{Bottom-Row}: The robot autonomously performing the learned book shelving motion using our stable NMODE framework, successfully executing the task from starting positions not contained in the original demonstrations}
%     \label{fig:robot-experiment}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/realExp2.png}
%     \put(-215,110){\color{red}\textbf{Start}}
%     \put(-215,100){\color{red}\textbf{Point}}
%     \put(-45,80){\color{red}\textbf{Goal}}
%     \put(-45,70){\color{red}\textbf{Point}}
%     \caption{Book Shelving Task. \emph{Top-Row}: A librarian teaches the robot how to place a lying book upright on a shelf. \emph{Bottom-Row}: The robot autonomously performing the learned book shelving motion using our stable NMODE framework, successfully executing the task from starting positions not contained in the original demonstrations}
%     \label{fig:robot-experiment}
% \end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/robot-experiment-1.png}
    \put(-215,40){\color{red}\textbf{Start}}
    \put(-214,30){\color{red}\textbf{Pose}}
    \put(-48,16){\color{red!85!black}\textbf{Goal}}
    \put(-47,6){\color{red!85!black}\textbf{Pose}}
    \\[.05cm]
    % \includegraphics[width=\linewidth]{figures/robot-experiment-2.png}\\[.05cm]
    \includegraphics[width=\linewidth]{figures/robot-experiment-3.png}
    \caption{
        \textbf{Book Shelving Task}. 
        \emph{Top Row}: A librarian teaches the robot how to place a lying book upright on a shelf.
        % \emph{Middle Row}: The robot autonomously performs the learned book shelving motion using our stable NMODE framework.
        % \emph{Bottom Row}: It can also perform the motion from starting positions not contained in the original demonstrations. 
        \emph{Bottom Row}: The robot autonomously performs the book shelving motion using \ourframework{}, successfully executing the task from starting poses not contained in the demonstrations.
    }\label{fig:robot-experiment}
\end{figure}

Finally, we demonstrate the real-world applicability of our approach by learning a practical, real-world motion and deploying our learned \ourframework{} on a Franka Emika Panda robot.
Many future robot applications in households, shops, or libraries require placing objects on shelves.
% Current robot applications in warehouses, as well as future household robotics, frequently require placing objects on shelves. 
We choose the library task of placing a lying book upright on a shelf, as illustrated in \cref{fig:robot-experiment}, for this experiment.
This task requires manipulating both the position and the orientation of the book, meaning that our demonstrations evolve on~\(\Reals^3 \times \unitquat\).
We collect~\(20\) human demonstrations and learn a \ourframework{} to follow these demonstrations.
\Cref{fig:robot-experiment} shows that the robot is able to perform the motion correctly, also from starting points that were not contained in the human demonstrations.
A video of this robot experiment is available in the supplementary material.

%===============================================================================

\section{Conclusion}
We propose \ourframework{}, a general and expressive framework based on neural manifold ODEs (NMODEs) for teaching robots complex motions from demonstrations.
By leveraging NMODEs, we enable robots to track data modalities that evolve on Riemannian manifolds, such as orientation.
Our theoretical analysis proves that \ourframework{}s are stable, thereby guaranteeing predictable, safe, and robust robot motions.
We provide an efficient training strategy for applying \ourframework{} in practice.
Our experimental results establish that \ourframework{} outperforms existing frameworks for learning stable dynamical systems on manifolds and demonstrate the practical applicability of our approach.
% Our experimental evaluation demonstrates that our training strategy accelerates learning.
% Additionally, our evaluation sheds light on the advantages of different numerical NMODE solvers and demonstrates the practical applicability of our approach.

% \section{Limitations}
% All experimental results reported in this paper were obtained using numerical NMODE solvers of and are, therefore, subject to approximation errors.
% A noteworthy limitation of our theoretical results is that stability guarantees only apply to the exact solutions of a NMODE, but do not extend to numerical simulations of NMODEs generated by numerical solvers. 
% Our approach requires that manifold operations like the inner product, the distance of two points on the manifold, or the projection to the tangent space can be computed efficiently.
% Our approach is not applicable to manifolds where these operations can not be computed efficiently.
% In any case, training NMODEs requires implementing manifold operations carefully to ensure numerical stability.

% \appendix
% 
% \section{Additional Lasa Shapes}\label{app:lasa-shapes}
% \Cref{fig:additional-lasa-shapes-good,fig:additional-lasa-shapes-bad} contain learned stable NMODE solutions for additional~\(\sphere\) Lasa shapes, besides the Lasa-W shape showcased in \cref{sec:experiments}.
% These shapes were learned using our three-stage training recipe while leveraging the exp-step NMODE solver.
% We reused the tuned hyperparameters for Lasa-W for these shapes.
% While reusing these hyperparameters allows fitting many of the Lasa shapes well (\cref{fig:additional-lasa-shapes-good}), it also fails for several shapes (\cref{fig:additional-lasa-shapes-bad}). 
% Fitting these shapes better would require additional hyperparameter tuning.
% However, \cref{fig:additional-lasa-shapes-bad} demonstrates an important property of our approach: also when training is unsuccessful, the learned trajectories converge towards the equilibrium point due to our stability guarantee, which is independent from training success.
% 
% \begin{figure}
%     \centering
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Angle-modeint_project-2025-05-01_01-12-47_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-DoubleBendedLine-modeint_project-2025-05-01_01-44-10_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-GShape-modeint_project-2025-05-01_00-35-52_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-JShape-modeint_project-2025-05-01_01-54-37_test_trajectories.pdf}
%     \end{subfigure} \\[8pt]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Khamesh-modeint_project-2025-05-01_00-52-51_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-LShape-modeint_project-2025-05-01_02-15-36_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Leaf_2-modeint_project-2025-05-01_01-25-01_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Line-modeint_project-2025-05-01_01-35-22_test_trajectories.pdf}
%     \end{subfigure} \\[8pt]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-NShape-modeint_project-2025-05-01_01-46-00_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-PShape-modeint_project-2025-05-01_01-56-28_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-RShape-modeint_project-2025-05-01_02-06-58_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Sine-modeint_project-2025-05-01_02-48-25_test_trajectories.pdf}
%     \end{subfigure}  \\[8pt]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Spoon-modeint_project-2025-05-01_00-25-56_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Trapezoid-modeint_project-2025-05-01_03-22-27_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Worm-modeint_project-2025-05-01_02-42-00_test_trajectories.pdf}
%     \end{subfigure}
%     \caption{Additional~\(\sphere\) Lasa shapes that were learned successfully using our three-stage training recipe and the exp-step NMODE solver without additional hyperparameter tuning.}
%     \label{fig:additional-lasa-shapes-good}
% \end{figure}
% 
% \begin{figure}
%     \centering
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-BendedLine-modeint_project-2025-05-01_01-22-54_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-CShape-modeint_project-2025-05-01_01-33-19_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-JShape_2-modeint_project-2025-05-01_02-05-06_test_trajectories.pdf}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Leaf_1-modeint_project-2025-05-01_01-14-34_test_trajectories.pdf}
%     \end{subfigure}  \\[8pt]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Saeghe-modeint_project-2025-05-01_02-17-22_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Sharpc-modeint_project-2025-05-01_02-38-19_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Snake-modeint_project-2025-05-01_03-00-30_test_trajectories.pdf}
%     \end{subfigure} 
%     \hfill
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Sshape-modeint_project-2025-05-01_03-12-54_test_trajectories.pdf}
%     \end{subfigure}   \\[8pt]
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-Zshape-modeint_project-2025-05-01_02-54-03_test_trajectories.pdf}
%     \end{subfigure}
%     \hspace{.23\textwidth}
%     \begin{subfigure}{.23\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/SphereLasa-heee-modeint_project-2025-05-01_03-06-26_test_trajectories.pdf}
%     \end{subfigure}
%     
%     \caption{Additional~\(\sphere\) Lasa shapes that were not learned successfully using our three-stage training recipe and the exp-step NMODE solver without additional hyperparameter tuning. While fitting these shapes better would require additional hyperparameter tuning, these cases demonstrate that our method ensures convergence towards the equilibrium point, also when learning is unsuccessful.}
%     \label{fig:additional-lasa-shapes-bad}
% \end{figure}
% 
% \section{Hyperparameters}\label{app:hyperparams}
% This section describes details of our training setup and the various hyperparameters of training.
% In all cases, we trained using Adam~\cite{AdamTrain} for a fixed number of epochs while using a fixed-step learning rate decay schedule.
% In all experiments, we randomly downsample the Lasa trajectories from~\(1000\) samples to~\(100\) samples in each iteration.
% The exponential stability rate is~\(\alpha = 10^{-4}\) in all our experiments.
% For the dynamic chart method, we switch charts whenever the distance to the chart reference point approaches~\(1.0\).
% 
% For each experiment, we separately optimize the learning rate and hidden layer width using Optuna hyperparameter tuning~\cite{Optuna}.
% Since we found that the initial random initialization of the network parameters also has a significant impact on performance, we additionally let Optuna select a random seed for training.
% When tuning hyperparameters, we use a decreased number of epochs to accelerate tuning.
% Whenever tuning hyperparameters, we run Optuna for~\(100\) trials.
% The results from hyperparameter tuning are contained in \cref{tab:tuned-hyperparams}.
% 
% \textbf{ODE Solver}.
% All numerical MODE solvers that we study build upon a numerical ODE solver. 
% In our experiments, using the Euler method as ODE solver provides sufficient accuracy while allowing for fast training.
% For this reason, we use it throughout our experiments.
% 
% \textbf{Network Architecture}.
% In all experiments, our neural vector fields are fully-connected neural networks with three hidden layers of the same width.
% The width of the hidden layers is optimized using Optuna.
% Our Lyapunov functions always consist of a fully-connected neural feature extractor~\(F\) and an input convex neural network~\cite{ICNN}~\(C\), both with two hidden layers of size~\(64\).
% 
% \textbf{Learning Rate}.
% In our three-stage training recipe, we tune the learning rate of the first stage, while using fixed learning rates of~\(1\times10^{-3}\) and~\(1\times10^{-4}\) for the second and third stage, respectively.
% We use a fixed-step learning rate schedule in all experiments.
% Concretely, for our three-stage training recipe described in \cref{sec:nmode-train}, we decay the learning rate by~\(0.33\) after~\(100\) and~\(250\) epochs when training the base vector field.
% When training the Lyapunov function and fine-tuning the stable vector field, we decay the learning rate by~\(0.1\) after~\(100\) epochs.
% When training the stable vector field from \cref{eqn:stable-nmode} directly, we similarly decay the learning rate by~\(0.33\) after~\(100\) and~\(250\) epochs.
% 
% \textbf{Number of Epochs}.
% We generally use~\(200\) epochs for training Lyapunov functions and fine-tuning stable vector fields.
% To make hyperparameter tuning efficient, we only train for~\(250\) epochs while tuning the hyperparameters of the base vector field in our three-stage training recipe.
% In the main training run after hyperparameter tuning, we train the base vector field for~\(500\) epochs.
% When training the stable vector field directly, we similarly train for~\(250\) epochs during hyperparameter tuning and for~\(900\) epochs in the main training run.
% 
% \begin{table}
%     \centering
%     \caption{Tuned Hyperparameters}\label{tab:tuned-hyperparams}
%     
%     \footnotesize
%     \begin{tabular}{cccccc}
%         \(\manif\) & \textbf{Training Approach} & \textbf{NMODE Solver} & \textbf{Neurons per Layer} & \textbf{Learning Rate} & \textbf{Random Seed}  \\ \midrule
%         \multirow{4}{*}{\(\sphere\)} 
%          & 3-stage recipe & Tangent Space & \(192\) & \(\approx 0.0022\) & \(75\) \\
%          & 3-stage recipe & Dynamic-Chart & \(142\) & \(\approx 0.0180\) & \(11\) \\
%          & 3-stage recipe & Exp-Step & \(237\) & \(\approx 0.0037\) & \(48\) \\
%          & direct training & Exp-Step & \(127\) & \(\approx 0.0130\) & \(27\) \\
%          \midrule
%         \multirow{3}{*}{\(\unitquat\)}
%          & 3-stage recipe & Tangent Space & \(165\) & \(\approx 0.0012\) & \(8\) \\
%          & 3-stage recipe & Dynamic-Chart & \(233\) & \(\approx 0.0011\) & \(77\) \\
%          & 3-stage recipe & Exp-Step & \(244\) & \(\approx 0.0011\) & \(36\) \\ \bottomrule
%     \end{tabular}
% \end{table}

%===============================================================================
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.


\end{document}
