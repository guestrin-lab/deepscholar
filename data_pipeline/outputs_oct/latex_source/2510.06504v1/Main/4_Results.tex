\section{Experiments}

\subsection{Experimental Setup}

\textbf{Dataset.} We use the InterHuman~\citep{liang2024intergen} dataset for training and evaluating our model. InterHuman contains 6,022 two-person interacting motions and 3 textural descriptions per motion in the training split, and 1,177 two-person interacting motions in the test split. Additionally, a synthesized dataset of 25,000 text-motion pairs before filtering and 1,200 text-motion pairs after filtering is used for fine-tuning. All models are first trained on the InterHuman training split. For fine-tuning, the model is fine-tuned on the InterHuman training split augmented by the filtered synthetic dataset. All metrics are calculated using the InterHuman test split. 

\textbf{Metrics.} Following standard practice in human-human interaction generation~\citep{liang2024intergen,ruiz2024in2in,javed2024intermask,cai2024digital}, we use the R-Precision (Top-1, 2, 3), Frechet Inception Distance (FID), Multimodal Distance (MM Dist), Diversity, and Multimodality (MModality) for evaluation our models. Please refer to InterGen~\citep{liang2024intergen} for the detailed definition of these metrics.

\textbf{Implementation Details.} Our model consists of 12 attention blocks and 12 word-level conditioning blocks, positioned in an interleaved manner. We utilize a frozen CLIP-ViT-L/14~\citep{radford2021learning} model for extracting as the text encoder. We set the number of diffusion~\citep{ho2020denoising} steps to 1,000 and use a cosine noise schedule~\citep{nichol2021improved}. The model is trained with 8 NVIDIA A100 GPUs for 200,000 steps, with a 5e-5 learning rate and a batch size of 16 with the AdamW~\citep{loshchilov2017decoupled} optimizer, cosine learning rate scheduling, and 1000-step warm-up. During sampling, we use the DDIM~\citep{song2020denoising} sampling with 50 timesteps, with a classifier-free guidance~\citep{ho2022classifier} weight of 3.5.



\begin{table*}[t]
\vspace{-6mm}
    \centering
    \makebox[\textwidth]{ 
    \scalebox{0.7}{
    \begin{tabular}{ l c c c c c c c}
    \toprule
\multirow{2}{*}{Method}  & \multicolumn{3}{c}{R Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM Dist$\downarrow$} & \multirow{2}{*}{Diversity$\rightarrow$} & \multirow{2}{*}{MModality$\uparrow$} \\
    \cmidrule{2-4}
    ~ & Top 1 & Top 2 & Top 3 \\
    \midrule
         Ground Truth & \et{0.452}{.008} & \et{0.610}{.009} & \et{0.701}{.008} & \et{0.273}{.007} & \et{3.755}{.008} & \et{7.948}{.064} & - \\
    \midrule
         T2M~\citep{guo2022generating} & \et{0.238}{.012} & \et{0.325}{.010} & \et{0.464}{.014} & \et{13.769}{.072} & \et{5.731}{.013} & \et{7.046}{.022} & \et{1.387}{.076} \\
         MDM~\citep{tevet2022human} & \et{0.153}{.012} & \et{0.260}{.009} & \et{0.339}{.012} & \et{9.167}{.056} & \et{7.125}{.018} & \et{7.602}{.045} & \et{\textbf{2.350}}{.080} \\
         ComMDM~\citep{shafir2023human} & \et{0.223}{.009} & \et{0.334}{.008} & \et{0.466}{.010} & \et{7.069}{.054} & \et{6.212}{.021} & \et{7.244}{.038} & \et{1.822}{.052} \\
         RIG~\citep{tanaka2023role} & \et{0.285}{.010} & \et{0.409}{.014} & \et{0.521}{.013} & \et{6.775}{.069} & \et{5.876}{.002} & \et{{7.311}}{.043} & \et{2.096}{.065} \\
         InterGen~\citep{liang2024intergen} & \et{0.371}{.010} & \et{0.515}{.012} & \et{0.624}{.010} & \et{5.918}{.079} & \et{5.108}{.014} & \et{7.387}{.029} & \et{ {2.141}}{.063} \\
         MoMat-MoGen~\citep{cai2024digital} & \et{0.449}{.004} & \et{0.591}{.003} & \et{0.666}{.004} & \et{5.674}{.085} & \et{3.790}{.001} & \et{8.021}{.350} & \et{1.295}{.023} \\
         in2IN~\citep{ruiz2024in2in} & \et{0.425}{.008} & \et{0.576}{.008} & \et{0.662}{.009} & \et{5.535}{.120} & \et{3.803}{.002} & \et{{7.953}}{.047} & \et{1.215}{.023} \\
         InterMask~\citep{javed2024intermask} & \et{0.449}{.004} & \et{0.599}{.005} & \et{0.683}{.004} & \et{\textbf{5.154}}{.061} & \et{3.790}{.002} & \et{\textbf{7.944}}{.033} & \et{1.737}{.020} \\
            \midrule
        
        Ours  & \et{\textbf{0.483}}{.005} & \et{\textbf{0.638}}{.005} & \et{\textbf{0.717}}{.005} & \et{{5.191}}{.055} & \et{\textbf{3.778}}{.001} & \et{7.900}{.030} & \et{1.051}{.031} \\
        
    
    
    
    
    
    
    
    
    
    
    \bottomrule
    \end{tabular}
    }
    }
    \caption{Performance on the InterHuman~\citep{liang2024intergen} test sets. $\pm$ indicates a 95\% confidence interval and $\rightarrow$ means the closer to ground truth the better. Boldface indicates the best result.}
    \label{tab:quantitative_eval}
    \vspace{-4mm}
\end{table*}
\subsection{Comparison with the State-of-the-arts}


\paragraph{Quantitative Comparison.} Tab.~\ref{tab:quantitative_eval} contains the quantitative comparison between \namegen and state-of-the-art methods. Each experiment is repeated 20 times, after which the mean and 95\% confidence interval of each metric is recorded. \namegen achieves state-of-the-art results on all three R-precision metrics, surpassing the previous state-of-the-art, InterMask~\citep{javed2024intermask}, by a significant margin, highlighting the effectiveness of the word-level conditioning design choice in text-motion alignment. In terms of motion quality, our \namegen also achieves the best MM Distance and the second-best FID, with a small FID margin (0.037) from the state-of-the-art, InterMask, and surpassing all other prior arts. Notably, our FID is within the 95\% confidence interval of InterMask's FID, highlighting an equal level of generation quality while exhibiting significantly improved R-Precision.

\begin{figure*}[htbp]
\centering
\makebox[\textwidth][c]{
  \begin{overpic}[width=\linewidth]{Figs/fig_baseline_compare}
  \end{overpic}
}
\caption{Qualitative comparisons of interaction generation results from \namegen and InterMask~\citep{javed2024intermask}. Our method produces results with better text-motion alignment and is more robust to implausible poses. A deeper color indicates a later time.}
\label{fig:baseline_compare}
\end{figure*}

\paragraph{Qualitative Comparison.} We provide a qualitative comparison between our method and the current state-of-the-art, InterMask~\citep{javed2024intermask}. As shown in Fig.~\ref{fig:baseline_compare}, our model exhibits stronger adherence to text, higher robustness, and more plausible interaction semantics. 

Specifically, in the first generation result of our method, the agent marked in red successfully pulls the agent marked in green, and the red agent emerges as the winner (frame 5). In contrast, the motion generated by InterMask only exhibits pulling, and does not reflect this final part of the motion. In the second row, the InterMask result exhibits implausible human pose outputs in frames 1 and 2, and does not reflect the final bowing action, while our model generates plausible results faithful to the complete semantic meanings of the text. In the third row, the kneeling human generated by InterMask again exhibits an implausible human pose in frames 2, 3, 4, and 5. These results highlight our \namegen's pose robustness over InterMask, an aspect not adequately measured by the evaluator and the FID metric, while confirming \namegen's lead in text to motion alignment.

\subsection{Further Evaluation}
\begin{table*}[htpb]
\vspace{-2mm}
    \centering
    \makebox[\textwidth]{ 
    \scalebox{0.7}{
    \begin{tabular}{ l c c c c c c c}
    \toprule
\multirow{2}{*}{Method}  & \multicolumn{3}{c}{R Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM Dist$\downarrow$} & \multirow{2}{*}{Diversity$\rightarrow$} & \multirow{2}{*}{MModality$\uparrow$} \\
    \cmidrule{2-4}
    ~ & Top 1 & Top 2 & Top 3 \\
    
    
    \midrule
        Before Fine-tuning & \et{0.485}{.010} & \et{0.644}{.007} & \et{0.721}{.009} & \et{5.701}{.065} & \et{3.777}{.001} & \et{7.904}{.033} & \et{1.081}{.019} \\
            \midrule
          Finetune ($0.25 < d < 0.6$) & \et{0.485}{.004} & \et{0.641}{.004} & \et{0.717}{.004} & \et{5.981}{.056} & \et{3.778}{.001} & \et{7.946}{.028} & \et{1.080}{.026} \\
       Finetune ($0.3 < d < 0.6$) & \et{0.480}{.007} & \et{0.635}{.004} & \et{0.715}{.004} & \et{5.682}{.100} & \et{3.779}{.002} & \et{7.909}{.030} & \et{1.058}{.030} \\
        Finetune ($0.35 < d < 0.6$) & \et{0.483}{.005} & \et{0.638}{.005} & \et{0.717}{.005} & \et{5.191}{.055} & \et{3.778}{.001} & \et{7.900}{.030} & \et{1.051}{.031} \\
    \bottomrule
    \end{tabular}
    }
    }
    \caption{Quantitative Results of \namegen after fine-tuning on synthetic data generated by \namesyn. $d$ denotes the Euclidean distance between a synthetic data sample point and its closest held-out data point in the embedding space of the neural evaluator.}
    \label{tab:ablation_sampling}
\end{table*}

\textbf{Quantitative Evaluation of Fine-Tuning and Filtering.} 
\label{sec:filtering_ablation} We present the results of fine-tuning the \namegen model on a combined dataset consisting of InterHuman~\citep{liang2024intergen} training split and filtered synthetic data for 50k steps, with a learning rate of 5e-6. As shown in Tab.~\ref{tab:ablation_sampling}, the model exhibits a similar level of text-to-motion matching (R-Precision) after fine-tuning and a significantly improved FID in the best case, highlighting the improvement in generalizability. Notably, the FID exhibits a clear increasing trend when the minimum Euclidean distance $d$ of the filtering process is increased within a reasonable range, confirming the effectiveness of the proposed filtering pipeline in achieving synthetic data quality and diversity at the same time. The results also indicates that the increased dataset diversity by synthetic data improves the model's generalizability.


\textbf{Qualitative Visualization of Synthetic Data.} 
In Fig.~\ref{fig:fig_synthetic}, we present exemplar results of our data synthesis pipeline, \namesyn. The agent marked yellow is generated by the single-person motion generator~\citep{guo2024momask} while the agent marked blue is generated by the reaction generation model. As shown in the figure, our pipeline synthesizes high-quality and diverse motions from single-person motion and text descriptions, with close adherence to the text. Moreover, the textual descriptions resemble real-life human-human interaction situations with emotional interactions or inter-person collaboration instilled into the text prompts. 

\textbf{User Study Results on Fine-Tuning.} 
We present the user preference study results conducted with 51 participants on 10 samples generated with out-of-distribution texts using our data generation pipeline. Fig.~\ref{fig:finetuning_user_study} shows the users' strong preference for the model after fine-tuning, in terms of both motion quality and text-motion matching. This result confirms our model's improved generalizability to out-of-distribution samples after fine-tuning.

\begin{figure*}[t]
\centering
\makebox[\textwidth][c]{
  \begin{overpic}[width=\linewidth]{Figs/fig_synthetic}
  \end{overpic}
}
\caption{Qualitative samples of \namesyn. Prompts are synthesized by an LLM~\citep{liu2024deepseek}. The yellow is synthesized by the single-person motion generator, while the blue is generated by the reaction model with the yellow as the condition. A deeper color indicates a later time.}
\label{fig:fig_synthetic}
\end{figure*}



\textbf{User Study Results on \namesyn vs \namegen.} Fig.~\ref{fig:filtering_user_study} presents a user preference study between motions synthesized with \namegen and generated by \namegen. Two studies are conducted for \namesyn  (a) without and (b) with distributional filtering, where only high-quality and novel motions are retained after the filtering process. The results show that: (a) \namegen shows stronger consistency in motion quality compared to \namesyn, rendering the former suitable for general text-to-interaction tasks; (b) with the distributional filtering step, motions from \namesyn have higher quality compared to motions generated by \namegen, confirming the quality of the synthesized and filtered motion dataset used for fine-tuning.
































\begin{figure}[htpb]
\vspace{-1mm}
\centering
\begin{minipage}[t]{0.35\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figs/fig_user_study_finetune.pdf}
  \caption{User preference study results of \namegen with and without fine-tuning on synthetic data.}
  \label{fig:finetuning_user_study}
\end{minipage}
\hfill
\begin{minipage}[t]{0.62\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figs/filtering_user_study.pdf}
  \caption{Comparison of motion generation results using \namesyn and \namegen, (a) without filtering, and (b) with filtering. The motion quality and text-motion matching of \namesyn surpass \namegen only after filtering.}
  \label{fig:filtering_user_study}
  \vspace{-3mm}
\end{minipage}







s\end{figure}








































        










\textbf{Ablation Study.} After thoroughly investigating the effectiveness of the data synthesis and fine-tuning pipeline, we now analyze the effectiveness of the proposed sub-components: the word-level conditioning module (WLC), the adaptive interaction loss (AIL), and the synthetic data fine-tuning process (FT). When removing the word-level conditioning module, we replace it with sentence-level condition injection by AdaLN; for ablation of adaptive interaction loss, we replace it with the flat-weight distance map function $\mathcal{L}_{DM}$ proposed by InterGen~\citep{liang2024intergen}; for ablation of fine-tuning, the model was trained only on InterHuman~\citep{liang2024intergen} without the fine-tuning process.
Tab.~\ref{tab:ablation_component} shows significant improvement of our model after adding each proposed component, in terms of text-motion matching (R-Precision), FID, Multimodal Distance, and diversity. 

\begin{table*}[htpb]
    \centering
    \makebox[\textwidth]{ 
    \scalebox{0.7}{
    \begin{tabular}{ l c c c c c c c}
    \toprule
\multirow{2}{*}{Method}  & \multicolumn{3}{c}{R Precision$\uparrow$} & \multirow{2}{*}{FID$\downarrow$} & \multirow{2}{*}{MM Dist$\downarrow$} & \multirow{2}{*}{Diversity$\rightarrow$} & \multirow{2}{*}{MModality$\uparrow$} \\
    \cmidrule{2-4}
    ~ & Top 1 & Top 2 & Top 3 \\
    \midrule
    
    

        w.o. All Proposed Components & \et{0.441}{.006} & \et{0.608}{.005} & \et{0.681}{.005} & \et{6.237}{.071} & \et{3.781}{.001} & \et{7.959}{.035} & \et{1.068}{.022} \\

        w.o. AIL, FT & \et{0.484}{.005} & \et{0.632}{.005} & \et{0.710}{.005} & \et{6.192}{.069} & \et{3.779}{.001} & \et{7.853}{.033} & \et{1.081}{.019} \\

        w.o. WLC, FT & \et{0.484}{.005} & \et{0.629}{.005} & \et{0.711}{.005} & \et{5.877}{.061} & \et{3.779}{.001} & \et{7.851}{.034} & \et{0.996}{.027} \\

        w.o. FT & \et{0.485}{.010} & \et{0.644}{.007} & \et{0.721}{.009} & \et{5.701}{.065} & \et{3.777}{.001} & \et{7.904}{.033} & \et{1.046}{.022} \\

        Ours & \et{0.483}{.005} & \et{0.638}{.005} & \et{0.717}{.005} & \et{5.191}{.055} & \et{3.778}{.001} & \et{7.900}{.030} & \et{1.051}{.031} \\
        
        

    \bottomrule
    \end{tabular}
    }
    }
    \caption{Ablation Study: Effect of removing one or more of the proposed components: Adaptive Interaction Loss (AIL), Synthetic Data Fine-Tuning (FT), Word-Level Conditioning (WLC).}
    \label{tab:ablation_component}
    \vspace{-3mm}
\end{table*}


















