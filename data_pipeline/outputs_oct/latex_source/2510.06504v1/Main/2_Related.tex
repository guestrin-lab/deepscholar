\section{Related Works}

\subsection{Text-to-Human Motion Generation}
\vspace{-0.5mm}


Text-to-motion generation aims to synthesize human motion sequences from natural language descriptions~\citep{fan2024freemotion, tanke2023social, jeong2024multi, jiang2023motiongpt, guo2022generating, zhang2023generating, wan2024tlcontrol, lu2024scamo, guo2024momask}. Early methods such as Text2Action~\citep{ahn2018text2action} and Language2Pose~\citep{ahuja2019language2pose} utilized GANs and sequence-to-sequence architectures to map text to motion, laying foundational work in this area. Subsequent approaches leveraged variational autoencoders (VAEs) for probabilistic generation, including Guo et al.~\citep{guo2022generating} and TEMOS~\citep{petrovich2022temos}, which improved motion diversity and fluency. More recent advancements have focused on powerful generative models. Diffusion-based approaches such as MDM~\citep{tevet2022human} and latent diffusion via MLD~\citep{chen2023executing} significantly improved motion realism and sample efficiency. T2M-GPT~\citep{zhang2023generating} employed autoregressive transformers for fine-grained motion synthesis, while MoMask~\citep{guo2024momask} introduced generative masked transformers to enhance fidelity under the autoregressive paradigm. ReMoDiffuse~\citep{zhang2023remodiffuse} further enhanced generation quality by retrieving reference motions from a motion database. Parallel to improving generation quality, increasing attention has been given to controllable text-to-motion generation. Techniques have explored conditioning on spatial trajectories~\citep{shafir2023human, karunratanakul2023guided, wan2024tlcontrol, xie2023omnicontrol} and linguistic constraints~\citep{wan2024tlcontrol, huang2024controllable} to provide more precise control over generated outputs. Additionally, MotionCLIP~\citep{tevet2022motionclip} aligned motion and language embeddings in a shared space, enabling zero-shot text-to-motion generation. Despite stellar results in single-person motion generation, extending them to two-person interactions introduces additional challenges such as modeling inter-agent coordination and handling semantically richer text descriptions. Our work builds on these foundations by proposing a scalable framework that composes diverse and semantically aligned two-person interactions from single-person motion priors and language models.




\subsection{Human-Human Interaction Generation}
\vspace{-0.5mm}
Although some progress has been achieved in multi-human interaction modeling~\citep{fan2024freemotion, tanke2023social, jeong2024multi}, prior works on human interaction modeling have been mostly focused on the two-person interaction problem. A pioneer work, ComMDM~\citep{shafir2023human}, explores two-person motion generation by using a bridge network to compose the outputs of two single-person motion diffusion models~\citep{tevet2022human}. RIG~\citep{tanaka2023role} and InterGen~\citep{liang2024intergen} first trained dedicated networks to directly model two-person interaction. in2IN~\citep{ruiz2024in2in} explores the simultaneous use of individual and interaction descriptions to enhance textual alignment and generation quality. MoMat-MoGen~\citep{cai2024digital} proposes to enhance generation quality by retrieving from a motion database and a generative framework that models interactive behaviors between agents, considering personality, motivations, and interpersonal relationships. InterMask~\citep{javed2024intermask} utilizes the generative masked transformer architecture and spatial-temporal attention to enhance generation quality and text-motion alignment. TIMotion~\citep{wang2024temporal}, a contemporaneous work, proposes to model the human interaction sequence in a causal sequence, leveraging the temporal and causal properties of human motions. Although these methods have achieved impressive results, there remains significant possibilities of improvement due to their common flaw of limited training corpus and inadequate text modeling granularity. In this paper, we aim to tackle these two key issues with our generative interaction composition framework and fine-grained word-level conditioning module.





