\clearpage
\appendix
\section*{\textbf{SUPPLEMENTARY MATERIALS}}

\section{\namegen Implementation Details}
\subsection{Word-level Tokenization}
We use the CLIP~\citep{radford2021learning} ViT-L/14 encoder for encoding the text. The text is tokenized by the CLIP tokenizer into word-level tokens for short words and sub-word-level tokens for long words, with \texttt{<SOT>} and \texttt{<EOT>} tokens inserted at the start and end of the text. The maximum number of text tokens is $75$. If the number of tokens after tokenization is longer than $75$, the text tokens are truncated and additional tokens are discarded. 

\subsection{Word-level Conditioning}

\begin{figure*}[htbp]
\centering
\makebox[\textwidth][c]{
  \begin{overpic}[width=0.6\linewidth]{Figs/fig_appendix_word_level}
  \end{overpic}
}
\vspace{-10mm}
\caption{Illustration of the \textbf{Word-Level Conditioning} Block}
\label{fig:fig_appendix_word_level}
\end{figure*}

Fig.~\ref{fig:fig_appendix_word_level} illustrates the details of the proposed Word-Level Conditioning block. One of the interacting agents' motion features (Single Motion Features) is passed through an Adaptive Layer Norm (AdaLN) for the injection of timestep information. The word-level tokens are passed through a separate AdaLN of the same structure but with different parameters. Then, the normalized and modulated features are passed through the linear layers to yield the query, key, and value tensors, where the query comes from the single motion features and the key and value come from the word-level tokens. Then, an attention output embedding is obtained using query $Q$, key $K$, and value $V$ with the attention mechanism~\citep{vaswani2017attention}:
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^\top}{\sqrt{d_k}} \right) V
\end{equation}
Finally, the output of the attention layer is passed through a linear layer for the reprojection and mixing of the attention head outputs.


\subsection{Motion-Motion Interaction}


\begin{figure}[htpb]
\vspace{-1mm}
\centering
\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figs/fig_appendix_self_attention.pdf}
  \caption{Illustration of the \textbf{Self-Attention} module in the Motion-Motion Interaction Block.}
  \label{fig:fig_appendix_self_attn}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
  \centering
  \includegraphics[width=\linewidth]{Figs/fig_appendix_cross_attention.pdf}
  \caption{Illustration of the \textbf{Cross-Attention} module in the Motion-Motion Interaction Block.}
  \label{fig:fig_appendix_cross_attn}
  \vspace{-3mm}
\end{minipage}
\end{figure}

\subsubsection{Self-Attention}
The Self-Attention module in the Motion-Motion Interaction block is responsible for the processing of one of the interacting agents' motion features. Fig.~\ref{fig:fig_appendix_self_attn} is an illustration of this. The motion features are first modulated by an Adaptive Layer Norm (AdaLN) block, which injects timestep information by scaling the features with mean and variance determined by the timestep. Then, projection layers calculate the query, key, and value tensors separately, which are used to calculate the attention output using the attention mechanism~\citep{vaswani2017attention}. Finally, the attention output is projected with a linear output projection layer to give the final output. 

\subsubsection{Cross-Attention}
The Cross-Attention module in the Motion-Motion Interaction block is responsible for modeling the inter-agent interaction. Fig.~\ref{fig:fig_appendix_cross_attn} provides an illustration. In the Cross-Attention module, the motion features of agent A and agent B (vice versa) are used to calculate features using separate AdaLN blocks for the timestep information. Then, the motion features of agent A are passed through the query projection layer to give the query features. The motion features of agent B are passed through the key and value projection layer to calculate the key and value features. The query, key, and value features undergo an attention mechanism to obtain the attention output feature, which is subsequently projected by an linear output layer to form the final output.



\section{\namesyn Implementation Details}
\subsection{Two Person Prompt Synthesis}
We use a prompt template for synthesizing diverse and plausible two-person interaction descriptions from an LLM~\citep{liu2024deepseek} based on coarse-grained themes and fine-grained tags, along with real examples from the InterHuman~\citep{liang2024intergen} dataset for styling reference. The complete template is provided below in Fig.~\ref{fig:two_person_prompt_template}.

\begin{figure}[ht]
\centering

\begin{tcolorbox}[promptstyle]
You write compact, vivid descriptions of **two-person interactions**. 
\\\\
Each output sentence MUST:

• mention exactly two unnamed people (“one person… the other person…”),  

• focus on body / arms / legs (ignore faces / fingers / appearance),  

• be <=25 words,  

• clearly match the given *Theme* and *Tags*,  

• be entirely different from the examples.
\\\\

Theme: \textbf{\{theme\}}

Tags : \textbf{\{tags\}}

Reference examples ({k}):

\textbf{\{example1\}}\\
\textbf{\{example2\}}\\
\dots\\
\textbf{\{examplek\}}
\\\\
Now craft \textbf{\{m\}} brand-new descriptions.  
Return **only** a JSON array of strings.
\end{tcolorbox}

\caption{Prompt template for generating two-person interaction descriptions.}
\label{fig:two_person_prompt_template}
\end{figure}

\subsection{Single Person Prompt Synthesis}
After obtaining two-person descriptions, we use a separate prompt template to synthesize pairs of single-person descriptions that are self-contained, coherent, and consistent with the given two-person descriptions. The LLM infers the corresponding single-person motion according to the provided two-person interaction information, while reasoning the plausible single-person motion if the two-person prompt does not provide complete information. The complete prompt template is given in Fig.~\ref{fig:single_person_prompt_template}. Examples of two-person prompts and corresponding single-person prompts are given in Fig.~\ref{fig:prompt_examples}.

\begin{figure}[ht]
\centering

\begin{tcolorbox}[promptstyle]
Given the following description of a two-person interaction:
\\\\
\textbf{\{two-person text\}}
\\\\
Independently describe the motion of each person involved, using only information
implied by the full interaction. Do not mention or refer to the other person in
either description. Focus only on body, arms, and legs — ignore facial
expressions, fingers, or appearance.
\\\\
Use "the person" to refer to each. Assume shared context (e.g., dancing,
greeting, arguing), but isolate each description.
\\\\
Output JSON in this exact format:

\quad     \{\{"1": \{\{"person1": "\{description1\}", "person2": "\{description2\}"\}\}\}\}
\\\\
Each description must be one sentence, <=15 words, specific, and motion-focused
with relevant context.
\end{tcolorbox}

\caption{Prompt template for generating single-person interaction descriptions.}
\label{fig:single_person_prompt_template}
\end{figure}

\begin{figure}[ht]
\centering

\begin{tcolorbox}[promptstyle]
\textbf{Example 1.} \\
\textbf{two-person text}: One person leans back, arms outstretched, while the other steps forward, pressing their chest lightly against the first's, hands resting on their hips.\\
\textbf{single-person text A}: The person leans back with arms outstretched. \\
\textbf{single-person text B}: The person steps forward, chest pressed lightly, hands on hips.
\\\\
\textbf{Example 2.} \\
\textbf{two-person text}: One person claps twice, and the other responds by jumping in place, their legs kicking out wildly with excitement.\\
\textbf{single-person text A}: The person raises both arms and brings hands together sharply twice. \\
\textbf{single-person text B}: The person leaps upward, legs swinging outward vigorously.
\\\\
\textbf{Example 3.} \\
\textbf{two-person text}: One person lunges with a punch, the other person blocks with crossed arms and counters with a swift kick to the thigh.
\\
\textbf{single-person text A}: The person steps forward, extending one arm sharply in a punching motion. \\
\textbf{single-person text B}: The person raises both arms to cross in front, then swings one leg outward quickly.
\\\\
\textbf{Example 4.} \\
\textbf{two-person text}: one person steps forward aggressively, arms raised, while the other person backs away, hands outstretched to resist the advancing confrontation.
\\
\textbf{single-person text A}: The person steps forward aggressively with arms raised. \\
\textbf{single-person text B}: The person backs away with hands outstretched to resist.
\\\\
\textbf{Example 5.} \\
\textbf{two-person text}: One person stumbles backward from alcohol, and the other person swiftly wraps an arm around their waist to steady them.
\\
\textbf{single-person text A}: The person stumbles backward, legs unsteady from alcohol. \\
\textbf{single-person text B}: The person moves an arm quickly to wrap around a waist. 
\\\\
\textbf{Example 6.} \\
\textbf{two-person text}: One person shoves the other's shoulder, causing them to stagger, then crosses their arms in defiance as the other retreats.
\\
\textbf{single-person text A}: The person extends their arm sharply, then pulls it back and crosses both arms tightly. \\
\textbf{single-person text B}: The person stumbles backward from a sudden force, then turns away while stepping back.

\end{tcolorbox}

\caption{Examples of generated and two-person interaction descriptions with corresponding single-person descriptions.}
\label{fig:prompt_examples}
\end{figure}

\subsection{Single Person Motion Generation}
We use MoMask~\citep{guo2024momask} to generate the single-person motion conditions for the subsequent reaction generation. In addition, we trained a length estimator on the InterHuman~\citep{liang2024intergen} text-motion pairs to estimate the correct length of the corresponding motion given a two-person motion prompt, and used the predicted length by the estimator to guide the single-person motion generation.

For each two-person prompt, we use the LLM~\citep{liu2024deepseek} to provide two prompts for the two interactants and generate two single-person motions, one with each prompt.

\subsection{Two Person Motion Composition}
We trained a reaction generation network that uses a given sequence of joints as condition $\mathbf{x}_{cond}\in \mathbb{R}^{T \times 22 \times 3}$ to generate the reaction $\mathbf{x}\in \mathbb{R}^{T \times 262}$, consisting of the full interaction in the complete InterGen~\citep{liang2024intergen} joint representation. 

The network is trained on the InterHuman~\citep{liang2024intergen} training dataset with one person's joints not noised and all other terms noised to simulate the reaction generation tasks. At test time, the condition joints are provided at each time step of denoising and after the final denoising step. 

\subsection{Motion Filtering}
Before filtering, we first use the trained neural motion evaluator to project all generated motions to the $512$-dimensional motion latent space, to compare with the latents of a $500$-sample held-out motion dataset. The motion filtering step consists of a $k$-nearest neighbors filtering with a maximum $20$ nearest neighbors for each sample in the held-out set. We also calculate the distances between each of the nearest neighbors with the held-out motion sample to make sure the distance is in the predefined annulus $d_{min} < r < d_{max}$. Finally, we use the neural motion evaluator to filter out all the remaining motions with text-motion cosine similarity less than $0.58$, an empirically set threshold.


\section{Motion Embedding Space Visualizations}

\begin{figure*}[t]
\centering
\makebox[\textwidth][c]{
  \begin{overpic}[width=\linewidth]{Figs/fig_embeddings.pdf}
  \end{overpic}
}
\caption{UMAP~\citep{mcinnes2018umap} visualizations of evaluator and CLIP~\citep{radford2021learning} embeddings of (a) text and (b) two-person motions from the InterHuman~\citep{liang2024intergen} held-out subset and filtered synthesized dataset. }
\label{fig:latent_space_visualization}
\end{figure*}

Fig.~\ref{fig:latent_space_visualization} demonstrates a dimensionality-reduced visualization of the text and two-person motion embeddings of the InterHuman~\citep{liang2024intergen} held-out dataset, extracted from CLIP~\citep{radford2021learning} and trained motion evaluator. We utilize UMAP~\citep{mcinnes2018umap} (Uniform Manifold Approximation and Projection), a popular dimension reduction technique that preserves the local and global structure of high-dimensional data in a low-dimensional space. As shown in the figure, both the generated text (Fig.~\ref{fig:latent_space_visualization} (a)) and motion (Fig.~\ref{fig:latent_space_visualization} (b)) descriptions from our pipeline have good coverage in most high-density areas of the held-out dataset, while covering many underrepresented areas that lacks held-out data samples, highlighting our pipeline's capability in enhancing data diversity. 

\section{User Study Details}
We conducted a user study to evaluate our \namegen model with and without fine-tuning. Human evaluators are asked to choose between 10 pairs of two-person interaction videos and determine the one out of each pair that is more faithful to the text prompt or more natural as an interaction. Fig.~\ref{fig:fig_user_study} is an illustration of how the user study is conducted.

\begin{figure*}[htbp]
\centering
\makebox[\textwidth][c]{
  \begin{overpic}[width=0.95\linewidth]{Figs/fig_user_study}
  \end{overpic}
}
\caption{A screenshot of the user study}
\label{fig:fig_user_study}
\end{figure*}


\section{Societal Impacts}
Our work on \name introduces a scalable framework for high-fidelity and diverse text-to-two-person interaction generation. While the technology has the potential to significantly benefit domains such as animation, virtual reality, assistive robotics, and embodied AI, it also raises several ethical and societal considerations.

\paragraph{Positive Impacts.} The proposed method can facilitate content creation in media, education, and human-computer interaction by automating the generation of complex, realistic human interactions. It may lower the barrier for creating high-quality motion data, particularly for under-resourced languages or motion types, and help simulate social interactions for training embodied agents or improving accessibility tools for individuals with disabilities.

\paragraph{Risks and Limitations.} As with many generative models, there is a potential risk of misuse, such as generating deceptive or misleading content (e.g., synthetic surveillance or manipulated footage). Although our method focuses solely on body motion and excludes facial expressions or identity features, generated motion could still be used out of context or embedded in misleading visual narratives. Additionally, there is a risk of dataset bias being amplified if the single-person priors or LLM-generated text reflect culturally specific or stereotyped behaviors. We recommend future users apply careful evaluation and transparency practices when deploying this technology.

\paragraph{Mitigations.} Our dataset curation and filtering process emphasizes diversity and alignment with real-world motion distributions to reduce representation biases. Furthermore, our model does not generate personally identifiable information or faces, and we encourage its use only in applications that respect human dignity, consent, and privacy.
